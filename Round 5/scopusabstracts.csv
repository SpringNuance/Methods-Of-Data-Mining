ID#TITLE#ABSTRACT
id1#Anomaly detection in wide area imagery [Geniş alan görüntülerinde anomali tespiti]#This study is about detecting anomalies in wide area imagery collected from an aircraft. The set of anomalies have been identified as anything out of the normal course of action. For this purpose, two different data sets were used and the experiments were carried out on these data sets. For anomaly detection, a convolutional neural network model that tries to generate the next image using past images is designed. The images were pre-processed before being given to the model. Anomaly detection is performed by comparing the estimated image and the true image. 
id2#Person re-identification with deep kronecker-product matching and group-shuffling random walk#Person re-identification (re-ID) aims to robustly measure visual affinities between person images. It has wide applications in intelligent surveillance by associating same persons' images across multiple cameras. It is generally treated as an image retrieval problem: Given a probe person image, the affinities between the probe image and gallery images (P2G affinities) are used to rank the retrieved gallery images. There exist two main challenges for effectively solving this problem. 1) Person images usually show significant variations because of different person poses and viewing angles. The spatial layouts and correspondences between person images are therefore vital information for tackling this problem. State-of-the-art methods either ignore such spatial variation or utilize extra pose information for handling the challenge. 2) Most existing person re-ID methods rank gallery images considering only P2G affinities but ignore the affinities between the gallery images (G2G affinity). Such affinities could provide important clues for accurate gallery image ranking but were only utilized in post-processing stages by current methods. In this article, we propose a unified end-to-end deep learning framework to tackle the two challenges. For handling viewpoint and pose variations between compared person images, we propose a novel Kronecker Product Matching operation to match and warp feature maps of different persons. Comparing warped feature maps results in more accurate P2G affinities. To fully utilize all available P2G and G2G affinities for accurately ranking gallery person images, a novel group-shuffling random walk operation is proposed. Both Kronecker Product Matching and Group-shuffling Random Walk operations are end-to-end trainable and are shown to improve the learned visual features if integrated in the deep learning framework. The proposed approach outperforms state-of-the-art methods on Market-1501, CUHK03 and DukeMTMC datasets, which demonstrates the effectiveness and generalization ability of our proposed approach. Code is available at https://github.com/YantaoShen/kpm_rw_person_reid. 
id3#Crack detection in images of masonry using cnns#While there is a significant body of research on crack detection by computer vision methods in concrete and asphalt, less attention has been given to masonry. We train a convolutional neural network (CNN) on images of brick walls built in a laboratory environment and test its ability to detect cracks in images of brick-and-mortar structures both in the laboratory and on real-world images taken from the internet. We also compare the performance of the CNN to a variety of simpler classifiers operating on handcrafted features. We find that the CNN performed better on the domain adaptation from laboratory to real-world images than these simple models. However, we also find that performance is significantly better in performing the reverse domain adaptation task, where the simple classifiers are trained on real-world images and tested on the laboratory images. This work demonstrates the ability to detect cracks in images of masonry using a variety of machine learning methods and provides guidance for improving the reliability of such models when performing domain adaptation for crack detection in masonry. 
id4#Towards an energy efficient code generator for mobile phones#Using a smartphone become the part of our everyday life in the last few years. These devices can help us in many areas of life (sport, job, weather etc.), but sometimes can be also very annoying because of the battery life time. That is why it is very important to find solutions, which can reduce the energy consumption of the smartphones. One possible method is the 'computation offloading' where a part of the processes are executed on a remote device (e.g. in the cloud). A lot of example has already shown, that computation offloading can reduce the energy usage of the mobile devices. However, the amount of energy saving may differ, as the decision making of the offloading process can be controlled with several techniques. Our decision making theory is based on the scheduling theory. In this paper, we are going to introduce a new system called ECGM (Energy efficient Code Generator for Mobile phones). ECGM decides automatically at compile time, which task should run on the smartphone and which task can be offloaded. The benefit of our system will be demonstrated through measurements based on our energy-efficiency scheduling technique. 
id5#Sub-polyhedral scheduling using (Unit-)two-variable-per-inequality polyhedra#Polyhedral compilation has been successful in the design and implementation of complex loop nest optimizers and parallelizing compilers. The algorithmic complexity and scalability limitations remain one important weakness. We address it using sub-polyhedral under-aproximations of the systems of constraints resulting from affine scheduling problems. We propose a sub-polyhedral scheduling technique using (Unit-)Two-Variable-Per-Inequality or (U)TVPI Polyhedra. This technique relies on simple polynomial time algorithms to under-approximate a general polyhedron into (U)TVPI polyhedra. We modify the state-of-the-art PLuTo compiler using our scheduling technique, and show that for a majority of the Polybench (2.0) kernels, the above under-approximations yield polyhedra that are non-empty. Solving the under-approximated system leads to asymptotic gains in complexity, and shows practically significant improvements when compared to a traditional LP solver. We also verify that code generated by our sub-polyhedral parallelization prototype matches the performance of PLuTo-optimized code when the under-approximation preserves feasibility. Copyright 
id6#Extracting multiple viewpoint models from relational databases#Much time in process mining projects is spent on finding and understanding data sources and extracting the event data needed. As a result, only a fraction of time is spent actually applying techniques to discover, control and predict the business process. Moreover, current process mining techniques assume a single case notion. However, in real-life processes often different case notions are intertwined. For example, events of the same order handling process may refer to customers, orders, order lines, deliveries, and payments. Therefore, we propose to use Multiple Viewpoint (MVP) models that relate events through objects and that relate activities through classes. The required event data are much closer to existing relational databases. MVP models provide a holistic view on the process, but also allow for the extraction of classical event logs using different viewpoints. This way existing process mining techniques can be used for each viewpoint without the need for new data extractions and transformations. We provide a toolchain allowing for the discovery of MVP models (annotated with performance and frequency information) from relational databases. Moreover, we demonstrate that classical process mining techniques can be applied to any selected viewpoint. 
id7#A Program Result Checker for the Lexical Analysis of the GNU C Compiler#In theory, program result checking has been established as a well-suited method to construct formally correct compiler frontends but it has never proved its practicality for real-life compilers. Such a proof is necessary to establish result checking as the method of choice to implement compilers correctly. We show that the lexical analysis of the GNU C compiler can be formally specified and checked within the theorem prover Isabelle/HOL utilizing program checking. Thereby we demonstrate that formal specification and verification techniques are able to handle real-life compilers. 
id8#Advances in visual object tracking algorithm based on correlation filter [基于相关滤波的视觉目标跟踪算法新进展]#With excellent comprehensive performance, correlation filter-based tracking algorithms have become a hotspot of the theoretical research and practical application in the field of visual object tracking. Despite many studies, there is still a lack of systematic analyses on the existing correlation filter-based tracking algorithms from the level of tracking framework. Therefore, starting from the basic framework of object tracking algorithms, the characteristics of correlation filter-based tracking algorithms are deeply analyzed, and their basic problems in each working stage are presented in this paper. On this basis, the main technological progress of correlation filter-based tracking algorithms and characteristics of corresponding algorithms in recent ten years are summarized, and 20 typical correlation filter-based tracking algorithms are evaluated and analyzed. Finally, the outstanding issues to be urgently solved and the future research directions of correlation filter-based tracking algorithms are discussed. 
id9#A Study on Various Database Models: Relational, Graph, and Hybrid Databases#Relational database is a popular database for storing various types of information. But due to the ever-increasing growth of data, it becomes hard to maintain and process the database. So, the graph model is becoming more and more popular since it can store and handle big data more efficiently compared to relational database. But both relational database and graph database have their own advantages and disadvantages. To overcome their limitations, they are combined to make a hybrid model. This paper discusses relational database, graph database, their advantages, their applications and also talks about hybrid model. 
id12#Better Reporting of Studies on Artificial Intelligence: CONSORT-AI and Beyond#An increasing number of studies on artificial intelligence (AI) are published in the dental and oral sciences. The reporting, but also further aspects of these studies, suffer from a range of limitations. Standards towards reporting, like the recently published Consolidated Standards of Reporting Trials (CONSORT)-AI extension can help to improve studies in this emerging field, and the Journal of Dental Research (JDR) encourages authors, reviewers, and readers to adhere to these standards. Notably, though, a wide range of aspects beyond reporting, located along various steps of the AI lifecycle, should be considered when conceiving, conducting, reporting, or evaluating studies on AI in dentistry. 
id13#Wearable System for Personalized and Privacy-preserving Egocentric Visual Context Detection using On-device Deep Learning#Wearable egocentric visual context detection raises privacy concerns and is rarely personalized or on-device. We created a wearable system, called PAL, with on-device deep learning so that the user images do not have to be sent to the cloud for processing, and can be processed on-device in a real-time, offline, and privacy-preserving manner. PAL enables human-in-the-loop context labeling using wearable audio input/output and a mobile/web application. PAL uses on-device deep learning models for object and face detection, low-shot custom face recognition (∼1 training image per person), low-shot custom context recognition (e.g., brushing teeth, ∼10 training images per context), and custom context clustering for active learning. We tested PAL with 4 participants, 2 days each, and obtained ∼1000 in-the-wild images. The participants found PAL easy-to-use and each model had gt80% accuracy. Thus, PAL supports wearable, personalized, and privacy-preserving egocentric visual context detection using human-in-the-loop, low-shot, and on-device deep learning. 
id14#Partial flow sensitivity#Compiler optimizations need precise and scalable analyses to discover program properties. We propose a partially flow-sensitive framework that tries to draw on the scalability of flow-insensitive algorithms while providing more precision at some specific program points. Provided with a set of critical nodes -basic blocks at which more precise information is desired -our partially flow-sensitive algorithm computes a reduced control-flow graph by collapsing some sets of non-critical nodes. The algorithm is more scalable than a fully flow-sensitive one as, assuming that the number of critical nodes is small, the reduced flow-graph is much smaller than the original flow-graph. At the same time, a much more precise information is obtained at certain program points than would had been obtained from a flow-insensitive algorithm. 
id15#Automated crack pattern recognition from images for condition assessment of concrete structures#Evaluating the appearance of cracks on concrete elements is necessary during condition assessment. Most imaging techniques related to crack identification focus on detecting cracks and measuring basic dimensions of cracks. However, condition assessment of an element from visible cracks requires further information, of which characterizing the crack pattern is necessary. To achieve this, this paper proposes an automated approach to recognize concrete crack patterns from images. By analyzing the characteristics of structural and non-structural cracks, a binary classification of crack patterns into isolated patterns and map patterns is proposed. The recognition of crack patterns is performed through similarity comparisons using DISTS index. Various parameters that may affect the performance are investigated through several experiments conducted using real-world images. According to the experimental results, the proposed method is shown to successfully recognize crack patterns with an accuracy of over 96%. Recommendations are further proposed to enhance the performance of the method. 
id16#Creating annotations for web ontology language ontology generated from relational databases#Many approaches that have been proposed that allow users to create a Web Ontology Language (OWL) ontology from a relational database fail to include metadata that are inherent to the database tables. Without metadata, the resulting ontology lacks annotation properties. These properties are key when performing ontology alignment. This paper proposes a method to include relevant metadata through annotation properties to OWL ontologies, which furthers the ability to integrate and use data from multiple unique ontologies. The described method is applied to geospatial data collected from The National Map, a data source hosted by the U. S. Geological Survey. Following that method, an ontology was manually created that used the metadata from The National Map. Because a manual approach is prone to human error, an automated approach to storing and converting metadata into annotation properties is discussed. 
id18#Towards a Passive Self-Assembling Macroscale Multi-Robot System#The combined efforts of theoretical computer science, biochemistry, and nanotechnology have enabled the design of tile-based systems capable of self-assembling intricate patterns in a massively parallel manner, with low error rates, and applications ranging from DNA computing to microelectronics. However, as the underlying physical and chemical principles do not directly translate from micro to the macroscale, the transition to centimeter-scale systems remains challenging. In this contribution, we propose a framework for designing macroscale passive robots (tiles) capable of targeted self-assembly under uncontrolled external mechanical excitation. Self-assembly at this scale is achieved by using properly designed magneto-mechanical locks (glues) to accomplish jamming-free assembly, a dedicated encoding of glues to guide tile interactions, and consistent formalization of geometrical constraints that ensure the valid assembly. The potential of our framework is demonstrated by the errorless assembly of a chessboard pattern, thereby showing its robustness, three-fold increase in error recovery, and two-fold increase in growth rate, when compared to a fully magnetic approach. 
id19#Compiling for reduced bit-width queue processors#Embedded systems are characterized by the requirement of demanding small memory footprint code. A popular architectural modification to improve code density in RISC embedded processors is to use a reduced bit-width instruction set. This approach reduces the length of the instructions to improve code size. However, having less addressable registers by the reduced instructions, these architectures suffer a slight performance degradation as more reduced instructions are required to execute a given task. On the other hand, 0-operand computers such as stack and queue machines implicitly access their source and destination operands making instructions naturally short. Queue machines offer a highly parallel computation model, unlike the stack model. This paper proposes a novel alternative for reducing code size by using a queue-based reduced instruction set while retaining the high parallelism characteristics in programs. We introduce an efficient code generation algorithm to generate programs for our reduced instruction set. Our algorithm successfully constrains the code to the reduced instruction set with the addition of only 4% extra code, in average. We show that our proposed technique is able to generate about 16% more compact code than MIPS16, 26% over ARM/Thumb, and 50% over MIPS32 code. Furthermore, we show that our compiler is able to extract about the same parallelism than fully optimized RISC code. 
id21#ARank: Toward specific model pruning via advantage rank for multiple salient objects detection#Pruning unnecessary regions from weight kernels and filters used in convolutional nerual networks has proven to be effective for model compression. However, prior studies have often over-optimized for generic application areas, which are not ideal for specific problems in salient object detection. In this study, a novel pruning strategy denoted advantage rank (ARank) is developed for multiple salient object detection (Mul-SOD), in which salient priority criterion (SPC) is used to evaluate parameter contributions. The proposed SPC is a measurable standard for object priority that can be quantified and calculated using ARank. Simulation experiments demonstrate that Mul-SOD can be optimized by removing interference from low-SPC parameters. ARank effectively decreased interference and distinguished high-priority salient objects from multiple background objects. 
id23#Rail Surface Inspection System Using Differential Topographic Images#In this article, a surface inspection system for rails is presented. Rails must meet the strict requirements of international quality standards; however, there are few commercial surface inspection systems for rails and also a lack of publications describing the design and configuration of inspection systems in detail. Therefore, manufacturers must develop their own systems or buy one of the few commercial ones available. These systems also need a long, cumbersome, and expensive configuration process that the manufacturer cannot perform without the assistance of the inspection system provider. The system proposed in this article needs a set of samples and the requirements of the international standards to carry out an automatic configuration process avoiding the cost of manual configuration. The system uses four profilometers to acquire the surface of the rail. The acquired data is compared to a mathematical model of the rail to generate differential topographic images of the surface of the rail. Then a computer vision algorithm is used to detect defects based on the tolerances established in the international quality standards. The system has been tested and validated using a set of rails and a rail pattern from ArcelorMittal, with better results than the other two systems installed in a factory.1 
id25#Post-quantum secure multi-party private set-intersection in star network topology#In many realistic scenarios, participants wish to perform some secret set operations such as intersection, union, cardinality of intersection, etc. on their private data sets. Private Set Intersection (PSI) plays a major role in addressing such problems. PSI is one of the widely used secure multi-party computation technique that allows the participants to securely compute the intersection of their private input sets and nothing beyond that. It is generally executed between two parties. When the number of entities is more than two, it is known as multi-party PSI (MPSI). Today, the security of all the existing MPSI protocols are based on number theoretic assumptions. However, these will become insecure once large enough quantum computers are built. As a consequence, designing of quantum computer resistant MPSI becomes an interesting direction of research work. This paper addresses the issue by presenting the first post-quantum MPSI protocol in the so-called star network topology, using lattice-based public key encryption scheme. We utilize space-efficient probabilistic data structure (Bloom filter) as building blocks of our design. It attains security in standard model (without random oracles) under the decisional learning with errors (DLWE) assumption. 
id27#Simulating and Compiling Code for the Sequential Quantum Random Access Machine#We present the SQRAM architecture for quantum computing, which is based on Knill's QRAM model. We detail a suitable instruction set, which implements a universal set of quantum gates, and demonstrate the operation of the SQRAM with Deutsch's quantum algorithm. The compilation of high-level quantum programs for the SQRAM machine is considered; we present templates for quantum assembly code and a method for decomposing matrices for complex quantum operations. The SQRAM simulator and compiler are discussed, along with directions for future work. 
id28#PICCO: A general-purpose compiler for private distributed computation#Secure computation on private data has been an active area of research for many years and has received a renewed interest with the emergence of cloud computing. In recent years, substantial progress has been made with respect to the efficiency of the available techniques and several implementations have appeared. The available tools, however, lacked a convenient mechanism for implementing a general-purpose}program in a secure computation framework suitable for execution in not fully trusted environments. This work fulfills this gap and describes a system, called PICCO, for converting a program written in an extension of C into its distributed secure implementation and running it in a distributed environment. The C extension preserves all current features of the programming language and allows variables to be marked as private and be used in general-purpose computation. Secure distributed implementation of compiled programs is based on linear secret sharing, achieving efficiency and information-theoretical security. Our experiments also indicate that many programs can be evaluated very efficiently on private data using PICCO. 
id29#Learning spectral normalized adversarial systems with stacked structure for high-quality 3D object generation#This paper proposes a new method for generating 3D objects based on generative adversarial networks (GANs). Recently, GANs have been used in 3D object generation, but it is still very challenging to generate high-quality 3D objects because of the complex data distribution over 3D objects. In this paper, we propose a system based on GAN that makes the generated objects more realistic. We use multiple generators and discriminators to enhance the ability of the model for learning complex distributions. Such a stacked structure can be considered as a coarse-to-fine or low-to-high–resolution mechanism. We employ the spectral normalization technology to control the Lipschitz constant of the discriminators by literally constraining the spectral norm of each layer to get a more stable training process. In this way, the proposed model can generate realistic and high-quality 3D objects. Moreover, our system can also recover incomplete 3D objects into complete 3D objects. Experiments demonstrate that our model performs better in the quality of the generated objects than the baselines. 
id30#Generalizations of the theory and deployment of triangular inequality for compiler-based strength reduction#Triangular Inequality (TI) has been used in many manual algorithm designs to achieve good efficiency in solving some distance calculation-based problems. This paper presents our generalization of the idea into a compiler optimization technique, named TI-based strength reduction. The generalization consists of three parts. The first is the establishment of the theoretic foundation of this new optimization via the development of a new form of TI named Angular Triangular Inequality, along with several fundamental theorems. The second is the revealing of the properties of the new forms of TI and the proposal of guided TI adaptation, a systematic method to address the difficulties in effective deployments of TI optimizations. The third is an integration of the new optimization technique in an open-source compiler. Experiments on a set of data mining and machine learning algorithms show that the new technique can speed up the standard implementations by as much as 134X and 46X on average for distance-related problems, outperforming previous TI-based optimizations by 2.35X on average. It also extends the applicability of TI-based optimizations to vector related problems, producing tens of times of speedup. 
id31#Machine Learning and Data Visualization to Evaluate a Robotics and Programming Project Targeted for Women#Around the world women end up being less interested in areas related to the sciences, technology, engineering and mathematics, or shortly STEM. Therefore, it is important that governments around the world maintain an active interest in getting women to continue in STEM careers. In this context, this work was divided into three main phases, the first was to conduct a search through a related works that were published involving the development of projects aimed at the engagement of girls students or female teachers/professionals within the context of STEM or Robotics, between the years 2018 and 2020. In this case, seven works were found within these criteria, including one Brazilian project. Subsequently, analyzes were carried out of the 85 projects that are being financed by the federal government of Brazil within STEM. The last analysis was a case study to evaluate the engagement of female teachers and students in a medium-sized city in the interior of the southeastern region of Brazil. In this case, we carried out the analysis with the teachers and students, as well as with an external audience. We carry out our analyzes through statistics and analysis of feelings and opinions, in addition to data visualizations. In the end, we conducted through data mining of unsupervised machine learning, analyzes of the groups of people we are interested in engaging, which are groups of young people, especially girls who are interested in STEM, but with little knowledge in Robotics. This strategy was put on the schedule, because we will aim to increase the knowledge of these girls in STEM, especially in robotics, which is the focus of our study and research group. Finally, results have shown that this project has improved a major social and encouraging role for these girls in the field of exact sciences, computing and engineering. 
id32#A Port Graph Rewriting Approach to Relational Database Modelling#We present new algorithms to compute the Syntactic Closure and the Minimal Cover of a set of functional dependencies, using strategic port graph rewriting. We specify a Visual Domain Specific Language to model relational database schemata as port graphs, including an extension to port graph rewriting rules. Using these rules we implement strategies to compute a syntactic closure, analyse it and find minimal covers, essential for schema normalisation. The graph program provides a visual description of the computation steps coupled with analysis features not available in other approaches. We show soundness and completeness of the computed closure, and implement it in PORGY. 
id33#FollowUpAR: Enabling Follow-up Effects in Mobile AR Applications#Existing smartphone-based Augmented Reality (AR) systems are able to render virtual effects on static anchors. However, today's solutions lack the ability to render follow-up effects attached to moving anchors since they fail to track the 6 degrees of freedom (6-DoF) poses of them. We find an opportunity to accomplish the task by leveraging sensors capable of generating sparse point clouds on smartphones and fusing them with vision-based technologies. However, realizing this vision is non-trivial due to challenges in modeling radar error distributions and fusing heterogeneous sensor data. This study proposes FollowUpAR, a framework that integrates vision and sparse measurements to track object 6-DoF pose on smartphones. We derive a physical-level theoretical radar error distribution model based on an in-depth understanding of its hardware-level working principles and design a novel factor graph competent in fusing heterogeneous data. By doing so, FollowUpAR enables mobile devices to track anchor's pose accurately. We implement FollowUpAR on commodity smartphones and validate its performance with 800,000 frames in a total duration of 15 hours. The results show that FollowUpAR achieves a remarkable rotation tracking accuracy of 2.3° with a translation accuracy of 2.9mm, outperforming most existing tracking systems and comparable to state-of-the-art learning-based solutions. FollowUpAR can be integrated into ARCore and enable smartphones to render follow-up AR effects to moving objects. 
id34#The energy-oriented management of public historic buildings: An integrated approach and methodology applications#In the European framework, there is a strong drive to develop integrated approaches aimed at understanding and improving the energy behavior of public historic buildings within urban contexts. However, the examples already provided tend to address the issue from mono-disciplinary perspectives, losing the opportunity for a coordinated view. The research suggests a methodology to reach the definition of a three-dimensional database, which incorporates spatial models and energy information, with the final goal of merging heterogeneous information that is useful to interpret the overall framework and to design sustainable development scenarios. The platform achieves GIS (Geographic Information System) and BIM (Building Information Modeling) integration by using the CityGML data model, for supporting multi-scale analyses without break of continuity, ranging from urban to building level. The discussion combines the applicative case with the theoretical background, deepening the role of a solid knowledge framework as a basis for sustainable interventions on public historic buildings. To better explain and test the methodology, a case study on the University built heritage of Pavia is presented and three possible outputs deriving from the database are discussed. The example demonstrates the strength of the approach, which is able to provide a variety of results coming from a unique source of information, ensuring coherence and unambiguousness at all levels of investigation. 
id35#Blockchain based secure smart city architecture using low resource IoTs#Internet of Things (IoT) is growing exponentially in research and industry. Although, many standard and conventional security solutions have been provided for IoT, it suffers from many privacy and security concerns. Standard security protocols are not suitable for majority of IoT devices because of its decentralized topology and resource-constraints. Blockchain (BC) finds its efficient application in IoT to preserve the five basic cryptographic primitives, such as confidentiality, authenticity, integrity, availability and non-repudiation. Adoption of conventional BC in IoT causes high energy consumption, delay and computational overheads which are not appropriate for various resource constrained IoT devices. To mitigate these problems, this work proposes a smart access control framework in a public and a private BC for a smart city application which makes it more efficient and secure as compared to the existing IoT applications. The proposed IoT based smart city architecture adopts BC technology for preserving all the cryptographic security and privacy issues. Moreover, proposed BC has minimal overhead as well. This work investigates the existing threat models and critical access control issues which handle multiple permissions of various processing nodes of IoT environment and detects relevant inconsistencies. Comparison in terms of all security issues with existing literature shows that the proposed architecture is competitively efficient in terms of security access control. The primary goal of this research article is to explore the possibility of BC as an alternative to standard security solutions for low resource IoT applications. 
id36#Cobalt: A Language for Writing Provably-Sound Compiler Optimizations#We overview the current status and future directions of the Cobalt project. Cobalt is a domain-specific language for implementing compiler optimizations as guarded rewrite rules. Cobalt optimizations operate over a C-like intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. The design of Cobalt engenders a natural inductive strategy for proving the soundness of optimizations. This strategy is fully automated by requiring an automatic theorem prover to discharge a small set of simple proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial dead assignment elimination, and simple forms of points-to analysis. The implementation of our soundness-checking strategy employs the Simplify automatic theorem prover, and we have used this implementation to automatically prove the above optimizations correct. An execution engine for Cobalt optimizations is implemented as part of the Whirlwind compiler infrastructure. 
id37#Query structure and data model mapping errors in information retrieval tasks#SQL query writing is a challenging task for novices, even after considerable training. Query writing is a programming task and a translation task where the writer must translate a user's request for information into code that conforms to the structure, constraints, and syntax of an SQL SELECT statement and that references specific tables and columns from a database. This paper investigates the impact of two instructional interventions on query errors under conditions of low and high query complexity. Data was collected from an experimental study of 63 undergraduate students nearing completion of a 15-week database course. Our analysis reveals specific areas of query writing where each of the interventions helped, and hindered, task performance. We discuss the implications of these findings for improving SQL training and for future research on SQL training effectiveness. 
id38#Behaviour recognition of pigs and cattle: Journey from computer vision to deep learning#The increasing demand for sustainable livestock products also demands new considerations in animal breeding. Breeding programs are now seeking to integrate animal behavioural phenotypes, as these relate to the productivity, health and welfare of the animals and thereby can influence yield and economic benefits in the industry. Traditional manual observation of pig behaviour is time-consuming, laborious, subjective, and difficult to achieve in continuous and large-scale operations. It is not surprising that computer vision technology with the advantages of being objective, non-invasive and continuous has been widely researched for its use in the recognition of livestock behaviours over recent years. Nevertheless, in studies of livestock behaviour recognition, computer vision technology faces some challenges, e.g., complex scenes, variable illumination, occlusion, touching and overlapping between livestock, which has limited the fast translation of technology to industry. On the other hand, deep learning technology has proven to solve these difficulties to a certain extent and is being adopted to recognise livestock behaviours. This paper mainly evaluates the recent developments in computer vision methods for recognition of these behaviours in pigs and cattle. The focus on these species is made possible by the number of studies exist quantifying behaviours that are of importance for their health, welfare and productivity such as aggression, drinking, feeding, lameness, mounting, posture, tail-biting and nursing. This review paper especially analyses the development of image segmentation, identification and behaviour recognition using tradition computer vision and more recent deep learning methods, and evaluates the evolution of key research in the field. We elaborate the research trend of livestock behaviour recognition from four aspects, i.e., development of robust livestock identification algorithms, recognition of livestock behaviours for different growth stages, further quantification of the results of behaviour recognition, and building evaluation system of growth status, health and welfare. 
id39#Novel denial-of-service attacks against cloud-based multi-robot systems#The development of robotics technology is accelerated by the strong support from cloud computing. Massive computation resources and services from the cloud make modern multi-robot systems more efficient and powerful. However, the introduction of cloud servers to multi-robot systems can also incur potential Denial-of-Service (DoS) threats, where an adversary can utilize the shared cloud resources to degrade or bring down the robot systems. In this paper, we conduct a comprehensive study about this security issue. By analyzing different attack vectors in cloud-robotic platforms, we propose three new DoS attacks, which manipulate the network resources, micro-architecture resources, and function parameters respectively. We conduct extensive evaluations and case studies to demonstrate the feasibility and severity of our techniques. We alert the robotics community to these catastrophic attacks on the safety and performance of cloud-robotic systems, and encourage building better defenses for higher reliability, in addition to automation and intelligence. 
id41#Sustainable educational robotics. Contingency plan during lockdown in primary school#New technologies have offered great alternatives for education. In this context, we place robotics and programming as innovative and versatile tools that adapt to active methodologies. With the arrival of COVID-19 and lockdowns, physical resources were kept out of use, and the virtual lectures did not propose to incorporate these elements in a meaningful way. This recent situation raises as an objective of study the need to evaluate if robotics and programming are content that can be taught virtually in these circumstances, without physical resources and without face-to-face lectures. To do this, a mixed methodology consisting of questionnaires and interviews has been incorporated, aimed at primary education teachers, families, and primary education grade students. The results suggest that the virtualization of robotics and programming is a feasible and beneficial alternative for students, which allows the development of digital skills, while it is enhanced with the use of audiovisual materials and online resources. Even though face-to-face classes have other benefits not offered by virtualization, and teacher training needs to be up to the task to face this situation, it is a matter of time to respond to these situations and to guarantee a high-quality distance education. 
id42#Vins: Visual search for mobile user interface design#Searching for relative mobile user interface (UI) design examples can aid interface designers in gaining inspiration and comparing design alternatives. However, finding such design examples is challenging, especially as current search systems rely on only text-based queries and do not consider the UI structure and content into account. This paper introduces VINS, a visual search framework, that takes as input a UI image (wireframe, high-fidelity) and retrieves visually similar design examples. We first survey interface designers to better understand their example finding process. We then develop a large-scale UI dataset that provides an accurate specification of the interface's view hierarchy (i.e., all the UI components and their specific location). By utilizing this dataset, we propose an object-detection based image retrieval framework that models the UI context and hierarchical structure. The framework achieves a mean Average Precision of 76.39% for the UI detection and high performance in querying similar UI designs. 
id43#Coordinate ascent MORE with adaptive entropy control for population-based regret minimization#Model-based Relative Entropy Policy Search (MORE) is a population-based stochastic search algorithm with desirable properties such as a well defined policy search objective, i.e., it optimizes the expected return, and exact closed form information theoretic update rules. This is in contrast with existing population-based methods, that are often referred to as evolutionary strategies, such as CMA-ES. While these methods work very well in practice, the updates of the search distribution are often based on heuristics and they do not optimize the expected return of the population but instead implicitly optimize the return of elite samples, which may yield a poor expected return and unreliable or risky solutions. We show that the MORE algorithm can be improved with distinct updates based on coordinate ascent on the mean and covariance of the search distribution, which considerably improves the convergence speed while maintaining the exact closed form updates. In this way, we can match the performance of elite samples of CMA-ES while also showing a considerably improved performance of the sample average. We evaluate our new algorithm on simulated robotic tasks and compare to the state of the art CMA-ES. 
id44#Performance of Texture Compression Algorithms in Low-Latency Computer Vision Tasks#Deep learning has been successfully used for computer vision tasks, but its high computational cost limits the adoption in lightweight devices such as camera sensors. For this reason, many low-latency vision systems offload the inference computation to a local server, requiring fast (de)compression of the source images. Texture compression is a compelling alternative to existing compression schemes, such as JPEG or HEVC, due to its low decoding overhead, straightforward parallelization, robustness, and a fixed compression ratio. In this paper, we study the impact of lightweight bounding box-based texture compression algorithms, BC1 and YCoCg-BC3, on the accuracy of two computer vision tasks: object detection and semantic segmentation. While JPEG achieves superior per-pixel error rate, the YCoCg-BC3 encoding can provide comparable vision accuracy. The BC1 encoding results in significant degradation of vision performance. However, by retraining the FasterSeg teacher network with a BC1-compressed dataset, we reduced its segmentation mIoU loss from 2.7 to 0.5 percent. Thus, both BC1 and YCoCg-BC3 encoders are suitable for use in low latency vision systems, since they both achieve significantly higher encoding speed than JPEG and their decoding overhead is negligible. 
id45#Punch Anticipation in a Karate Combat with Computer Vision#Fighting in karate martial art requires great dexterity and ability of multiple physical and psychological factors. A key and fundamental skill for the success of this endeavor is the anticipation of the opponent's movements. Anticipation is an innate attribute, but it can also be worked on with training. Vision training, in the search for peripheral vision that allows the opponent's body to be monitored, is continuously worked on in martial arts training. Nonetheless, peripheral vision can be of use outside the martial arts domain, such as when driving (to be able to notice the environment) or reading (to increase reading speed). New technologies can bring new training methods that enhance peripheral vision by training motion anticipation. For this, a tool designed for karate training is used to evaluate if computer vision filters can facilitate motion anticipation performance in karate practice. Our research aims to model the interaction of the fighters in order to improve the body reading of the opponent as well as the dexterity in the anticipation of the attacks made by the opponent, aimed to build a personalized system for psychomotor learning. A user study is carried out to evaluate whether computer vision can be used to improve the prediction of punch attacks launched by the rival as well as the response time to them. 
id46#Research on malicious traffic identification technology in encrypted traffic [加密流量中的恶意流量识别技术]#The encrypted transmission of network traffic is one of the development trends of the Internet. The identification of malicious traffic in encrypted traffic is an important way to maintain the security of cyberspace. One of the prior tasks of identifying malicious traffic is to classify encrypted traffic into the encrypted/unencrypted, different kinds of the application programs and encryption algorithms in order to improve the efficiency of identification. Then they are transformed into the image, matrix, n-gram or other forms which will be sent into the machine learning training model, so as to realize the binary classification and multi classification of benign malicious traffic. However, the machine learning based way relies seriously on the number and quality of samples, and can not effectively deal with the data after traffic shaping or confusion. Fortunately, cryptography based malicious traffic identification can search malicious keywords over encrypted traffic to avoid such problems, which must integrate searchable encryption technology, deep packet inspection and a provable security model to protect both data and rules. Finally, some unsolved problems of malicious traffic identification technology in encrypted traffic are presented. 
id47#Calling hardware procedures in a reconfigurable accelerator using RPC-FPGA#RPC-FPGA is a remote procedure call protocol implementation of the Open Network Computing ONC-RPC specification for use in FPGA accelerators. The implementation involves the generation of High-Level Synthesis (HLS) interface stubs to call a hardware procedure and stream the data between the processor and the FPGA. The RPC protocol is extended to accept variable-length arrays with multiple dimensions. This extension is required to support optimizing nested loop transformations on multidimensional arrays stored in the reconfigurable logic. The major benefits of RPC-FPGA are: hardware procedures running on an FPGA accelerator become accessible for any client in the network, multiple hardware procedures run in a truly parallel fashion and the development time of the communication interface between the processor and the FPGA is largely reduced. Using RPC-FPGA a speedup gain of 17 to 20 is demonstrated on a square matrix multiplication of N=4096 with network speeds 100 Mbps and 1 Gbps respectively. 
id49#A Benchmarking Platform for Learning-Based Grasp Synthesis Methodologies#Benchmarking is a common practice employed to quantify the performance of various approaches toward the same task. By maintaining a consistent test environment, the inherent behaviours between methods may be distinguished—which is key to progressing a research field. In robotic manipulation research there is a current lack of standardisation, making it challenging to fairly assess and compare various approaches throughout literature. This paper proposes new criteria in conjunction with a benchmarking platform to measure the effectiveness of a grasping pipeline. The proposed benchmarking template offers a testing platform for 2-fingered, vision-based grasp synthesis methodologies. A prototype system was constructed. The prototype was shown to serve as a suitable benchmarking platform for the deployment of various grasp synthesis methodologies. 4000 trials were conducted to evaluate the differing approaches. Results showed that the proposed metrics provide useful insights into the quality of grasp poses produced by a grasp synthesis methodology. Moreover, such metrics provide more comprehensive insights into grasp outcome than traditional methods used to quantify performance of a methodology and present a fair baseline for comparison between different approaches. 
id50#Identity-based encryption in the internet of things [Nesnelerin internetinde kimlik tabanli şifreleme]#Internet of Things (IoT) has been developing rapidly with the use of new technologies today. In addition to offering new applications that make our lives easier, it also reveals serious security and privacy problems. Hundreds of sensors around us or the applications we use can share our private data with other objects without our knowledge. Encryption systems can be used to prevent this emerging security problem. However, it is difficult to determine the appropriate encryption system due to limited power resources and computational capabilities. In this study, an identity-based encryption system has been proposed for secure communication between objects included in the Internet of Things. 
id51#Fusing a Transformation Language with an Open Compiler#Program transformation systems provide powerful analysis and transformation frameworks as well as concise languages for language processing, but instantiating them for every subject language is an arduous task, most often resulting in half-completed frontends. Compilers provide mature frontends with robust parsers and type checkers, but solving language processing problems in general-purpose languages without transformation libraries is tedious. Reusing these frontends with existing transformation systems is therefore attractive. However, for this reuse to be optimal, the functional logic found in the frontend should be exposed to the transformation system - simple data serialization of the abstract syntax tree is not enough, since this fails to expose important compiler functionality, such as import graphs, symbol tables and the type checker. In this paper, we introduce a novel and general technique for combining term-based transformation systems with existing language frontends. The technique is presented in the context of a scriptable analysis and transformation framework for Java built on top of the Eclipse Java compiler. The framework consists of an adapter automatically extracted from the abstract syntax tree of the compiler and an interpreter for the Stratego program transformation language. The adapter allows the Stratego interpreter to rewrite directly on the compiler AST. We illustrate the applicability of our system with scripts written in Stratego that perform framework and library-specific analyses and transformations. 
id52#MPI-aware compiler optimizations for improving communication-computation overlap#Several existing compiler transformations can help improve communication-computation overlap in MPI applications. However, traditional compilers treat calls to the MPI library as a black box with unknown side effects and thus miss potential optimizations. This paper's contributions enable the development of an MPI-aware optimizing compiler that can perform transformations exploiting knowledge of MPI call effects to increase communication-computation overlap. We formulate a set of data flow equations and rules to describe the side effects of key MPI functions so an MPI-aware compiler can automatically assess the safety of transformations. After categorizing existing compiler transformations based on their effect on the application code, we present an optimization algorithm that specifies when and how to apply these optimizing transformations to achieve improved communication-computation overlap. By manually applying the optimization algorithm to kernels extracted from HYCOM and the NAS benchmarks, we show that even when transforming these highly optimized codes, execution time can be decreased by an average of over 30%. Copyright 2009 ACM.
id53#Hybrid flow graphs: Towards the transformation of sequential code into parallel pipeline networks#Transforming procedural code for execution by specialized parallel platforms requires a model of computation sufficiently close to both the sequential programming languages and the target parallel environment. In this paper, we present Hybrid Flow Graphs, encompassing both control flow and data flow in a unified, pipeline based model of computation. Besides the definition of the Hybrid Flow Graph, we introduce a formal framework based on graph rewriting, used for the specification of Hybrid Flow Graph semantics as well as for the proofs of correctness of the associated code transformations. As a formalism particularly close to pipeline-based runtime environments which include many modern database engines, the Hybrid Flow Graphs may become a powerful means for the automatic parallelization of sequential code under these environments.
id54#Automated system to measure Tandem Gait to assess executive functions in children#As mobile technologies have become ubiquitous in recent years, computer-based cognitive tests have become more popular and efficient. In this work, we focus on assessing motor function in children by analyzing their gait movements. Although there has been a lot of research on designing automated assessment systems for gait analysis, most of these efforts use obtrusive wearable sensors for measuring body movements. We have devised a computer vision-based assessment system that only requires a camera which makes it easier to employ in school or home environments. A dataset has been created with 27 children performing the test. Furthermore, in order to improve the accuracy of the system, a deep learning based model was pre-trained on NTU-RGB+D 120 dataset and then it was fine-tuned on our gait dataset. The results highlight the efficacy of proposed work for automating the assessment of children's performances by achieving 76.61% classification accuracy. 
id56#Towards an automatic data value analysis method for relational databases#Data is becoming one of the world’s most valuable resources and it is suggested that those who own the data will own the future. However, despite data being an important asset, data owners struggle to assess its value. Some recent pioneer works have led to an increased awareness of the necessity for measuring data value. They have also put forward some simple but engaging survey-based methods to help with the first-level data assessment in an organisation. However, these methods are manual and they depend on the costly input of domain experts. In this paper, we propose to extend the manual survey-based approaches with additional metrics and dimensions derived from the evolving literature on data value dimensions and tailored specifically for our use case study. We also developed an automatic, metric-based data value assessment approach that (i) automatically quantifies the business value of data in Relational Databases (RDB), and (ii) provides a scoring method that facilitates the ranking and extraction of the most valuable RDB tables. We evaluate our proposed approach on a real-world RDB database from a small online retailer (MyVolts) and show in our experimental study that the data value assessments made by our automated system match those expressed by the domain expert approach. Copyright 
id57#Improving whole-program locality using intra-procedural and inter-procedural transformations#Exploiting spatial and temporal locality is essential for obtaining high performance on modern computers. Writing programs that exhibit high locality of reference is difficult and error-prone. Compiler researchers have developed loop transformations that allow the conversion of programs to exploit locality. Recently, transformations that change the memory layouts of multi-dimensional arrays - called data transformations - have been proposed. Unfortunately, both data and loop transformations have some important drawbacks. In this work, we present an integrated framework that uses loop and data transformations in concert to exploit the benefits of both approaches while minimizing the impact of their disadvantages. Our approach works inter-procedurally on acyclic call graphs, uses profile data to eliminate layout conflicts, and is unique in its capability of resolving conflicting layout requirements of different references to the same array in the same nest and in different nests for regular array-based applications. The optimization technique presented in this paper has been implemented in a source-to-source translator. We evaluate its performance using standard benchmark suites and several math libraries (complete programs) with large input sizes. Our experimental results show that the proposed approach improves the performance of the applications optimized by using the current state-of-the-art techniques by 8.2% on the average. This reduction comes from three important characteristics of the technique, namely, resolving layout conflicts between references to the same array in a loop nest, determining a suitable order to propagate layout modifications across loop nests, and propagating layouts between different procedures in the program - all in a unified framework. The locality optimization technique presented in this paper tries to exploit locality in the innermost loop positions. This strategy, in most cases, generates dependence-free outer loops, which can be safely parallelized. 
id58#ESS-IBAA: Efficient, short, and secure ID-based authentication algorithm for wireless sensor network#Designing an efficient, short, and secure authentication algorithm for resource-constrained sensor nodes of wireless sensor networks (WSNs) is a challenging task. Authentication in WSNs is mainly performed by the digital signature algorithm. In this paper, we propose an efficient, short, and secure pairing computation-free ID-based authentication algorithm (signature scheme) ESS-IBAA for WSNs which completely follows the rule of identity-based cryptosystem. Identity-based schemes, unlike traditional public-key infrastructure (PKI)-based schemes, remove the need for public-key certificates for public-key validation. It also removes extra costs associated with the public-key certificate and traffic management. Further, due to the requirement of low power and fast authentication, bilinear pairing computation-free identity-based signature schemes are applicable in WSNs. Keeping this in mind, ESS-IBAA scheme is proposed, which is pairing computation free and uses a general cryptographic hash function in the place of a costly map-to-point hash function. The proposed scheme ESS-IBAA is secure against existential forgery on adaptive chosen message and ID attack in the random oracle model under the hardness of the elliptic curve discrete logarithm problem (ECDLP). Moreover, comparative performance analysis shows that the proposed scheme ESS-IBAA is much more efficient in both communication cost and computation cost from the existing related schemes. 
id59#Toward an automatic parallelization of sparse matrix computations#In this paper, we propose a generic method of automatic parallelization for sparse matrix computation. This method is based on both a refinement of the data-dependence test proposed by Bernstein and an inspector-executor scheme which is specialized to each input program of the compiler. This analysis mixes compilation process and run-time process. The sparsity of underlying data-structure determines a specific parallelism which increases the degree of parallelism of an algorithm. Such a source of parallelism had already been applied to many numerical algorithms such as the usual Cholesky factorization or LU-decomposition algorithms considered as the gold standards of parallelization based on sparsity. The standard automatic parallelization method cannot tackle such source of parallelism because it is based on the value of cells arrays and not merely on the memory addressing function. Addressing the automatization of this parallelism requires to develop a mixed compile-time and runtime approach integrated in a inspector-executor process. The compilation step provides a dedicated inspector devoted to the analyzed program. The inspector computes the dependence graph at runtime which allows a dynamic parallelization of the execution. As expressed just before, the generic scheme developed in this paper follows the design principles which have been applied, but at each time in an ad hoc way, to many sparse parallelization of numerical algorithms such as Cholesky algorithm. As far as we know, no general formal framework has been proposed to automate such a method of sparse parallelization. In this paper, we propose a generic framework of sparse parallelization (i.e. numerical program independent) which can be applied to any numerical programs satisfying the usual syntactic constraints of parallelization. 
id61#Compiling graph transformation rules into a procedural language for behavioral modeling#Graph transformation rules provide an opportunity to specify model transformations in a declarative way at a high level of abstraction. So far, compilers have translated graph transformation rules into conventional programming languages such as Java, C, or C
id62#Ethical considerations in user modeling and personalization#Ethical considerations are getting increased attention with regards to providing responsible personalization for robots and autonomous systems. This is partly as a result of the currently limited deployment of such systems in human support and interaction settings. The tutorial will give an overview of the most commonly expressed ethical challenges and ways being undertaken to reduce their impact using the findings in an earlier undertaken review supplemented with recent work and initiatives. The tutorial will exemplify the challenges related to privacy, security and safety through several examples from own and others' work. 
id63#Artificial Intelligence in Computer Vision: Cardiac MRI and Multimodality Imaging Segmentation#Purpose of Review: Anatomical segmentation has played a major role within clinical cardiology. Novel techniques through artificial intelligence-based computer vision have revolutionized this process through both automation and novel applications. This review discusses the history and clinical context of cardiac segmentation to provide a framework for a survey of recent manuscripts in artificial intelligence and cardiac segmentation. We aim to clarify for the reader the clinical question of “Why do we segment?” in order to understand the question of “Where is current research and where should be?” Recent Findings: There has been increasing research in cardiac segmentation in recent years. Segmentation models are most frequently based on a U-Net structure. Multiple innovations have been added in terms of pre-processing or connection to analysis pipelines. Cardiac MRI is the most frequently segmented modality, which is due in part to the presence of publically available, moderately sized, computer vision competition datasets. Further progress in data availability, model explanation, and clinical integration are being pursued. Summary: The task of cardiac anatomical segmentation has experienced massive strides forward within the past 5 years due to convolutional neural networks. These advances provide a basis for streamlining image analysis, and a foundation for further analysis both by computer and human systems. While technical advances are clear, clinical benefit remains nascent. Novel approaches may improve measurement precision by decreasing inter-reader variability and appear to also have the potential for larger-reaching effects in the future within integrated analysis pipelines. 
id64#Moving Recursion out of the RDBMS for Transactional Graph Workloads#This paper presents work in progress that focuses on querying transactional graph data that is stored in a relational database system (RDBMS). We focus on transactional workloads where there are frequent insert and update operations. Although these types of workloads are common in social network, scientific, and business applications, much of the prior work has focused on graph analytics workloads where there is little to no change to the data over time. We introduce an approach that combines simple database queries with parallel programming, and compare our approach to the recursive SQL operations that are known to have poor performance. Our initial experiments and results provide guidance for the future directions of this project where we will refine the parallel programing approach and structure of the experiment in order to better compare these two approaches. 
id65#Using decision trees to learn ontology taxonomies from relational databases#Relational databases are widely used; they are at the backend of the majority of information systems. However, these databases are semantically poor. To solve this issue, it is necessary to build ontologies. In this paper, we propose an automatic approach to learn ontologies from relational databases by using classification techniques, more specifically decision trees. Finally, we evaluate our approach by conducting tests and comparing our results with results from previous works. The results were satisfactory in terms of extracting taxonomies from relational databases. 
id66#Register allocation via graph coloring using an evolutionary algorithm#Register allocation is one of most important compiler optimization techniques. The most famous method for register allocation is graph coloring and the solution presented in this paper (RAGCES) is based on the graph coloring too; for coloring interference graph, evolutionary algorithm is used. In this method graph coloring and selecting variables for spilling is taken place at the same time. This method was tested on several graphs and results were compared with the results of the previous methods. 
id67#A Method for Transforming Object-relational to Document-oriented Databases#Object-relational databases have emerged to improve relational ones by adding properties of object-oriented approach such as references, polymorphism, inheritance, etc. However, these extended relational databases have become a huge amount of data and the database management systems (DBMS) cannot handle them. Due to the emergence of NoSQL databases for ensuring the storage and the processing of large data scale, it is necessary to propose a method for transforming object-relational to NoSQL databases. This paper presents a new method for transforming the object-relational database to one of the popular NoSQL data stores so-called document-oriented database. The proposed method is based on a set of matching between the schemata of object-relational and document-oriented databases. The method is terminated by the generation of a set of JSON files which represent collections of semi-structured documents. These files can be imported and represented by BSON format that will be managed by document-oriented DBMS such as MongoDB. 
id69#Automated maintenance feasibility testing on the EU DEMO Automated Inspection and Maintenance Test Unit (AIM-TU)#In order to reach commercially-relevant availability, future fusion power plants must minimise the duration of maintenance shutdowns. However, as radiation levels increase, so too will the number of maintenance tasks that must be performed without human access. To meet these conflicting constraints, remote maintenance systems must therefore significantly increase their capabilities, performing tasks faster and in parallel. Unfortunately, the current teleoperation-based maintenance approach is inherently limited in these regards. Automation emerges as a potential solution, having demonstrated dramatic productivity gains in manufacturing across the world. The European DEMO project has begun to explore the feasibility of automated maintenance as a route to meeting the challenging availability targets. To this end, the Automated Inspection and Maintenance Test Unit (AIM-TU) has been designed and constructed, providing a highly versatile test platform able to adapt to evolving R&D programmes. It consists of a modular robot cell equipped with two robot arms mounted on linear rails, interchangeable end-of-arm-tooling, a variety of vision sensors and a fully integrated safety system. The hardware is supported by a modular software architecture, which permits different control input modes and provides a digital twin simulation of the cell for virtual verification and validation of control algorithms. This paper provides an overview of the system's capabilities and reports on the first automated maintenance feasibility tests performed on the cell: automatic replacement of JET reactor tiles and automatic contactless inspection of tile anomalies. 
id71#A Comparison of Two Database Partitioning Approaches that Support Taxonomy-Based Query Answering#In this paper we address the topic of identification of cohorts of similar patients in a database of electronic health records. We follow the conjecture that retrieval of similar patients can be supported by an underlying distributed database design. Hence we propose a fragmentation based on partitioning the health records and present a benchmark of two implementation variants in comparison to an off-the-shelf data distribution approach provided by Apache Ignite. While our main use case in this paper is cohort identification, our approach has advantages for taxonomy-based query answering in other (non-medical) domains. 
id72#Extending Abstract Interpretation to Dependency Analysis of Database Applications#Dependency information (data- and/or control-dependencies) among program variables and program statements is playing crucial roles in a wide range of software-engineering activities, e.g., program slicing, information flow security analysis, debugging, code-optimization, code-reuse, code-understanding. Most existing dependency analyzers focus on mainstream languages and they do not support database applications embedding queries and data-manipulation commands. The first extension to the languages for relational database management systems, proposed by Willmor et al. in 2004, suffers from the lack of precision in the analysis primarily due to its syntax-based computation and flow insensitivity. Since then no significant contribution is found in this research direction. This paper extends the Abstract Interpretation framework for static dependency analysis of database applications, providing a semantics-based computation tunable with respect to precision. More specifically, we instantiate dependency computation by using various relational and non-relational abstract domains, yielding to a detailed comparative analysis with respect to precision and efficiency. Finally, we present a prototype semDDA semDDA, a semantics-based Database Dependency Analyzer integrated with various abstract domains, and we present experimental evaluation results to establish the effectiveness of our approach. We show an improvement of the precision on an average of 6 percent in the interval, 11 percent in the octagon, 21 percent in the polyhedra and 7 percent in the powerset of intervals abstract domains, as compared to their syntax-based counterpart, for the chosen set of Java Server Page (JSP)-based open-source database-driven web applications as part of the GotoCode project. 
id73#Write Like You: Synthesizing Your Cursive Online Chinese Handwriting via Metric-based Meta Learning#In this paper, we propose a novel Sequence-to-Sequence model based on metric-based meta learning for the arbitrary style transfer of online Chinese handwritings. Unlike most existing methods that treat Chinese handwritings as images and are unable to reflect the human writing process, the proposed model directly handles sequential online Chinese handwritings. Generally, our model consists of three sub-models: a content encoder, a style encoder and a decoder, which are all Recurrent Neural Networks. In order to adaptively obtain the style information, we introduce an attention-based adaptive style block which has been experimentally proven to bring considerable improvement to our model. In addition, to disentangle the latent style information from characters written by any writers effectively, we adopt metric-based meta learning and pre-train the style encoder using a carefully-designed discriminative loss function. Then, our entire model is trained in an end-to-end manner and the decoder adaptively receives the style information from the style encoder and the content information from the content encoder to synthesize the target output. Finally, by feeding the trained model with a content character and several characters written by a given user, our model can write that Chinese character in the user's handwriting style by drawing strokes one by one like humans. That is to say, as long as you write several Chinese character samples, our model can imitate your handwriting style when writing. In addition, after fine-tuning the model with a few samples, it can generate more realistic handwritings that are difficult to be distinguished from the real ones. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our method. 
id74#Interval Data Type Creation with Object-Relational Mapping#When processing a large amount of interval-valued data, stored in a relational database, developers cannot use the pre-property provided by the DBMS without dividing a single logical unit into lower and upper boundaries and storing them in different fields of the tables, thus jeopardizing the integrity of the database. The analysis of approaches to the implementation of methods used for storage and manipulating the data containing interval values shows, it is needed to develop a design procedure for the user-defined interval-valued data type used to create application software and design a DBMS. The article describes the implemented approach of creating an interval data type in SQL Server as well as the possibilities of its implementation with object-relational mapping technology that allow to solve the problem. The advantages and disadvantages of this approach and the complexity of its implementation are revealed. Research methods based on the interval arithmetic and interval analysis theory, object-oriented programming, the theory of relational databases designing. 
id75#Vehicle detection through instance segmentation using mask R-CNN for intelligent vehicle system#The recent advancement in artificial intelligence approach or deep learning techniques explored the ways to facilitate automation in various sectors. The application of deep learning with computer vision field has resulted in realization of intelligent systems. Vehicle detection plays a key role in Intelligent Vehicle System and Intelligent Transport System as it assists critical components of these systems like road scene classification, detecting obstacle vehicles to find an unhindered pathway, and even preventing accidents. This paper presents an implementation of Mask R-CNN state-of-the-art method using transfer learning technique for vehicle detection via instance wise segmentation which produces bounding box and object mask simultaneously. As the autonomous systems demands precise and flawless identification of the vehicles thus segmentation based approach is adopted. The model performs satisfactorily for occluded and small sized objects as well. This study is accomplished using an online GPU and cloud services provided by Google Colab by using Tensorflow and Keras framework. A mAP of 90.27% and mAR of 92.38% is achieved by using a combination of benchmark datasets. 
id76#Ocean front detection and tracking using a team of heterogeneous marine vehicles#Ocean monitoring is an expensive and time consuming endeavor, but it can be made more efficient through the use of teams of autonomous robots. In this paper, we present a system for the autonomous identification and tracking of ocean fronts by coordinating the sampling efforts of a heterogeneous team of autonomous surface vehicles (ASVs) and autonomous underwater vehicles (AUVs). The primary contributions of this study are (1) our algorithm for performing autonomous coordination using general autonomy principles: Sequential Allocation Monte Carlo Tree Search (SA-MCTS) which incorporates domain knowledge into the environmental estimation through both augmenting a standard Gaussian process with a nearest neighbors prior and planning in a drifting reference frame, (2) our decision support user interface to help human operators oversee the autonomous system, and (3) the demonstration of the system's operation in a 2-week long deployment in the Gulf of Mexico using a heterogeneous team of four Slocum gliders and two robotic ocean surface samplers. With these contributions, we aim to bridge the gap between state of the art autonomy algorithms and marine vehicle planning methods that have been tested in large-scale field trials. This paper presents the first deployment of a general, heuristic-based, multi-robot coordination algorithm for an extended sampling mission. 
id77#Surface crack detection using deep learning with shallow CNN architecture for enhanced computation#Surface cracks on the concrete structures are a key indicator of structural safety and degradation. To ensure the structural health and reliability of the buildings, frequent structure inspection and monitoring for surface cracks is important. Surface inspection conducted by humans is time-consuming and may produce inconsistent results due to the inspectors’ varied empirical knowledge. In the field of structural health monitoring, visual inspection of surface cracks on civil structures using deep learning algorithms has gained considerable attention. However, these vision-based techniques require high-quality images as inputs and depend on high computational power for image classification. Thus, in this study, shallow convolutional neural network (CNN)-based architecture for surface concrete crack detection is proposed. LeNet-5, a well-known CNN architecture, is optimized and trained for image classification using 40,000 images in the Middle East Technical University (METU) dataset. To achieve maximum accuracy for crack detection with minimum computation, the hyperparameters of the proposed model were optimized. The proposed model enables the employment of deep learning algorithms using low-power computational devices for a hassle-free monitoring of civil structures. The performance of the proposed model is compared with those of various pretrained deep learning models, such as VGG16, Inception, and ResNet. The proposed shallow CNN architecture was found to achieve a maximum accuracy of 99.8% in the minimum computation. Better hyperparameter optimization in CNN architecture results in higher accuracy even with a shallow layer stack for enhanced computation. The evaluation results confirm the incorporation of the proposed method with autonomous devices, such as unmanned aerial vehicle, for real-time inspection of surface crack with minimum computation. 
id78#A powerful LL(K) covering transformation#"k-transformable grammars have been conjectured to be the uppermost class of LL(k) covering transformable grammars. PLR(k) grammars have been known as a well characterized subclass of k-transformable grammars. Being contrary to those claims, this paper shows that some PLR(k) grammars are not k-transformable, and so k-transformable grammars are not the true uppermost. A powerful LL(k) covering transformation is suggested in this paper. It is a generalization of the transformations of k-transformable grammars and PLR(k) grammars. A remarkable aspect of the new transforming process is the deterministic property, where ""deterministic"" means that the transformation is obtained in a single process without requiring any heuristic, unlike k-transformable grammars' transformation for which a heuristic is required. The transformable grammar class is shown to be larger than k-transformable grammars and PLR(k) grammars. "
id79#Towards optimal task positioning in multi-robot cells, using nested meta-heuristic swarm algorithms#While multi-robot cells are being used more often in industry, the problem of work-piece position optimization is still solved using heuristics and the human experience and, in most industrial cases, even a feasible solution takes a considerable amount of trials to be found. Indeed, the optimization of a generic performance index along a path is complex, due to the dimension of the feasible-configuration space. This work faces this challenge by proposing an iterative layered-optimization method that integrates a Whale Optimization and an Ant Colony Optimization algorithm, the method allows the optimization of a user-defined objective function, along a working path, in order to achieve a quasi-optimal, collision free solution in the feasible-configuration space. 
id80#Efficient scalar multiplication of ECC using SMBR and fast septuple formula for IoT#In order to solve the problem between low power of Internet of Things devices and the high cost of cryptography, lightweight cryptography is required. The improvement of the scalar multiplication can effectively reduce the complexity of elliptic curve cryptography (ECC). In this paper, we propose a fast formula for point septupling on elliptic curves over binary fields using division polynomial and multiplexing of intermediate values to accelerate the computation by more than 14%. We also propose a scalar multiplication algorithm based on the step multi-base representation using point halving and the septuple formula we proposed, which significantly reduces the computational cost. The experimental results show that our method is more efficient over binary fields and contributes to reducing the complexity of ECC. 
id81#Optical multiple information hiding via azimuth multiplexing#We propose an optical information hiding scheme based on azimuth multiplexing. Azimuth is first introduced into optical information security, which effectively improves the space utilization. Also, the security of the system is ensured in many aspects because of the combination of free-space optics and visual cryptography. In the hiding process, multiple secret messages are encoded by visual cryptography and hidden in a cascaded phase structure of different azimuthal states. When extracting, different information can be extracted by rotating pure phase masks to a specific azimuth. The simulations and analysis also demonstrate that the system is robust to common perturbations, such as noise and blocking. 
id82#The 1st International Project Competition for Structural Health Monitoring (IPC-SHM, 2020): A summary and benchmark problem#To promote the development of structural health monitoring around the world, the 1st International Project Competition for Structural Health Monitoring (IPC-SHM, 2020) was initiated and organized in 2020 by the Asia-Pacific Network of Centers for Research in Smart Structures Technology, Harbin Institute of Technology, the University of Illinois at Urbana-Champaign, and four leading companies in the application of structural health monitoring technology. The goal of this competition was to attract more young scholars to engage in the study of structural health monitoring, encouraging them to provide creative and effective solutions for full-scale applications. Recognizing the recent advent and importance of artificial intelligence in structural health monitoring, three competition projects were set up with the data from full-scale bridges: (1) image-based identification of fatigue cracks in bridge girders, (2) data anomaly detection for structural health monitoring, and (3) condition assessment of stay cables using cable tension data. Three corresponding data sets were released at http://www.schm.org.cn and http://sstl.cee.illinois.edu/ipc-shm2020. Participants were required to be full-time undergraduate students, M.S. students, Ph.D. students, or young scholars within 3 years after obtaining their Ph.D. Both individual and teams (each team had no more than five individuals) could compete. Submissions for the competition included a 10- to 15-page technical paper, a 10-min presentation video with PowerPoint slides, and commented code. The organizing committee then conducted the validation, review, and evaluation. A total of 330 participants in 112 teams from 70 universities and institutions in 12 countries registered for the competition, resulting in 75 papers from 56 teams from 57 different affiliations finally being submitted. Of those submitted, 31, 30, and 14 papers were for Projects 1, 2, and 3, respectively. After completion of the review by the organization committee and awards committee, the top 10, 10, and 5 teams were selected as the prize winners for the three competition projects. 
id83#Sensorless force estimation for industrial robots using disturbance observer and neural learning of friction approximation#Contact force estimation enables robots to physically interact with unknown environments and to work with human operators in a shared workspace. Most heavy-duty industrial robots without built-in force/torque sensors rely on the inverse dynamics for the sensorless force estimation. However, this scheme suffers from the serious model uncertainty induced by the nonnegligible noise in the estimation process. This paper proposes a sensorless scheme to estimate the unknown contact force induced by the physical interaction with robots. The model-based identification scheme is initially used to obtain dynamic parameters. Then, neural learning of friction approximation is designed to enhance estimation performance for robotic systems subject with the model uncertainty. The external force exerted on the robot is estimated by a disturbance observer which models the external disturbance. A momentum observer is modified to develop a disturbance Kalman filter-based approach for estimating the contact force. The neural network-based model uncertainty and measurement noise level are analysed to guarantee the robustness of the Kalman filter-based force observer. The proposed scheme is verified by the measurement data from a heavy-duty industrial robot with 6 degrees of freedom (KUKA AUGLIS six). The experimental results are used to demonstrate the estimation performance of the proposed approach by the comparison with the existing schemes. 
id84#A high-level language and compiler to configure the Multi-Core Debug Solution (MCDS)#With the rise of multi-core System-on-Chips (SoC) debug adds new requirements and challenges to the system visibility and control. Complex on-chip trace and debug hardware like Infineon's Multi-Core Debug Solution (MCDS) offer the benefit of high degree of observability without affecting the run-time behavior of the system. Highly sophisticated programmable trace qualification capabilities allow the definition of analysis tasks, tailored to the users' needs. This paper introduces a high-level trace qualification language and compiler which enables the user defining analysis tasks efficiently and fully utilize the powerful and complex features of MCDS without the need of getting into the internals. The language and the compiler are already in industrial use where software development is based on MCDS enabled SoCs to support the developers to achieve better product quality and shorter product development cicles. 
id85#Managing Technical Debt in Database Normalization#Database normalization is one of the main principles for designing relational databases, which is the most popular database model, with the objective of improving data and system qualities, such as performance. Refactoring the database for normalization can be costly, if the benefits of the exercise are not justified. Developers often ignore the normalization process due to the time and expertise it requires, introducing technical debt into the system. Technical debt is a metaphor that describes trade-offs between short-term goals and applying optimal design and development practices. We consider database normalization debts are likely to be incurred for tables below the fourth normal form. To manage the debt, we propose a multi-attribute analysis framework that makes a novel use of the Portfolio Theory and the TOPSIS method (Technique for Order of Preference by Similarity to Ideal Solution) to rank the candidate tables for normalization to the fourth normal form. The ranking is based on the tables estimated impact on data quality, performance, maintainability, and cost. The techniques are evaluated using an industrial case study of a database-backed web application for human resource management. The results show that the debt-aware approach can provide an informed justification for the inclusion of critical tables to be normalized, while reducing the effort and cost of normalization. IEEE
id86#Multiphysics Simulation of Magnetically Actuated Robotic Origami Worms#Multiphysics simulation of magnetically actuated origami robots promises a range of applications such as synthetic data generation, design parameter optimization, predicting the robot's performance, and implementing various control algorithms, but it has rarely been explored. This letter presents a realistic multiphysics simulation of magnetically actuated origami robots with focus on real-time interactions between the origami and magnets. Due to the interaction between multiple magnets, the complex motion dynamics of a worm-like robot are generated and analyzed. We further show the possibility of accurately simulating origami structures made of different materials and permanent magnets of different shapes, sizes, and magnetic strength. The simulation's unknown parameters are determined by conducting similar practical and simulated experiments and comparing their characteristics. Further, we show the close resemblance between the real and simulated behavior of the origami robot. &copy; 2021 IEEE. 
id88#Data preparation for precise elevator advertising system#In recent years, elevator media has gradually become an important media, and more and more attention has been paid to it by businesses and media. At present, elevator media mainly use advertising screen in the elevator to broadcast advertisements in turn, and the advertising effect is difficult to evaluate. With the development of information technology such as big data and artificial intelligence, people begin to pay attention to the influence of big data on elevator media. People prefer to advertise by analyzing the commercial attributes of elevators. Meanwhile, advertising recommendation model based on click through rate is widely used in Internet advertising. Due to the lack of passenger commercial attributes (details of users) and feedback information of passengers watching the advertisements (the interactions), the advertising recommendation system of elevator media is mostly based on business logic or big data analysis. It is of great significance to effectively identify the commercial attributes of passengers and judge whether passengers watch advertisements for the accurate delivery of elevator media advertisements. This paper builds computer vision models to effectively identify the elevator passenger attributes and judge whether the passengers pay attention to the elevator advertising. This paper gives the solution of elevator media precise advertising recommendation for the first time, which fill the vacuum in this topic and is of great significance to elevator media precise advertising recommendation. 
id89#Dynamic simulation-guided design of tumbling magnetic microrobots#The design of robots at the small scale is a trial-and-error based process, which is costly and time-consuming. There are few dynamic simulation tools available to accurately predict the motion or performance of untethered microrobots as they move over a substrate. At smaller length scales, the influence of adhesion and friction, which scales with surface area, becomes more pronounced. Thus, rigid body dynamic simulators, which implicitly assume that contact between two bodies can be modeled as point contact, are not suitable. In this paper, we present techniques for simulating the motion of microrobots where there can be intermittent and non-point contact between the robot and the substrate. We use these techniques to study the motion of tumbling microrobots of different shapes and select shapes that are optimal for improving locomotion performance. Simulation results are verified using experimental data on linear velocity, maximum climbable incline angle, and microrobot trajectory. Microrobots with improved geometry were fabricated, but limitations in the fabrication process resulted in unexpected manufacturing errors and material/size scale adjustments. The developed simulation model can incorporate these limitations and emulate their effect on the microrobot's motion, reproducing the experimental behavior of the tumbling microrobots, further showcasing the effectiveness of having such a dynamic model. 
id90#Trivial Cryptographic Protocol for Resource-Constraint IoT Device Security Using OECC-KA#The IoT structures use data as a general rule; the data arrangement from contraptions can moreover be a goal of cyberattacks. It is a direct result of this that countermeasures dependent on encryption are as of now picking up in significance. Trivial (lightweight) cryptosystem is an encryption approach which includes slight impress with low-computational multifaceted nature. A few difficulties can block the fruitful execution of an IoT framework and its associated gadgets, including security, interoperability, power/handling capacities, adaptability, and accessibility. Many of these can be addressed with IoT gadget the board, for example, by accepting standard IoT conventions like lightweight M2M or utilising a comprehensive IoT gadget the executives stage offered by a reliable supplier as part of a SaaS model. In this paper, we propose a simple cryptography protocol for securing resource-constrained devices that uses Optimised ECC with Karatsuba algorithm (OECC-KA) for fast multiplication. Several efficient cryptographic algorithms have been widely used for resource constraint device; it may not be suitable with its limited computing power, bandwidth and CPU power. Elliptic curve cryptography is the best choice for limited power usage with high-level security. The traditional elliptic curve cryptography occupies 80% of execution time for scalar multiplication. The proposed optimized ECC with Karatsuba algorithm increases the execution time with fast multiplication algorithm. 
id91#Detecting visual design principles in art and architecture through deep convolutional neural networks#Visual design is associated with the use of some basic design elements and principles. Those are applied by the designers in the various disciplines for aesthetic purposes, relying on an intuitive and subjective process. Thus, numerical analysis of design visuals and disclosure of the aesthetic value embedded in them are considered as hard. However, it has become possible with emerging artificial intelligence technologies. This research aims at a neural network model, which recognizes and classifies the design principles over different domains. The domains include artwork produced since the late 20th century; professional photos; and facade pictures of contemporary buildings. The data collection and curation processes, including the production of computationally-based synthetic dataset, is genuine. The proposed model learns from the knowledge of myriads of original designs, by capturing the underlying shared patterns. It is expected to consolidate design processes by providing an aesthetic evaluation of the visual compositions with objectivity. 
id93#Axiomatic semantics for compiler verification#Based on constructive type theory, we study two idealized imperative languages GC and IC and verify the correctness of a compiler from GC to IC. GC is a guarded command language with underspecified execution order defined with an axiomatic semantics. IC is a deterministic low-level language with linear sequential composition and lexically scoped gotos defined with a small-step semantics. We characterize IC with an axiomatic semantics and prove that the compiler from GC to IC preserves specifications. The axiomatic semantics we consider model total correctness and map programs to continuous predicate transformers. We define the axiomatic semantics of GC and IC with elementary inductive predicates and show that the predicate transformer described by a program can be obtained compositionally by recursion on the syntax of the program using a fixed point operator for loops and continuations. We also show that two IC programs are contextually equivalent if and only if their predicate transformers are equivalent. 
id94#PD Tracking for a Class of Underactuated Robotic Systems with Kinetic Symmetry#In this letter, we study stability properties of Proportional-Derivative (PD) controlled underactuated robotic systems for trajectory tracking applications. Stability of PD control laws for fully actuated systems is an established result, and we extend it for the class of underactuated robotic systems. We will first show some well known examples where PD tracking control laws do not yield tracking; some of which can even lead to instability. We will then show that for a subclass of robotic systems, PD tracking control laws, indeed, yield desirable tracking guarantees. We will show that for a specified time interval, and for sufficiently large enough PD gains (input saturations permitting), local boundedness of the tracking error can be guaranteed. In addition, for a class of systems with the kinetic symmetry property, stronger conditions like convergence to desirable bounds can be guaranteed. This class is not restrictive and includes robots like the acrobot, the cart-pole, and the inertia-wheel pendulums. Towards the end, we will provide necessary simulation results in support of the theoretical guarantees presented. 
id95#Modernization of the Second Normal Form and Boyce-Codd Normal Form for Relational Theory#The task of designing relational databases has always been the subject of scientific research, as it is associated with a number of interrelated steps. The result of each step is the development of models for presenting the future database with further refinement and, finally, the creation of an adequate relational database as a set of relations with the corresponding links between them. The article focuses on the normalization of databases, as one of the steps to create a datalogical model, namely, the use of the first three Normal Forms. As a result of the analysis carried out in the article, it was concluded that the definition of the Second Normal Form can be modernized and thus achieve two goals: to ensure the correct creation of potential primary keys and, thereafter, the correct external connections between relations, even before creating the data schema in a specific relational database. Moreover, to reconsider the necessity of applying the so-called “strengthened” or a higher version of the Third Normal Form, which speaks of mutual dependencies between key and non-key attributes. 
id96#A Survey on the Application of Fault Analysis on Lightweight Cryptography#With a growing number of connected devices the demand for secure communication increases steadily. Therefore, the National Institute of Standards and Technology started the Lightweight Cryptography competition for authenticated encryption with associated data and hash applications. Its goal is to identify small, power efficient and secure cryptographic primitives. The 32 second round candidates of the Lightweight Cryptography competition are classified into three categories according to their architecture, i.e. block, sponge, or stream based ciphers. Fault Analysis is an approach to perform implementation level attacks and is composed of a broad spectrum of attack strategies. These strategies are classified into analytical and statistical approaches. In the following we provide an overview of state-of-The-Art fault attacks and their application. We will then categorize Fault Analysis strategies by their application on block, sponge, or stream based ciphers. This gives an idea of the applicability of different kinds of Fault Analysis on the second round candidates of the Lightweight Cryptography competition. 
id97#Deep learning computer vision for the separation of Cast- and Wrought-Aluminum scrap#In consequence of the electrification and the increased adoption of lightweight structures in the automotive industry, global demand for wrought Aluminum (Al) is expected to rise while demand for cast Al will stagnate. Since cast alloys can only be converted to wrought alloys by energy-intensive processes, the most promising strategy to avoid the emergence of excess Al cast alloys scrap is to sort cast from wrought alloys. To date, the separation of complex mixes of non-ferrous metals often implies the use of either or both sink-float techniques and/or X-ray fluorescence (XRF) based sorting. Therefore, the presented research develops an efficient method to classify cast and wrought (C&W) alloys in a real-time system with a conveyor belt using transfer learning methods, such as fine-tuning and feature extraction. Five CNNs are evaluated to classify C&W alloys using colour and depth images and transfer learning methods. In addition, the early fusion and late fusion of colour and depth images of C&W Al are investigated. For early fusion, data is added as an extra input channel to the first convolution layer of the CNN, and for later fusion, the images are fed in two separate subnetworks with the same architecture, where the parameters of the fully-connected layers are concatenated in both subnetworks. Our approach shows that late fusion CNN DenseNet allows obtaining the best performances and can achieve up to 98% accuracy. 
id98#To Share or Not to Share? Performance Guarantees and the Asymmetric Nature of Cross-Robot Experience Transfer#In the robotics literature, experience transfer has been proposed in different learning-based control frameworks to minimize the costs and risks associated with training robots. While various works have shown the feasibility of transferring prior experience from a source robot to improve or accelerate the learning of a target robot, there are usually no guarantees that experience transfer improves the performance of the target robot. In practice, the efficacy of transferring experience is often not known until it is tested on physical robots. This trial-and-error approach can be extremely unsafe and inefficient. Building on our previous work, in this letter we consider an inverse module transfer learning framework, where the inverse module of a source robot system is transferred to a target robot system to improve its tracking performance on arbitrary trajectories. We derive a theoretical bound on the tracking error when a source inverse module is transferred to the target robot and propose a Bayesian-optimization-based algorithm to estimate this bound from data. We further highlight the asymmetric nature of cross-robot experience transfer that has often been neglected in the literature. We demonstrate our approach in quadrotor experiments and show that we can guarantee positive transfer on the target robot for tracking random periodic trajectories. 
id99#An end-to-end framework for unconstrained monocular 3D hand pose estimation#This work addresses the challenging problem of unconstrained 3D hand pose estimation using monocular RGB images. Most of the existing approaches assume some prior knowledge of hand (such as hand locations and side information) is available for 3D hand pose estimation. This restricts their use in unconstrained environments. Therefore, we present an end-to-end framework that robustly predicts hand prior information and accurately infers 3D hand pose by learning ConvNet models while only using keypoint annotations. To enhance the hand detector's robustness, we propose a novel keypoint-based method to simultaneously predict hand regions and side labels, unlike existing methods that suffer from background color confusion caused by using segmentation or detection-based technology. Moreover, inspired by the human hand's biological structure, we introduce two geometric constraints directly into the 3D coordinates prediction that further improves its performance. Experimental results show that our proposed framework outperforms the state-of-art methods on standard benchmark datasets while providing robust predictions. 
id100#Non-contact vibration sensor using deep learning and image processing#This paper proposes a non-contact vibration measurement method based on deep learning and image processing. The deep learning method is used to realize the automatic and efficient selection of effective pixels and the optical flow method is used to extract vibration signals to realize non-contact and targetless visual vibration measurement. In this study, a carbon plate board and aluminum C-beam structure were measured and verified under artificial and non-human excitation in a laboratory environment. Additionally, bridge and cable structures in an outdoor environment were selected as measurement targets to verify the reliability of the proposed method. This paper compares the experimental results of Canny and Sobel edge detection algorithms and deep learning methods to verify the efficiency of deep learning. The results demonstrate that our method is robust, even under real-world unfavorable conditions, meaning it can serve as a novel measurement method in the field of vibration measurement. 
id102#Relational Database Watermarking for Data Tracing#Data should be distributed to clients from a centralized database. But distribution may result in a number of problems such as unauthorized copies and alteration of data. As an important branch of information hiding technology, digital watermarking technology is an effective method to realize the protection of copyright of relational database. However, the vast majority of relational digital watermarking solutions have the problem of limited use times of watermarks. In this paper, we propose a watermark scheme based on clustering, along with watermark embedding algorithm, watermark detection algorithm, and data recovery algorithm. By controlling the relevant parameters, we can verify the watermark for several times. We also elaborated how to use this scheme for database traceability. In the end of this paper, the calculation performance experiment of the algorithm shows that the scheme has good feasibility; the anti-Attack ability comparison experiment shows that the scheme has good robustness. 
id103#Vizgen: Accelerating visual computing prototypes in dynamic languages#This paper introduces a novel domain-specific compiler, which translates visual computing programs written in dynamic languages to highly efficient code. We define “dynamic” languages as those such as Python and MATLAB, which feature dynamic typing and flexible array operations. Such language features can be useful for rapid prototyping, however, the dynamic computation model introduces significant overheads in program execution time. We introduce a compiler framework for accelerating visual computing programs, such as graphics and vision programs, written in general-purpose dynamic languages. Our compiler allows substantial performance gains (frequently orders of magnitude) over general compilers for dynamic languages by specializing the compiler for visual computation. Specifically, our compiler takes advantage of three key properties of visual computing programs, which permit optimizations: (1) many array data structures have small, constant, or bounded size, (2) many operations on visual data are supported in hardware or are embarrassingly parallel, and (3) humans are not sensitive to small numerical errors in visual outputs due to changing floating-point precisions. Our compiler integrates program transformations that have been described previously, and improves existing transformations to handle visual programs that perform complicated array computations. In particular, we show that dependent type analysis can be used to infer sizes and guide optimizations for many small-sized array operations that arise in visual programs. Programmers who are not experts on visual computation can use our compiler to produce more efficient Python programs than if they write manually parallelized C, with fewer lines of application logic. 
id105#Hardware scripting in gel#Gel is a hardware description language that enables quick scripting of high level designs and can be easily extended to new design patterns. It is expression oriented and extremely succinct. Modules are described as functions and composed through function calls. Types and bit widths are inferred automatically to guarantee correctness. Together these features reduce hardware development time, allowing complex designs to be scripted quickly. A simulator and logic analyzer are available to help in the development process. A compiler has been developed that translates Gel to Verilog, and a number of applications have been demonstrated. This paper introduces the core language, demonstrates its extensibility, and shows how design patterns can easily be created. Finally, we compare a few applications written in Gel against equivalents written in Verilog. 
id106#Implementation of the NewHope Protocol for Post-Quantum Cryptography#In this paper, we propose a faster and less resource intensive implementation of the NewHope protocol to address the problems of resource-intensive random number generators and slow running NTT modules in NewHope implementations. In the random number generation module, choose the lightweight pseudo-random number generator (PRNG) Trivium instead of the SHAKE function to reduce the resource usage in the random number generation module. In the NTT transformation module, a pipeline structure is used in combination with a parallel structure of four butterfly units to shorten the cycle time of butterfly operations and achieve acceleration. The experimental results show that, compared with Kuo's design, the overall time for key exchange in this paper is reduced by 25.3%, and the consumption of FFs and LUTs is reduced by 20.2% and 38.0%, respectively. 
id108#Authentication strategies in vehicular communications: a taxonomy and framework#In intelligent vehicular networks, vehicles have enhanced sensing capabilities and carry computing and communication platforms to enable new versatile systems known as Vehicular Communication (VC) systems. Vehicles communicate with other vehicles and with nearby fixed equipment to support different applications, including those which increase driver awareness of the surroundings. This should result in improved safety and may optimize traffic. However, VC systems are vulnerable to cyber attacks involving message manipulation. Research aimed at tackling this problem has resulted in the proposal of multiple authentication protocols. Several existing survey papers have attempted to classify some of these protocols based on a limited set of characteristics. However, to date there is no generic framework to support the comparison of these protocols and provide guidance for design and evaluation. Most existing classifications either use computation complexity of cryptographic techniques as a criterion, or they fail to make connections between different important aspects of authentication. This paper provides such a framework, proposing a new taxonomy to enable a consistent means of classifying authentication schemes based upon seven main criteria. The main contribution of this study is a framework to enable protocol designers and investigators to adequately compare and select authentication schemes when deciding on particular protocols to implement in an application. Our framework can be applied in design, making choices appropriate for the intended context in both intra-vehicle and inter-vehicle communications. We demonstrate the application of our framework using two different types of case study: individual analysis and hypothetical design. Additionally, this work makes several related contributions. We present the network model, outline the applications, list the communication patterns and the underlying standards, and discuss the necessity of using cryptography and key management in VC systems. We also review the threats, authentication, and privacy requirements in vehicular networks. 
id109#Vibration investigation for telecom structures with smartphone camera: case studies#Telecom structures such as high guyed masts are tall and flexible, so that not only the main structure but also the components (i.e., guy cable and antenna) suffer from vibrations induced by wind or earthquakes. The installation of contact inertial accelerometers for high guyed mast cables or antenna can be logistically challenging and the original vibration state may be influenced by these sensors. With convenient implementation and acceptable accuracy, computer vision technologies have been applied for vibration tests both in labs and field. In this paper, videos taken with smartphone cameras are processed to extract guy cable and antenna vibration information for telecom structures and, as a result, providing an efficient cost-effective method for vibration investigation of this type of structure. The video processing method can also be used in similar cases for other structures. 
id110#Bilateral Teleoperation with Adaptive Impedance Control for Contact Tasks#This letter presents an adaptive impedance control architecture for robotic teleoperation of contact tasks featuring continuous interaction with the environment. We use Learning from Demonstration (LfD) as a framework to learn variable stiffness control policies. Then, the learnt state-varying stiffness is used to command the remote manipulator, so as to adapt its interaction with the environment based on the sensed forces. Our system only relies on the on-board torque sensors of a commercial robotic manipulator and it does not require any additional hardware or user input for the estimation of the required stiffness. We also provide a passivity analysis of our system, where the concept of energy tanks is used to guarantee a stable behavior. Finally, the system is evaluated in a representative teleoperated cutting application. Results show that the proposed variable-stiffness approach outperforms two standard constant-stiffness approaches in terms of safety and robot tracking performance. 
id111#Design and Implementation of an Interactive Information System for University Education under the Cloud Service Model#This paper designs and implements an interactive information system for university education under the cloud service model. The Ruby-based Rails framework and Apana Studio integration environment are used to develop the system, and the object-relational database Posture SQL is chosen to manage structured data and support multiple types of Value. With two levels of global and local authentication, user information is synchronized and interconnected between homogeneous clouds, enabling identity management capabilities. The overall framework of the system is built with the model-view-control architecture pattern and RESTful architecture style. After testing, the platform is a comprehensive teaching and learning system that can perform the functions of creating new courses, managing assignments, initiating discussions, stage evaluation, etc. The visual interface is easy to operate, which can make the traditional education mode more diversified, and can also refer to the operation mode of Mujiao class. After the system and module testing, the test results show that it meets the design requirements of the platform and module. 
id112#Building subject domain ontology for a corporate web application#The technology of automated construction of the subject domain ontology, based on information extracted from the comments of the TATNEFT oil company relational databases, is considered. The technology is based on building a converter (compiler) translating the logical data model of Epicenter Petrotechnical Open Software Corporation (POSC), presented in the form of ER diagrams and a set of the EXPRESS object-oriented language descriptions, into the OWL ontology description language, recommended by the W3C consortium. The basic syntactic and semantic aspects of the transformation are described. Copyright 
id113#An automated heterogeneous robotic system for radiation surveys: Design and field testing#During missions involving radiation exposure, unmanned robotic platforms may embody a valuable tool, especially thanks to their capability of replacing human operators in certain tasks to eliminate the health risks associated with such an environment. Moreover, rapid development of the technology allows us to increase the automation rate, making the human operator generally less important within the entire process. This article presents a multirobotic system designed for highly automated radiation mapping and source localization. Our approach includes a three-phase procedure comprising sequential deployment of two diverse platforms, namely, an unmanned aircraft system (UAS) and an unmanned ground vehicle (UGV), to perform aerial photogrammetry, aerial radiation mapping, and terrestrial radiation mapping. The central idea is to produce a sparse dose rate map of the entire study site via the UAS and, subsequently, to perform detailed UGV-based mapping in limited radiation-contaminated regions. To accomplish these tasks, we designed numerous methods and data processing algorithms to facilitate, for example, digital elevation model-based terrain following for the UAS, automatic selection of the regions of interest, obstacle map-based UGV trajectory planning, and source localization. The overall usability of the multirobotic system was demonstrated by means of a 1-day, authentic experiment, namely, a fictitious car accident including the loss of several radiation sources. The ability of the system to localize radiation hotspots and individual sources has been verified. 
id116#SRPS-deep-learning-based photometric stereo using superresolution images#This paper introduces a novel deep-learning-based photometric stereo method that uses superresolution (SR) images: SR photometric stereo. Recent deep-learning-based SR algorithms have yielded great results in terms of enlarging images without mosaic effects. Supposing that the SR algorithms successfully enhance the feature and colour information of original images, implementing SR images using the photometric stereo method facilitates the use of considerably more information on the object than existing photometric stereo methods. We built a novel deep-learning-based network for the photometric stereo technique to optimize the input-output of SR image inputs and normal map outputs. We tested our network using the most widely used benchmark dataset and obtained better results than existing photometric stereo methods. 
id117#Fine-Grained Cryptography Revisited#Fine-grained cryptographic primitives are secure against adversaries with bounded resources and can be computed by honest users with less resources than the adversaries. In this paper, we revisit the results by Degwekar, Vaikuntanathan, and Vasudevan in Crypto 2016 on fine-grained cryptography and show constructions of three key fundamental fine-grained cryptographic primitives: one-way permutation families, hash proof systems (which in turn implies a public-key encryption scheme against chosen chiphertext attacks), and trapdoor one-way functions. All of our constructions are computable in NC1 and secure against (non-uniform) NC1 circuits under the widely believed worst-case assumption NC1⊊ ⊕ L/ poly. 
id118#Horizontal review on video surveillance for smart cities: Edge devices, applications, datasets, and future trends#The automation strategy of today’s smart cities relies on large IoT (internet of Things) systems that collect big data analytics to gain insights. Although there have been recent reviews in this field, there is a remarkable gap that addresses four sides of the problem. Namely, the application of video surveillance in smart cities, algorithms, datasets, and embedded systems. In this paper, we discuss the latest datasets used, the algorithms used, and the recent advances in embedded systems to form edge vision computing are introduced. Moreover, future trends and challenges are addressed. 
id119#Measurement Method for Helicopter Rotor Motion Parameters#Objetive:To realize real-time measurement and monitoring of helicopter motion state, based on fully considering the rotor measurement environment, power supply, information storage, processing, and information transmission, a set of rotor motion parameter test system based on vision is designed. The system adopts a non-contact measurement method, which can realize the real-time measurement and monitoring of the rotor state. Method: First, the overall design of the rotor motion state test scheme is given, and the working principle and flow chart are provided. Then combined with the actual measurement environment, it presents the concrete method of camera calibration and data processing. Result: The measurement accuracy under this type of selection and layout is evaluated. After satisfying the design accuracy, the test environment is set up in the laboratory, and the measurement accuracy is assessed through the actual test. Experimental results indicate that the measurement accuracy of the system is better than 1cm for the rotor of 8m, when the measurement baseline is 350mm, and the image recognition is better than 0.5 pixels. Conclusion: It can satisfy the system requirements of online, state accuracy measurement. 
id120#Gamification of LR algorithm: Engaging students by playing in compiler principle course#"Theoretical knowledge of the compiler principle courses is often abstract and uninteresting, which may easily make students lose their confidence and interest in learning such important computer science courses. To solve this problem, we introduce the concept of gamification into the compiler principle course for teaching aids, and design the card game called Into the Stack for the LR parsing algorithm, an important theory that must be mastered by students. When playing the game, players think over the quickest way to reduce their hands, which is based on the ""shift-reduce""actions in LR parsing. Simulating such core actions of LR algorithm in this game, the whole abstract theory and algorithm has been demonstrated in a way of entertainment. It would help learners overcome their fear of theoretical knowledge, stimulate their interest in learning theories and enhance their ability of applying algorithms. Applied in realistic compiler principle courses, this game and such teaching method have achieved satisfactory teaching results. "
id121#Passive reconfigurable end effector for underwater simulation on humanoids#The underwater environment uses to be a harsh place to test aquatic robots just after analyze their performance with computational simulations. Apart from the necessary tasks, the robot has to perform additional algorithms in its first tests, like handling with drag or buoyancy forces. A mechanical simulator of the underwater environment is proposed to avoid this gap between virtual simulations and experiments in submerged spaces. This simulator is based on a passive reconfigurable mechanism attached to the robot's trunk or a human body for training before immersion. This mechanism acts as a wearable end effector of a cable-driven robot. This configuration allows to exert desired wrenches in two different links, and it is suitable for humanoid bodies. These forces focus on simulating the underwater force distributed throughout the trunk by exerting specific forces in the hip and the chest of a humanoid robot or a scuba diver. The elastic behavior of different compliant mechanisms related to this mechanism is analyzed. Theoretical simulations and real experiments are developed. 
id122#C to D-Wave: A high-level C compilation framework for quantum annealers#A quantum annealer solves optimization problems by exploiting quantum effects. Problems are represented as Hamiltonian functions that define an energy landscape. The quantum-annealing hardware relaxes to a solution corresponding to the ground state of the energy landscape. Expressing arbitrary programming problems in terms of real-valued Hamiltonian-function coefficients is unintuitive and challenging. This paper addresses the difficulty of programming quantum annealers by presenting a compilation framework that compiles a subset of C code to a quantum machine instruction (QMI) to be executed on a quantum annealer. Our work is based on a modular software stack that facilitates programming D-Wave quantum annealers by successively lowering code from C to Verilog to a symbolic 'quantum macro assembly language' and finally to a device-specific Hamiltonian function. We demonstrate the capabilities of our software stack on a set of problems written in C and executed on a D-Wave 2000Q quantum annealer. 
id123#Automatic mapping of C to FPGAs with the DEFACTO compilation and synthesis system#The DEFACTO compilation and synthesis system is capable of automatically mapping computations expressed in high-level imperative programming languages as C to FPGA-based systems. DEFACTO combines parallelizing compiler technology with behavioral VHDI, synthesis tools to guide the application of high-level compiler transformations in the search of high-quality hardware designs. In this article we illustrate the effectiveness of this approach in automatically mapping several kernel codes to an FPGA quickly and correctly. We also present a detailed example of the comparison of the performance of an automatically generated design against a manually generated implementation of the same computation. The design-space-exploration component of DEFACTO is able to explore a large number of designs for a particular computation that would otherwise be impractical for any designers. 
id124#RESEARCH on MULTI-SOURCE SATELLITE IMAGE DATABASE MANAGEMENT SYSTEM#For the exploration and analysis of electricity, it is necessary to continuously acquire multi-star source, multi-temporal, multi-level remote sensing images for analysis and interpretation. Since the overall data has a variety of features, a data structure for multi-sensor data storage is proposed. On the basis of solving key technologies such as real-time image processing and analysis and remote sensing image normalization processing, the .xml file and remote sensing data geographic information file are used to realize effective organization between remote sensing data and remote sensing data. Based on GDAL design relational database, the formation of a relatively complete management system of data management, shared publishing and application services will maximize the potential value of remote sensing images in electricity remote sensing. 
id126#A sensitive dynamic mutual encryption system based on a new 1D chaotic map#In this research paper, we propose a novel one-dimensional chaotic system based on the fraction of cosine over sine (1-DFCS). Evaluation of 1-DFCS indicates the existence of a complex chaotic behavior, an infinite chaotic range, and a high initial state sensitivity. We further propose a new sensitive dynamic mutual image encryption scheme (SDME) using 1-DFCS. SDME is designed with a dynamic diffusion and confusion, which gives a unique encryption process for each image. To ensure the dynamicity, SDME is enriched with a new proposed plain image sensitivity function (PISF). PISF uses the 1-DFCS, plain image, and secret key, which makes it unpredictable and sensitive to a tiny change. PISF is recommended to many image encryption schemes to raise their ability toward the differential attacks. Besides, we propose a mutual part encryption technique. This technique refers to two parties encrypting each other, where a small change in one part reflects the other part. These three characteristics make our scheme able to achieve a high-security level in a single round, which provides a combination of security and time efficiency. The experiment results prove that SDME provides better performance when compared with several state-of-the-art image encryption systems. 
id127#MAGIC: Machine-Learning-Guided Image Compression for Vision Applications in Internet of Things#The emergent ecosystems of intelligent edge devices in diverse Internet-of-Things (IoT) applications, from automatic surveillance to precision agriculture, increasingly rely on recording and processing a variety of image data. Due to resource constraints, e.g., energy and communication bandwidth requirements, these applications require compressing the recorded images before transmission. For these applications, image compression commonly requires: 1) maintaining features for coarse-grain pattern recognition instead of the high-level details for human perception due to machine-to-machine communications; 2) high compression ratio that leads to improved energy and transmission efficiency; and 3) large dynamic range of compression and an easy tradeoff between compression factor and quality of reconstruction to accommodate a wide diversity of IoT applications as well as their time-varying energy/performance needs. To address these requirements, we propose, MAGIC, a novel machine learning (ML)-guided image compression framework that judiciously sacrifices the visual quality to achieve much higher compression when compared to traditional techniques, while maintaining accuracy for coarse-grained vision tasks. The central idea is to capture application-specific domain knowledge and efficiently utilize it in achieving high compression. We demonstrate that the MAGIC framework is configurable across a wide range of compression/quality and is capable of compressing beyond the standard quality factor limits of both JPEG 2000 and WebP. We perform experiments on representative IoT applications using two vision data sets and show $42.65\times $ compression at similar accuracy with respect to the source. We highlight low variance in compression rate across images using our technique as compared to JPEG 2000 and WebP. 
id128#Adaptive Control of Soft Robots Based on an Enhanced 3D Augmented Rigid Robot Matching#Despite having proven successful in generating precise motions under dynamic conditions in highly deformable soft-bodied robots, model based techniques are also prone to robustness issues connected to the intrinsic uncertain nature of the dynamics of these systems. This letter aims at tackling this challenge, by extending the augmented rigid robot formulation to a stable representation of three dimensional motions of soft robots, under Piecewise Constant Curvature hypothesis. In turn, the equivalence between soft-bodied and rigid robots permits to derive effective adaptive controllers for soft-bodied robots, achieving perfect posture regulation under considerable errors in the knowledge of system parameters. The effectiveness of the proposed control design is demonstrated through extensive simulations. 
id129#Cellular Automata based Cryptography Model for Reliable Encryption Using State Transition in Wireless Network Optimizing Data Security#Authors propose a hardware based approach for reliable encryption in wireless network using Cellular Automata (CA). Distinct layers of encryption have been utilized in proposed model for enhancing data security. Transmitter and receiver modules are designed for performing state transition based encrypted activities. Transmitter module is required for capturing environmental turbulences (noise) from a specific geographical location and send encrypted signal to receiver module through wireless network. Receiver module analyses received signal after decryption for desired activities based on real time situations. Different hardware components are selected and compared based on market study targeting efficient construction of proposed modules. Time complexity and code break complexity are measured using different key lengths, keysets, and data sizes for proposed CA based encryption scheme achieving unique results in hardware based cryptography system. Encryption key length, index, and keyset are required for computing code break complexity in design model. Large data size is effectively managed to calculate protection ratio and reliability factor for realizing efficient protectiveness and accuracy. Comparative study between existing encryption technique and proposed CA based encryption technique is established based on time complexity and code break complexity. 
id131#Slip-Based Autonomous ZUPT through Gaussian Process to Improve Planetary Rover Localization#The zero-velocity update (ZUPT) algorithm provides valuable state information to maintain the inertial navigation system (INS) reliability when stationary conditions are satisfied. Employing ZUPT along with leveraging non-holonomic constraints can greatly benefit wheeled mobile robot dead-reckoning localization accuracy. However, determining how often they should be employed requires consideration to balance localization accuracy and traversal rate for planetary rovers. To address this, we investigate when to autonomously initiate stops to improve wheel-inertial odometry (WIO) localization performance with ZUPT. To do this, we propose a 3D dead-reckoning approach that predicts wheel slippage while the rover is in motion and forecasts the appropriate time to stop without changing any rover hardware or major rover operations. We validate with field tests that our approach is viable on different terrain types and achieves a 3D localization accuracy of ∼97 bf% over 650 m drives on rough terrain. 
id132#Compiler management of communication and parallelism for quantum computation#Quantum computing (QC) offers huge promise to accelerate a range of computationally intensive benchmarks. Quantum computing is limited, however, by the challenges of decoherence: i.e., a quantum state can only be maintained for short windows of time before it decoheres. While quantum error correction codes can protect against decoherence, fast execution time is the best defense against decoherence, so efficient architectures and effective scheduling algorithms are necessary. This paper proposes the Multi-SIMD QC architecture and then proposes and evaluates effective schedulers to map benchmark descriptions onto Multi-SIMD architectures. The Multi-SIMD model consists of a small number of SIMD regions, each of which may support operations on up to thousands of qubits per cycle. Efficient Multi-SIMD operation requires efficient scheduling. This work develops schedulers to reduce communication requirements of qubits between operating regions, while also improving parallelism. We find that communication to global memory is a dominant cost in QC. We also note that many quantum benchmarks have long serial operation paths (although each operation may be data parallel). To exploit this characteristic, we introduce Longest-Path-First Scheduling (LPFS) which pins operations to SIMD regions to keep data in-place and reduce communication to memory. The use of small, local scratchpad memories also further reduces communication. Our results show a 3% to 308% improvement for LPFS over conventional scheduling algorithms, and an additional 3% to 64% improvement using scratchpad memories. Our work is the most comprehensive software-to-quantum toolflow published to date, with efficient and practical scheduling techniques that reduce communication and increase parallelism for full-scale quantum code executing up to a trillion quantum gate operations. Copyright 
id133#Construction of a SSA-based queue compiler#Queue processors are a non-conventional computer architecture that uses a first-in firstout queue to execute programs. Such computing model allows queue programs to be compact and unrestricted in terms of parallelism. These characteristics are promising for the design of high-performance processors for modern applications where low power consumption, high computation speed, and small memory footprint are required. This paper presents the design and development principles of a queue compiler that employs static single assignment (SSA) as intermediate representation. In SSA form, all assignments to variables are done to distinct variables, therefore each variable is assigned only once. We show that for a queue compiler, this form not only facilitates compiler optimizations but it matches the fundamental driving principle of queue computing: single assignment of variables. Therefore, a SSA-based queue compiler offers a solid infrastructure for the generation of optimized queue code. We present the methods to build a queue compiler that takes advantage of SSA form to extract parallelism and improve the quality of the compiled programs. Furthermore, we discuss how our infrastructure can be extended with detailed pointer and data dependency information. Our experimental results show that a SSA-based queue compiler is able to generate better programs than a non-SSA queue compiler.
id134#Informational and Analytical System for Diagnostics of the Electric Power Equipment Condition#The presented work is devoted to the decision of a problem of increase of operational reliability of the electric systems equipment, by developing information-analytical system for diagnosing a condition of the electric power equipment. To solve this problem, author's methods of diagnostics have been created and software implemented, which, unlike existing analogues, allow to detect equipment defects at an early stage of their development, to determine the maximum permissible and boundary values of equipment isolation indicators, to recognize the predicted defect type. The description of the structure and principles of database organization in the information-analytical system is given and its functional capabilities are analysed. 
id135#Gas Pipeline Inspection Using Autonomous Robots with Omni-Directional Cameras#Maintenance of pipelines is a major concern for oil and gas companies and millions of dollars are spent on preventive inspections every year. The type of equipment used in carrying out inspections is expensive and very sophisticated requiring very specialized manpower. Here, a novel approach to pipeline inspection is presented using a small mobile robot and a catadioptric omni-directional vision system. The system captures panoramic videos using conical and/or hyperbolic mirrors, which are transmitted to a monitoring station to examine the inner surface of gas pipelines. The videos are prepped, unwrapped, and un-warped to obtain a realistic flat image of the whole inner surface of the pipeline. The Simulated Annealing optimization method is implemented to optimize the system parameters using different mirror shapes and unwrapping and un-warping techniques. Simulations and experimental studies using a mobile robot equipped with a CCD camera and a conical mirror are presented here to demonstrate the viability of this inspection technique. The developed robotic system can be used to visually pre-scan pipelines before using much more expensive tools such as magnetic flux leakage (MFL) or ultrasound (UT) based inspection systems. These latter tools cost tens of thousands of dollars per hour and will be used only after suspicious areas are visually detected. This will reduce the inspection time and cost significantly. 
id136#R-PMA: Rapid Efficient Statistical Reporting on Heterogeneous Resources#In contemporary organizations, different units of the corporation use a diverse system to produce, save, and search their important data. However, these enterprises synchronize the diversified data of these different systems to understand the full value of the data. Data federation is an approach to integrate the data of heterogeneous data sources. R-PMA is a web-based reporting tool, that federates the heterogeneous data sources e.g. RDF, Relational Databases, and text data in different formats. R-PMA imports all features of datasets and builds interactive reports, based on the imported data resources. It can interact with sparkR for statistical analysis of large datasets. R-PMA also provides an SQL interface to query SparkSQL and MySQL for procuring the required information for reporting. Finally, we have developed it as a plugin in PHPMy Admin to facilitate the organizations that have been using rational databases for years. 
id137#Using data compression for increasing memory system utilization#The memory system presents one of the critical challenges in embedded system design and optimization. This is mainly due to the ever-increasing code complexity of embedded applications and the exponential increase seen in the amount of data they manipulate. The memory bottleneck is even more important for multiprocessor-system-on-a-chip (MPSoC) architectures due to the high cost of off-chip memory accesses in terms of both energy and performance. As a result, reducing the memory-space occupancy of embedded applications is very important and will be even more important in the next decade. While it is true that the on-chip memory capacity of embedded systems is continuously increasing, the increases in the complexity of embedded applications and the sizes of the data sets they process are far greater. Motivated by this observation, this paper presents and evaluates a compiler-driven approach to data compression for reducing memory-space occupancy. Our goal is to study how automated compiler support can help in deciding the set of data elements to compress/ decompress and the points during execution at which these compressions/decompressions should be performed. We first study this problem in the context of single-core systems and then extend it to MPSoCs where we schedule compressions and decompressions intelligently such that they do not conflict with application execution as much as possible. Particularly, in MPSoCs, one needs to decide which processors should participate in the compression and decompression activities at any given point during the course of execution. We propose both static and dynamic algorithms for this purpose. In the static scheme, the processors are divided into two groups: those performing compression/ decompression and those executing the application, and this grouping is maintained throughout the execution of the application. In the dynamic scheme, on the other hand, the execution starts with some grouping but this grouping can change during the course of execution, depending on the dynamic variations in the data access pattern. Our experimental results show that, in a single-core system, the proposed approach reduces maximum memory occupancy by 47.9% and average memory occupancy by 48.3% when averaged over all the benchmarks. Our results also indicate that, in an MPSoC, the average energy saving is 12.7% when all eight benchmarks are considered. While compressions and decompressions and related bookkeeping activities take extra cycles and memory space and consume additional energy, we found that the improvements they bring from the memory space, execution cycles, and energy perspectives are much higher than these overheads. 
id138#An integrated environmental monitoring approach through the development of coal mine, a GIS open source application#Coal related fires may occur in un-mined outcrops, during coal mining, in abandoned mines, during storage and transportation and in coal waste deposits. The self-burning of coal mobilizes large amounts of pollutants, for instance, particulate matter, organic compounds and toxic trace elements that can be emitted, released or leached to soils, waters and air of the surrounding environment. The S. Pedro da Cova (Porto, Portugal) coal mine was exploited between 1795 and 1972 and had an important role on the economic development of the region. Nowadays a waste pile of about 28,000 m2 is still deposited in the mine, suffering from selfcombustion since 2005. Geographical Information System (GIS) and spatial databases are frequently used for monitoring this type of processes. The main objective of this work was to integrate, manipulate and combine the spatial information obtained in the field with other datasets (geospatial and alphanumerical) in a GIS open source application connected to a relational database (PostGIS), in order to monitor and assess environmental conditions in the S. Pedro da Cova coal mine. This is an ongoing project where some campaigns were conducted and some spatial information was obtained (thermal images, Digital Elevation Model) and also water and soil samples. 
id139#Learning Humanoid Robot Running Motions with Symmetry Incentive through Proximal Policy Optimization#This article contributes with a methodology based on deep reinforcement learning to develop running skills in a humanoid robot with no prior knowledge. Specifically, the algorithm used for learning is the Proximal Policy Optimization (PPO). The chosen application domain is the RoboCup 3D Soccer Simulation (Soccer 3D), a competition where teams composed by 11 autonomous agents each compete in simulated soccer matches. In our approach, the state vector used as the neural network’s input consists of raw sensor measurements or quantities which could be obtained through sensor fusion, while the actions are the joint positions, which are sent to joint controllers. Our running behavior outperforms the state-of-the-art in terms of sprint speed by approximately 50%. We present results regarding the training procedure and also evaluate the controllers in terms of speed, reliability, and human similarity. Since the running policies with top speed display asymmetric motions, we also investigate a technique to encourage symmetry in the sagittal plane. Finally, we discuss key factors that lead us to surpass previous results in the literature and share some ideas for future research. 
id140#Regional pet database: An information system to guarantee pet-population control#An epidemiological approach to the study of pet population is needed to ensure the application of effective public health policies, addressed in particular to the management of monitoring plans and veterinary controls in the urban environments, where the human-pet interaction is closer. Since 1993, a regional database for pet-animals (dogs, cats and ferrets), called BAC, has been implemented in North-Eastern Italy (Veneto Region), involving both private and public Veterinary Services of the Local Public Health Units (LPHUs). BAC has been developed using the Oracle RDBMS platform with two applications ANACANI and ACWEB. The information system includes data about owners and/or keepers, animals, their movements and events like vaccinations, neutering and dogs aggressions/bites. Data are processed and organized in reports useful to plan stray dog and cat population control strategies. Moreover, public Veterinary Services can use the system to monitor some social phenomena like owned-dog aggression and bites or pet abandon. Copyright 
id141#A decoupled local memory allocator#"Compilers use software-controlled local memories to provide fast, predictable, and power-efficient access to critical data. We show that the local memory allocation for straight-line, or linearized programs is equivalent to a weighted interval-graph coloring problem. This problem is new when allowing a color interval to ""wrap around,"" and we call it the submarine-building problem. This graph-theoretical decision problem differs slightly from the classical ship-building problem, and exhibits very interesting and unusual complexity properties. We demonstrate that the submarine-building problem is NP-complete, while it is solvable in linear time for not-so-proper interval graphs, an extension of the the class of proper interval graphs. We propose a clustering heuristic to approximate any interval graph into a not-so-proper interval graph, decoupling spill code generation from local memory assignment. We apply this heuristic to a large number of randomly generated interval graphs reproducing the statistical features of standard local memory allocation benchmarks, comparing with state-of-the-art heuristics. "
id142#Research on heterogeneous data exchange technology based on shadow table#In the process of continuous development of enterprise informatization, with the expansion of old applications and the increasing of new applications, users will face the problem of data exchange between different hardware platforms, different network environments and different databases. Due to the coexistence of multiple application modes, the problems of data exchange between multiple systems are not standardized, network data sharing is not easy, data synchronization is not guaranteed, and data security is not guaranteed, which results in the phenomenon of “information island” and brings great inconvenience to users. The functions provided by traditional system software or tool software can not solve the problem of heterogeneous data exchange well. The research and implementation of a flexible, efficient, concise and transparent heterogeneous data exchange system is very necessary. This paper plans to study and implement a user configurable heterogeneous data exchange system which can shield complex interfaces between various data systems and has high security and reliability. 
id143#Rushmore: Securely displaying static and animated images using TrustZone#We present Rushmore, a system that securely displays static or animated images using TrustZone. The core functionality of Rushmore is to securely decrypt and display encrypted images (sent by a trusted party) on a mobile device. Although previous approaches have shown that it is possible to securely display encrypted images using TrustZone, they exhibit a critical limitation that significantly hampers the applicability of using TrustZone for display security. The limitation is that, when the trusted domain of TrustZone (the secure world) takes control of the display, the untrusted domain (the normal world) cannot display anything simultaneously. This limitation comes from the fact that previous approaches give the secure world exclusive access to the display hardware to preserve security. With Rushmore, we overcome this limitation by leveraging a well-known, yet overlooked hardware feature called an IPU (Image Processing Unit) that provides multiple display channels. By partitioning these channels across the normal world and the secure world, we enable the two worlds to simultaneously display pixels on the screen without sacrificing security. Furthermore, we show that with the right type of cryptographic method, we can decrypt and display encrypted animated images at 30 FPS or higher for medium-to-small images and at around 30 FPS for large images. One notable cryptographic method we adapt for Rushmore is visual cryptography, and we demonstrate that it is a light-weight alternative to other cryptographic methods for certain use cases. Our evaluation shows that in addition to providing usable frame rates, Rushmore incurs less than 5% overhead to the applications running in the normal world. 
id144#Comparative analysis of CNN and Viola-Jones for face mask detection#According to the World Health Organization, the Coronavirus (COVID-19) pandemic is causing a worldwide emergency, and one safe way to cover oneself is to wear masks. This pandemic constrained governments everywhere in the world to force lock-downs to avoid the transmission of infection. Reports show that wearing masks at work diminishes the danger of infection. We assemble our model by utilizing the concept of deep neural learning and AI. The dataset comprises pictures with masked faces and non-masked faces. Several computer algorithms are there for face detection. But this analysis centers around two of the most widely recognized procedures: The Viola-Jones algorithm and the Convolution Neural Networks. We will check whether the individual in the image/video wears a mask or not with a CV and Deep neural learning. Not only finding out about face mask detection, but this project also introduced the chance to delve into the field of computer algorithms. 
id145#A Recommendation System Design and Development for the best Tourist Attraction#The best tourist attractions is the most concerned issue of consumers for our choice with development of information technology and the application of big data technology have solved this demand of consumers. The best tourist attractions recommendation system developed by Internet platform, MySQL database, JAVA JSP technology, B/S design mode and other technologies are widely used in the selection and decision-making process of the best tourist attractions for the optimal Economy Recommendation System. 
id148#Enhanced quality monitoring during black tea processing by the fusion of NIRS and computer vision#Polyphenol and catechin are key components in black tea processing, contributing to both taste and color quality. However, the rapid detection methods that are applicable throughout the processing stages are lacking. Here, we explored the potential of miniature near-infrared spectroscopy and self-built computer vision. Fresh tea leaves, and the samples from withering, rolling, fermentation, and drying steps were collected for in-situ data acquisition in a tea factory. Data from two sensors were fused, competitive adaptive reweighted sampling and Pearson correlation analysis were employed to select effective variables from spectral and color variables, respectively. And the linear partial least squares (PLS) were used for modeling. The results showed that PLS models based on low-level data fusion could not effectively improve the prediction accuracies compared to single data. By contrast, middle-level data fusion achieved the best prediction accuracies for both polyphenol and catechin, with average root mean square error of prediction of 0.66 ± 0.12 and 1.06 ± 0.11 g/100 g, and residual prediction deviations of 5.41 ± 0.99 and 4.03 ± 0.38, respectively. Overall, this study demonstrated the enhanced predictive capability of fused spectral and imaging systems for polyphenols, overcoming the low predictive accuracy of single sensors. 
id151#A Compiler for Automatic Selection of Suitable Processing-in-Memory Instructions#Although not a new technique, due to the advent of 3D-stacked technologies, the integration of large memories and logic circuitry able to compute large amount of data has revived the Processing-in-Memory (PIM) techniques. PIM is a technique to increase performance while reducing energy consumption when dealing with large amounts of data. Despite several designs of PIM are available in the literature, their effective implementation still burdens the programmer. Also, various PIM instances are required to take advantage of the internal 3D-stacked memories, which further increases the challenges faced by the programmers. In this way, this work presents the Processing-In-Memory cOmpiler (PRIMO). Our compiler is able to efficiently exploit large vector units on a PIM architecture, directly from the original code. PRIMO is able to automatically select suitable PIM operations, allowing its automatic offloading. Moreover, PRIMO concerns about several PIM instances, selecting the most suitable instance while reduces internal communication between different PIM units. The compilation results of different benchmarks depict how PRIMO is able to exploit large vectors, while achieving a near-optimal performance when compared to the ideal execution for the case study PIM. PRIMO allows a speedup of 38× for specific kernels, while on average achieves 11.8 × for a set of benchmarks from PolyBench Suite. 
id153#Agriculture Resources for Plant-Leaf Disease Identification using Deep Learning Techniques#Agriculture is an essential food supply. In developing countries like India, agriculture provides farmers with large-scale livelihood opportunities. The recent advances in computer vision brought on by in-depth learning have paved the way for detecting and diagnosing plant diseases by using a camera to take pictures. This study is an important means of distinguishing different diseases in various plant species. The system has been developed to detect and classify many plant varieties, including apples, wheat, grapes, potatoes, sugar cane and tomatoes. The computer is also able to diagnose a host of herbal diseases. The experts were able to create profound learning models that identified and differentiated plant diseases and non-attention of ailments with 25000 images of infected sound plant leaves and disease. The model produced was 95,3 percent accurate, and the gadget was able to report the accuracy up to 100 percent to classify and differentiate between the plant variety and the types of diseases that were infected by the plant. 
id155#Hardware architecture for supersingular isogeny diffie-hellman and key encapsulation using a fast montgomery multiplier#Public key cryptography lies among the most important bases of security protocols. The classic instances of these cryptosystems are no longer secure when a large-scale quantum computer emerges. These cryptosystems must be replaced by post-quantum ones, such as isogeny-based cryptographic schemes. Supersingular isogeny Diffie-Hellman (SIDH) and key encapsulation (SIKE) are two of the most important such schemes. To improve the performance of these protocols, we have designed several modular multipliers. These multipliers have been implemented for all the prime fields used in SIKE round 3, on a Virtex-7 FPGA, showing a time and area-time product improvement of up to 60.1% and 64.5%, respectively. These multipliers are also suitable for applications such as RSA, as shown by implementations for 512-bit, 1024-bit, and 2048-bit generic moduli on a Virtex-7 FPGA. Our fastest multiplier has been used in the implementation of SIDH and SIKE round 3. Employing six instances of this multiplier, SIDH completes after 7.33, 8.93, 13.39, and 18.67 milliseconds and the encapsulation and the decapsulation of SIKE is performed in 7.13, 8.68, 13.08, and 18.16 milliseconds over p434, p503, p610, p751, respectively, which yields a least improvement factor of 1.23. 
id156#Verify a Valid Message in Single Tuple: A Watermarking Technique for Relational Database#The leakage of sensitive digital assets is a major problem which causes huge legal risk and economic loss. Service providers need to ensure data security for owners. Robust watermarking techniques play a critical role in ownership protection of relational databases. In this paper, we proposed a new Double-layer Ellipse Model called DEM that embeds a valid message in each candidates tuples. Each watermark can independently prove ownership. The main idea of DEM is to use watermarks itself to locate and make the most of contextual information to verify validity of watermarks. Under the framework of DEM, we propose a robust and semi-blind reversible watermarking scheme. Our scheme handles non-significant data for locating and embedding. The scheme generates watermark by exchanging data groups extracted from scattered attributes using Computation and Sort-Exchange step. Key information such as primary key, most significant bit (MSB) become a assistant feature for verifying the validity of embedded watermark. Our scheme can be applied on all type of numerical attributes (e.g. Integer, float, double, boolean). In robust experiments, the scheme is proved to be extremely resilient to insertion/detection/alteration attacks in both normal and hard situations. From a practical point of view, our solution is easy to implement and has good performance in statistics, incremental updates, adaptation. 
id158#Tracking and analysing social interactions in dairy cattle with real-time locating system and machine learning#There is a need for reliable and efficient methods for monitoring the activity and social behaviour in cows, in order to optimise management in modern dairy farms. This research presents an embedded system that could track individual cows using Ultra-wideband technology. At the same time, social interactions between individuals around the feeding area were analysed with a computer vision module. Detections of the dairy cows’ negative and positive interactions were performed on foreground video stream using a Long-term Recurrent Convolution Networks model. The sensor fusion system was implemented and tested on seven dairy cows during 45 days in an experimental dairy farm. The system performance was evaluated at the feeding area. The real-time locating system based on Ultra-wideband technology reached an accuracy with mean error 0.39 m and standard deviation 0.62 m. The accuracy of detecting the affiliative and agonistic social interactions reached 93.2%. This study demonstrates a potential system for monitoring social interactions between dairy cows. 
id159#DME: An Efficient Encryption Technique for Body Sensor Network in Healthcare Internet of Things (HIoT)#In order to secure the precious data and make a secure communication way between the user and their health assistance, the authors of this paper are going to suggest a novel encryption technique named Dynamic Matrix Encryption (DME). This method uses the concept of primary and secondary key concept where the primary key is actually a system-generated verifier and the secondary key will be provided by the medical assistance to whom we want to send our personal medical data or by the user who wants their personal report back. Here the authors explain the suggested encryption technique with a suitable example. In the suggested model every bit of data, including keys, will be in encrypted mode while traveling in the network. 
id161#Automatic Force-Based Probe Positioning for Precise Robotic Ultrasound Acquisition#The correct orientation of an ultrasound (US) probe is one of the main parameters governing the US image quality. With the rise of robotic ultrasound systems (RUSS), methods that can automatically compute the orientation promise repeatable, automatic acquisition from predefined angles resulting in high-quality US imaging. In this article, we propose a method to automatically position a US probe orthogonally to the tissue surface, thereby improving sound propagation and enabling RUSS to reach predefined orientations relatively to the surface normal at the contact point. The method relies on the derivation of the underlying mechanical model. Two rotations around orthogonal axes are carried out, while the contact force is being recorded. Then, the force data are fed into the model to estimate the normal direction. Accordingly, the probe orientation can be computed without requiring visual features. The method is applicable to the convex and linear probes. It has been evaluated on a phantom with varying tilt angles and on multiple human tissues (forearm, upper arm, lower back, and leg). As a result, it has outperformed existing methods in terms of accuracy. The mean (pmSD) absolute angular difference on the in-vivo tissues averaged over all anatomies and probe types is 2.9pm 1.6° and 2.2pm 1.5° on the phantom. 
id162#Lightweight Multi-party Authentication and Key Agreement Protocol in IoT-based E-Healthcare Service#Internet of Things (IoT) is playing a promising role in e-healthcare applications in the recent decades; nevertheless, security is one of the crucial challenges in the current field of study. Many healthcare devices (for instance, a sensor-augmented insulin pump and heart-rate sensor) collect a user's real-time data (such as glucose level and heart rate) and send them to the cloud for proper analysis and diagnosis of the user. However, the real-time user's data are vulnerable to various authentication attacks while sending through an insecure channel. Besides that, the attacks may further open scope for many other subsequent attacks. Existing security mechanisms concentrate on two-party mutual authentication. However, an IoT-enabled healthcare application involves multiple parties such as a patient, e-healthcare test-equipment, doctors, and cloud servers that requires multi-party authentication for secure communication. Moreover, the design and implementation of a lightweight security mechanism that fits into the resource constraint IoT-enabled healthcare devices are challenging. Therefore, this article proposes a lightweight, multi-party authentication and key-establishment protocol in IoT-based e-healthcare service access network to counter the attacks in resource constraint devices. The proposed multi-party protocol has used a lattice-based cryptographic construct such as Identity-Based Encryption (IBE) to acquire security, privacy, and efficiency. The study provided all-round analysis of the scheme, such as security, power consumption, and practical usage, in the following ways. The proposed scheme is tested by a formal security tool, Scyther, to testify the security properties of the protocol. In addition, security analysis for various attacks and comparison with other existing works are provided to show the robust security characteristics. Further, an experimental evaluation of the proposed scheme using IBE cryptographic construct is provided to validate the practical usage. The power consumption of the scheme is also computed and compared with existing works to evaluate its efficiency. 
id163#Tenodesis Grasp Detection in Egocentric Video#Objective: Cervical spinal cord injury (cSCI) can impair motor function in the upper limbs. Video from wearable cameras (egocentric video) has the potential to provide monitoring of rehabilitation outcomes at home, but methods for automated analysis of this data are needed. Wrist flexion and extension are essential elements to track grasping strategies after cSCI, as they may reflect the use of the tenodesis grasp, a common compensatory strategy. However, there is no established method to evaluate wrist flexion and extension from egocentric video. Methods: We propose a machine-learning-based approach comprising three steps - hand detection, pose estimation, and arm orientation estimation - to estimate wrist angle data, leading to the detection of tenodesis grasp. Results: The hand detection in conjunction with the pose estimation algorithm correctly located wrist and index finger metacarpophalangeal coordinates in 63% and 76% of 15,319 annotated frames, respectively, extracted from egocentric videos of individuals with cSCI performing activities of daily living in a home simulation laboratory. The arm orientation algorithm had a mean absolute error of 2.76 +/- 0.39 degrees in 12,863 labeled frames. Using these estimates, the presence of a tenodesis grasp was correctly detected in 72% +/- 11% of frames in videos of 6 activities. Conclusion: The results provided a clear indication of which participants relied on tenodesis grasp and which did not. Significance: This paradigm provides the first method that can enable clinicians and researchers to monitor the use of the tenodesis grasp by individuals with cSCI at home, with implications for remote therapeutic guidance. 
id164#The Perceptual Belief Problem: Why Explainability Is a Tough Challenge in Social Robotics#The explainability of robotic systems depends on people's ability to reliably attribute perceptual beliefs to robots, i.e., what robots know (or believe) about objects and events in the world based on their perception. However, the perceptual systems of robots are not necessarily well understood by the majority of people interacting with them. In this article, we explain why this is a significant, difficult, and unique problem in social robotics. The inability to judge what a robot knows (and does not know) about the physical environment it shares with people gives rise to a host of communicative and interactive issues, including difficulties to communicate about objects or adapt to events in the environment. The challenge faced by social robotics researchers or designers who want to facilitate appropriate attributions of perceptual beliefs to robots is to shape human-robot interactions so that people understand what robots know about objects and events in the environment. To meet this challenge, we argue, it is necessary to advance our knowledge of when and why people form incorrect or inadequate mental models of robots' perceptual and cognitive mechanisms. We outline a general approach to studying this empirically and discuss potential solutions to the problem. 
id166#Communication optimization for efficient dynamic task allocation in swarm robotics#Cooperation is a central idea to the usage of swarm robotics. It enables the solution of complex problems with a coordinated execution of simple tasks by a large group of small robot, which together lead to the achievement of the swarm common goal. This coordination is only possible with an efficient task allocation. Inspired by the strategy of the particle swarm optimization algorithm, we propose a novel algorithm called the Clustered Dynamic Task Allocation. This algorithm performs task allocation in a swarm robotic system in a fully distributed manner. It performs a guided search of the allocation space using the concept of adaptive speed. This search process requires information exchange between robots. This robot communication process must be planned carefully so as to achieve two conflicting objectives: the knowledge acquired by a given robot must flow throughout the swarm so that the optimization process may converge yet this communication must be limited so it does not hinder the efficiency of the task allocation process regarding large swarms. This paper proposes the use of a clustered communication topology between the swarm robots, aiming to optimize the underlying communication process, and thus enabling efficient task allocation for large robotic swarms. The results obtained with the cluster-based topology are compared to those obtained with the full mesh-based topology. On average, the results show a clear improvement in terms of execution time and battery charge requirements. Moreover, the performance of the proposed algorithm and the stability of the produced allocation are compared to other well-known models, demonstrating its better applicability to real-world swarm robotics based systems. 
id167#Patterns of urban foot traffic dynamics#Using publicly available traffic camera data in New York City, we quantify time-dependent patterns in aggregate pedestrian foot traffic. These patterns exhibit repeatable diurnal behaviors that differ for weekdays and weekends but are broadly consistent across neighborhoods in the borough of Manhattan. Weekday patterns contain a characteristic 3-peak structure with increased foot traffic around 9:00 am, 12:00–1:00 pm, and 5:00 pm aligned with the “9-to-5” work day in which pedestrians are on the street during their morning commute, during lunch hour, and then during their evening commute. Weekend days do not show a peaked structure, but rather increase steadily until sunset. Our study period of June 28, 2017 to September 11, 2017 contains two American holidays, the 4th of July and Labor Day, and their foot traffic patterns are quantitatively similar to weekend days despite the fact that they fell on weekdays. Projecting all days in our study period onto the weekday/weekend phase space (by regressing against the average weekday and weekend day) we find that Friday foot traffic can be represented as a mixture of both the 3-peak weekday structure and non-peaked weekend structure. We also show that anomalies in the foot traffic patterns can be used for detection of events and network-level disruptions. Finally, we show that clustering of foot traffic time series generates associations between cameras that are spatially aligned with distributions of commercial, residential, and unbuilt space proximal to the camera locations, indicating that foot traffic dynamics encode information about the surrounding built environment. 
id168#Towards automated generation of time-predictable code#Knowledge of the worst-case execution time of software components is essential in safety-critical hard real-time systems. The analysis thereof is not trivial as the execution time depends on many factors, including the underlying hardware platform, the program structure, and the code produced by the compiler. Often, the execution time is variable and highly sensitive to the input data the program has to process. This paper presents a code transformation applicable in a compiler backend that produces time-predictable code. The resulting code contains a single input-data independent execution path, in order to obtain programs of stable timing behaviour. The transformation technique has been validated by applying it on a number of benchmarks. Experiments show a reduction of execution time variability, at acceptable costs for the single execution path. 
id169#Enabling accuracy-aware Quantum compilers using symbolic resource estimation#Approximation errors must be taken into account when compiling quantum programs into a low-level gate set. We present a methodology that tracks such errors automatically and then optimizes accuracy parameters to guarantee a specified overall accuracy while aiming to minimize the implementation cost in terms of quantum gates. The core idea of our approach is to extract functions that specify the optimization problem directly from the high-level description of the quantum program. Then, custom compiler passes optimize these functions, turning them into (near-)symbolic expressions for (1) the total error and (2) the implementation cost (e.g., total quantum gate count). All unspecified parameters of the quantum program will show up as variables in these expressions, including accuracy parameters. After solving the corresponding optimization problem, a circuit can be instantiated from the found solution. We develop two prototype implementations, one in C++ based on Clang/LLVM, and another using the Q
id170#Electrical Impedance Tomography for Robot-Aided Internal Radiation Therapy#High dose rate brachytherapy (HDR) is an internal based radiation treatment for prostate cancer. The treatment can deliver radiation to the site of dominant tumor growth within the prostate. Imaging methods to delineate the dominant tumor are imperative to ensure the maximum success of HDR. This paper investigates the feasibility of using electrical impedance tomography (EIT) as the main imaging modality during robot-aided internal radiation therapy. A procedure utilizing brachytherapy needles in order to perform EIT for the purpose of robot-aided prostate cancer imaging is proposed. It is known that cancerous tissue exhibits different conductivity than healthy tissue. Using this information, it is hypothesized that a conductivity map of the tissue can be used to locate and delineate cancerous nodules via EIT. Multiple experiments were conducted using eight brachytherapy needle electrodes. Observations indicate that the imaging procedure is able to observe differences in tissue conductivity in a setting that approximates transperineal HDR and confirm that brachytherapy needles can be used as electrodes for this purpose. The needles can access the tissue at a specific depth that traditional EIT surface electrodes cannot. The results indicate the feasibility of using brachytherapy needles for EIT for the purpose internal radiation therapy. 
id171#Hybrid evolutionary algorithm based solution for register allocation for embedded systems#Embedded systems have an ever-increasing need for optimizing compilers to produce high quality codes with a limited general purpose register set. Either memory or registers are used to store the results of computation of a program. As compared to memory, accessing a register is much faster, but they are scarce resources and have to be utilized very efficiently. The optimization goal is to hold as many live variables as possible in registers in order to avoid expensive memory accesses. We present a hybrid evolutionary algorithm for graph coloring register allocation problem based on a new crossover operator called crossover by conflict-free sets(CCS) and a new local search function. 
id172#Dual-Hiding Side-Channel-Attack Resistant FPGA-Based Asynchronous-Logic AES: Design, Countermeasures and Evaluation#We present a side-channel-attack (SCA) resistant asynchronous-logic (async-logic) Advanced Encryption Standard (AES) accelerator with dual-hiding SCA countermeasures, i.e. the amplitude moderation (vertical dimension) and the time moderation (horizontal dimension). There are five contributions in this paper. First, we propose an async-logic design flow with relative timing to simplify the AES realization in Field-Programmable-Gate-Array (FPGA). Second, we optimize completion detection circuits therein to achieve a low power/overhead solution. Third, we propose a randomized delay-line control and a data-propagation control to amplify the dual-hiding SCA countermeasures for our async-logic AES accelerator. Fourth, we validate the async-logic design flow based on two commercially-available Sakura-X and Arty-A7 FPGA boards. Fifth, we comprehensively evaluate 74 SCA attacking models for our async-logic AES accelerator on these two boards, and compare the results against a benchmarking AES based on synchronous-logic (sync-logic). We show that our async-logic AES accelerator is unbreakable within 1 million electromagnetic (EM) traces where the sync-logic counterpart is breakable within < 30K EM traces. To our best knowledge, our async-logic AES accelerator is the first async-logic AES design evaluated comprehensively at the first/last round, at various attacking locations (i.e. before/after Substitute-Box), and with various Hamming weight/distance, bit model, and zero-model of SCAs. 
id173#Measurement of micro burr and slot widths through image processing: Comparison of manual and automated measurements in micro‐milling#In this study, the burr and slot widths formed after the micro‐milling process of Inconel 718 alloy were investigated using a rapid and accurate image processing method. The measurements were obtained using a user‐defined subroutine for image processing. To determine the accuracy of the developed imaging process technique, the automated measurement results were compared against results measured using a manual measurement method. For the cutting experiments, Inconel 718 alloy was machined using several cutting tools with different geometry, such as the helix angle, axial rake angle, and number of cutting edges. The images of the burr and slots were captured using a scanning electron microscope (SEM). The captured images were processed with computer vision software, which was written in C++ programming language and open‐sourced computer library (Open CV). According to the results, it was determined that there is a good correlation between automated and manual measurements of slot and burr widths. The accuracy of the proposed method is above 91%, 98%, and 99% for up milling, down milling, and slot measurements, respectively. The conducted study offers a user‐friendly, fast, and accurate solution using computer vision (CV) technology by requiring only one SEM image as input to characterize slot and burr formation. 
id174#Marker-tracking and deep-learning-based pose estimation for automatic crane work − Study on applying computer vision to construction sites [マーカトラッキングと深層学習を併用した位置姿勢推定によるクレーンの自動揚重支援− 施 工 現 場 に お け る コ ン ピ ュ ー タ ビ ジョンの活用に関する研究−]#In response to the labor shortages caused by an aging society, many attempts have been made to improve productivity using information technology, such as sensors and automated heavy machinery. Crane work is one of the labor-intensive tasks in construction sites; hence, automation is expected. This study reports on a computer vision-based application for supporting crane work, specifically item-hoisting work. A pose estimation method is developed using marker-tracking and deep-learning-based global feature-tracking, which calculates position correction data for a crane and manipulation for a gyro instrument that controls the turn of a hoisted item. 
id175#Performance Analysis of Dynamic Bijective S-BOX Construction Algorithms#Dynamic substitution boxes (S-boxes) are a vital building block of many cryptosystems. The general process of the dynamic construction of a bijective S-box consists of two main components: a random number generator, and a construction algorithm. In this paper, we study the effect of the choice of the construction algorithm on various aspects of the dynamic S-box, including its randomness and it cryptographic strength. We identify the different algorithms found in the literature and present a comparative analysis of their computational complexity and effective key space. Moreover, we study the cryptographic characteristics of the resulting S-boxes. We conclude with some guidelines to help cipher designers choose the S-box construction method most suited for their design objectives. 
id176#LiDARTag: A Real-Time Fiducial Tag System for Point Clouds#Image-based fiducial markers are useful in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and vision-based simultaneous localization and mapping (SLAM). The state-of-the-art fiducial marker detection algorithms rely on the consistency of the ambient lighting. This article introduces LiDARTag, a novel fiducial tag design and detection algorithm suitable for light detection and ranging (LiDAR) point clouds. The proposed method runs in real-time and can process data at 100 Hz, which is faster than the currently available LiDAR sensor frequencies. Because of the LiDAR sensors' nature, rapidly changing ambient lighting will not affect the detection of a LiDARTag; hence, the proposed fiducial marker can operate in a completely dark environment. In addition, the LiDARTag nicely complements and is compatible with existing visual fiducial markers, such as AprilTags, allowing for efficient multi-sensor fusion and calibration tasks. We further propose a concept of minimizing a fitting error between a point cloud and the marker's template to estimate the marker's pose. The proposed method achieves millimeter error in translation and a few degrees in rotation. Due to LiDAR returns' sparsity, the point cloud is lifted to a continuous function in a reproducing kernel Hilbert space where the inner product can be used to determine a marker's ID. The experimental results, verified by a motion capture system, confirm that the proposed method can reliably provide a tag's pose and unique ID code. The rejection of false positives is validated on the Google Cartographer indoor dataset and the Honda H3D outdoor dataset. All implementations are coded in C++ and are available at http://github.com/UMich-BipedLab/LiDARTag. &copy; 2021 IEEE. 
id177#Effect of loss functions on domain adaptation in semantic segmentation [Yitim fonksiyonlarinin anlamsal bölütlemede alan uyarlamasina etkisi]#In this paper, it is analyzed how different loss functions affect the performance of domain adaptation in the field of semantic segmentation. Semantic segmentation is a pixel-wise classification problem of an image. Large amounts of annotated data are required to train successfully in multi-parameter deep learning architectures. In recent years, several works have demonstrated that synthetic datasets are a good alternative since they are automatically annotated in virtual environments. However, due to the different distribution of source and target datasets, there is a decrease in performance. Domain adaptation methods address this problem by decreasing gap between source and target data. In this study, it is investigated that the effect of Cross- Entropy, Lovasz-Softmax, Dice Coefficient, Tversky and mean Intersection-over-Union Loss functions on domain adaptation in semantic segmentation. For our study, KITTI and Virtual KITTI datasets are used for real and synthetic images respectively. By evaluating the quantitative results, it is observed that the Dice Coefficient is relatively more successful. 
id178#Multi-emitter MAP-elites: Improving quality, diversity and data efficiency with heterogeneous sets of emitters#Quality-Diversity (QD) optimisation is a new family of learning algorithms that aims at generating collections of diverse and high-performing solutions. Among those algorithms, the recently introduced Covariance Matrix Adaptation MAP-Elites (CMA-ME) algorithm proposes the concept of emitters, which uses a predefined heuristic to drive the algorithm's exploration. This algorithm was shown to outperform MAP-Elites, a popular QD algorithm that has demonstrated promising results in numerous applications. In this paper, we introduce Multi-Emitter MAP-Elites (ME-MAP-Elites), an algorithm that directly extends CMA-ME and improves its quality, diversity and data efficiency. It leverages the diversity of a heterogeneous set of emitters, in which each emitter type improves the optimisation process in different ways. A bandit algorithm dynamically finds the best selection of emitters depending on the current situation. We evaluate the performance of ME-MAP-Elites on six tasks, ranging from standard optimisation problems (in 100 dimensions) to complex locomotion tasks in robotics. Our comparisons against CMA-ME and MAP-Elites show that ME-MAP-Elites is faster at providing collections of solutions that are significantly more diverse and higher performing. Moreover, in cases where no fruitful synergy can be found between the different emitters, ME-MAP-Elites is equivalent to the best of the compared algorithms. 
id180#Video7 Extended Architecture: Project Design and Statistical Analysis#The increasing number of both digital audio and video, brings up the necessity of appropriate tools for the storage and management of those kind of data. As options for the storage, there are non-relational databases (NoSQL). The diversity of existing systems provokes the interest in proposing an architecture for the management of that content, in different types of databases. This work deepens the Video7 architecture for storing and retrieving streaming audio and video files stored in non-relational key-value, tabular, document databases. Based on the architecture and suggested project design, a tool was implemented that makes use of the Apache HBase, Apache Cassandra, Project Voldemort, Redis and MongoDB databases, and is subjected to stressful routines. The purpose of stress routines is to measure insertion and queries times, in addition to their transfer rates in response to requests to a media server. The kruskal-wallis test was used to validated the measurements. Redis database presents better performance in the submitted routines, while Project Voldemort and Apache Cassandra perform poorly than other databases. 
id181#Geonote: A Field Notebook and Database for Geology#Field studies in Geology typically require taking notes in a paper based field notebook that is then stockpiled for future references. The so stored data is hardly accessible over the years, and information sharing is also difficult because of searching time, and the particularities in notation from different experts. Although organizations like the Open Geospatial Consortium favor the standardization, the aforementioned issues exhibit the need for information systems to store geologic data that enable further analysis. In this paper, a system is proposed for geological data capture and storage, based on requirements specified by Geology experts in Peru. The resulting system maintains a hierarchical organization of geologic information, and suggests an architecture based on a normalized relational database. Finally, the system is capable to retrieve information from the geologic database, and generate a set of user defined reports. 
id183#Towards Understanding People's Experiences of AI Computer Vision Fitness Instructor Apps#The recent rise in on-device AI computer vision and dialogue systems has facilitated a growing number of AI fitness related instructional apps. However, these technologies have yet to be explored within the HCI community. To investigate this domain we recruited 12 participants and asked them to engage with five recently launched AI fitness instructor apps. We interviewed participants and thematically analysed transcripts to understand their experience and expectations of these technologies. Our qualitative analysis outlines five main themes focusing on; limitations of computer vision, visual feedback, dialogue with the AI, adapting to the user, and working out with the instructor. Based upon our findings we present five design considerations for designers that relate to three key areas: feedback and motivation, personalising the experience, and building a relationship with the AI. We contribute a first look into people's initial experiences with on-device AI fitness instructor applications and we provide design considerations to guide future contextually-aware AI research in this domain. 
id184#Cross-boundary behavioural reprogrammability reveals evidence of pervasive universality#We exhaustively explore the reprogrammability capabilities and the intrinsic universality of the Cartesian product P × C of the space P of all possible computer programs of increasing size and the space C of all possible compilers of increasing length such that p ∈ P emulates pʹ ∈ P with T |pʹ| = |p| under a coarse-graining transformation T . Our approach yields a novel perspective on the complexity, controllability, causality and (re)programmability discrete dynamical systems. We find evidence that the density of (qualitatively different) computer programs that can be reprogrammed grows asymptotically as a function of program and compiler size. To illustrate these findings we show a series of behavioural boundary crossing results, including emulations (for all initial conditions) of Wolfram class 2 Elementary Cellular Automata (ECA) by Class 1 ECA, emulations of Classes 1, 2 and 3 ECA by Class 2 and 3 ECA, and of Classes 1, 2 and 3 by Class 3 ECA, along with results of even greater emulability for general CA (neighbourhood r = 3/2), including Class 1 CA emulating Classes 2 and 3, and Classes 3 and 4 emulating all other classes (1, 2, 3 and 4). The emulations occur with only a linear overhead and can be considered computationally efficient. We also found that there is no hacking strategy to compress the search space based on compiler profiling in terms of e.g. similarity or complexity, suggesting that no strategy other than exhaustive search is viable. We also introduce emulation networks, derive a topologicallybased measure of complexity based upon out- and in-degree connectivity, and establish bridges to fundamental ideas of complexity, universality, causality and dynamical systems. 
id185#Beyond Relational Databases: Preserving the Data#Relational databases are one of the main technologies supporting information assets in today’s organizations. They are designed to store, organize and retrieve digital information, and are such a fundamental part of information systems that most would not be able to function without them. Very often, the information contained in databases is irreplaceable or prohibitively expensive to reacquire; therefore, steps must be taken to ensure that the information within databases is preserved. This paper describes a methodology for long-term preservation of relational databases based on information extraction and format migration to a preservation format. It also presents a tool that was developed to support this methodology: Database Preservation Toolkit (DBPTK), as well as the processes and formats needed to preserve databases. The DBPTK connects to live relational databases and extracts information into formats more adequate for long-term preservation. Supported preservation formats include the SIARD 2, created by a cooperation between the Swiss Federal Archives and the E-ARK project that is becoming a standard in the area. DBPTK has a flexible plugin-based architecture enabling its use for other purposes like database upgrade and database migration between different systems. Presented real case scenarios demonstrate the usefulness, correctness and performance of the tool. 
id186#Evolving soft robotic jamming grippers#Jamming Grippers are a novel class of soft robotic actuators that can robustly grasp and manipulate objects of arbitrary shape. They are formed by placing a granular material within a flexible skin connected to a vacuum pump and function by pressing the un-jammed gripper against a target object and evacuating the air to transition the material to a jammed (solid) state, gripping the target object. However, due to the complex interactions between grain morphology and target object shape, much uncertainty still remains regarding optimal constituent grain shapes for specific gripping applications. We address this challenge by utilising a modern Evolutionary Algorithm, NSGA-III, combined with a Discrete Element Method soft robot model to perform a multi-objective optimisation of grain morphology for use in jamming grippers for a range of target object sizes. Our approach optimises the microscopic properties of the system to elicit bespoke functional granular material performance driven by the complex relationship between the individual particle morphologies and the related emergent behaviour of the bulk state. Results establish the important contribution of grain morphology to gripper performance and the critical role of local surface curvature and the length scale governed by the relative sizes of the grains and target object. 
id188#A Gate-Level Approach to Compiling for Quantum Computers#Programming language constructs generally operate on data words, and so does most compiler analysis and transformation. However, individual word-level operations often harbor pointless, yet resource and power hungry, lower-level operations. By transforming complete programs into gate-level operations on individual bits, and optimizing operations at that level, it is possible to dramatically reduce the total amount of work needed to execute the program's algorithm. This gate-level representation can be in terms of any complete set of logic gate types; earlier work targeted conventional multiplexor gates, but the work reported here centers on targeting CSWAP (FredKin) gates without fanout-a form that can be implemented on a quantum computer. This paper will overview the approach, describe the current state of the prototype compiler, and suggest some ways in which compiler automatic parallelization technology might be extended to allow ordinary programs to take advantage of the unique properties of quantum computers. 
id189#Integration of Relational and Graph Databases Functionally#In today's multi-model database world there is an effort to integrate databases expressed in different data models. The aim of the article is to show possibilities of integration of relational and graph databases with the help of a functional data model and its formal language - a typed lambda calculus. We suppose the existence of a data schema both for the relational and graph database. In this approach, relations are considered as characteristic functions and property graphs as sets of single-valued and multivalued functions. Then it is possible to express a query over such integrated heterogeneous database by one query expression expressed in a version of the typed lambda calculus. A more user-friendly version of such language could serve as a powerful query tool in practice. We discuss also queries sent to the integrated system and translated into queries in SQL and Cypher - the graph query language for Neo4j. 
id190#Real-Time Semantic Segmentation via Auto Depth, Downsampling Joint Decision and Feature Aggregation#To satisfy the stringent requirements for computational resources in the field of real-time semantic segmentation, most approaches focus on the hand-crafted design of light-weight segmentation networks. To enjoy the ability of model auto-design, Neural Architecture Search (NAS) has been introduced to search for the optimal building blocks of networks automatically. However, the network depth, downsampling strategy, and feature aggregation method are still set in advance and nonadjustable during searching. Moreover, these key properties are highly correlated and essential for a remarkable real-time segmentation model. In this paper, we propose a joint search framework, called AutoRTNet, to automate all the aforementioned key properties in semantic segmentation. Specifically, we propose hyper-cells to jointly decide the network depth and the downsampling strategy via a novel cell-level pruning process. Furthermore, we propose an aggregation cell to achieve automatic multi-scale feature aggregation. Extensive experimental results on Cityscapes and CamVid datasets demonstrate that the proposed AutoRTNet achieves the new state-of-the-art trade-off between accuracy and speed. Notably, our AutoRTNet achieves 73.9% mIoU on Cityscapes and 110.0 FPS on an NVIDIA TitanXP GPU card with input images at a resolution of 768 × 1536. 
id191#The rise of NoSQL systems: Research and pedagogy#Transaction processing systems are primarily based on the relational model of data and offer the advantages of decades of research and experience in enforcing data quality through integrity constraints, allowing concurrent access and supporting recoverability. From a performance standpoint, they offer joins-based query optimization and data structures to promote fast reads and writes, but are usually vertically scalable from a hardware standpoint. NoSQL (Not Only SQL) systems follow different data representation formats than relations, such as key-value pairs, graphs, documents or column-families. They offer a flexible data representation format as well as horizontal hardware scalability so that Big Data can be processed in real time. In this review article, we review recent research on each type of system, and then discuss how teaching of NoSQL may be incorporated into traditional undergraduate database courses in information systems curricula. Copyright 
id192#Urban Landscape Ecological Design and Stereo Vision Based on 3D Mesh Simplification Algorithm and Artificial Intelligence#With the development of urban road construction in China, urban road landscape design is also making progress. Aesthetic design is a kind of behavior that does not depend on the subjectivity of science. When this process is going on, we need to consider the various components of landscape, that is, landscape elements. Only in this way, works of art that conform to the aesthetic concept of a specific group of people can be created. The landscape ecological design of urban road is guided by the ecological theory, which can meet the normal traffic function of the road and realize the beautiful green landscape. This paper introduces three-dimensional network reconstruction method and three-dimensional vision technology, and puts forward the visualization scheme of urban landscape ecological design. Based on the model design and simulations, the perfomance of the proposed framework is validated. Compared with the state-of-the-art models, the proposed outperforms. 
id193#On-cmos image sensor processing for lane detection#This paper presents a CMOS image sensor (CIS) with built-in lane detection computing circuits for automotive applications. We propose on-CIS processing with an edge detection mask used in the readout circuit of the conventional CIS structure for high-speed lane detection. Further-more, the edge detection mask can detect the edges of slanting lanes to improve accuracy. A prototype of the proposed CIS was fabricated using a 110 nm CIS process. It has an image resolution of 160 (H) × 120 (V) and a frame rate of 113, and it occupies an area of 5900 μm × 5240 μm. A comparison of its lane detection accuracy with that of existing edge detection algorithms shows that it achieves an acceptable accuracy. Moreover, the total power consumption of the proposed CIS is 9.7 mW at pixel, analog, and digital supply voltages of 3.3, 3.3, and 1.5 V, respectively. 
id194#Emerging nondestructive approaches for meat quality and safety evaluation—A review#Meat is one of the most consumed agro-products because it contains proteins, minerals, and essential vitamins, all of which play critical roles in the human diet and health. Meat is a perishable food product because of its high moisture content, and as such there are concerns about its quality, stability, and safety. There are two widely used methods for monitoring meat quality attributes: subjective sensory evaluation and chemical/instrumentation tests. However, these methods are labor-intensive, time-consuming, and destructive. To overcome the shortfalls of these conventional approaches, several researchers have developed fast and nondestructive techniques. Recently, electronic nose (e-nose), computer vision (CV), spectroscopy, hyperspectral imaging (HSI), and multispectral imaging (MSI) technologies have been explored as nondestructive methods in meat quality and safety evaluation. However, most of the studies on the application of these novel technologies are still in the preliminary stages and are carried out in isolation, often without comprehensive information on the most suitable approach. This lack of cohesive information on the strength and shortcomings of each technique could impact their application and commercialization for the detection of important meat attributes such as pH, marbling, or microbial spoilage. Here, we provide a comprehensive review of recent nondestructive technologies (e-nose, CV, spectroscopy, HSI, and MSI), as well as their applications and limitations in the detection and evaluation of meat quality and safety issues, such as contamination, adulteration, and quality classification. A discussion is also included on the challenges and future outlooks of the respective technologies and their various applications. 
id195#A domain-specific compiler theory based framework for automated reaction network generation#Catalytic chemical reaction networks are often very complicated because of the numerous species and reactions involved. Hence, automating the network generation process is necessary as it is quite labor intensive and error prone to write down all the reactions manually. We present an automated integrated framework for reaction network generation based on domain-specific compiler theory using a knowledge base of chemistry rules. The chemistry rules represent basic reaction mechanisms that the reactants can undergo. The system's domain-specific compiler takes the rules and initial reactants as inputs, parses the rule text, generates the intermediate representation, and finally produces the reaction network by interpreting the intermediate representation. We chose the Abstract Syntax Tree (AST) as the intermediate representation because of its transparency and ease of search. The system executes the AST using the initial reactants, and generates the reaction network. The Reaction Description Language (RDL) has been extended to describe the chemistry rules for catalytic systems, and the molecules are represented by Simplified Molecular Input Line Entry System (SMILES). This framework separates the molecules and the behavior of catalysts, represented by the chemistry rules. This approach accelerates the speed of generating hypotheses for building the kinetic models for catalytic systems. 
id196#A Logic Framework for P2P Deductive Databases#This paper presents a logic framework for modeling the interaction among deductive databases in a peer-to-peer (P2P) environment. Each peer joining a P2P system provides or imports data from its neighbors by using a set of mapping rules, that is, a set of semantic correspondences to a set of peers belonging to the same environment. By using mapping rules, as soon as it enters the system, a peer can participate and access all data available in its neighborhood, and through its neighborhood it becomes accessible to all the other peers in the system. A query can be posed to any peer in the system and the answer is computed by using locally stored data and all the information that can be consistently imported from the neighborhood. Two different types of mapping rules are defined: mapping rules allowing to import a maximal set of atoms not leading to inconsistency (called maximal mapping rules) and mapping rules allowing to import a minimal set of atoms needed to restore consistency (called minimal mapping rules). Implicitly, the use of maximal mapping rules states it is preferable to import as long as no inconsistencies arise; whereas the use of minimal mapping rules states that it is preferable not to import unless a inconsistency exists. The paper presents three different declarative semantics of a P2P system: (i) the Max Weak Model Semantics, in which mapping rules are used to import as much knowledge as possible from a peer's neighborhood without violating local integrity constraints; (ii) the Min Weak Model Semantics, in which the P2P system can be locally inconsistent and the information provided by the neighbors is used to restore consistency, that is, to only integrate the missing portion of a correct, but incomplete database; (iii) the Max-Min Weak Model Semantics that unifies the previous two different perspectives captured by the Max Weak Model Semantics and Min Weak Model Semantics. This last semantics allows to characterize each peer in the neighborhood as a resource used either to enrich (integrate) or to fix (repair) the knowledge, so as to define a kind of integrate-repair strategy for each peer. For each semantics, the paper also introduces an equivalent and alternative characterization, obtained by rewriting each mapping rule into prioritized rules so as to model a P2P system as a prioritized logic program. Finally, results about the computational complexity of P2P logic queries are investigated by considering brave and cautious reasoning. 
id198#Plant recognition based on deep belief network classifier and combination of local features [Derin inanç aǧlari ve yerel özniteliklerin birlikte kullanimi ile bitki tanima]#During the past decades, recognition of plant types has attracted the attention of numerous researchers due to its outstanding applications including precision agriculture. Applying to the video frames, this paper proposes a hybrid method which combines the features extracted from the images using the SIFT, HOG and GIST descriptors and classifies the plants by means of the deep belief network. First, in order to avoid ineffective features, a pre-processing course is performed on the image. Then, the mentioned descriptors extract several features from the image. Due to the problems of working with a large number of the features, a small and distinguishing feature set is produced using the bag of words technique. Finally, these reduced features are given to a deep belief network in order to recognize the plants. Comparing the results of the proposed method with some other existing methods demonstrates an improvement in the accuracy, precision and recall measures for the approach of this work in the plant recognition. 
id200#Advancing the E-Tendering Information System to Counter Corruption by Proposing Anti-Corruption SMART Tools#The number of corruption can reduce by corruption prevention initiative. This research aims to improve the E-Tendering Electronic Procurement System (EPS) to be more transparent, accountable, and effective in preventing corruption and support the Corruption Eradication Commission (CEC). Management Information System and Business Process Reengineering methods are used in this study. The information system design proposed in this research made with a structured system development method consists of 4 stages; the creation of entity-relationship diagrams (ERD), relational database, use case diagram, and data flow diagram (DFD). This research provides three choices of scenarios that are modeled and simulated by iGrafx software. The most realistic scenario to be implemented right now is the first scenario with database integration for government agencies' administrative documents. The first scenario's benefit is reducing average cycle time by 34.20%, faster bid evaluation process, and eliminating face-to-face processes to prevent collusion. Scenario 3 is the ideal scenario to be implemented in a long-term project. The third scenario's benefit is reducing average cycle time by 18.34%, a faster bid evaluation process, eliminating face-to-face processes to prevent collusion, strengthening E-Tendering supervision with Self Monitoring, Analysis, Reporting Technology (SMART), and increase information transparency. 
id201#Street view imagery: Methods and applications based on artificial intelligence [街景影像-基于人工智能的方法与应用]#"Street view imagery is a promising and growing big geo-data that provides current and historical images in more than 200 countries for urban physical environment representation and audit. Such data not only describes the visual details of the urban physical environment but also contains information about urban functions, socioeconomics, and human dynamics. Street view imagery has the potential to complement new and traditional data, such as remote sensing imagery and social sensing data. However, traditional digital image processing techniques for street view imagery handling are limited. Extracting rich semantic information from street view imagery efficiently has always been a challenging issue. Until recently, the development of artificial intelligence has led to numerous breakthroughs in image processing and machine learning. Indeed, the last few years have witnessed the fast development of deep learning and computer vision techniques, which facilitate the understanding of scene semantics from street view imagery and the quantitative representation of the urban physical and built environments. Many new applications, novel methods, and thoughts regarding street view imagery have emerged, covering research fields, such as geography, urban planning, urban design, urban economics, public health, environmental psychology, and energy. This trend has provided new perspectives for big geo-data-driven urban environment analysis, human-land relationship study, and spatial data mining and knowledge discovery. To summarize this research trend, this paper reviews the recent works on urban physical environment analysis using street view imagery. The key supporting techniques for street view imagery analytics are discussed in terms of two dimensions: deep learning and computer vision. Deep learning has been applied recently to various computer vision tasks, such as image classification, image segmentation, and object detection. The success of deep learning techniques is attributed to their ability to learn rich high-level image representations as opposed to the hand-designed low-level features used in other image understanding methods. Additionally, this paper summarizes the street view imagery applications in three aspects: place representation, sense of place, and place semantics reasoning. ""Place representation"" includes works that extract visual elements that constitute the urban physical environment; ""sense of place"" refers to works that use street view imagery to understand how people respond to their surrounding environments regarding perceptions and emotions; and ""place semantics reasoning"" refers to studies that attempt to infer and estimate invisible factors, socio-economics, demographics, and human dynamics from street view imagery. More importantly, the issues of this research field, such as the spatio-temporal uniformity of street view image data, and the lack of solid workflow of data analytics, are highlighted. Finally, the development prospects of street view imagery are discussed. Crowdsourcing platforms and the field of the autonomous vehicle will increase the number of street view image sources. More issues, including how physical environments are involved and whether a universal law exists regarding the distribution of physical elements in space, are expected to be explored in future works. "
id202#Compile-time type-checking for custom type qualifiers in java#The Checker Framework enables adding custom type qualifiers to the Java language in a backward-compatible way. The Checker Framework allows programmers to write type qualifiers in their programs and to create compiler plug-ins that enforce the semantics of these qualifiers at compile time. Using the plug-ins, programmers can improve the quality of their code without disturbing their development workflow. Our case studies demonstrate that the Checker Framework is scalable, easy to use, and effective in detecting and preventing errors. It has been used to detect real errors in null pointer dereferencing, side effects, equality tests, and initialization, among others.
id203#Survey of Side-channel Attacks and Countermeasures on Post-quantum Cryptography [后量子密码算法的侧信道攻击与防御综述]#To solve the threat of quantum computing to the security of public-key cryptography, post-quantum cryptography has become a frontier focus in the field of cryptography. Post-quantum cryptography guarantees the security of the algorithm through mathematical theories, but it is vulnerable to side-channel attacks in specific implementation and applications, which will seriously threaten the security of post-quantum cryptography. This study is based on the round 2 candidates in the NIST post-quantum cryptography standardization process and the round 2 candidates in the CACR public key cryptography competition in China. First, classification investigations of various post-quantum cryptographic algorithms are conducted, including lattice-based, code-based, hash-based, and multivariate-based cryptographic algorithms. Then, their security status against side-channel attacks and existing protection strategies are analyzed. To analyze the methods of side-channel attack against post-quantum cryptography, it is summarized that the commonly used post-quantum cryptography side-channel attack methods, attack targets, and attack evaluation indexes for various post-quantum cryptography according to the classification of core operators and attack types. Furthermore, following the attack types and attack targets, the existing countermeasures for attack and the costs of defense strategies are sorted out. Finally, in the conclusion part, some security suggestions are put forward according to the attack method, protection means, and protection cost, and also the potential side-channel attack methods and defense strategies in the future are analyzed. 
id204#Estimating the aquatic-plant area on a pond surface using a hue-saturation-component combination and an improved Otsu method#Accurately estimating the aquatic-plant area on a pond surface is of great significance for regulating nutrients, such as nitrogen and phosphorus, increasing the dissolved-oxygen content, and ensuring the healthy growth of aquaculture organisms. Currently, remote-sensing methods for monitoring pond-surface aquatic plants have a high cost, long cycle, and poor real-time performance, which limits the practicability of these methods in aquaculture. To address the above problems, a pond-surface aquatic-plant area estimation method based on a hue-saturation (H-S)-component combination and an improved Otsu method, was proposed, which utilizes the advantages of fast and non-destructive computer-vision technology. First, the pond-surface images collected by the camera were preprocessed using a median filter. Second, a visual-saliency map of the aquatic plants was generated by combining the H and S components of the HSV color space. Then, the appropriate Otsu optimal threshold-selection criterion formula was selected to determine the optimal segmentation thresholds of the aquatic plants and background water. Finally, the aquatic plants in the image were segmented using the binary method, and the aquatic-plant area on the pond surface was estimated, based on the proportion of aquatic-plant pixels. In this study, an improved Otsu method was proposed to adaptively select the Otsu optimal threshold-selection criterion formula by judging the position of the highest and second highest peaks of the gray histogram of the visual-saliency map, which solved the problem of finding the appropriate segmentation threshold for different proportions of aquatic plants and background water. The proposed method was tested on real pond-surface images with a mean misclassification error of 0.0284, mean F1 score of 0.8396, mean precision of 0.8428, and mean recall of 0.9146. The experimental results indicated that the proposed method could accurately segment aquatic plants to accurately estimate the aquatic-plant area on the pond surface. 
id205#Challenges in compiler construction for secure two-party computation#"The problem of secure two-party computation has received great attention in the years that followed its introduction by Yao. The solutions proposed follow one of the two research directions of either using homomorphic encryption techniques or implementing Yao's ""Garbled Circuit"" solution. The latter requires circuits to implement a given functionality. Recently, the compiler CBMC-GC was introduced, the first compiler capable of translating programs written in a general purpose language (ANSI-C) into circuits suitable for secure two-party computation. In this paper, we discuss the current limitations of CBMC-GC and propose directions for future research. "
id207#3D-Printed Self-Healing Elastomers for Modular Soft Robotics#Advances in materials, designs, and controls are propelling the field of soft robotics at an incredible rate; however, current methods for prototyping soft robots remain cumbersome and struggle to incorporate desirable geometric complexity. Herein, a vat photopolymerizable self-healing elastomer system capable of extreme elongations up to 1000% is presented. The material is formed from a combination of thiol/acrylate mixed chain/step-growth polymerizations and uses a combination of physical processes and dynamic-bond exchange via thioethers to achieve full self-healing capacity over multiple damage/healing cycles. These elastomers can be three dimensional (3D) printed with modular designs capable of healing together to form highly complex and large functional soft robots. Additionally, these materials show reprogrammable resting shapes and compatibility with self-healing liquid metal electronics. Using these capabilities, subcomponents with multiple internal channel systems were printed, healed together, and combined with functional liquid metals to form a high-wattage pneumatic switch and a humanoid-scale soft robotic gripper. The combination of 3D printing and self-healing elastomeric materials allows for facile production of support-free parts with extreme complexity, resulting in a paradigm shift for the construction of modular soft robotics. 
id208#Exploiting In-Hand Knowledge in Hybrid Joint-Cartesian Mapping for Anthropomorphic Robotic Hands#Replication of human hand motions on anthropomorphic robotic hands is typically treated in literature as the combination of two sub-problems: the measurement of human hand motions, and the mapping of such motions on the robotic hand. In this letter we focus on the second one. Different approaches have been proposed to deal with this problem, but none of them preserves both master finger shapes and fingertip positions on the robotic hand, i.e. ensuring predictability and natural motion for the teleoperator. In this article, we propose a novel hybrid approach that combines both joint and Cartesian mappings in a single solution. In particular, we exploit the a priori, in-hand information related to the areas of the workspace in which thumb and finger fingertips can get in contact. This allows to define, for each finger, a zone of transition from joint to Cartesian mapping. As a consequence, both hand shape during volar grasps and correctness of the fingertip positions for precision grasps are preserved, despite the master-slave kinematic dissimilarities. The proposed hybrid mapping is presented and experimentally evaluated both in simulation and with a real slave anthropomorphic robotic hand. 
id209#Two control computation transformation methods for obfuscating Java soft software#Protection of Java code from malicious modifications is an important issue. A defense against reverse engineering is obfuscation. Two control flow obfuscation methods for protecting Java code at the bytecode level are proposed. They are no initial variable obfuscation (NIVO) and breaking for-loop obfuscation (BFLO). The new methods were tested against the decompiler Jad. Both the no initial variable obfuscation and the breaking for-loop obfuscation successfully defeated the decompiler Jad.
id210#CUDA-on-CL: A compiler and runtime for running NVIDIA® CUDA™ C++11 applications on OpenCL™ 1.2 devices#In the machine learning domain, machine learning frameworks are predominantly written and maintained in NVIDIA® CUDA™ language. There have been attempts to port these frameworks to OpenCL®, notably the ports of Caffe framework by Gu et al; Tschopp; and Engel; and of Torch framework by Perkins. The authors of these frameworks found merging their work into the mainstream framework challenging, and maintain their forks as separate branches or repositories. CUDA-on-CL addresses this problem by leaving the reference implementation entirely in NVIDIA CUDA, both host-side and device-side, and providing a compiler and a runtime component, so that any CUDA C++11 application can in theory be compiled and run on any OpenCL 1.2 device. We use Tensorflow framework as a case-study, and demonstrate the ability to run unary, binary and reduction Tensorflow and Eigen kernels, with no modification to the original CUDA source-code. Performance studies are undertaken, using the Tensorflow kernels. For buffer sizes of 1MB or more, performance is comparable between CUDA and CUDA-on-CL, across unary operations, binary operations and single-Axis reductions. Full reduction is around 14 times slower on CUDA-on-CL than on CUDA. We think this may be because of the absence of the low-level hardware shfl operation. The asymptotic time for zero buffer sizes is double that of CUDA, possibly because of the overhead of additional kernel boilerplate needed to workaround limitations in the OpenCL 1.2 standard. 
id212#A Lightweight Neural Network for Monocular View Generation with Occlusion Handling#In this article, we present a very lightweight neural network architecture, trained on stereo data pairs, which performs view synthesis from one single image. With the growing success of multi-view formats, this problem is indeed increasingly relevant. The network returns a prediction built from disparity estimation, which fills in wrongly predicted regions using a occlusion handling technique. To do so, during training, the network learns to estimate the left-right consistency structural constraint on the pair of stereo input images, to be able to replicate it at test time from one single image. The method is built upon the idea of blending two predictions: a prediction based on disparity estimation and a prediction based on direct minimization in occluded regions. The network is also able to identify these occluded areas at training and at test time by checking the pixelwise left-right consistency of the produced disparity maps. At test time, the approach can thus generate a left-side and a right-side view from one input image, as well as a depth map and a pixelwise confidence measure in the prediction. The work outperforms visually and metric-wise state-of-the-art approaches on the challenging KITTI dataset, all while reducing by a very significant order of magnitude (5 or 10 times) the required number of parameters (6.5 M). 
id213#Application of ultraviolet, visible, and infrared light imaging in protein-based biopharmaceutical formulation characterization and development studies#Imaging is increasingly more utilized as analytical technology in biopharmaceutical formulation research, with applications ranging from subvisible particle characterization to thermal stability screening and residual moisture analysis. This review offers a comprehensive overview of analytical imaging for scientists active in biopharmaceutical formulation research and development, where it presents the unique information provided by the ultraviolet (UV), visible (Vis), and infrared (IR) sections in the electromagnetic spectrum. The main body of this review consists of an outline of UV, Vis, and IR imaging techniques for several (bio)physical properties that are commonly determined during protein-based biopharmaceutical formulation characterization and development studies. The review concludes with a future perspective of applied imaging within the field of biopharmaceutical formulation research. 
id214#Use of Ontology Learning in Information System Integration: A Literature Survey#Ontology-based information integration is a useful method to integrate heterogeneous data at the semantic level. However, there are some bottlenecks of the traditional method for constructing ontology, i.e., time-consuming, error-prone, and semantic loss. Ontology learning is a kind of ontology construction approach based on machine learning, it provides a new opportunity to tackle the above bottlenecks. Especially, it could be employed to construct ontologies and integrate large-scale and heterogeneous data from various information systems. This paper surveys the latest developments of ontology learning and highlights how they could be adopted and play a vital role in the integration of information systems. The recent techniques and tools of ontology learning from text and relational database are reviewed, the possibility of using ontology learning in information integration were discussed based on the mapping results of the aforementioned bottlenecks and features of ontology learning. The potential directions for using ontology learning in information systems integration were given. 
id215#Development of flexible tactile sensor for the envelop of curved robotic hand finger in grasping force sensing#Intelligent robotic hand with integrated tactile sensor is able to perform real-time tactile information sensing, which is crucial for dexterous grasping and manipulation. This paper presents a novel flexible tactile sensor with 18 sensing units to envelop a complex-shaped robotic hand finger for grasping force measurement. A new approach based on point cloud fitting and triangular searching is developed to determine the special configuration of these 18 sensing units on the complex finger surface, and it is used for the structural design of the flexible tactile sensor. The fabrication process to make the tactile sensor is developed and fabricated tactile sensor features high flexibility and can be perfectly enveloped around robotic hand finger. Calibration results show that the sensor has relatively high sensitivities for three-axis force sensing: 1.87 V/N at 0.1 ~ 0.7 N and 0.47 V/N at 0.7 ~ 3 N for z-axis normal force sensing, 1.59 V/N and 1.49 V/N at −1 ~ 1 N for x- and y-axes shear force sensing, respectively. Besides, the tactile sensor has good reproducibility and generally low cross-talk interferences between the sensing units. After being wrapped onto robotic hand finger, this sensor is utilized to measure the distributed contact forces when the robotic hand grasps different objects. The effects of grasping movements on the generated contact forces are analyzed. The obtained results demonstrate that the developed tactile sensor has great potentials for the perception of tactile information in human-robotic interactions. 
id216#An improved algorithm for loop dead optimization#Loop dead variables are the variables, which are defined in a loop, but not used in that loop. On successive execution of loop, these get different value, however all values (except last value) are not used. Hence in optimized program, the definition of a loop dead variable can be moved outside the loop (after the loop), rather than computing every time, inside the loop. In our discussion, we have assumed only simple loop. Every loop has one entry and one exit point.
id217#PTP: Parallelized Tracking and Prediction with Graph Neural Networks and Diversity Sampling#Multi-object tracking (MOT) and trajectory prediction are two critical components in modern 3D perception systems that require accurate modeling of multi-agent interaction. We hypothesize that it is beneficial to unify both tasks under one framework in order to learn a shared feature representation of agent interaction. Furthermore, instead of performing tracking and prediction sequentially which can propagate errors from tracking to prediction, we propose a parallelized framework to mitigate the issue. Also, our parallel track-forecast framework incorporates two additional novel computational units. First, we use a feature interaction technique by introducing Graph Neural Networks (GNNs) to capture the way in which agents interact with one another. The GNN is able to improve discriminative feature learning for MOT association and provide socially-aware contexts for trajectory prediction. Second, we use a diversity sampling function to improve the quality and diversity of our forecasted trajectories. The learned sampling function is trained to efficiently extract a variety of outcomes from a generative trajectory distribution and helps avoid the problem of generating duplicate trajectory samples. We evaluate on KITTI and nuScenes datasets showing that our method with socially-aware feature learning and diversity sampling achieves new state-of-the-art performance on 3D MOT and trajectory prediction. Project website is: http://www.xinshuoweng.com/projects/PTP. 
id218#Stumped nature hyperjerk system with fractional order and exponential nonlinearity: Analog simulation, bifurcation analysis and cryptographic applications#This article presents the construction and consumption of hyperjerk system having chaotic nature with the commensurate ordered fractional derivative on rapidly digitalized emerging technologies. The system analyzed analytically using Lipschitz condition and numerically with the help of predictor corrector approaches. Originality of constructed system is confirmed using log error plots which gives satisfactory small values on finite time scale. The dynamical performances of the complex system are termed in multiple phase planes by the mean of their significant nature. Stability and bifurcation analysis around the fractional derivative is also studies for the various parameter of system to check the visibility of chaotic solution. The system is also designed in analog circuit simulator with the aid of operational amplifier, anti-parallel semiconductor diodes for validation of the system. With the help of random number generators (RNGs), binary array generated from the hyperjerk system and plugged in to the NIST 800–22 test suite to measure the randomness. The array having high randomness is used as strong cypher key for the cryptographic execution straightforwardly in both direction encryption and decryption. 
id220#Autonomous Hierarchical Surgical State Estimation during Robot-Assisted Surgery through Deep Neural Networks#Many operations in robot-Assisted surgery (RAS) can be viewed in a hierarchical manner. Each surgical task is represented by a superstate, which can be decomposed into finer-grained states. The estimation of these discrete states at different levels of temporal granularity provides a temporal perception of the current surgical scene during RAS, which is a crucial step towards many automated surgeon-Assisting functionalities. We propose Hierarchical Estimation of Surgical States through Deep Neural Networks (HESS-DNN), a deep learning-based system that concurrently estimates the current super-and fine-grained states. HESS-DNN incorporates endoscopic vision, robot kinematics, and system events data from the da Vinci Xi surgical system. HESS-DNN is evaluated on a real-world robotic inguinal hernia repair surgery dataset: HERNIA-20, and achieves accurate state estimates of both surgical superstate and the corresponding fine-grained surgical state. We show that HESS-DNN improves state-of-The-Art fine-grained state estimation across the entire HERNIA-20 RAS procedure through its hierarchical design. We also analyze the relative contributions of each input data type and HESS-DNN's design to surgical (super)state estimation accuracy. 
id221#Prevention of FDI Attacks in Smart Meter by providing Multi-Layer Authentication using ElGamal and SHA#With intent of reducing energy consumption, cost and preventing electricity wastage and power theft, the automated smart grid architecture is revolutionising the traditional electric power grid system. Smart Grid (SG) is a combination of traditional power grid with Information and communication Technologies (ICT). The main feature of smart grid is the advanced metering infrastructure (AMI) unit which integrates several smart meters, communicating networks and data management system, enabling two way communications between utilities and customers. But, these systems are more vulnerable to attacks due to several security loopholes and are a constant target for hackers and attackers intending energy theft. Among many, a major vulnerability of the smart meter is the false data injection (FDI) attack wherein the meter readings are at high risk and can be compromised. The Network Interface Card (NIC) in Smart meter is more prone to get effected by FDI attacks. In this paper, a novel approach is proposed to prevent the FDI attacks on NIC thereby smart meters using the asymmetric key cryptography algorithm, ElGamal and a hashing algorithm SHA512. Bit-masking, salting is also performed for data integrity. This paper also simulates the digital signature verification and authentication to detect tampering of meter reading. 
id222#Towards Eclipse Plug-ins for Automated Data Warehouse Design from an Ontology#The Semantic Web is experiencing a rise in recent years. As a result, ontologies have become ubiquitous in many areas while attracting growing interest from researchers. However, this knowledge representation model escapes to multidimensional analysis due to the lack of tools allowing the transition from the ontological model to the multidimensional model. In this contribution, we present our approach for the implementation of our multi-approaches model for the automatic generation of a Data Warehouse model from an ontology. In this context, we have first developed an Eclipse Plug-in for generating a conceptual model of a relational database from an ontology. Secondly, we have developed another Eclipse Plug-in for generating a Data Warehouse model from a database model. And finally, we have integrated these two plug-ins, based on model-to-model transformation and to build an automated design solution for a Data Warehouse model from an ontology. 
id223#A Survey on Healthcare Data: A Security Perspective#With the remarkable development of internet technologies, the popularity of smart healthcare has regularly come to the fore. Smart healthcare uses advanced technologies to transform the traditional medical system in an all-round way, making healthcare more efficient, more convenient, and more personalized. Unfortunately, medical data security is a serious issue in the smart healthcare systems. It becomes a fundamental challenge that requires the development of efficient innovative strategies towards fulfilling the healthcare needs and supporting secure healthcare transfer and delivery. This article provides a comprehensive survey on state-of-the-art techniques for health data security and their new trends for solving challenges in real-world applications. We survey the various notable cryptography, biometrics, watermarking, and blockchain-based security techniques for healthcare applications. A comparative analysis is also performed to identify the contribution of reviewed techniques in terms of their objective, methodology, type of medical data, important features, and limitations. At the end, we discuss the open issues and research directions to explore the promising areas for future research. 
id224#A Software/Hardware Co-Design of Crystals-Dilithium Signature Scheme#As quantum computers become more affordable and commonplace, existing security systems that are based on classical cryptographic primitives, such as RSA and Elliptic Curve Cryptography (ECC), will no longer be secure. Hence, there has been interest in designing post-quantum cryptographic (PQC) schemes, such as those based on lattice-based cryptography (LBC). The potential of LBC schemes is evidenced by the number of such schemes passing the selection of NIST PQC Standardization Process Round-3. One such scheme is the Crystals-Dilithium signature scheme, which is based on the hard module-lattice problem. However, there is no efficient implementation of the Crystals-Dilithium signature scheme. Hence, in this article, we present a compact hardware architecture containing elaborate modular multiplication units using the Karatsuba algorithm along with smart generators of address sequence and twiddle factors for NTT, which can complete polynomial addition/multiplication with the parameter setting of Dilithium in a short clock period. Also, we propose a fast software/hardware co-design implementation on Field Programmable Gate Array (FPGA) for the Dilithium scheme with a tradeoff between speed and resource utilization. Our co-design implementation outperforms a pure C implementation on a Nios-II processor of the platform Altera DE2-115, in the sense that our implementation is 11.2 and 7.4 times faster for signature and verification, respectively. In addition, we also achieve approximately 51% and 31% speed improvement for signature and verification, in comparison to the pure C implementation on processor ARM Cortex-A9 of ZYNQ-7020 platform. 
id225#A new data structure for representation of relational databases for application in the normalization process#In this paper, a new data structure named relational tree is proposed for the representation of the relational database in computer memory. Relational database schema represented using relational tree(s) appears to be more promising for semiautomating the process of normalization in a very efficient manner, which is prime motive of this paper. This paper provides all the fundamental concepts required for the understanding of the representation of relational schema using relational tree so that efficient algorithms of various normal forms can be designed by using this representation. This automation will considerably reduce manual efforts and errors in the process of normalization in software industries. Space requirements also improved, compared to previously proposed approaches. It is expected that application of various normalization algorithms on this way of representation is very efficient since the relational tree can be easily manipulated. 
id226#In-database connected component analysis#We describe a Big Data-practical, SQL-implementable algorithm for efficiently determining connected components for graph data stored in a Massively Parallel Processing (MPP) relational database. The algorithm described is a linear-space, randomised algorithm, always terminating with the correct answer but subject to a stochastic running time, such that for any ϵ>0 and any input graph G = (V, E) the algorithm terminates after O(log|V |) SQL queries with probability of at least, which we show empirically to translate to a quasi-linear runtime in practice. 
id227#On compiling Boolean circuits optimized for secure multi-party computation#Secure multi-party computation (MPC) allows two or more distrusting parties to jointly evaluate a function over private inputs. For a long time considered to be a purely theoretical concept, MPC transitioned into a practical and powerful tool to build privacy-enhancing technologies. However, the practicality of MPC is hindered by the difficulty to implement applications on top of the underlying cryptographic protocols. This is because the manual construction of efficient applications, which need to be represented as Boolean or arithmetic circuits, is a complex, error-prone, and time-consuming task. To facilitate the development of further privacy-enhancing technology, multiple compilers have been proposed that create circuits for MPC. Yet, almost all presented compilers only support domain specific languages or provide very limited optimization methods. In this work (this is an extended and revised version of the paper ‘Secure Two-party Computations in ANSI C’ (Holzer et al., in: ACM CCS, 2012) that reflects the progress in secure computation and describes the current optimization tool chain of CBMC-GC) we describe our compiler CBMC-GC that implements a complete tool chain from ANSI C to circuit. Moreover, we give a comprehensive overview of circuit minimization techniques, which we have identified and adapted for the creation of efficient circuits for MPC. With the help of these techniques, our compilation approach allows for a high level of abstraction from the cryptographic primitives used in MPC protocols, as well as the complex design of digital circuits. By using the model checker CBMC as a compiler frontend, we illustrate the link between MPC, formal methods, and digital logic design. Our experimental results illustrate the effectiveness of the implemented optimizations techniques for various example applications. In particular, compared with other state-of-the-art compilers, we show that CBMC-GC compiles circuits from the same source code that are up to four times smaller. 
id228#Spatial segment-aware clustering based dynamic reliability threshold determination (SSC-DRTD) for unsupervised person re-identification#Person Re-Identification (re-ID) in a crowded multi-camera surveillance environment is a highly challenging task. The traditional benchmark datasets contain less number of occluded images due to the pre-planned setup and limited duration of the videos recorded. Unlike the traditional benchmark person re-ID datasets, real-world surveillance environment possess high static and dynamic occlusions. The analysis of different image segments captured in diverse environments by using a static reliability threshold leads to a poor matching accuracy. To resolve this issue of poor reliability threshold determination and to handle the occluded person re-ID images efficiently, we propose an unsupervised spatial segmented clustering model (SSC-DRTD) which determines a dynamic segment-wise reliability threshold. The unlabeled person re-ID images are segmented into k-parts to determine the segment-wise reliability threshold and the optimal number of segments for a given dataset. A cluster refinement strategy is proposed by incorporating the determined dynamic reliability threshold values to match the occluded noisy images with its appropriate ground truth identities for robust cluster formation. An improved rank evaluation has been performed on the benchmark person re-ID datasets such as DukeMTMC re-ID, Market1501, CUHK03, and MSMT17. The experimental results show the improved performance of our proposed SSC-DRTD model in handling occluded person re-ID images over the state-of-the-art unsupervised person re-ID methods. To further prove the efficiency of our proposed model, an exploratory analysis is performed by increasing the number of occluded query images to simulate the real-world surveillance scenario. 
id229#How should compilers explain problems to developers?#Compilers primarily give feedback about problems to developers through the use of error messages. Unfortunately, developers routinely find these messages to be confusing and unhelpful. In this paper, we postulate that because error messages present poor explanations, theories of explanation-such as Toulmin's model of argument-can be applied to improve their quality. To understand how compilers should present explanations to developers, we conducted a comparative evaluation with 68 professional software developers and an empirical study of compiler error messages found in Stack Overflow questions across seven different programming languages. Our findings suggest that, given a pair of error messages, developers significantly prefer the error message that employs proper argument structure over a deficient argument structure when neither offers a resolution-but will accept a deficient argument structure if it provides a resolution to the problem. Human-authored explanations on Stack Overflow converge to one of the three argument structures: Those that provide a resolution to the error, simple arguments, and extended arguments that provide additional evidence for the problem. Finally, we contribute three practical design principles to inform the design and evaluation of compiler error messages. 
id231#Impact of energy efficiency on the morphology and behaviour of evolved robots#Most evolutionary robotics studies focus on evolving some targeted behavior without considering energy usage. In this paper, we extend our simulator with a battery model to take energy consumption into account in a system where robot morphologies and controllers evolve simultaneously. The results show that including the energy consumption in the fitness in a multi-objective fashion reduces the average size of robot bodies while reducing their speed. However, robots generated without size reduction can achieve speeds comparable to robots from the baseline set. 
id232#Light-weight UAV object tracking network based on strategy gradient and attention mechanism#Most existing object tracking methods have poor adaptability to complex scenes, and cannot achieve a good balance between tracking accuracy and real-time performance. To solve the above problems, this paper proposes a lightweight UAV object real-time tracking algorithm based on strategy gradient and attention. Firstly, a lightweight E-Mobile Net is designed as the backbone network of feature extraction; secondly, a feature enhanced attention assistant module is designed to enhance the adaptability and discrimination ability of the model; with multi-layer feature fusion regional suggestion network, foreground background classification and boundary box regression response map are obtained by cross-correlation, and the tracking results are calculated. The strategy network based on strategy gradient is used to optimize the template update and re detection strategy, which improves the overall tracking accuracy and efficiency of the model. Simulation experiments on an embedded device and multiple standard data sets show that compared with the current mainstream algorithms, the tracking accuracy is significantly improved 20%∼30%, the algorithm robustness also has obvious advantages, and the tracking speed on an embedded device is 56 fps can meet the real-time requirements. 
id233#Applying unifying theories of programming to real-time programming#"This paper introduces an approach to verify the correctness of the implementation of real-time languages. We apply the techniques presented in Hoare and He's ""Unifying Theories of Programming"" to reason about the correctness of compilers and schedulers for real-time languages, using high-level abstractions such as algebraic laws. In the compilation process, the existence of unique fixed-points is exploited to verify the implementation of crucial real-time operators such as asynchronous input, delay and timeout. It is developed an abstract model for scheduling real-time programs into a uniprocessor machine. The applicability of the model is shown by instancing it with two types of schedulers: a round-robin scheduler, employed when the participating parallel processes do not include deadline constraints, and a priority-based scheduler, used when each participating process is periodic and possesses an associated deadline."
id234#The Performance of Geoinformation Software for Fire Weather Forecast#The survey suggests targeted geoinformation software for rating and predicting fire danger for vegetation caused by natural and natural-Anthropogenic forcers that have been observed for a long time. The study applies weather fire danger indices represented by Nesterov and their varieties in the territory of the Russian Federation. The MySQL 5 open-source relational database management system is used for data storing/holding. Spatial processing and data verification are carried out using a verification tool called MapInfo Professional geographic information system and Delphi 2010 integrated development environment. A series of electronic maps was constructed during a fire season according to computer experiments in order to verify the reliability of the fire danger forecast in spring-And-fall and summer-Time, calculate statistical criteria rates, assess a permissible error in the forecast of weather effects and a probability of forest fires. The values obtained are reported in the paper. The authors suggest the criteria and methods for assessing and predicting the forest fire danger according to Nesterov index with various charts for using actual and forecasted weather data. The researchers also determine system requirements for the geoinformation software; develop the structure of a multidimensional relational database and processing algorithms of meteorological data and vegetation fire data. The authors also spot probable fires in the centers of forestry areas in the territory of the Jewish Autonomous Region every day within the fire danger season. The authors found out a high confident present day forecast and the one for the next day according to the short-Term forecast available in the whole territory of the Russian Federation. 
id237#Copyright protection scheme for color images using extended visual cryptography#Most of the existing visual cryptography based copyright protection schemes create random looking shares which can create suspicion of some secret information being shared. In order to handle this issue, a copyright protection scheme based on curvelet transform and extended visual cryptography is proposed for color images. Robustness of the scheme against various attacks is achieved by using inter-layer low and middle frequency coefficients obtained by applying discrete curvelet transform on R,G, and B components of host image. The curvelet inter-layer coefficients are used to create a master share. This master share and the watermark are used to create the ownership share by using the codebook. XOR-superimposition of master and ownership shares retrieve the watermark to prove the copyright. Security of the scheme is achieved by creating meaningful ownership shares; and by using Baker's Map for scrambling watermark and transformed host image. The scheme is robust to withstand several image processing attacks as well as provides better imperceptibility. Effectiveness of the scheme is shown by comparing it with the existing copyright protection schemes. 
id238#Computer Vision Based Two-stage Waste Recognition-Retrieval Algorithm for Waste Classification#The main objective of this study is to classify domestic waste via computer vision and sort it automatically according to the four-category regulation. A novel two-stage Waste Recognition-Retrieval algorithm (W2R) is proposed. Its first stage was to train a Recognition Model (RegM) recognizing waste into one of thirteen subcategories. The second stage was to construct the Recognition-Retrieval Model (RevM) classifying the recognized subcategory into one of four categories. Meanwhile, a one-stage waste Classification Model (ClfM) was trained as a comparison. Both best-performing models were selected and installed respectively onto the automatic sorting machine for contrast experiment classifying a set of waste. The machine consisted of three main modules: the Computer-Vision Module, the Sorting Module, and the Customized Module. It was also a platform for data collection. Ten participants also classified and sorted the same set of waste in the experiment of Manual Sorting (MS). The experimental results show that the average accuracy of the RevM, 94.71% ± 1.69, was significantly higher than that of the ClfM-VGG, 69.66% ± 3.43, and that of the MS, 72.50% ± 11.37. 
id240#Register file partitioning and compiler support for reducing embedded processor power consumption#Register file (RF) in modern embedded processors contributes a substantial budget in the energy consumption due to its large switching capacitance and long working time. For embedded processors, on average 25% of registers count for 83% of RF accessing time. This motivates us to partition the RF into hot and cold regions, with the most frequently used registers placed in the hot region, and the rarely accessed ones in the cold region. We employ the techniques of bit-line splitting and drowsy register cell to reduce the overall accessing power of RF. We propose a novel approach to partition the RF in a way that can achieve the largest power saving. We formulate the RF partitioning process into a graph partitioning problem, and apply an effective algorithm to obtain the optimal result. We evaluate our algorithm on MiBench and SPEC2000 applications, and an average saving of 58.3% and 54.4% over the non-partitioned RF accessing power is achieved for the SimpleScalar PISA system, respectively. The area overhead is negligible, and the execution time overhead is acceptable. 
id241#Review of vision-based occupant information sensing systems for occupant-centric control#Vision-based (camera-based) systems, which can effectively sense occupant information, have garnered attention as a core technology in the Fourth Industrial Revolution. A detailed understanding of vision-based sensing systems is required to detect occupant information based on vision and use it for occupant-centric control. Therefore, in this study, we performed a comprehensive and structural literature review of vision-based occupant information systems. The contributions of this review can be summarized in the following six points: (1) a five-tier taxonomy of vision-based occupant information is proposed, (2) a systematic summary of vision-based occupant information is presented, (3) the quantitative and qualitative performance of sensing systems is reviewed, (4) an analysis of the applicability of deep-learning-based computer vision techniques is presented, (5) a summary of privacy-preserving techniques is included, and (6) a summary of vision-based control strategies and energy saving potential analysis is provided. The analysis in this review is an important contribution toward addressing the challenges in the field of research. 
id242#An enhanced scalable and secure RFID authentication protocol for WBAN within an IoT environment#Nowadays, Internet of Things (IoT)-based E-healthcare represents an emergent research field due to the fast development of wireless technologies and cloud computing. Radio Frequency Identification (RFID) is an integral technology in IoT thanks to its low cost and autonomous data collection and transfer. These features made it useful in Wireless Body Area Network (WBAN) for healthcare applications. However, data security and patient privacy remain major challenges in WBANs. In this context, many authentication protocols have been designed trying to satisfy both security and implementation requirements. Most recently, Naeem et al. have proposed an RFID authentication scheme for IoT which is claimed to be secure and provides scalability. Unfortunately, we have found that their protocol does not provide authentication and anonymity and it is vulnerable to numerous attacks. To overcome these security issues, we propose, in this paper, an efficient extended and improved IoT-based RFID authentication scheme for WBANs. Our proposed protocol could resist to various attacks and ensure mutual authentication from the tag to the medical server, in addition to patients data security. For this, elliptic curve cryptography (ECC) encryption mechanism and elliptic curve digital signature with message recovery (ECDSMR) have been adopted. Formal and informal analysis have proved that our proposed protocol succeeded to provide many security features and offer reliable data security with a considerably small computational and storage cost compared to existing schemes. 
id245#Cybersecurity in the Quantum Era-A Study of Perceived Risks in Conventional Cryptography and Discussion on Post Quantum Methods#Information security in communication networks is a persistent problem and essentially requires the usage of encryption methods. Quantum computing was first used to break encryption codes in the latter half of the 20th century with the introduction of the SHOR algorithm. Though the recent developments in QC capabilities have increased confidentiality, integrity, and availability of networks by protecting them against passive attacks like eavesdropping yet the transformation of classical to quantum computation can bear catastrophic implications as it has the potential to put the currently secure methods of transactions in jeopardy. This paper aims at the evaluation and comparison of traditional cryptographic techniques by the application of a SWOT framework. It takes up an exploratory study of the advanced quantum computing capabilities that can pose a massive risk to network security. The various security enhancements that can be adopted in data transmission to curtail these risks post-quantum are also discussed. 
id246#A secure and efficient group signature scheme based on multivariate public key cryptography#Group signatures are significant primitive for anonymity, which allow group members to sign messages while hiding in the group, however, the signers remain accountable. Most of the existing schemes on group signature are relying on traditional cryptographic primitives, whereas rapid advancements in quantum computing suggest an originating threat to usual cryptographic primitives. This makes the necessity of quantum computer resistant cryptographic primitives. Multivariate public key cryptography (MPKC) is one of the promising options that may withstand quantum attacks. Its constructions are potential candidates for post-quantum (PQ) cryptography as they are very fast and require only modest computational resources. There are many existing secure and practical multivariate digital signatures. However, there is a deficiency of more advanced multivariate group signature scheme. The existing multivariate group signature has weaknesses in terms of security and efficiency. This paper introduces a new multivariate group signature scheme employing a 5-pass identification protocol and multivariate signature scheme as its building blocks. The proposed signature scheme possesses unforgeability, user's anonymity, unlinkability, exculpability and traceability property. Unlike most of the existing post-quantum group signatures, the sizes of the signatures and the public parameters are not dependent on the number of group users in our construction. It depends only on the security parameters. In particular, our construction is the first MPKC based group signature, where signature size and public parameter size are independent of the number of group users. 
id247#Unified phase compiler by use of 3-D representation space#A novel unified phase compiler framework for embedded VLIWs and DSPs is shown. In this compiler, a given program is represented in 3-D representation space, which enables quantitatively estimating required resources and elapsed time. Transformation of a 3-D representation graph that corresponds to a code optimization method for a specific processor architecture is also proposed. The proposal compiler and the code optimization methods are compared with an ordinary compiler in terms of their generated codes. The results demonstrate their effectiveness. Copyright 
id248#Database generator to support product derivation in SPL#Software product line (SPL) is a methodology to develop application variants with feature variability to accommodate user needs in a specific market. This methodology can be supported by the Abstract Behavioral Specification (ABS) modeling language. With the use of delta-oriented programming (DOP) in ABS, the development process to produce application variants in SPL can be automated. However, feature variability of an application variant could affect the design and implementation of the database schema of the application during product derivation phase in SPL, if the feature is related to the data storage of the application. This crucial thing is not handled by the management technique of variability in SPL which makes database schema might be inconsistent and incompatible with the application requirements. In this study, we successfully implement a tool to generate a compatible relational database schema for an application variant. This tool has also been adopted to support the automation process of our working product line for charity organizations. 
id249#Topology of parametrized motion planning algorithms#We introduce and study a new concept of parametrized topological complexity: A topological invariant motivated by the motion planning problem of robotics. In the parametrized setting, a motion planning algorithm has a high degree of universality and flexibility and can function under a variety of external conditions (such as positions of obstacles). We explicitly compute the parametrized topological complexity of obstacle-avoiding collision-free motion of many particles (robots) in 3-dimensional space. Our results show that the parametrized topological complexity can be significantly higher than the standard (nonparametrized) invariant. 
id251#Black hole Entropic Fuzzy Clustering-based image indexing and Tversky index-feature matching for image retrieval in cloud computing environment#Due to the expansion of social websites, the data owner accumulates multimedia data and stores in cloud server. The owner encrypts the images before uploading it in cloud server for security. However, the conventional encryption method failed to support feasible retrieval on the encrypted images. This paper proposes novel technique, namely Black Hole Entropic Fuzzy Clustering-based Tversky index for effective image retrieval. Here, the SLBP (Spatial Local Binary Pattern) features, semantic features, statistical features, and low image features are considered in the feature extraction. In addition, encryption of feature vector using Elliptic Curve Cryptography (ECC) is done for encrypting the images contained in cloud server. The Black Hole Entropic Fuzzy Clustering (BHEFC) is adapted for grouping the images. Whenever users request an input query then, the query image is fed to feature extraction, and encryption phase, wherein the Tversky index is applied for matching the similarity between the images for retrieval. The proposed BHEFC-based Tversky index outperformed other methods with maximal accuracy of 95.737%, maximal precision of 83.563%, maximal recall of 94.697%, and maximal F-measure of 83.014%. 
id252#Improving Participation and Learning of Compiler Theory Using Educational Simulators#Due to the fact that compiler theory is pretty abstract, students' interest in learning the topics of this course decreases. In order to better prepare students for this challenge, a new approach to teaching and practicing compilers is necessary. The new approach should be complementary to existing curricula. This paper presents two educational interactive tools for visual representation of lexical and syntax analysis. It describes the way of applying simulation tools in order to improve the teaching process of the compiler construction course. These software systems evaluations were performed by checking the correctness of the implemented functionalities and examining the user experience by a quantitative student survey. 
id253#Article towards lifespan automation for caenorhabditis elegans based on deep learning: Analysing convolutional and recurrent neural networks for dead or live classification#The automation of lifespan assays with C. elegans in standard Petri dishes is a challenging problem because there are several problems hindering detection such as occlusions at the plate edges, dirt accumulation, and worm aggregations. Moreover, determining whether a worm is alive or dead can be complex as they barely move during the last few days of their lives. This paper proposes a method combining traditional computer vision techniques with a live/dead C. elegans classifier based on convolutional and recurrent neural networks from low‐resolution image se-quences. In addition to proposing a new method to automate lifespan, the use of data augmentation techniques is proposed to train the network in the absence of large numbers of samples. The proposed method achieved small error rates (3.54% ± 1.30% per plate) with respect to the manual curve, demonstrating its feasibility. 
id254#Smart solution for pain detection in remote rehabilitation#In this article, we present a low cost open source robotic solution for remote rehabilitation. In a pandemic context, where the trend is to manage our services remotely, the proposed system guarantees that the physiotherapist remotely controls and supervises the rehabilitation process. In the developed solution, it is not necessary to directly touch either the subject or the system, since the proposed architecture ensures control of the device remotely and using only a simple gesture. According to experts, pain is considered one of the major constraints that prevent patients from easily accepting rehabilitation sessions. To our knowledge, there is no method or tool presented in the literature to measure pain. The proposed solution aims to estimate the pain then to manage the control of the robot according to this estimate. In fact, the idea is to estimate pain through a decision support system based on cascading fuzzy logic. The first compartment of this fuzzy system was used to estimate muscle contact based on the EMG signal. The second compartment was set up to estimate pain according to three parameters: muscle contraction, the patient's resistance force and the last angle reached. A user-friendly human machine interface (HMI) has been developed to ensure communication between the robot and the physiotherapist. The control is done by gestures taking advantage of the Kinect technology. Sensory information about strength, range of motion (RoM) and estimated pain level is communicated to the physiotherapist's board. Patient information, health changes and tracked activities are all recorded, providing an electronic health record. Experimental studies carried out on a set of patients in a clinical environment have shown the efficiency and reliability of the developed robotic solution. 
id255#Software support for heterogeneous computing#Heterogeneous computing, materialized in the form of multiprocessor system-on-chips (MPSoC) comprising of various processing elements such as general-purpose cores with differing characteristics, GPUS, DSPs, non-programmable accelerators, and reconfigurable computing, are expected to dominate the current and the future consumer device landscape. The heterogeneity enables a computational kernel with specific requirements to be paired with the processing element(s) ideally suited to perform that computation, leading to substantially improved performance and energy-efficiency. While heterogeneous computing is an attractive proposition in theory, considerable software support at all levels is essential to fully realize its promises. The system software needs to orchestrate the different on-chip compute resources in a synergistic manner with minimal engagement from the application developers. We present compiler time and runtime techniques to unleash the full potential of heterogeneous multi-cores towards high-performance energy-efficient computing on consumer devices. 
id256#Area-Time Efficient Hardware Architecture for Signature Based on Ed448#In this brief, we proposed a highly-optimized FPGA-based implementation of the Ed448 digital signature algorithm. Despite significant progress in elliptic curve cryptography (ECC) implementations, Ed448 hardware architecture, to the best of our knowledge, has not been investigated in the literature. In this work, we demonstrate a high throughput while maintaining low resource architecture for Ed448 by employing a new combined algorithm for refined Karatsuba-based multiplier with precise scheduling. Furthermore, a compact distributed memory unit is developed to increase speed while keeping the area low. Our variable-base-point Ed448 architecture performs 327 signatures and 189 verifications per second at a notably higher security level of 224 bits, using not more than 6,617 Slices and 16 DSPs on a Xilinx Artix-7 FPGA. We also proposed possible countermeasures and extensions to Ed448 to counter the physical attacks. 
id257#Security analysis of reversible logic cryptography design with LFSR key on 32-bit microcontroller#"This paper presents a detailed security analysis of the research article on the digital image encryption scheme entitled ""Reversible Logic Cryptography Design (RLCD) with Linear Feedback Shift Register (LFSR) key"" (Karunamurthi S, and Natarajan VK, Microprocessors and Microsystems, 2019). Although the inadequate length of its 4-bit LFSR key makes the scheme extremely vulnerable to quick brute force attack, analyzing the various error metrics concerning the security of the encrypted images, this scheme provides statistically pleasing results. The major shortcoming identified on this RLCD-LFSR scheme is the traceable patterns that appear on its encrypted images due to the absence of confusion to break the pixels' correlation. In addition to the chosen plaintext attack, edge detection based cryptanalysis proposed in this paper to be sufficient to crack the RLCD-LFSR scheme. The enhancement made by the insertion of a confusion module in RLCD-LFSR scheme wipes out the perceptible patterns and edges from the encrypted images to resist the attacks. The failure of enhanced RLCD-LFSR under NIST tests confirms the flaws in the design of the Reversible Logic Gate (RLG) based diffusion process and its ineffectiveness for image encryption. Besides the security analysis, the performance of RLCD-LFSR scheme and the proposed improved version of the same is implemented on a 32-bit microcontroller to evaluate their suitability for real-time embedded applications. "
id258#Two-stage deep learning model for fully automated pancreas segmentation on computed tomography: Comparison with intra-reader and inter-reader reliability at full and reduced radiation dose on an external dataset#Purpose: To develop a two-stage three-dimensional (3D) convolutional neural networks (CNNs) for fully automated volumetric segmentation of pancreas on computed tomography (CT) and to further evaluate its performance in the context of intra-reader and inter-reader reliability at full dose and reduced radiation dose CTs on a public dataset. Methods: A dataset of 1994 abdomen CT scans (portal venous phase, slice thickness ≤ 3.75-mm, multiple CT vendors) was curated by two radiologists (R1 and R2) to exclude cases with pancreatic pathology, suboptimal image quality, and image artifacts (n = 77). Remaining 1917 CTs were equally allocated between R1 and R2 for volumetric pancreas segmentation [ground truth (GT)]. This internal dataset was randomly divided into training (n = 1380), validation (n = 248), and test (n = 289) sets for the development of a two-stage 3D CNN model based on a modified U-net architecture for automated volumetric pancreas segmentation. Model’s performance for pancreas segmentation and the differences in model-predicted pancreatic volumes vs GT volumes were compared on the test set. Subsequently, an external dataset from The Cancer Imaging Archive (TCIA) that had CT scans acquired at standard radiation dose and same scans reconstructed at a simulated 25% radiation dose was curated (n = 41). Volumetric pancreas segmentation was done on this TCIA dataset by R1 and R2 independently on the full dose and then at the reduced radiation dose CT images. Intra-reader and inter-reader reliability, model’s segmentation performance, and reliability between model-predicted pancreatic volumes at full vs reduced dose were measured. Finally, model’s performance was tested on the benchmarking National Institute of Health (NIH)-Pancreas CT (PCT) dataset. Results: Three-dimensional CNN had mean (SD) Dice similarity coefficient (DSC): 0.91 (0.03) and average Hausdorff distance of 0.15 (0.09) mm on the test set. Model’s performance was equivalent between males and females (P = 0.08) and across different CT slice thicknesses (P > 0.05) based on noninferiority statistical testing. There was no difference in model-predicted and GT pancreatic volumes [mean predicted volume 99 cc (31cc); GT volume 101 cc (33 cc), P = 0.33]. Mean pancreatic volume difference was −2.7 cc (percent difference: −2.4% of GT volume) with excellent correlation between model-predicted and GT volumes [concordance correlation coefficient (CCC)=0.97]. In the external TCIA dataset, the model had higher reliability than R1 and R2 on full vs reduced dose CT scans [model mean (SD) DSC: 0.96 (0.02), CCC = 0.995 vs R1 DSC: 0.83 (0.07), CCC = 0.89, and R2 DSC:0.87 (0.04), CCC = 0.97]. The DSC and volume concordance correlations for R1 vs R2 (inter-reader reliability) were 0.85 (0.07), CCC = 0.90 at full dose and 0.83 (0.07), CCC = 0.96 at reduced dose datasets. There was good reliability between model and R1 at both full and reduced dose CT [full dose: DSC: 0.81 (0.07), CCC = 0.83 and reduced dose DSC:0.81 (0.08), CCC = 0.87]. Likewise, there was good reliability between model and R2 at both full and reduced dose CT [full dose: DSC: 0.84 (0.05), CCC = 0.89 and reduced dose DSC:0.83(0.06), CCC = 0.89]. There was no difference in model-predicted and GT pancreatic volume in TCIA dataset (mean predicted volume 96 cc (33); GT pancreatic volume 89 cc (30), p = 0.31). Model had mean (SD) DSC: 0.89 (0.04) (minimum–maximum DSC: 0.79 −0.96) on the NIH-PCT dataset. Conclusion: A 3D CNN developed on the largest dataset of CTs is accurate for fully automated volumetric pancreas segmentation and is generalizable across a wide range of CT slice thicknesses, radiation dose, and patient gender. This 3D CNN offers a scalable tool to leverage biomarkers from pancreas morphometrics and radiomics for pancreatic diseases including for early pancreatic cancer detection. 
id259#Optimal superblock scheduling using enumeration#The superblock is a scheduling region that is used by compilers for exploiting instruction-level parallelism across basic blocks. Many heuristic techniques have been proposed for solving this difficult scheduling problem, but none accurately approximates the optimal solution. This paper presents a new technique that finds provably optimal solutions to superblock scheduling problems. The technique is based on reducing the problem of finding branch combinations that yield incrementally increasing weighted execution times to a subset-sum problem, which is solved by dynamic programming. An enumerative approach that employs a number of powerful pruning techniques to efficiently explore the solution space is then used to search for a feasible schedule for each branch combination. Experimental evaluation using the SPEC CPU fp2000 and int2000 benchmarks shows that, within a per-problem time limit of one second, this combination of dynamic programming and enumeration optimally solves about 99% of the hard superblock scheduling problems with an average solution time of 9 milliseconds per problem. For 80% of the hard problems, the optimal schedule is improved compared to the schedule produced by an established heuristic technique. 
id260#Language engineering in the context of a popular, inexpensive robot platform#Language engineering - the theory and practice of building language processors and compilers, has long been recognized as important subject in Computer Science curricula. However, due to lack of suitable target systems, educators face significant challenges to teach language engineering classes effectively. Leveraging the emerging inexpensive robot devices, this paper presents a new approach of using robots as system context to teach language engineering topics. We designed the Chirp-Scribbler Language, which targets the popular Scribbler robot; combined together, they provide an engaging and feature-rich platform to teach a wide range of topics in language engineering. This paper describes the Chirp-Scribbler Language, its integration with the target robot, and the teaching practice of using them to teach language translation basics in an undergraduate programming course.
id261#Roofpedia: Automatic mapping of green and solar roofs for an open roofscape registry and evaluation of urban sustainability#Sustainable roofs, such as those with greenery and photovoltaic panels, contribute to the roadmap for reducing the carbon footprint of cities. However, research on sustainable urban roofscapes is rather focused on their potential and it is hindered by the scarcity of data, limiting our understanding of their current content, spatial distribution, and temporal evolution. To tackle this issue, we introduce Roofpedia, a set of three contributions: (i) automatic mapping of relevant urban roof typology from satellite imagery; (ii) an open roof registry mapping the spatial distribution and area of solar and green roofs of more than one million buildings across 17 cities; and (iii) the Roofpedia Index, a derivative of the registry, to benchmark the cities by the extent of sustainable roofscape in term of solar and green roof penetration. This project, partly inspired by its street greenery counterpart ‘Treepedia’, is made possible by a multi-step pipeline that combines deep learning and geospatial techniques, demonstrating the feasibility of an automated methodology that generalises successfully across cities with an accuracy of detecting sustainable roofs of up to 100% in some cities. We offer our results as an interactive map and open dataset so that our work could aid researchers, local governments, and the public to uncover the pattern of sustainable rooftops across cities, track and monitor the current use of rooftops, complement studies on their potential, evaluate the effectiveness of existing incentives, verify the use of subsidies and fulfilment of climate pledges, estimate carbon offset capacities of cities, and ultimately support better policies and strategies to increase the adoption of instruments contributing to the sustainable development of cities. 
id262#Implementation of Scale Invariant Feature Transform detector on FPGA for low-power wearable devices for prostheses control#In this paper we describe an FPGA implementation of the Scale Invariant Feature Transform (SIFT) algorithm. The FPGA is required as its a lightweight device which makes it ideal for vision-guided hybrid neuro-prostheses utilised for upper limbs replacement. SIFT point detection is needed for computation of coordinates of the object-to-grasp in a wearable multi-camera system. A modified SIFT algorithm is proposed and an implementation of it into C/C++ language on Xilinx ZCU102 FPGA board. The proposed hybrid hardware/software solution is compared to other hardware or hybrid implementations of the SIFT algorithm and with the baseline software detector OpenSIFT. The algorithm optimised for FPGA gives an average precision of 0.84 and the average recall of 0.94 in SIFT-point detection compared to the baseline. The proposed solution has lower dissipated power than other solutions like CPU or GPU, and it has better computational speed. This solution allows for processing medium-sized images in real-time with low power consumption. 
id263#An Efficient Privacy-Preserving ID Centric Authentication in IoT Based Cloud Servers for Sustainable Smart Cities#Smart cities or Smart societies require Internet of Things (IoT), for connecting numerous devices to enormous asset pools in cloud computing. This coordination of embedded tools plus cloud servers conveys the extensive applicability of IoT in Smart Cities. However, authentication and data protection, play a major job in secure coordination of these two technologies. Considering this, in 2017, Chang et al.s system introduced famous verification system dependent on an elliptic curve cryptography (ECC) for IoT plus cloud servers for Smart Cities and guaranteed that it fulfills need of security protocols and is safe to different sorts of assaults. Nevertheless, in this paper, we demonstrate that Chang et.al. system is defenseless to a privileged insider intrusion, server impersonation intrusion, known session-specific information intrusion and offline password guessing intrusion. In addition, it does not accomplish device anonymity and mutual authentication. Considering this weakness of existing system, we propose an authentication system dependent on ECC for IoT and cloud servers in Smart Cities. The suggested system accomplishes mutual authentication and supports fundamental safety necessities. The informal security examination, performance analysis and contrast of the suggested system with existing systems prove that the suggested method is powerful, effective and stout as a counter to manifold security threats faced by Smart Cities. The formal confirmation of the suggested procedure is performed by AVISPA tools, which affirms its safety strength within the sight of a conceivable invader. 
id264#Review on Quantum Communication and Quantum Computation#Quantum communication has made great breakthroughs in recent years. Because of its characteristics for strict information security transmission and high speed, it has received attention from related research fields around the world. This paper briefly reviews the progress in the field of quantum computing and communication. The basic functions of quantum mechanics related to the quantum communication are first introduced, including qubit, logic gates, postulates, polarization and quantum entanglement. Then the applications of quantum communication are discussed including teleportation, cryptography and quantum networks. Finally, advantages and disadvantages for the current applications are analyzed and the challenges remained in the current research are demonstrated. 
id265#Opportunities for big data analytics in healthcare information systems development for decision support#Nowadays, an enormous volume of heterogeneous healthcare and medical data are generated routinely. These heterogeneous data have to be integrated and stored in a standard manner and format to perform appropriate big data analysis and visualization and improve decision-making. These data are generated from different sources such as mobile devices, sensors, national public health institutions, laboratory tests, clinical notes, social media, and various omics data that can be structured, semi-structured or unstructured. These data structure varieties necessitate these big data to be stored not only in the relational databases but also in NoSQL databases. To provide effective data analysis, besides the application of appropriate data mining techniques, excellent design and implementation of healthcare information systems are needed. These software solutions have to solve patient data security and privacy issues by employing proper big data governance policies. The design and implementation of healthcare information knowledge-based systems should provide to the patients more well-organized and economical healthcare services, and on the other hand, a boosted knowledge-based basis for decision-making to the managers in healthcare institutions and insurance companies and benefits for the all involved stakeholders. In this paper, we overview and suggest suitable development framework that will cover patient-, clinical- and population oriented approaches to decision-making and to reveal valuable knowledge and insights from these healthcare and medical big data. Moreover, on specific occasions, this knowledge should enable a rapid and reliable response to the healthcare hazards and help to decision-makers worldwide as well on the national level. Copyright 
id266#Towards an interface for translating natural language questions to SQL: A conceptual framework from a systematic review#Querying relational databases using human language is a complex process that must take into consideration the lack of knowledge of SQL by the end users who use the content of these systems. Many studies published in the last decade are based on limited, unbalanced or even small datasets, thing that is by far not realistic for training and evaluating a model. The SQL structure is rich and the scale of complexity is very large. Moreover, the difference between the natural language, which is a language of communication, and the SQL language that contains details of implementation specific to relational databases, adds more complications to the problem. In this paper, we present a full systematic review of the existing approaches in the task of text to SQL and we introduce the details of our suggested architecture as well as our new approach to tackle the problem of nested queries. Copyright 
id267#A Multi-Contact-Aided Continuum Manipulator with Anisotropic Shapes#Cable-driven continuum manipulators have shown excellent benefits to work for endoluminal intervention. Demand of small diameter for confined anatomy limits the usage of multiple actuations, leading to limited DOFs and bending shapes. To address this problem, this letter proposes a multi-contact-aided continuum manipulator with anisotropic bending shapes. First, contact-aided compliant mechanisms (CCMs) are configured at different locations to introduce the specific constraints to enable the anisotropy. Then, the forward and inverse kinematic models considering contact blocks are built. The reachable workspace of the continuum manipulator is given, and simulation cases are studied to demonstrate the superiority of the proposed manipulator. Finally, a 3D-printed physical prototype is fabricated, and preliminary experiments are conducted to evaluate the model accuracy. Results show an average shape error of 1.98 mm (4.2%) and a maximum of 3.58 mm (7.6%). A robotic bronchoscopy platform integrated with the continuum manipulator is developed to conduct the airway phantom experiments. The shape deviation of the proposed manipulator from the centerline of the bronchus is 31.8% smaller than that of the constant curvature manipulator. The experiments validate the system-level feasibility and effectiveness of the developed continuum manipulator. 
id268#FPGA design of elliptic curve cryptosystem (ECC) for isomorphic transformation and EC elgamal encryption#Attacking or tampering with sensitive data continues to increase risks to economic processes or human activities. These risks are significant key factors to improve the development and implementation of security systems. Therefore, improving cryptography is essentially needed to enhance the security of critical data. For example, elliptic curve cryptography (ECC) over the Galois field GF(2163) is one of the public-key (asymmetric) cryptographic techniques, in which demands mapping a message (163-bit) to a point in the prime subgroup of the elliptic curve. To the best of our knowledge, mapping methods are not yet available on Field-Programmable Gate Arrays (FPGAs). Also, asymmetric encryption schemes often do not consider encrypting/decrypting data packets because of their computation complexity and performance limitations. In this letter, we propose and develop a concurrent reconfigurable cryptosystem to encrypt and decrypt stream of data using ECC on FPGA. First, we present hardware design and implementation to map a plain message on the elliptic curve based on isomorphic transformation, then second, we architect the elliptic curve ElGamal public-key encryption method by using point addition and multiplication on Koblitz elliptic curve on FPGA. Our proposed cryptosystem is synthesized and implemented on Intel Cyclone 10 GX and Xilinx Kintex-7 FPGAs to evaluate throughput, and it achieves 25.73-57.1 Mbps. 
id269#Towards machine recognition of facial expressions of pain in horses#Automated recognition of human facial expressions of pain and emotions is to a certain degree a solved problem, using approaches based on computer vision and machine learning. How-ever, the application of such methods to horses has proven difficult. Major barriers are the lack of sufficiently large, annotated databases for horses and difficulties in obtaining correct classifications of pain because horses are non-verbal. This review describes our work to overcome these barriers, using two different approaches. One involves the use of a manual, but relatively objective, classification system for facial activity (Facial Action Coding System), where data are analyzed for pain expressions after coding using machine learning principles. We have devised tools that can aid manual labeling by identifying the faces and facial keypoints of horses. This approach provides promising results in the automated recognition of facial action units from images. The second approach, recurrent neural network end-to-end learning, requires less extraction of features and representations from the video but instead depends on large volumes of video data with ground truth. Our preliminary results suggest clearly that dynamics are important for pain recognition and show that combinations of recurrent neural networks can classify experimental pain in a small number of horses better than human raters. 
id270#A novel algorithm for callgraph traversal in cross-module compilation based on graph theory#Due to limitation of compile time and resource, traditional compilers generally run with functions or procedures in applications. Cross-Module interprocedural optimization (IPO) is an effective compilation for exploiting performance opportunities. And alternative IPO techniques have been proposed to decrease compilation consumptions including partial compilations, transformations of IR files and link-time IR. This paper presents a novel algorithm for callgraph traversal in cross-module IPO framework. The new algorithm tends to find an optimized traversal path which cuts down the cost of file access based on flow information of global callgraph during IPO. Firstly, calling flows are transformed into priority hierarchy in candidate queue. Then a locally optimal choice is selected in hope to obtain a globally optimal solution with greedy strategy to get minimum-spanning tree. Experiments show the proposed algorithm achieves 49% decrement on compile time with regular memory cost. Copyright 
id271#Towards security recommendations for public-key infrastructures for production environments in the post-quantum era#Quantum computing technologies pose a significant threat to the currently employed public-key cryptography protocols. In this paper, we discuss the impact of the quantum threat on public key infrastructures (PKIs), which are used as a part of security systems for protecting production environments. We analyze security issues of existing models with a focus on requirements for a fast transition to post-quantum solutions. Although our primary focus is on the attacks with quantum computing, we also discuss some security issues that are not directly related to the used cryptographic algorithms but are essential for the overall security of the PKI. We attempt to provide a set of security recommendations regarding the PKI from the viewpoints of attacks with quantum computers. 
id272#Online Learning of Parameterized Uncertain Dynamical Environments with Finite-Sample Guarantees#We present a novel online learning algorithm for a class of unknown and uncertain dynamical environments that are fully observable. First, we obtain a novel probabilistic characterization of systems whose mean behavior is known but which are subject to additive, unknown subGaussian disturbances. This characterization relies on recent concentration of measure results and is given in terms of ambiguity sets. Second, we extend the results to environments whose mean behavior is also unknown but described by a parameterized class of possible mean behaviors. Our algorithm adapts the ambiguity set dynamically by learning the parametric dependence online, and retaining similar probabilistic guarantees with respect to the additive, unknown disturbance. We illustrate the results on a differential-drive robot subject to environmental uncertainty. 
id273#Geometric region-based swarm robotics path planning in an unknown occluded environment#This article presents a geometrical region-based shape control methodology for navigating a cohesive swarm-robotic structure toward the goal even in a field occluded by unknown obstacles. In this control approach, initially, the robotic swarm is conceived to lie within a well-defined virtual circular region thus preserving a strict interagent cohesiveness among them. However, during the progression, for evading severely constricted obstacles, the virtual circle has been allowed to change its shape and in the process, varied elliptical shapes are made to evolve. In essence, for a collision-free solution, this shrinking aspect (from circle to ellipse) depends entirely on the number of agents in the swarm and at the same time also reliance on the sensed distance between two nearest obstacles through which the shrunken circle or the virtual ellipse will be able to pass. Consequently, shape switching is a dynamic as well as a stochastic process throughout the journey of the swarm. For achieving these objectives, a two-level hierarchical control strategy has been employed. Moreover, during aggregating toward the target, the actuation failure of any agent or agents may occur. In this perspective, the proposed control law has been updated adaptively throughout the route such that agent failure does not encumber the mission. Finally, the extensive simulation results along with the hardware experimentation are provided to demonstrate the efficacy of the proposed scheme. 
id274#OptQC v1.3: An (updated) optimized parallel quantum compiler#We present a revised version of the OptQC program of Loke et al. (2014) [1]. We have removed the simulated annealing process in favour of a descending random walk. We have also introduced a new method for iteratively generating permutation matrices during the random walk process, providing a reduced total cost for implementing the quantum circuit. Lastly, we have also added a synchronization mechanism between threads, giving quicker convergence to more optimal solutions. New version program summary Program title: OptQC v1.3 Catalogue identifier: AEUA_v1_3 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEUA_v1_3.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 240903 No. of bytes in distributed program, including test data, etc.: 632395 Distribution format: tar.gz Programming language: Fortran, MPI. Computer: Any computer with Fortran compiler (not gfortran4.9 or earlier) and MPI library. Operating system: Linux. Classification: 4.15. Catalogue identifier of previous version: AEUA_v1_3 Journal reference of previous version: Comput. Phys. Comm. 185(2014)3307 External routines: Intel MKL LAPACK routines and MPI routines. Does the new version supersede the previous version?: Yes Nature of problem: It aims to minimize the number of quantum gates required to implement a given unitary operation. Solution method: It utilizes a descending random walk to select permutation matrices P and Q for a given unitary matrix U such that the number of gates in the quantum circuit of U=QTPTU'PQ is minimized, where U' is equivalent to U up to a permutation. The decomposition of a unitary operator is performed by recursively applying the cosine–sine decomposition. Reasons for new version: Simulated annealing process was found to give suboptimal results compared to a normal descending random walk. Computation time was also bloated by the necessity of running the CS decomposition thrice (for U',P and PT) for each iteration of the optimization process. Summary of revisions: • Simulated annealing process was replaced by a descending random walk (equivalent to setting the threshold value β to 0), due to poor convergence of the simulated annealing process, and due to the lack of pathological instances of local minima in this particular search space.• Introduced an iterative method for generating the general permutation matrix P, in which we start with P=I and build up the quantum circuit (and modify P correspondingly) by adding gates onto the existing circuit. This removes the requirement of running the CS decomposition on P and PT to find the quantum circuit implementation, since it is already known by construction.• Added a synchronization mechanism between threads (after some prescribed number of iterations) in which the current state of the top 10% processes with the fittest solutions is copied over to the remaining 90%. This works so as to discard the less fit solutions and focuses the searching algorithm in the state space with the fittest solutions.Additional comments: The program contains some Fortran2003 features and will not compile with gcc4.9 or earlier. Running time: As before, running time increases with the size of the unitary matrix, as well as the prescribed maximum number of iterations for qubit permutation selection and the descending random walk. All simulation results presented in this paper are obtained from running the program on the Fornax supercomputer managed by iVEC@UWA with Intel Xeon X5650 CPUs. A comparison of running times is also given in Table 1. References: [1] T. Loke, J.B. Wang, Y.H. Chen, Comput. Phys. Comm. 185 (2014) 3307. 
id275#What we see in a photograph: content selection for image captioning#We propose and experimentally investigate the usefulness of several features for selecting image content (objects) suitable for image captioning. The approach taken explores three broad categories of features, namely geometric, conceptual, and visual. Experiments suggest that widely known geometric ‘rules’ in art–aesthetics or photography (such as the golden ratio or the rule-of-thirds) and facts about the human visual system (such as its wider horizontal angle than its vertical) provide no useful information for the task. Human captioners seem to prefer large, elongated (but not in the golden ratio) objects, positioned near the image center, irrespective of orientation. Conceptually, the preferred objects are either too specific or too general, and animate things are almost always mentioned; furthermore, some evidence is found for selecting diverse objects in order to achieve maximal image coverage in captions. Visual object features such as saliency, depth, edges, entropy, and contrast, are all found to provide useful information. Beyond evaluating features in isolation, we investigate how well these are combined by performing feature and feature-category ablation studies, leading to an effective set of features which can be proven useful for operational systems. Moreover, we propose alternative ways for feature engineering and evaluation, dealing with the drawbacks of the evaluation methodology proposed in past literature. 
id276#Using a Hybrid Approach to Data Management in Relational Database and Blockchain: A Case Study on the E-health Domain#Relational Databases (RDBs) have been widely used for decades. However, new persistence technologies are emerging, such as Blockchain, which is disruptive and has relevant properties, such as immutability and no third parties. Therefore, applications that use RDB can benefit from these properties by migrating part of their data to Blockchains. This article presents the MOON, a hybrid approach to manage data in RDB and Blockchain, which receives SQL queries. A case study was performed with a real health dataset using three scenarios. The conclusion is that the MOON responds to requests correctly and provides RDB and Blockchain features. Moreover, its response time was intermediate between RDB and Blockchains. 
id277#Towards Improved and Interpretable Action Quality Assessment with Self-Supervised Alignment#Action Quality Assessment (AQA) is a video understanding task aiming at the quantification of the execution quality of an action. One of the main challenges in relevant, deep learning-based approaches is the collection of training data annotated by experts. Current methods perform fine-tuning on pre-trained backbone models and aim to improve performance by modeling the subjects and the scene. In this work, we consider embeddings extracted using a self-supervised training method based on a differential cycle consistency loss between sequences of actions. These are shown to improve the state-of-the-art without the need for additional annotations or scene modeling. The same embeddings are also used to temporally align the sequences prior to quality assessment which further increases the accuracy, provides robustness to variance in execution speed and enables us to provide fine-grained interpretability of the assessment score. The experimental evaluation of the method on the MTL-AQA dataset demonstrates significant accuracy gain compared to the state-of-the-art baselines, which grows even more when the action execution sequences are not well aligned. 
id278#A survey on video content rating: taxonomy, challenges and open issues#Rating a video based on its content is one of the most important solutions to classify videos for audience age groups. In this regard, Film content rating and TV programmes rating are the only two most common rating systems which have been accomplished by the professional committees. However, due to the huge number of short videos shared in social media, it is impossible to review and rate their contents manually by a committee. Therefore, a proper solution is by utilizing computer vision capabilities to analyze the video content and rate it. Automatic Video Content Rating (VCR) system rates a short video to classify it for audience age groups. Inspired by the current manually film and TV programmes rating systems, VCR depends on five main components that comprise violence, profanity language, nudity, pornography, and substance abuse. To date, several reviews and survey papers have addressed advancements and innovations in video content analysis such as violence, nudity, and pornography detection. However, the lack of a comprehensive survey paper to investigate a VCR system and explain taxonomy, challenges, and open issues is discovered; thus, this study is undertaken. In this paper, in addition, to fill this gap, we review deep learning studies related to the relevant subjects of VCR. Moreover, we have investigated recently published works related to VCR based on the audio, static visual and, motion visual aspects of a video. Furthermore, related current datasets are investigated as well as the performances of published models in these datasets are compared. Finally, the challenges and the future of VCR are discussed. 
id279#Developing a mold-free approach for complex glulam production with the assist of computer vision technologies#With the increasing use of glulam in construction industry, low efficiency of material and time in complex glulam production process has been widely recognized. While single curved glulam components are normally achieved with the aid of heavy molds, double curved ones are difficult to be produced directly without massive subtractive fabrication. In this context, a mold-free approach for complex glulam production is proposed, which consists of a mechanical system that spatially shapes the curved beam, and a vision system to inform the fabrication directly with design model. Through the integration of digital design, simulation and physical process, different types of curved glulam could be produced following the same workflow. This approach could eliminate the use of complex molds in curved glulam production, greatly reduce wastes in post-processing process. With the feasibility initially verified through fabrication experiments, the system will be further developed so as to be transferred to industrial practice. 
id280#Online visual tracking via cross-similarity-based siamese network#Among deep-learning-based trackers, the siamese-based method inspires many researchers due to its effectiveness and simplicity. However, the traditional siamese tracker has not achieved satisfactory performance due to the limited representation ability and the lack of appropriate model update strategy. To cover the shortage of siamese models, we proposed a cross-similarity-based siamese network with three contributions. First, we introduce a novel cross similarity module into the SiameseFC framework, which could improve the matching ability of fully convolutional networks during the tracking process. Second, we propose a novel attention weighting layer to emphasize various contributions of matching scores in different positions. This adaptive attention weighting scheme makes our tracker well adapt to the appearance change caused by pose variation, partial occlusion, and so on. Third, we develop a simple yet effective model update strategy, which exploits an independent classification model to invoke the model fine-tuning process. Experimental results on the standard tracking benchmark show that our tracker performs much better than the baseline SiameseFC method and also achieves promising results in comparisons to other state-of-the-art algorithms. 
id282#Propels in compiler construction for versatile figuring#This paper shows a compiler machine for adaptable figuring. Our technique assembles the flexibility and comfort in a way that grants to port the structure to different centers with an irrelevant effort. In light of a present arrangement stream, we endeavor to accomplish another tier of handiness in the way of exploring and dividing programs written in C on most hoisted able to be done delineation tier. We show that the examination on this level is more successful than on lower ones as a result of usage of more communicative fabricate of programming. The better examination comes to fruition merged with Static single assignment based estimation for data way creation might provoke upper game plan nature of the last structure setup. 
id283#Eye Health, COVID-19, and the Occupational Health Professional: Round Table#Background: Eye health has garnered increased attention since the COVID-19 pandemic. This Round Table explored the impact mask wearing, delays in eye examinations, and increased screen time have on vision and ultimately the worker. Methods: Leading experts in the areas of occupational health, risk management, eye health, and communication were identified and invited to participate in a Round Table discussion. Questions posed to experts were based on literature that addressed eye health, such as mask wearing, communication and managing expectations when accessing professional eye health appointments, and increased screen time. Findings: Experts agreed that eye health considerations must be in place. These considerations should address not only clinical care of the patient but ways to protect workers from occupational injury associated with the eye. Conclusion/Application to practice: The occupational health professional is a key resource for assessment and training that pertains to eye health. 
id284#Computer vision applied to dual-energy computed tomography images for precise calcinosis cutis quantification in patients with systemic sclerosis#Background: Although treatments have been proposed for calcinosis cutis (CC) in patients with systemic sclerosis (SSc), a standardized and validated method for CC burden quantification is necessary to enable valid clinical trials. We tested the hypothesis that computer vision applied to dual-energy computed tomography (DECT) finger images is a useful approach for precise and accurate CC quantification in SSc patients. Methods: De-identified 2-dimensional (2D) DECT images from SSc patients with clinically evident lesser finger CC lesions were obtained. An expert musculoskeletal radiologist confirmed accurate manual segmentation (subtraction) of the phalanges for each image as a gold standard, and a U-Net Convolutional Neural Network (CNN) computer vision model for segmentation of healthy phalanges was developed and tested. A validation study was performed in an independent dataset whereby two independent radiologists manually measured the longest length and perpendicular short axis of each lesion and then calculated an estimated area by assuming the lesion was elliptical using the formula long axis/2 × short axis/2 × π, and a computer scientist used a region growing technique to calculate the area of CC lesions. Spearman’s correlation coefficient, Lin’s concordance correlation coefficient with 95% confidence intervals (CI), and a Bland-Altman plot (Stata V 15.1, College Station, TX) were used to test for equivalence between the radiologists’ and the CNN algorithm-generated area estimates. Results: Forty de-identified 2D DECT images from SSc patients with clinically evident finger CC lesions were obtained and divided into training (N = 30 with image rotation × 3 to expand the set to N = 120) and test sets (N = 10). In the training set, five hundred epochs (iterations) were required to train the CNN algorithm to segment phalanges from adjacent CC, and accurate segmentation was evaluated using the ten held-out images. To test model performance, CC lesional area estimates calculated by two independent radiologists and a computer scientist were compared (radiologist 1 vs. radiologist 2 and radiologist 1 vs. computer vision approach) using an independent test dataset comprised of 31 images (8 index finger and 23 other fingers). For the two radiologists’, and the radiologist vs. computer vision measurements, Spearman’s rho was 0.91 and 0.94, respectively, both p &lt; 0.0001; Lin’s concordance correlation coefficient was 0.91 (95% CI 0.85–0.98, p &lt; 0.001) and 0.95 (95% CI 0.91–0.99, p &lt; 0.001); and Bland-Altman plots demonstrated a mean difference between radiologist vs. radiologist, and radiologist vs. computer vision area estimates of − 0.5 mm2 (95% limits of agreement − 10.0–9.0 mm2) and 1.7 mm2 (95% limits of agreement − 6.0–9.5 mm2, respectively. Conclusions: We demonstrate that CNN quantification has a high degree of correlation with expert radiologist measurement of finger CC area measurements. Future work will include segmentation of 3-dimensional (3D) images for volumetric and density quantification, as well as validation in larger, independent cohorts. 
id285#External Quality Grading Method of Fuji Apple Based on Deep Learning [基于DXNet模型的富士苹果外部品质分级方法研究]#The research and development of high-precision and low-cost apple intelligent grading technology is the core issue to extend the apple industrial chain and improve the quality and efficiency of the fruit industry. In order to solve the problems of low accuracy and weak robustness of traditional computer vision technology in apple external quality classification, an apple appearance classification method based on deep learning (multiple convolutional neural network DXNet model) was proposed. Firstly, totally 15 000 apple images covering different appearance levels were taken in Yan'an supermarkets, orchards and other places, and then labeled manually. A database of apple images with extensive coverage of external quality information and large sample size was established. Then, on the basis of comparing and analyzing the classical convolution network model, the classical model was optimized and improved by the method of model fusion, and the convolution part of the classical model was extracted and fused to be the feature extractor, and the fully connected layer was shared to be the classifier, batch normalization and regularization techniques were used to prevent the model from over fitting. Totally 15 000 images were used for training and 4 500 images were used for testing. The results showed that the classification accuracy of the improved DXNet model was higher than that of the classical model, and the classification accuracy reached 97.84%, the validity of the method applied to apple external quality classification was verified. 
id286#Bi-GISIS KE: Modified key exchange protocol with reusable keys for IoT security#We propose a new bilateral generalization inhomogeneous short integer solution (Bi-GISIS)-based key exchange protocol with reusable key feature for post-quantum IoT security. It is aimed to reduce the time consumption in the key generation of key exchange protocols to be used in IoT devices. To obtain reusable key, we define modified bilateral pasteurization in the random oracle model. By ensuring reusable keys, the same key becomes available in several executions of the proposed protocol. This feature allows efficient usage of reusable keys in resource-constrained IoT architectures. The proposed scheme is suitable for quantum secure key exchange in D2D-aided fog computing environment. A key exchange protocol with improved key management process is constructed for D2D. 
id287#An energy-efficient crypto-extension design for RISC-V#With the prevailing of Internet-of-Things (IoT) technology, information security for ever-growing connected devices is an inevitable issue and gaining more attention. However, implementation of cryptography algorithms on battery-powered IoT devices is challenging due to limited power-budget. In this paper, we present an energy-efficient crypto-coprocessor. This coprocessor is designed with a unified pipelined structure for cryptography primitives of 128-bit or 256-bit data path and supports cryptography algorithms including AES, ECC and SHA. Since the clock tree and the sequential circuits dissipate a large percentage of the chip power, a conditional-charged flip-flop is proposed to reduce the clock tree power. Our design is integrated with an open-sourced RISC-V core as a crypto-extension, and shows both good flexibility and high energy-efficiency. This work is implemented in 28 nm technology and the power consumption for different cryptography applications is evaluated with post-layout simulation. When simulated with NIST prime fields curve P-256 and binary fields curve K-233, the energy consumed for one base point scalar multiplication is 43.54 μJ and 20.40 μJ, respectively. The proposed design consumes 0.0568 nJ/bit and 0.0288 nJ/bit for the AES-GCM mode and the AES-CBC mode, respectively. As for SHA-256, each bit requires 0.0874 nJ. Compared with previous works, this work provides both flexibility and high energy performance. 
id288#Game description language compiler construction#We describe a multilevel algorithm compiling a general game description in GDL into an optimized reasoner in a low level language. The aim of the reasoner is to efficiently compute game states and perform simulations of the game. This is essential for many General Game Playing systems, especially if they use simulation-based approaches. Our compiler produces a faster reasoner than similar approaches used so far. The compiler is implemented as a part of the player Dumalion. Although we concentrate on compiling GDL, the developed methods can be applied to similar Prolog-like languages in order to speed up computations. 
id289#A Comprehensive Review of Group Activity Recognition in Videos#Human group activity recognition (GAR) has attracted significant attention from computer vision researchers due to its wide practical applications in security surveillance, social role understanding and sports video analysis. In this paper, we give a comprehensive overview of the advances in group activity recognition in videos during the past 20 years. First, we provide a summary and comparison of 11 GAR video datasets in this field. Second, we survey the group activity recognition methods, including those based on handcrafted features and those based on deep learning networks. For better understanding of the pros and cons of these methods, we compare various models from the past to the present. Finally, we outline several challenging issues and possible directions for future research. From this comprehensive literature review, readers can obtain an overview of progress in group activity recognition for future studies. 
id290#The MADMAX data set for visual-inertial rover navigation on Mars#Planetary rovers increasingly rely on vision-based components for autonomous navigation and mapping. Developing and testing these components requires representative optical conditions, which can be achieved by either field testing at planetary analog sites on Earth or using prerecorded data sets from such locations. However, the availability of representative data is scarce and field testing in planetary analog sites requires a substantial financial investment and logistical overhead, and it entails the risk of damaging complex robotic systems. To address these issues, we use our compact human-portable DLR Sensor Unit for Planetary Exploration Rovers (SUPER) in the Moroccan desert to show resource-efficient field testing and make the resulting Morocco-Acquired data set of Mars-Analog eXploration (MADMAX) publicly accessible. The data set consists of 36 different navigation experiments, captured at eight Mars analog sites of widely varying environmental conditions. Its longest trajectory covers 1.5 km and the combined trajectory length is 9.2 km. The data set contains time-stamped recordings from monochrome stereo cameras, a color camera, omnidirectional cameras in stereo configuration, and from an inertial measurement unit. Additionally, we provide the ground truth in position and orientation together with the associated uncertainties, obtained by a real-time kinematic-based algorithm that fuses the global navigation satellite system data of two body antennas. Finally, we run two state-of-the-art navigation algorithms, ORB-SLAM2 and VINS-mono, on our data to evaluate their accuracy and to provide a baseline, which can be used as a performance reference of accuracy and robustness for other navigation algorithms. The data set can be accessed at https://rmc.dlr.de/morocco2018. 
id292#Improved extended progressive visual cryptography scheme using pixel harmonization#Security of information is of much concern in the modern internet era. Secret sharing schemes provide mechanism of encrypting secret information to prevent illicit usage. Visual cryptography is a secret sharing technique that facilitates encryption of a secret image. Visual cryptography allows us to effectively and efficiently share secrets among a number of trusted parties by hiding secrets within images. These images are encoded into multiple shares as per the rules indicated in basis matrices and later decoded by stacking required number of shares. Progressive visual cryptography has a specialty of recovering secret image as soon as more than one shares received gradually. Existing progressive visual cryptography schemes have severe limitations like data disclose on shares and higher pixel expansion. Improved progressive visual cryptography scheme deals with these limitations. Improved extended progressive visual cryptography scheme solves the issue of management of noise like meaningless shares by creating meaningful shares without any pixel expansion efficiently. Copyright 
id293#Citus: Distributed PostgreSQL for Data-Intensive Applications#Citus is an open source distributed database engine for PostgreSQL that is implemented as an extension. Citus gives users the ability to distribute data, queries, and transactions in PostgreSQL across a cluster of PostgreSQL servers to handle the needs of data-intensive applications. The development of Citus has largely been driven by conversations with companies looking to scale PostgreSQL beyond a single server and their workload requirements. This paper describes the requirements of four common workload patterns and how Citus addresses those requirements. It also shares benchmark results demonstrating the performance and scalability of Citus in each of the workload patterns and describes how Microsoft uses Citus to address one of its most challenging data problems. 
id294#A Quantum-Based Blockchain Approach to Voting Protocol Using Hyperledger Sawtooth#Protection measures are essential to present day blockchain innovation ever, since they can exist short of empowered outsider, which implies that there may not be a disclosed trustworthy individual or group responsible for frameworks. Security of the present frameworks depends on estimating the firmness assumptions and large numbers of the benchmark cryptographic functions proven to be powerless for crucial monetary and a variety of applications against the approach of undeniable quantum machines. Upgrading blockchain innovation with the future of quantum states in a shared manner will enhance the degree of protection and security by-laws of physical science, which is never feasible from non-quantum data hypothetical perspectives. In this article, we propose a quantum-built way to deal with harness of security for a democratic application with the execution, utilizing Hyperledger Sawtooth. 
id295#Online anomaly detection in surveillance videos with asymptotic bound on false alarm rate#Anomaly detection in surveillance videos is attracting an increasing amount of attention. Despite the competitive performance of recent methods, they lack theoretical performance analysis, particularly due to the complex deep neural network architectures used in decision making. Additionally, online decision making is an important but mostly neglected factor in this domain. Much of the existing methods that claim to be online, depend on batch or offline processing in practice. Motivated by these research gaps, we propose an online anomaly detection method in surveillance videos with asymptotic bounds on the false alarm rate, which in turn provides a clear procedure for selecting a proper decision threshold that satisfies the desired false alarm rate. Our proposed algorithm consists of a multi-objective deep learning module along with a statistical anomaly detection module, and its effectiveness is demonstrated on several publicly available data sets where we outperform the state-of-the-art algorithms. All codes are available at https://github.com/kevaldoshi17/Prediction-based-Video-Anomaly-Detection-. 
id296#Development and Implementation of a Laser-Camera-UAV System to Measure Total Dynamic Transverse Displacement#Railroad bridge inspectors are interested in measuring the maximum total transverse displacement of railroad bridges under trains, but these values are generally not easy to obtain in the field without sensors. Engineers use LVDTs, analog accelerometers, or wireless smart sensors (WSS). However, these sensors need to be attached to the bridge prior to the train-crossing event, which requires time, costs money, and is unsafe for engineers. This paper describes the design of a new sensor-equipped, low-cost unmanned aerial vehicle (UAV) system that enables the safe, cost-efficient, and noncontact total transverse displacements measurement of railroad bridges. The new system integrates laser and camera measurements from a UAV flying near the moving structure. The design and assembly of the new system is followed by methodology, field experiment, and results. The authors compared the estimations of the new system with ground-truth data obtained using an LVDT to quantify the capabilities of the new system. The results support the value of the proposed method to measure noncontact railroad bridge displacements in the field. 
id297#Region proposals optimization algorithm combining neural networks and superpixels [融合神经网络与超像素的候选区域优化算法]#In order to solve the low recall problem of the region proposals in object detection, the object region proposals algorithm, which combines neural networks and superpixels, was proposed. The edge features, which can be represented clearly by neural networks, were extracted from the images to be detected, and the score of edge information for per sliding window was computed by the strategy of edge clustering and the affinities between the edge groups. The several superpixels of this images were obtained by simple linear iterative clustering algorithm, and the salient object score of a superpixel was calculated using the location, integrity of this superpixel and the contrast with neighbors. The salient objects score of per sliding window was received by these saliency scores of superpixels according to the Euler distance strategy between the sliding window and these superpixels. The region proposals were determined by two components including edge information scores and salient object scores. The comparative experiments were conducted in PASCAL VOC 2007 test set, and the experiment results show that the proposed algorithm can fast generate a set of region proposal with higher localization. 
id298#Privacy preserving getup detection#The ageing population leads to an increase of people requiring long-term care. Assisting people when getting out of bed and fast reactions to falls can help to reduce costs and the risk of injury. We describe the possibility of detecting getting up behavior from a bed using different deep learning models and depth data as a proof of concept. The hereby used computer vision approach uses unobtrusive depth data in order to protect people's privacy. We gather data from different subjects, postures, views and rooms, with which we then train the network. Both classification and object detection methods are able to reliably detect getting up behavior. Situations that could not be correctly classified were when a person was changing from lying to sitting or when the legs of the person were covered with a blanket. Our results show that using pretrained networks is the key contributor in training. We also demonstrate that convolutional neural networks are capable of extracting high-level task-dependent features from depth data which can be utilized in developing ambient intelligent systems. The practicality of the network can be adapted from getting up from a bed to sitting and walking inside the room, based on the purpose of the real-life application. 
id299#Implementation and performance evaluation of RNS variants of the BFV homomorphic encryption scheme#Homomorphic encryption is an emerging form of encryption that provides the ability to compute on encrypted data without ever decrypting them. Potential applications include aggregating sensitive encrypted data on a cloud environment and computing on the data in the cloud without compromising data privacy. There have been several recent advances resulting in new homomorphic encryption schemes and optimized variants. We implement and evaluate the performance of two optimized variants, namely Bajard-Eynard-Hasan-Zucca (BEHZ) and Halevi-Polyakov-Shoup (HPS), of the most promising homomorphic encryption scheme in CPU and GPU. The most interesting (and also unexpected) result of our performance evaluation is that the HPS variant in practice scales significantly better (typically by 15-30 percent) with increase in multiplicative depth of the computation circuit than BEHZ, implying that the HPS variant will always outperform BEHZ for most practical applications. For the multiplicative depth of 98, our fastest GPU implementation performs homomorphic multiplication in 51 ms for 128-bit security settings, which is faster by two orders of magnitude than prior results and already practical for cloud environments supporting GPU computations. Large multiplicative depths supported by our implementations are required for applications involving deep neural networks, logistic regression learning, and other important machine learning problems. 
id301#Optimal Cryptography Scheme and Efficient Neutrosophic C-Means Clustering for Anomaly Detection in Cloud Environment#This paper introduces an efficient and scalable cloud-based privacy preserving model using a new optimal cryptography scheme for anomaly detection in large-scale sensor data. Our proposed privacy preserving model has maintained a better tradeoff between reliability and scalability of the cloud computing resources by means of detecting anomalies from the encrypted data. Conventional data analysis methods have used complex and large numerical computations for the anomaly detection. Also, a single key used by the symmetric key cryptographic scheme to encrypt and decrypt the data has faced large computational complexity because the multiple users can access the original data simultaneously using this single shared secret key. Hence, a classical public key encryption technique called RSA is adopted to perform encryption and decryption of secure data using different key pairs. Furthermore, the random generation of public keys in RSA is controlled in the proposed cloud-based privacy preserving model through optimizing a public key using a new hybrid local pollination-based grey wolf optimizer (LPGWO) algorithm. For ease of convenience, a single private server handling the organization data within a collaborative public cloud data center when requiring the decryption of secure sensor data are allowed to decrypt the optimal secure data using LPGWO-based RSA optimal cryptographic scheme. The data encrypted using an optimal cryptographic scheme are then encouraged to perform data clustering computations in collaborative public servers of cloud platform using Neutrosophic c-Means Clustering (NCM) algorithm. Hence, this NCM algorithm mainly focuses for data partitioning and classification of anomalies. Experimental validation was conducted using four datasets obtained from Intel laboratory having publicly available sensor data. The experimental outcomes have proved the efficiency of the proposed framework in providing data privacy with high anomaly detection accuracy. 
id302#Algebraic foundations for effect-dependent optimisations#We present a general theory of Gifford-style type and effect annotations, where effect annotations are sets of effects. Generality is achieved by recourse to the theory of algebraic effects, a development of Moggi's monadic theory of computational effects that emphasises the operations causing the effects at hand and their equational theory. The key observation is that annotation effects can be identified with operation symbols. We develop an annotated version of Levy's Call-by-Push-Value language with a kind of computations for every effect set; it can be thought of as a sequential, annotated intermediate language. We develop a range of validated optimisations (i.e., equivalences), generalising many existing ones and adding new ones. We classify these optimisations as structural, algebraic, or abstract: structural optimisations always hold; algebraic ones depend on the effect theory at hand; and abstract ones depend on the global nature of that theory (we give modularly-checkable sufficient conditions for their validity). 
id303#Robotic arm material characterisation using LIBS and Raman in a nuclear hot cell decommissioning environment#Material characterisation in nuclear environments is an essential part of decommissioning processes. This paper explores the feasibility of deploying commercial off the shelf (COTS) laser induced breakdown spectroscopy (LIBS) and Raman spectroscopy, for use in a decommissioning hot cell environment, to inform waste operation decision making. To operate these techniques, adapters and probes were designed and constructed, for each instrument, to form tools that a robotic arm could pick up and operate remotely from an isolated control room. The developed instrumentation successfully returned live measurement data to a control room for saving and further analysis (e.g. material classification/identification). Successful testing of the solutions was performed for contact LIBS, contact Raman and stand-off Raman on a PaR M3000 robotic arm, in a simulated hot cell environment and the limitations identified. Data obtained by the techniques are analysed, classified and presented in a 3D virtual environment. The spectral data collected by a basic COTS LIBS showed potential for use in contamination identification (beryllium is used as example). Potential for COTS, LIBS and Raman in decommissioning is established and improvements to the hardware, the measurement processes and how the data is stored and used, are identified. 
id304#The use of closed-circuit television and video in suicide prevention: narrative review and future directions#"Background: Suicide is a recognized public health issue, with approximately 800,000 people dying by suicide each year. Among the different technologies used in suicide research, closed-circuit television (CCTV) and video have been used for a wide array of applications, including assessing crisis behaviors at metro stations, and using computer vision to identify a suicide attempt in progress. However, there has been no review of suicide research and interventions using CCTV and video. Objective: The objective of this study was to review the literature to understand how CCTV and video data have been used in understanding and preventing suicide. Furthermore, to more fully capture progress in the field, we report on an ongoing study to respond to an identified gap in the narrative review, by using a computer vision-based system to identify behaviors prior to a suicide attempt. Methods: We conducted a search using the keywords ""suicide,"" ""cctv,"" and ""video"" on PubMed, Inspec, and Web of Science. We included any studies which used CCTV or video footage to understand or prevent suicide. If a study fell into our area of interest, we included it regardless of the quality as our goal was to understand the scope of how CCTV and video had been used rather than quantify any specific effect size, but we noted the shortcomings in their design and analyses when discussing the studies. Results: The review found that CCTV and video have primarily been used in 3 ways: (1) to identify risk factors for suicide (eg, inferring depression from facial expressions), (2) understanding suicide after an attempt (eg, forensic applications), and (3) as part of an intervention (eg, using computer vision and automated systems to identify if a suicide attempt is in progress). Furthermore, work in progress demonstrates how we can identify behaviors prior to an attempt at a hotspot, an important gap identified by papers in the literature. Conclusions: Thus far, CCTV and video have been used in a wide array of applications, most notably in designing automated detection systems, with the field heading toward an automated detection system for early intervention. Despite many challenges, we show promising progress in developing an automated detection system for preattempt behaviors, which may allow for early intervention. "
id305#Deep Learning-based Approximate Graph-Coloring Algorithm for Register Allocation#Graph-coloring is an NP-hard problem which has a myriad of applications. Register allocation, which is a crucial phase of a good optimizing compiler, relies on graph coloring. Hence, an efficient graph-coloring algorithm is of paramount importance. In this work we try to 'learn' a good heuristic for coloring interference graphs that are used in the register allocation phase. We aim to handle moderate-sized interference graphs which have 100 nodes or less. For such graphs we can get the optimal allocation of colors to the nodes. Such optimal coloring is then used to train our Deep Learning (DL) network which is based on several layers of LSTM that output a color for each node of the graph. However, the trained network may allocate the same color to the nodes connected by an edge resulting in an invalid coloring of the interference graph. Since it is difficult to encode constraints in an LSTM to avoid invalid coloring, we augment our deep learning network with a color correction phase that runs after the colors have been allocated by the DL network. Thus, our algorithm is approximate or hybrid in nature consisting of a mix of a DL algorithm followed by a more traditional correction phase. The color correction phase handles the edges with invalid coloring by first trying to reuse a color allocated to other nodes that are not connected to the invalid nodes, failing which it adds a totally new color-thereby breaking the invalid allocation. Our experience with many graphs shows that around 10%-30% edges may get an invalid coloring. We have trained our DL network using several thousand random graphs of varying sparsity(density). On application of our approximate algorithm to various popular graphs found in literature we see that our algorithm does very well when compared to the optimal coloring of these graphs. We have also run our algorithm against LLVM's popular greedy register allocator (GRA) for several SPEC CPU 2017 benchmarks and notice that the approximate algorithm performs on par or better than GRA for most of these benchmarks. 
id306#PLCC: A programming language compiler compiler#"This paper describes PLCC, a compiler-compiler tool to support courses in programming languages, compilers, and computational theory. This tool has proven to be useful for implementing interpreters, building compilers, and creating parsers for context-free languages. PLCC is a Perl program that takes an input file that specifies the tokens, syntax, and semantics of a language and that generates a complete set of Java files that implement the semantics of the language. PLCC stands for ""Programming Language Compiler-Compiler"". PLCC is not intended to be a production-quality tool. Rather, it supports understanding and implementing the essential elements of lexical analysis, parsing, and semantics without having to wrestle with the complexities of dealing with ""industrial-strength"" compiler-compiler tools. Students quickly learn how to write PLCC ""grammar"" files for small languages that have straightforward syntax and semantics and use PLCC to build Java-based parsers, interpreters, or compilers for these languages that run out-of-the-box. Input to PLCC is a text file with a token definition section that defines language tokens as simple regular expressions, a syntax section that specifies the grammar rules of an LL(1) language as simple Backus-Naur Form (BNF) productions, and a semantics section that defines the language semantics as Java methods. PLCC generates a set of Java source files that are entirely self-contained and that import only standard elements of java.util in JDK5 and above. For testing purposes, PLCC generates a read-eval-print loop that (1) reads standard input, (2) scans, parses, and evaluates the input, and (3) prints the evaluation to standard output."
id307#Survey and performance analysis of deep learning based object detection in challenging environments#Recent progress in deep learning has led to accurate and efficient generic object detection networks. Training of highly reliable models depends on large datasets with highly textured and rich images. However, in real-world scenarios, the performance of the generic object detection system decreases when (i) occlusions hide the objects, (ii) objects are present in low-light images, or (iii) they are merged with background information. In this paper, we refer to all these situations as challenging environments. With the recent rapid development in generic object detection algorithms, notable progress has been observed in the field of deep learning-based object detection in challenging environments. However, there is no consolidated reference to cover the state of the art in this domain. To the best of our knowledge, this paper presents the first comprehensive overview, covering recent approaches that have tackled the problem of object detection in challenging environments. Furthermore, we present a quantitative and qualitative performance analysis of these approaches and discuss the currently available challenging datasets. Moreover, this paper investigates the performance of current state-of-the-art generic object detection algorithms by benchmarking results on the three well-known challenging datasets. Finally, we highlight several current shortcomings and outline future directions. 
id308#Self-Supervised Human Activity Recognition by Augmenting Generative Adversarial Networks#This article proposes a novel approach for augmenting generative adversarial network (GAN) with a self-supervised task in order to improve its ability for encoding video representations that are useful in downstream tasks such as human activity recognition. In the proposed method, input video frames are randomly transformed by different spatial transformations, such as rotation, translation and shearing or temporal transformations such as shuffling temporal order of frames. Then discriminator is encouraged to predict the applied transformation by introducing an auxiliary loss. Subsequently, results prove superiority of the proposed method over baseline methods for providing a useful representation of videos used in human activity recognition performed on datasets such as KTH, UCF101 and Ball-Drop. Ball-Drop dataset is a specifically designed dataset for measuring executive functions in children through physically and cognitively demanding tasks. Using features from proposed method instead of baseline methods caused the top-1 classification accuracy to increase by more then 4%. Moreover, ablation study was performed to investigate the contribution of different transformations on downstream task. 
id310#TutNorBD: Assistant for teaching and learning process of relational database normalization up to 3NF from a universal table#"Relational database normalization is a formal data organization process that seeks to make the database more flexible by eliminating data redundancy and inconsistencies. Informally, a relational database relationship is often described as ""normalized""if it conforms to the third normal form. Most relationships in 3NF are free of insert, update, and delete anomalies. In this work the design of a normalization assistant is presented. The purpose of the presented proposal is to support the teaching and learning process of relational database normalization, making the user take his universal data table to third normal form in an agile and simple way. "
id311#Joint Iterative Color Correction and Dehazing for Underwater Image Enhancement#The captured underwater images suffer from color cast and haze effect caused by absorption and scattering. These interdependent phenomena jointly degrade images, resulting in failure of autonomous machines to recognize image contents. Most existing learning-based methods for underwater image enhancement (UIE) treat the degraded process as a whole and ignore the interaction between color correction and dehazing. Thus, they often obtain unnatural results. To this end, we propose a novel joint network to optimize the results of color correction and dehazing in multiple iterations. Firstly, a novel triplet-based color correction module is proposed to obtain color-balanced images with identical distribution of color channels. By means of inherent constraints of the triplet structure, the information of channel with less distortion is utilized to recover the information of other channels. Secondly, a recurrent dehazing module is designed to alleviate haze effect in images, where the Gated Recurrent Unit (GRU) as the memory module optimizes the results in multiple cycles to deal with severe underwater distortions. Finally, an iterative mechanism is proposed to jointly optimize the color correction and dehazing. By learning transform coefficients from dehazing features, color features and basic features of raw images are progressively refined, which maintains color balanced during the dehazing process and further improves clarity of images. Experimental results show that our network is superior to the existing state-of-the-art approaches for UIE and provides improved performance for underwater object detection. 
id312#Cloud-Based Quadratic Optimization with Partially Homomorphic Encryption#This article develops a cloud-based protocol for a constrained quadratic optimization problem involving multiple parties, each holding private data. The protocol is based on the projected gradient ascent on the Lagrange dual problem and exploits partially homomorphic encryption and secure communication techniques. Using formal cryptographic definitions of indistinguishability, the protocol is shown to achieve computational privacy. We show the implementation results of the protocol and discuss its computational and communication complexity. We conclude this article with a discussion on privacy notions. 
id313#Application of computer vision for estimation of moving vehicle weight#Heavy vehicle weights need to be closely monitored for preventing fatigue-induced deterioration and critical fractures to highway infrastructure, among many other purposes, but development of a cost-effective weigh-in-motion (WIM) system remains challenging. This paper describes the creation and experimental validations of a computer vision-based non-contact WIM system. The underlining physics is that the force exerted by each tire onto the road is the product of the tire-road contact pressure and contact area. Computer vision is applied (1) to measure the tire deformation parameters so that the tire-roadway contact area can be accurately estimated; and (2) to recognize the marking texts on the tire sidewall so that the manufacturer-recommended tire inflation pressure can be found. In this research, a computer vision system is developed, which is comprised of a camera and computer vision software for measuring tire deformation parameters and recognizing the tire sidewall markings from images of individual tires of a moving vehicle. Computer vision techniques such as edge detection and optical character recognition are applied to enhance the measurement and recognition accuracy. Field experiments were conducted on fully loaded or empty concrete trucks and the truck weights estimated by this novel computer vision-based non-contact WIM system agreed well with the curb weights verified by static weighing. This research has demonstrated a novel application of the computer vision technology to solve a challenging vehicle WIM problem. Requiring no sensor installation on the roadway or the vehicle, this cost-effective non-contact computer vision system has demonstrated a great potential to be implemented. 
id315#Visual identification of individual Holstein-Friesian cattle via deep metric learning#Holstein-Friesian cattle exhibit individually-characteristic black and white coat patterns visually akin to those arising from Turing's reaction-diffusion systems. This work takes advantage of these natural markings in order to automate visual detection and biometric identification of individual Holstein-Friesians via convolutional neural networks and deep metric learning techniques. Existing approaches rely on markings, tags or wearables with a variety of maintenance requirements, whereas we present a totally hands-off method for the automated detection, localisation, and identification of individual animals from overhead imaging in an open herd setting, i.e. where new additions to the herd are identified without re-training. We find that deep metric learning systems show strong performance even when many cattle unseen during system training are to be identified and re-identified – achieving 93.8% accuracy when trained on just half of the population. This work paves the way for facilitating the non-intrusive monitoring of cattle applicable to precision farming and surveillance for automated productivity, health and welfare monitoring, and to veterinary research such as behavioural analysis, disease outbreak tracing, and more. Key parts of the source code, network weights and underpinning datasets are available publicly (OpenCows2020). 
id316#A Survey of Microarchitectural Side-channel Vulnerabilities, Attacks, and Defenses in Cryptography#Side-channel attacks have become a severe threat to the confidentiality of computer applications and systems. One popular type of such attacks is the microarchitectural attack, where the adversary exploits the hardware features to break the protection enforced by the operating system and steal the secrets from the program. In this article, we systematize microarchitectural side channels with a focus on attacks and defenses in cryptographic applications. We make three contributions. (1) We survey past research literature to categorize microarchitectural side-channel attacks. Since these are hardware attacks targeting software, we summarize the vulnerable implementations in software, as well as flawed designs in hardware. (2) We identify common strategies to mitigate microarchitectural attacks, from the application, OS, and hardware levels. (3) We conduct a large-scale evaluation on popular cryptographic applications in the real world and analyze the severity, practicality, and impact of side-channel vulnerabilities. This survey is expected to inspire side-channel research community to discover new attacks, and more importantly, propose new defense solutions against them. 
id317#Feasibility and preliminary efficacy of a combined virtual reality, robotics and electrical stimulation intervention in upper extremity stroke rehabilitation#Background: Approximately 80% of individuals with chronic stroke present with long lasting upper extremity (UE) impairments. We designed the perSonalized UPper Extremity Rehabilitation (SUPER) intervention, which combines robotics, virtual reality activities, and neuromuscular electrical stimulation (NMES). The objectives of our study were to determine the feasibility and the preliminary efficacy of the SUPER intervention in individuals with moderate/severe stroke. Methods: Stroke participants (n = 28) received a 4-week intervention (3 × per week), tailored to their functional level. The functional integrity of the corticospinal tract was assessed using the Predict Recovery Potential algorithm, involving measurements of motor evoked potentials and manual muscle testing. Those with low potential for hand recovery (shoulder group; n = 18) received a robotic-rehabilitation intervention focusing on elbow and shoulder movements only. Those with a good potential for hand recovery (hand group; n = 10) received EMG-triggered NMES, in addition to robot therapy. The primary outcomes were the Fugl-Meyer UE assessment and the ABILHAND assessment. Secondary outcomes included the Motor Activity Log and the Stroke Impact Scale. Results: Eighteen participants (64%), in either the hand or the shoulder group, showed changes in the Fugl-Meyer UE or in the ABILHAND assessment superior to the minimal clinically important difference. Conclusions: This indicates that our personalized approach is feasible and may be beneficial in improving UE function in individuals with moderate to severe impairments due to stroke. Trial registration: ClinicalTrials.gov NCT03903770. Registered 4 April 2019. Registered retrospectively. 
id319#Continuous Tongue Robot Mapping for Paralyzed Individuals Improves the Functional Performance of Tongue-Based Robotic Assistance#Individuals with tetraplegia have a challenging life due to a lack of independence and autonomy. Assistive robots have the potential to assist with the activities of daily living and thus improve the quality of life. However, an efficient and reliable control interface for severely disabled individuals is still missing. An intraoral tongue-computer interface (ITCI) for people with tetraplegia has previously been introduced and tested for controlling a robotic manipulator in a study deploying discrete tongue robot mapping. To improve the efficiency of the interface, the current study proposed the use of virtual buttons based on the ITCI and evaluated them in combination with a joystick-like control implementation, enabling continuous control commands. Twelve able-bodied volunteers participated in a three-day experiment. They controlled an assistive robotic manipulator through the tongue to perform two tasks: Pouring water in a cup (PW) and picking up a roll of tape (PUT). Four different tongue-robot mapping methods were compared. The results showed that using continuous commands reduced the task completion time by 16% and the number of commands of the PUT test by 20% compared with discrete commands. The highest success rate for completing the tasks was 77.8% for the PUT test and 100% for the PW test, both achieved by the control methods with continuous commands. Thus, the study demonstrated that incorporating continuous commands can improve the performance of the ITCI system for controlling robotic manipulators. 
id320#Ajalon: Simplifying the authoring of wearable cognitive assistants#Wearable Cognitive Assistance (WCA) amplifies human cognition in real time through a wearable device and low-latency wireless access to edge computing infrastructure. It is inspired by, and broadens, the metaphor of GPS navigation tools that provide real-time step-by-step guidance, with prompt error detection and correction. WCA applications are likely to be transformative in education, health care, industrial troubleshooting, manufacturing, assisted driving, and sports training. Today, WCA application development is difficult and slow, requiring skills in areas such as machine learning and computer vision that are not widespread among software developers. This paper describes Ajalon, an authoring toolchain for WCA applications that reduces the skill and effort needed at each step of the development pipeline. Our evaluation shows that Ajalon significantly reduces the effort needed to create new WCA applications. 
id321#High-capacity measurement-device-independent deterministic secure quantum communication#Deterministic secure quantum communication (DSQC) is an important branch of quantum cryptography and has attracted continuous attention. However, in practical DSQC, the receiver’s detectors can be subjected to detector-side-channel attacks launched by the outside eavesdropper. Moreover, encoding the information in only one degree of freedom (DOF) of photons makes DSQC inefficient. Here, to remove all the detector side channels and increase single-photons’ channel capacity, we report the first high-capacity measurement-device-independent DSQC (HC-MDI-DSQC) protocol by using photons’ polarization-spatial-mode DOFs. This method is similar to the idea of MDI quantum key distribution. Theoretical analyses show that it is advantageous in terms of security and efficiency compared with the state-of-the-art DSQC protocols. 
id322#Specifying and optimizing robotic motion for visual quality inspection#Installation or even just modification of robot-supported production and quality inspection is a tedious process that usually requires full-time human expert engagement. The resulting parameters, e.g. robot velocities specified by an expert, are often subjective and produce suboptimal results. In this paper, we propose a new approach for specifying visual inspection trajectories based on CAD models of workpieces to be inspected. The expert involvement is required only to select – in a CAD system – the desired points on the inspection path along which the robot should move the camera. The rest of the approach is fully automatic. From the selected path data, the system computes temporal parametrization of the path, which ensures smoothness of the resulting robot trajectory for visual inspection. We then apply a new learning method for the optimization of robot speed along the specified path. The proposed approach combines iterative learning control and reinforcement learning. It takes a numerical estimate of image quality as input and produces the fastest possible motion that does not result in the degradation of image quality as output. In our experiments, the algorithm achieved up to 53% cycle time reduction from an initial, manually specified motion, without degrading the image quality. We show experimentally that the proposed algorithm achieves better results compared to some other policy learning approaches. The described approach is general and can be used with different types of learning and feedback signals. 
id323#Early-Anomaly Prediction in Surveillance Cameras for Security Applications#In the last decade, the number of surveillance cameras has increased significantly, with much research conducted to automate the process of surveillance, as humans cannot manage to monitor all these cameras individually, which may cause errors in public safety or abnormal situations. Also, humans may overlook key details in such abnormal behaviours in surveillance cameras. The proposed approach predicts abnormal behaviour using generative adversarial networks (GANs). GANs are trained using different datasets that contain various behaviours to predict future frames. These future frames are transmitted to a deep learning neural network to classify them as normal or abnormal activities, and future anomalies can be detected before they happen. Our initial results show that depending on the future frames extracted by the GAN model is possible, as these extracted frames either improve the accuracy of the detection model or do not affect it, but they can also be further enhanced to detect more frames at a longer duration and predict anomalies before they happen. Anomalies in surveillance will not only be detected but also predicted before they happen, which will result in the prevention of crimes, reductions in surveillance costs and a safer environment overall. 
id324#A verified compiler for an impure functional language#We present a verified compiler to an idealized assembly language from a small, untyped functional language with mutable references and exceptions. The compiler is programmed in the Coq proof assistant and has a proof of total correctness with respect to bigstep operational semantics for the source and target languages. Compilation is staged and includes standard phases like translation to continuation-passing style and closure conversion, as well as a common subexpression elimination optimization. In this work, our focus has been on discovering and using techniques that make our proofs easy to engineer and maintain. While most programming language work with proof assistants uses very manual proof styles, all of our proofs are implemented as adaptive programs in Coq's tactic language, making it possible to reuse proofs unchanged as new language features are added. In this paper, we focus especially on phases of compilation that rearrange the structure of syntax with nested variable binders. That aspect has been a key challenge area in past compiler verification projects, with much more effort expended in the statement and proof of binder-related lemmas than is found in standard penciland-paper proofs. We show how to exploit the representation technique of parametric higher-order abstract syntax to avoid the need to prove any of the usual lemmas about binder manipulation, often leading to proofs that are actually shorter than their pencil-andpaper analogues. Our strategy is based on a new approach to encoding operational semantics which delegates all concerns about substitution to the meta language, without using features incompatible with general-purpose type theories like Coq's logic. Copyright 
id325#Partitioning variables across register windows to reduce spill code in a low-power processor#Low-power embedded processors utilize compact instruction encodings to achieve small code size. Such encodings place tight restrictions on the number of bits available to encode operand specifiers and, thus, on the number of architected registers. As a result, performance and power are often sacrificed as the burden of operand supply is shifted from the register file to the memory due to the limited number of registers. In this paper, we investigate the use of a windowed register file to address this problem by providing more registers than allowed in the encoding. The registers are organized as a set of identical register windows where, at each point in the execution, there is a single active window. Special window management instructions are used to change the active window and to transfer values between windows. This design gives the appearance of a large register file without compromising the instruction encoding. To support the windowed register file, we designed and implemented a graph partitioning-based compiler algorithm that partitions program variables and temporaries referenced within a procedure across multiple windows. On a 16-bit embedded processor, an average of 11 percent improvement in application performance and 25 percent reduction in system power was achieved as an 8-register design was scaled from one to two windows. 
id327#Deep convolutional neural network for automatic fault recognition from 3D seismic datasets#With the explosive growth in seismic data acquisition and the successful application of deep convolutional neural networks (DCNN) to various image processing tasks within multidisciplinary fields, many researchers have begun to research DCNN based automatic seismic interpretation techniques. Due to the vast number of parameters considered in deep neural networks, deep learning methods usually require a large amount of data for training. However, collecting a large number of expert interpretations is very time consuming, so related research usually uses synthetic datasets and ignores the practical problems of field datasets. In this paper, we open-source a multi-gigabyte expert-labelled field dataset in response to the challenge of accessing large-scale expert-labelled field datasets. We show that 2D fault recognition within this dataset is an image segmentation or edge detection problem in the computer vision field, that can be expressed as a pixel-level fault/non-fault binary classification. Both types of DCNNs are compared, and we propose a novel fault recognition workflow, which involves processing and screening of seismic images and labels, training DCNNs and automatic numerical evaluation. We have also demonstrated for three case study datasets that effective image augmentation methods can reduce the required labelled crosslines while maintaining satisfactory performance. Our experimental results show that our workflow not only outperforms two state-of-the-art DCNN solutions but also achieves performance comparable to humans on an expert labelled image dataset, even predicting subtle faults that an expert interpreter did not annotate. We suggest that the proposed workflow could reduce the fault interpretation life cycle from months to hours and improve the quality, and define the confidence, of fault interpretation results. 
id328#Exploring the risc-v vector extension for the classic mceliece post-quantum cryptosystem#The dawn of quantum computers threatens the security guarantees of classical public-key cryptography. This gave rise to a new class of so-called quantum-resistant cryptography algorithms and a need to efficiently implement them on embedded hardware platforms. This paper investigates how we can exploit the most recent RISC-V Vector Extension Version 0.9 (RVV0.9) to accelerate the quantum-resistant code-based Classic McEliece cryptosystem. We focused on the Gaussian Elimination Algorithm (GEA) that is essential for the key generation of the McEliece scheme. The GEA offers high potential for acceleration by vector instructions of the RVV extension. In order to evaluate the possible gains, we adopted a rapid prototyping approach based on an instruction set simulator (ISS). We extended the simulator ETISS with a SoftVector library, which allows to quickly model the instructions of RVV. Using the rapid prototyping environment, the GEA was re-implemented and verified for RVV0.9.The final performance gain heavily depends on the memory interface of the vector unit. For different configurations of the memory system, we could profile performance gains of 6 up to 18 for the GEA. This clearly shows the benefit of RVV for implementing quantum-resistant cryptosystems. 
id329#Falcon: A false ceiling inspection robot#Frequent inspections are essential for false ceilings to maintain the service infrastructures, such as mechanical, electrical, and plumbing, and the structure of false ceilings. Human-labor-based conventional inspection procedures for false ceilings suffer many shortcomings, including safety concerns. Thus, robot-aided solutions are demanded for false ceiling inspections similar to other building maintenance services. However, less work has been conducted on developing robot-aided solutions for false ceiling inspections. This paper proposes a novel design for a robot intended for false ceiling inspections named Falcon. The compact size and the tracked wheel design of the robot allow it to traverse obstacles such as runners and lighting fixtures. The robot’s ability to autonomously follow the perimeter of a false ceiling can improve the productivity of the inspection process since the heading of the robot often changes due to the nature of the terrain, and continuous heading correction is an overhead for a teleoperator. Therefore, a Perimeter-Following Controller (PFC) based on fuzzy logic was integrated into the robot. Experimental results obtained by deploying a prototype of the robot design to a false ceiling testbed confirmed the effectiveness of the proposed PFC in perimeter following and the robot’s features, such as the ability to traverse on runners and fixtures in a false ceiling. 
id330#Low area PRESENT cryptography in FPGA using TRNG-PRNG key generation#Lightweight Cryptography (LWC) iswidely used to provide integrity, secrecy and authentication for the sensitive applications. However, the LWC is vulnerable to various constraints such as high-power consumption, time consumption, and hardware utilization and susceptible to the malicious attackers. In order to overcome this, a lightweight block cipher namely PRESENT architecture is proposed to provide the security against malicious attacks. The True Random Number Generator-Pseudo Random Number Generator (TRNG-PRNG) based key generation is proposed to generate the unpredictable keys, being highly difficult to predict by the hackers. Moreover, the hardware utilization of PRESENT architecture is optimized using the Dual port Read Only Memory (DROM). The proposed PRESENT-TRNGPRNG architecture supports the 64-bit input with 80-bit of key value. The performance of the PRESENT-TRNG-PRNG architecture is evaluated by means of number of slice registers, flip flops, number of slices Look Up Table (LUT), number of logical elements, slices, bonded input/output block (IOB), frequency, power and delay. The input retrieval performances analyzed in this PRESENT-TRNG-PRNG architecture are Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM) and Mean-Square Error (MSE). The PRESENT-TRNG-PRNG architecture is compared with three different existing PRESENT architectures such as PRESENT On-The- Fly (PERSENT-OTF), PRESENT Self-Test Structure (PRESENT-STS) and PRESENT-Round Keys (PRESENT-RK). The operating frequency of the PRESENT-TRNG-PRNG is 612.208 MHz for Virtex 5, which is high as compared to the PRESENT-RK. 
id331#Optimising the workflow for fish detection in didson (Dual-frequency identification sonar) data with the use of optical flow and a genetic algorithm#DIDSON acoustic cameras provide a way to collect temporally dense, high-resolution imaging data, similar to videos. Detection of fish targets on those videos takes place in a manual or semi-automated manner, typically assisted by specialised software. Exploiting the visual nature of the recordings, tools and techniques from the field of computer vision can be applied in order to facilitate the relatively involved workflows. Furthermore, machine learning techniques can be used to minimise user intervention and optimise for specific detection and tracking scenarios. This study explored the feasibility of combining optical flow with a genetic algorithm, with the aim of automating motion detection and optimising target-to-background segmentation (masking) under custom criteria, expressed in terms of the result. A 1000-frame video sequence sample with sparse, smoothly moving targets, reconstructed from a 125 s DIDSON recording, was analysed under two distinct scenarios, and an elementary detection method was used to assess and compare the resulting foreground (target) masks. The results indicate a high sensitivity to motion, as well as to the visual characteristics of targets, with the resulting foreground masks generally capturing fish targets on the majority of frames, potentially with small gaps of undetected targets, lasting for no more than a few frames. Despite the high computational overhead, implementation refinements could increase computational feasibility, while an extension of the algorithms, in order to include the steps of target detection and tracking, could further improve automation and potentially provide an efficient tool for the automated preliminary assessment of voluminous DIDSON data recordings. 
id332#Hybrid cryptosystem in wireless body area networks using message authentication code and modified and enhanced lattice-based cryptography (MAC-MELBC) in healthcare applications#Wireless Body Area Networks (WBAN) is a network of sensor devices that are connected together located in the clothes, on the body or underneath a human's skin to monitor patient's health continuously and communicate with required resources. Recently, WBAN offers many applications like remote health monitoring, sports, military, healthcare, and so forth. In healthcare, it improves the patient's life quality based on a faster, accurate diagnosis of diseases and better treatment of patients. Security is a major challenging task to protect the life critical data against various security threats. Many health security systems have been proposed by various researchers for WBAN. Even though, none of the security algorithms achieved high security with efficient time. Hybrid encryption technique plays an essential role to provide high security as compared to previous methods. Therefore, this paper introduces Hybrid Encryption Algorithms (HEA) by combining symmetric key (Message Authentication Code [MAC]) and asymmetric key cryptographic techniques (Modified and Enhanced Lattice-Based Cryptography [MELBC]) are used to provide strong security, because symmetric techniques provide a high level of security and asymmetric provides key administration. Based on experimental results, it is proven that the proposed algorithm HEA provides high security than other security algorithms. 
id333#Robotics: Considerations for Practice#The term robotics traditionally refers to the utilization of machines to perform a mechanized task or series of tasks automatically. The author in this article traces some ethical precepts from the literature and proposes that future developments include nursing theory. This would provide the opportunity for bringing forth person-centered utilization of technologies within any healthcare context. 
id334#Computer vision-based interior construction progress monitoring: A literature review and future research directions#Computer vision (CV)-based technologies have been used to automate construction progress monitoring. The automation attempts to maximise precision and minimise human intervention in onsite progress monitoring. Such attempts have mainly focussed on exterior construction environments while there are significantly lesser number of studies on interior construction. This imbalance impedes automation of the onsite progress monitoring as a whole. Thus, the core intent of this study is to pave the way for advancing automated indoor progress monitoring by providing a systematic survey of extant literature. Main contributions of this survey include 1) presenting a full spectrum of CV-based approaches, tools, and algorithms adopted for indoor construction progress monitoring (ICPM) 2) portraying a succinct reference to the shortcomings, technical challenges, and scope limitations of the past studies on ICPM. The study then synthesises a readily usable agenda for hybridising CV with other data-driven technologies to improve automation in ICPM. 
id335#Optimized Implementation of Gift Cipher#The Internet of Things is an emerging area which deals with transfer of the data through the wired or wireless network. The prime thing that needs to be addressed in this is the security of the data that must be transferred within the optimized time limit. In this paper, throughput and time delay are need to be considered for the optimized data transfer and while concentrating on this, there is a possibility of allowing the data to be vulnerable to attacks. Security algorithms currently available may be adequate for the wired system and not as the same for wireless scenario. PRESENT cipher is a one of the popular cryptosystem used in wireless which falls under the light weight cryptography category. Gift cipher is an enhanced version of PRESENT cipher. Which aims that maximizing the throughput. In this, iteration structure used for encryption. This can still be improved and optimized in terms of increased data rate and reduced time delay. In this paper, implements the optimization technique of the existing GIFT cipher and throughput is considered as the performance metrics. Pipeline and sub-stage pipeline techniques are used for enhancing the architecture. 
id337#Constrained multiple planar reconstruction for automatic camera calibration of intelligent vehicles#In intelligent vehicles, extrinsic camera calibration is preferable to be conducted on a regular basis to deal with unpredictable mechanical changes or variations on weight load distribution. Specifically, high-precision extrinsic parameters between the camera coordinate and the world coordinate are essential to implement high-level functions in intelligent vehicles such as distance estimation and lane departure warning. However, conventional calibration methods, which solve a Perspective-n-Point problem, require laborious work to measure the positions of 3D points in the world coordinate. To reduce this inconvenience, this paper proposes an automatic camera calibration method based on 3D reconstruction. The main contribution of this paper is a novel reconstruction method to recover 3D points on planes perpendicular to the ground. The proposed method jointly optimizes reprojection errors of image features projected from multiple planar surfaces, and finally, it significantly reduces errors in camera extrinsic parameters. Experiments were conducted in synthetic simulation and real calibration environments to demonstrate the effectiveness of the proposed method. 
id338#Ensemble feature extraction for multi-container quality-diversity algorithms#Quality-Diversity algorithms search for large collections of diverse and high-performing solutions, rather than just for a single solution like typical optimisation methods. They are specially adapted for multi-modal problems that can be solved in many different ways, such as complex reinforcement learning or robotics tasks. However, these approaches are highly dependent on the choice of feature descriptors (FDs) quantifying the similarity in behaviour of the solutions. While FDs usually needs to be hand-designed, recent studies have proposed ways to define them automatically by using feature extraction techniques, such as PCA or Auto-Encoders, to learn a representation of the problem from previously explored solutions. Here, we extend these approaches to more complex problems which cannot be efficiently explored by relying only on a single representation but require instead a set of diverse and complementary representations. We describe MC-AURORA, a Quality-Diversity approach that optimises simultaneously several collections of solutions, each with a different set of FDs, which are, in turn, defined automatically by an ensemble of modular auto-encoders. We show that this approach produces solutions that are more diverse than those produced by single-representation approaches. 
id339#Certificateless Signcryption Scheme from Lattice#Certificateless signcryption can simultaneously provide certificateless signature and encryption. In recent years, many certificateless signcryption schemes have been proposed. However, these schemes are based on traditional mathematical theory and not have the ability of resisting the quantum computing attacks. Up to now, lattice-based certificateless signature or encryption schemes have been proposed; however, these schemes only have one function and cannot fulfill two functions of certificateless signature and encryption at the same time. In consideration of this reason, in this article, a certificateless signcryption scheme from lattice (L-CLSS) is constructed. L-CLSS has three advantages. First, in the random oracle model, it is provably indistinguishable against adaptive ciphertext-chosen attacks under the intractability of the learning with errors (LWE) problem and unforgeable against adaptive message-chosen attacks under the hardness of small integer solution (SIS) problem. Second, comparison with the cryptographic algorithms under the difficulty of large integer decomposition, L-CLSS can resist the quantum computing attacks. Third, L-CLSS has higher computation efficiency and a lower communication cost than the existing schemes. 
id340#Mining Associations Rules Between Attribute Value Clusters#The approach presented in this paper clusters values of each attribute in a relational database and then finds associations between two clusters belonging to different attributes. The first experiment has shown that the approach proposed is accurate in discovering clusters and associations between them. The second experiment conducted on real dataset has discovered clusters of values of attribute ‘petiole thickness,’ wherein each cluster is uniquely associated with a particular species of a plant. 
id341#I know you are looking to me: Enabling eye-gaze communication between small children and parents with visual impairments#Eye-gaze interaction is a relevant mean of communication from the early infancy. The bonding between infants and their care-takers is Strengthened through eye contact. Parents with visual impairments are excluded of this type of interaction with their children. Thus, nowadays computer vision technologies allow to track eye-gaze with different purposes, even users with visual impairments are enable to recognize faces. This work starts from the following research question: Can current available eye tracking solutions aid parents with visual impairments to have eye-gaze interaction with their young infants children? We devised a software prototype based on currently available eye tracking technologies which was tested with three sets of visually impaired parents and their young infant children to explore the possibility to assist those parents to have eye-gaze interaction with their children. The experience was documented as semi-structured interviews which were processed with a content analysis technique. The approach got positive feedback in the functionality and Emotional interaction aspects. 
id342#Detection and recognition of batteries on X-Ray images of waste electrical and electronic equipment using deep learning#The trend of increased use of lithium-ion batteries, challenges the cost-effectiveness and safety of manual battery separation during the end-of-life treatment of Waste Electric and Electronic Equipment (WEEE). Therefore, the need for novel techniques to separate and sort batteries from WEEE is increasingly important. For this reason, the presented research investigates the potential to facilitate the development of novel techniques for battery extraction and sorting by examining the technical feasibility of predicting the presence, location, and type of batteries inside electronic devices with a deep learning object detection network using X-Ray images of the internal structure of WEEE. To determine the required X-ray imaging parameters, 532 electronic devices were arbitrarily collected from a recycling facility. From each product, two X-Ray Transmission (XRT) images were captured at two different X-Ray source configurations. Results obtained with the limited dataset are promising, demonstrating a 91% true positive rate and only a 6% false positive rate for classifying battery-containing devices. Moreover, a precision of 89% and a recall of 81% are demonstrated for battery detection, and an average precision of 85% and an average recall of 76% are demonstrated to distinguish amongst the following six battery technologies: cylindrical nickel-metal hydride or nickel-cadmium, cylindrical alkaline, cylindrical zinc-carbon, cylindrical lithium-ion, pouch lithium-ion, and button cell batteries. These results demonstrate the potential of using deep learning object detection on XRT-generated images for both automated battery extraction and sorting, regardless of the condition or shape of the products. 
id343#A detailed analysis of primal attack and its variants#Primal attack is a typically considered strategy to estimate the hardness of cryptosystem based on learning with errors problem (LWE), it reduces the LWE problem to the unique-SVP by embedding technique and then employs lattice reduction such as BKZ to find the shortest vector. The main reason for the popularity of primal attack is its conservative estimation, in general, the complexity of primal attack is estimated by the hardness of core-SVP as T= 2 0.292b. In this work, we first revisit primal attack and give supplemental proof of the scaling factor in Bai-Galbraith embedding, whose value was given according to the experimental results. Then we refine primal attack in two special cases and analyze the variants in detail. One is that, for sparse secret LWE (or sparse secret-error LWE), primal attack with dropping makes a trade-off between guessing zero components and solving dimension-reduced problems to improve the complexity. The other is that, when TBKZ(b) = poly (d) ⋅ TSieve(b) holds in practice, primal attack with preprocessing reduces the time complexity by a factor of 26−210 through dividing primal attack into three steps and considering them independently. 
id344#Street vendor detection: Helping municipalities make decisions with actionable insights#Street vendors are quite common in countries across the world. By the prevalence of mobile surveillance systems, increasing demand for automatic detection of street vendors for further decisions and planning by the city administrators emerged. In this paper, an object detector is developed using a MobileNet SSD object detection algorithm to detect vendors on the street. For this study images were used, however, in the future this technique could be used for real time video footage from street cameras. Since this is the first study tackling this issue, a data set was created from scratch. The accuracy achieved by the algorithm is promising considering the size of the data set and the minimal computational power available. The goal of this research is to pave the way for more work to be done in this area and help municipalities improve their decision making process regarding street vendor activities in countries like Mexico, Pakistan, China, Turkey, etc. 
id345#Verifying functional formalizations - A type-theoretical case study in PVS#In this case study we investigate the use of PVS for developing type theoretical concepts and verifying the correctness of a typing algorithm. PVS turns out to be very useful for the efficient development of a sound basic theory about polymorphic typing. The PVS formalization is also intended as the first step towards a functional training vehicle for the education of compiler construction. 
id346#Qcompiler: Quantum compilation with the CSD method#In this paper, we present a general quantum computation compiler, which maps any given quantum algorithm to a quantum circuit consisting a sequential set of elementary quantum logic gates based on recursive cosine-sine decomposition. The resulting quantum circuit diagram is provided by directly linking the package output written in LaTeX to Qcircuit.tex <http://www. cquic.org/Qcircuit >. We illustrate the use of the Qcompiler package through various examples with full details of the derived quantum circuits. Besides its accuracy, generality and simplicity, Qcompiler produces quantum circuits with significantly reduced number of gates when the systems under study have a high degree of symmetry. 
id348#Salient object detection using feature clustering and compactness prior#Salient object detection has been challenging computer vision though some advances have been made recently. In this study, we propose a novel salient object detection method by using feature clustering and compactness prior, in the situation of the absence of any prior information. The proposed method consists of four rigorous steps. Superpixel preprocessing is first employed to segment image into superpixels for suppressing noise and reducing computational complexity. Then, clustering algorithm is applied to get the classification of color features. Furthermore, two-dimensional entropy is used to measure the compactness of each cluster and build the background model. Finally, the salient feature is defined as the contrast between background region and other regions, and enhanced by designing a Gauss filter. To better evaluate the salient object detection accuracy, detailed experimental analysis is carried out by using 7 evaluation indexes. Our proposed method outperforms some peers in extensive experiments. It will inspire more similar techniques to be developed in this research topic. 
id349#A Novel False Alarm Suppression Method for CNN-Based SAR Ship Detector#Synthetic aperture radar (SAR) ship detection is an important part of remote sensing applications. With the development of computer vision, SAR ship detection methods based on convolutional neural network (CNN) can directly perform end-to-end detection of near-shore ship targets. However, CNN-based methods are prone to generate false targets on land areas, especially when using a rotatable bounding box (RBox) for detection. Therefore, how to reduce the false alarm rate becomes a key direction in research for SAR ship detection. In this letter, the problem of negative sample intraclass imbalance in the training stage of CNN-based detection methods is pointed out for the first time, which is considered to be an important reason for the excessive false alarm rate in the land area. Then, a method is proposed to reduce the false targets generated in the land area by CNN-based detection methods. First, an RBox-based model is proposed as the basic architecture for detection. Then, a new loss function is adopted to guide the model to balance the loss contribution of different negative samples during the training stage. The experimental results prove that the proposed method can effectively reduce the false alarm rate of the model and boost the performance of CNN-based detection methods. 
id350#Ascon v1.2: Lightweight Authenticated Encryption and Hashing#Authenticated encryption satisfies the basic need for authenticity and confidentiality in our information infrastructure. In this paper, we provide the specification of Ascon-128 and Ascon-128a. Both authenticated encryption algorithms provide efficient authenticated encryption on resource-constrained devices and on high-end CPUs. Furthermore, they have been selected as the “primary choice” for lightweight authenticated encryption in the final portfolio of the CAESAR competition. In addition, we specify the hash function Ascon-Hash, and the extendable output function Ascon-Xof. Moreover, we complement the specification by providing a detailed overview of existing cryptanalysis and implementation results. 
id351#Tilting at windmills with Coq: Formal verification of a compilation algorithm for parallel moves#This article describes the formal verification of a compilation algorithm that transforms parallel moves (parallel assignments between variables) into a semantically-equivalent sequence of elementary moves. Two different specifications of the algorithm are given: an inductive specification and a functional one, each with its correctness proofs. A functional program can then be extracted and integrated in the Compcert verified compiler. 
id353#Backwards-compatible array bounds checking for C with very low overhead#The problem of enforcing correct usage of array and pointer references in C and C++ programs remains unsolved. The approach proposed by Jones and Kelly (extended by Ruwase and Lam) is the only one we know of that does not require significant manual changes to programs, but it has extremely high overheads of 5x-6x and 11x-12x in the two versions. In this paper, we describe a collection of techniques that dramatically reduce the overhead of this approach, by exploiting a fine-grain partitioning of memory called Automatic Pool Allocation. Together, these techniques bring the average overhead checks down to only 12% for a set of benchmarks (but 69% for one case). We show that the memory partitioning is key to bringing down this overhead. We also show that our technique successfully detects all buffer overrun violations in a test suite modeling reported violations in some important real-world programs. Copyright 2006 ACM.
id354#What level of mathematical reasoning can computer science demand of a software implementer?#The article starts out from the observation that software engineering splits in two large activity areas: Software specification with its verification and software implementation with its verification. To find answers to the question in the title the article studies a practical systems software engineering area where theory is better developed than compared to other areas: Compiler construction. Our answer is a conclusion from work in the DFG-project Verifix, U.Karlsruhe, U.Kiel, U.Ulm, 1995-2003. One very complex cooperational task has been construction of a so called initial correct compiler for a realistic high level programming (and compiler writing) language correctly implemented and executed on a real life host processor. The interface between compiling specification and compiler implementation is given by algebraic-style, conditional formula transformation or program term rewriting rules which the specifier figures out and must prove correct w. r. t. source program and target processor semantics and data and states representations. Intensive cooperation of compiling specifiers and compiler implementers has revealed that the implementer's mathematical reasoning is algebraic reasoning of moderate depth. The specifier overtakes semantical issues and does induction proofs, a field of much more intricate mathematical reasoning.
id355#Protecting medical images by using fused cryptographic technique with fog computing#Medical Service providers are moving towards Cloud Computing as information sharing is a prevalent stage for any medical organization. With the enhancement of innovation, a titanic measure of information is creating with time. Cloud Computing gives a tremendous information stockpiling limit with the adaptability of getting to it without the time and place limitations with virtualized assets. To access information from any topographical area, the medical services industry is moving towards cloud processing. Tremendous expansion in clinical pictures presents a major test for medical services suppliers as they need to oversee, process and share these data with low cost. Medical images show restraint's computerized records, which incorporate touchy data of patients. Apart from all the capable assistance given by cloud processing, security is an essential worry for different associations. To address the security issue, a few cryptographic strategies carried out by analysts around the world. In this paper, an efficient fused Cryptographic technique which is a combination of Elliptic Curve Cryptography and DNA Cryptography with Fog computing facility is discussed. 
id356#Encryption based on multilevel security for relational database EBMSR#Cryptography is one of the most important sciences today because of the importance of data and the possibility of sharing data via the Internet. Therefore, data must be preserved when stored or transmitted over the Internet. Encryption is used as a solution to protect information during the transmission via an open channel. If the information is obtained illegally, the opponent/ enemy will not be able to understand the information due to encryption. In this paper we have developed a cryptosystem for testing the concepts of multi security level. The information is encrypted using more than one encryption algorithm based on the security level. The proposed cryptosystem concerns of Encryption Based on Multilevel Security (MLS) Model for DBMS. The cryptosystem is designed for both encryption and decryption. 
id357#Lean meat yield estimation using a prototype 3D imaging approach#Lean Meat Yield (LMY, %) of carcass is an important industry trait, which currently is not routinely measured in Australian beef abattoirs. Objective on-line technology to determine LMY is key for wider adoption. This paper presents a proof-of-concept approach for estimating the LMY of beef carcasses from the 3D information provided by RGB-D cameras. Moreover, a specifically designed on-line data acquisition system for abattoir applications is presented, consisting of three cameras moving on a scanning rig to generate 3D carcass side reconstructions. The hindquarter is then segmented consistently across all the 3D models to extract curvature information and LMY estimated via non-linear regression based on Gaussian Process models. Sides from 119 carcasses at two different commercial abattoirs were used to evaluate this approach. Results from this preliminary study (RMSE = 3.91%, R2 = 0.69) using curvature, P8 fat and HSCW indicate that 3D imaging of beef carcasses is a viable and relatively accurate technology to estimate LMY. 
id358#Visual question answering model based on graph neural network and contextual attention#Visual Question Answering (VQA) has recently appeared as a hot research area in the field of computer vision and natural language processing. A VQA model uses both image and question features and fuses them to predict an answer for a given natural question related to an image. However, most VQA approaches using attention mechanism mainly concentrate on extraction of visual information from regions of interests for answer prediction and ignore the relation between the regions of interests together with the reasoning among these regions. Apart from this limitation, VQA approaches also ignore the regions which are previously attended for answer generation. These regions which are attended in past can guide the selection of the subsequent regions of attention. In this paper, a novel VQA model is presented and formulated that utilizes this relationship between the regions and employs visual context based attention that takes into account the previously attended visual content. Experimental results demonstrate that the proposed VQA model boosts the accuracy of answer prediction on publically available datasets VQA 1.0 and VQA 2.0. 
id359#Tensor Methods in Computer Vision and Deep Learning#Tensors, or multidimensional arrays, are data structures that can naturally represent visual data of multiple dimensions. Inherently able to efficiently capture structured, latent semantic spaces and high-order interactions, tensors have a long history of applications in a wide span of computer vision problems. With the advent of the deep learning paradigm shift in computer vision, tensors have become even more fundamental. Indeed, essential ingredients in modern deep learning architectures, such as convolutions and attention mechanisms, can readily be considered as tensor mappings. In effect, tensor methods are increasingly finding significant applications in deep learning, including the design of memory and compute efficient network architectures, improving robustness to random noise and adversarial attacks, and aiding the theoretical understanding of deep networks. This article provides an in-depth and practical review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on visual data analysis and computer vision applications. Concretely, besides fundamental work in tensor-based visual data analysis methods, we focus on recent developments that have brought on a gradual increase in tensor methods, especially in deep learning architectures and their implications in computer vision applications. To further enable the newcomer to grasp such concepts quickly, we provide companion Python notebooks, covering key aspects of this article and implementing them, step-by-step with TensorLy. 
id361#Compiler-directed channel allocation for saving power in on-chip networks#Increasing complexity in the communication patterns of embedded applications parallelized over multiple processing units makes it difficult to continue using the traditional bus-based on-chip communication techniques. The main contribution of this paper is to demonstrate the importance of compiler technology in reducing power consumption of applications designed for emerging multi-processor, NoC (Network-on-Chip) based embedded systems. Specifically, we propose and evaluate a compiler-directed approach to NoC power management in the context of array-intensive applications, used frequently in embedded image/video processing. The unique characteristic of the compiler-based approach proposed in this paper is that it increases the idle periods of communication channels by reusing the same set of channels for as many communication messages as possible. The unused channels in this case take better advantage of the underlying power saving mechanism employed by the network architecture. However, this channel reuse optimization should be applied with care as it can hurt performance if two or more simultaneous communications are mapped onto the same set of channels. Therefore, the problem addressed in this paper is one of reducing the number of channels used to implement a set of communications without increasing the communication latency significantly. To test the effectiveness of our approach, we implemented it within an optimizing compiler and performed experiments using twelve application codes and a network simulation environment. Our experiments show that the proposed compiler-based approach is very successful in practice and works well under both hardware based and software based channel turn-off schemes. Copyright 
id362#Post-processing integration and semi-Automated analysis of eye-Tracking and motion-capture data obtained in immersive virtual reality environments to measure visuomotor integration#Mobile eye-Tracking and motion-capture techniques yield rich, precisely quantifiable data that can inform our understanding of the relationship between visual and motor processes during task performance. However, these systems are rarely used in combination, in part because of the significant time and human resources required for post-processing and analysis. Recent advances in computer vision have opened the door for more efficient processing and analysis solutions. We developed a post-processing pipeline to integrate mobile eye-Tracking and full-body motion-capture data. These systems were used simultaneously to measure visuomotor integration in an immersive virtual environment. Our approach enables calculation of a 3D gaze vector that can be mapped to the participant's body position and objects in the virtual environment using a uniform coordinate system. This approach is generalizable to other configurations, and enables more efficient analysis of eye, head, and body movements together during visuomotor tasks administered in controlled, repeatable environments. 
id364#Objective Evaluation of Fabric Flatness Grade Based on Convolutional Neural Network#As an important indicator for the appearance and intrinsic quality of textiles, fabric flatness is the immediate cause affecting the aesthetic appearance and performance of textiles. In this paper, the objective evaluation system of fabric flatness based on 3D scanner and convolutional neural network (CNN) is constructed by using the height data of AATCC flatness template. The 3D scanner is responsible for the collection of the height value data of the sample. The effect of different sub-sample cutting sizes, cutting offsets, and network model depths on the objective evaluation coincidence rate of multiple flatness level was studied. The experimental results show that the coincidence rate of the system reaches 98.9% when the collected sample data are cut into subsamples of 20 pixel × 20 pixel with 12 pixel cutting offsets and the 11-layer network model is selected. Finally, this scheme is used to evaluate the flatness of four real fabrics with different colors and textures. The result shows that all of the samples can achieve a higher coincidence rate, which further verifies the adaptability and stability of the objective evaluation system constructed in this paper for fabric flatness evaluation. 
id365#Pig-posture recognition based on computer vision: Dataset and exploration#Posture changes in pigs during growth are often precursors of disease. Monitoring pigs’ behavioral activities can allow us to detect pathological changes in pigs earlier and identify the factors threatening the health of pigs in advance. Pigs tend to be farmed on a large scale, and manual observation by keepers is time consuming and laborious. Therefore, the use of computers to monitor the growth processes of pigs in real time, and to recognize the duration and frequency of pigs’ pos-tural changes over time, can prevent outbreaks of porcine diseases. The contributions of this article are as follows: (1) The first human-annotated pig-posture-identification dataset in the world was established, including 800 pictures of each of the four pig postures: standing, lying on the stomach, lying on the side, and exploring. (2) When using a deep separable convolutional network to classify pig postures, the accuracy was 92.45%. The results show that the method proposed in this paper achieves adequate pig-posture recognition in a piggery environment and may be suitable for livestock farm applications. 
id367#A review on lightweight cryptography for Internet-of-Things based applications#The Internet-of-Things (IoT) is a combination of an intelligent infrastructure combined with various self-organizing devices. These devices are used to monitor the environment and help to exchange sensitive data over the Internet without much human interference. Such a huge network of unmanned devices are subjected to various security and privacy concern. As these devices are battery powered and have low inbuilt resources, it is important to enable secure and resource-constrained security solutions to secure the devices. Thereby, to address the security and privacy of these devices and the data, the authentication plays an important role along with data integrity. Through this paper, we have analyzed the various lightweight solution and their security threats under the authentication and data integrity of the IoT applications. From the study, it can be seen that the major security concern of these protocols is to perform with less computation and resist to attacks like man-in-the-middle, replay attacks, denial of service attacks, forgery and chosen-ciphertext attacks. Also, this review provides an insight into using the Microsoft threat modeling tool used for IoT based applications. 
id369#Compiler transformations for effectively exploiting a zero overhead loop buffer#A Zero Overhead Loop Buffer (ZOLB) is an architectural feature that is commonly found in DSP processors. This buffer can be viewed as a compiler managed cache that contains a sequence of instructions that will be executed a specified number of times without incurring any loop overhead. Unlike loop unrolling, a loop buffer can be used to minimize loop overhead without the penalty of increasing code size. In addition, a ZOLB requires relatively little space and power, which are both important considerations for most DSP applications. This paper describes strategies for generating code to effectively use a ZOLB. We have found that many common code improving transformations used by optimizing compilers on conventional architectures can be easily used to (1) allow more loops to be placed in a ZOLB, (2) further reduce loop overhead of the loops placed in a ZOLB, and (3) avoid redundant loading of ZOLB loops. The results given in this paper demonstrate that this architectural feature can often be exploited with substantial improvements in execution time and slight reductions in code size for various signal processing applications. Copyright 
id370#A Survey of Robot Learning Strategies for Human-Robot Collaboration in Industrial Settings#Increased global competition has placed a premium on customer satisfaction, and there is a greater demand for manufacturers to be flexible with their products and services. This challenge is usually addressed with the introduction of human operators for precise tasks that require dexterity, flexibility and cognitive decision making. On the other hand, robots, through automation, are very effective in carrying out repetitive, non-ergonomic tasks. Owing to the complementary nature of robots’ and humans’ capabilities, there is an increased interest towards a shared workspace for humans and robots to work together collaboratively, forming the motivation behind the field of human-robot collaboration (HRC). Research in HRC in industry is concerned with the safety of the humans and robots, extent, and modes of collaboration among them, and the level of autonomy and adaptability of robots that can be trained for different tasks. This paper introduces a novel taxonomy of levels of interaction between humans and robots along the lines of SAEs guidelines for autonomous vehicles in response to a need for standard definitions and evolving nature of the field. Research into modes of communication for HRC driven by machine learning are reviewed followed by broad definitions of the types of machine learning. The authors also present a comprehensive review of the machine learning (ML) methodologies and industrial applications of the same in the context of adaptable collaborative robots. 
id371#Mammobot: A miniature steerable soft growing robot for early breast cancer detection#This letter presents MAMMOBOT, one of the first millimetre-scale steerable soft growing robots for medical applications. MAMMOBOT aims to access the breast through the nipple and navigate the mammary ducts to detect precursors of invasive breast cancers. Addressing limitations of the state-of-the-art, MAMMOBOT maintains a hollow inner lumen throughout its soft body, enabling the passing of instruments such as miniature endoscopes, biopsy needles, and optical probes for in situ histopathology. MAMMOBOT is developed by a novel manufacturing approach entailing dual LDPE sheet adhesion with localised heat treatment. MAMMOBOT's steerability is achieved through a sub-millimetre profiled tendon-driven catheter that passes through its inner lumen. A duty cycle controller governs steering versus growing to achieve navigation in complex environments within a human-in-the-loop framework. Benchtop experimental evaluation demonstrates the robot's capabilities and agreement with a Reduced-Order Mode (ROM) of its dynamics. Finally, experimental evaluation on a bespoke breast phantom developed for the purposes of this project demonstrates the clinical relevance and potential impact of MAMMOBOT. 
id372#Basketball shooting technology based on acceleration sensor fusion motion capture technology#Computer vision recognition refers to the use of cameras and computers to replace the human eyes with computer vision, such as target recognition, tracking, measurement, and in-depth graphics processing, to process images to make them more suitable for human vision. Aiming at the problem of combining basketball shooting technology with visual recognition motion capture technology, this article mainly introduces the research of basketball shooting technology based on computer vision recognition fusion motion capture technology. This paper proposes that this technology first performs preprocessing operations such as background removal and filtering denoising on the acquired shooting video images to obtain the action characteristics of the characters in the video sequence and then uses the support vector machine (SVM) and the Gaussian mixture model to obtain the characteristics of the objects. Part of the data samples are extracted from the sample set for the learning and training of the model. After the training is completed, the other parts are classified and recognized. The simulation test results of the action database and the real shot video show that the support vector machine (SVM) can more quickly and effectively identify the actions that appear in the shot video, and the average recognition accuracy rate reaches 95.9%, which verifies the application and feasibility of this technology in the recognition of shooting actions is conducive to follow up and improve shooting techniques. 
id373#Secure image encryption scheme using 4D-Hyperchaotic systems based reconfigurable pseudo-random number generator and S-Box#This paper introduces the design of a hardware efficient reconfigurable pseudorandom number generator (PRNG) using two different feedback controllers based four-dimensional (4D) hyperchaotic systems i.e. Hyperchaotic-1 and -2 to provide confidentiality for digital images. The parameter's value of these two hyperchaotic systems is set to be a specific value to get the benefits i.e. all the multiplications (except a few multiplications) are performed using hardwired shifting operations rather than the binary multiplications, which doesn't utilize any hardware resource. The ordinary differential equations (ODEs) of these two systems have been exploited to build a generic architecture that fits in a single architecture. The proposed architecture provides an opportunity to switch between two different 4D hyperchaotic systems depending on the required behavior. To ensure the security strength, that can be also used in the encryption process in which encrypt the input data up to two times successively, each time using a different PRNG configuration. The proposed reconfigurable PRNG has been designed using Verilog HDL, synthesized on the Xilinx tool using the Virtex-5 (XC5VLX50T) and Zynq (XC7Z045) FPGA, its analysis has been done using Matlab tool. It has been found that the proposed architecture of PRNG has the best hardware performance and good statistical properties as it passes all fifteen NIST statistical benchmark tests while it can operate at 79.101-MHz or 1898.424-Mbps and utilize only 0.036 %, 0.23 %, and 1.77 % from the Zynq (XC7Z045) FPGA's slice registers, slice LUTs, and DSP blocks respectively. Utilizing these PRNGs, we design two 16 × 16 substitution boxes (S-boxes). The proposed S-boxes fulfill the following criteria: Bijective, Balanced, Non-linearity, Dynamic Distance, Strict Avalanche Criterion (SAC) and BIC non-linearity criterion. To demonstrate these PRNGs and S-boxes, a new three different scheme of image encryption algorithms have been developed: a) Encryption using S-box-1, b) Encryption using S-box-2 and, c) Two times encryption using S-box-1 and S-box-2. To demonstrate that the proposed cryptosystem is highly secure, we perform the security analysis (in terms of the correlation coefficient, key space, NPCR, UACI, information entropy and image encryption quantitatively in terms of (MSE, PSNR and SSIM)). 
id374#A semi-automatic semantic consistency-checking method for learning ontology from relational database#To tackle the issues of semantic collision and inconsistencies between ontologies and the original data model while learning ontology from relational database (RDB), a semi-automatic semantic consistency checking method based on graph intermediate representation and model checking is presented. Initially, the W-Graph, as an intermediate model between databases and ontologies, was utilized to formalize the semantic correspondences between databases and ontologies, which were then transformed into the Kripke structure and eventually encoded with the SMV program. Meanwhile, description logics (DLs) were employed to formalize the semantic specifications of the learned ontologies, since the OWL DL showed good semantic compatibility and the DLs presented an excellent expressivity. Thereafter, the specifications were converted into a computer tree logic (CTL) formula to improve machine readability. Furthermore, the task of checking semantic consistency could be converted into a global model checking problem that could be solved automatically by the symbolic model checker. Moreover, an example is given to demonstrate the specific process of formalizing and checking the semantic consistency between learned ontologies and RDB, and a verification experiment was conducted to verify the feasibility of the presented method. The results showed that the presented method could correctly check and identify the different kinds of inconsistencies between learned ontologies and its original data model. 
id378#RDBMS Based Hadoop Metadata and Log Data Management Optimization#At the moment, metadata is one of the fastest growing sub-segments of enterprise data management. While metadata is growing, it is not able to keep pace with the rapid increase of Big Data projects being currently initiated by organizations. Nowadays, it refers to this as the 'Big Data Gap'. This paper introduces novel approach by bringing Apache Hadoop and Relational database together to minimize the query time, resource usage, and increase the fault tolerance, and efficiency. Hadoop's metadata and log files are synchronously being migrated to the PGSQL and easily controlled through the graphical user interface. The experiment part has used 100.000's of movie rates dataset and decreased the resource usage of NameNode by giving the task of log and metadata analysis to the PGSQL. The query time in PGSQL is 1.5 times faster than Hadoop and the data format is in structured format comparing to Hadoop. Although, the technique implemented on a single node, it outperformed existing hadoop on premise and on cloud. The technique makes the metadata and log data management easier through the GUI that uses charts and graphs. The results suggest that the proposed approach performs better than existing solution and sharply decreases the usage of Big Data hardware systems and budget as well. 
id380#FisOmics: A portal of fish genomic resources#An online portal, accessible at URL: http://mail.nbfgr.res.in/FisOmics/, was developed that features different genomic databases and tools. The portal, named as FisOmics, acts as a platform for sharing fish genomic sequences and related information in addition to facilitating the access of high-performance computational resources for genome and proteome data analyses. It provides the ability for quarrying, analysing and visualizing genomic sequences and related information. The featured databases in FisOmics are in the World Wide Web domain already. The aim to develop portal was to provide a nodal point to access the featured databases and work conveniently. Presently, FisOmics includes databases on barcode sequences, microsatellite markers, mitogenome sequences, hypoxia-responsive genes and karyology of fishes. Besides, it has a link to other molecular resources and reports on the on-going activities and research achievements. 
id381#Adaptive source-level data assignment to dual memory banks#Dual memory banks provide extra memory bandwidth to DSP applications and enable simultaneous access to two operands if the data is partitioned appropriately. Fully automated and compiler integrated approaches to data partitioning and memory bank assignment have, however, found little acceptance by DSP software developers. In this article we present a novel source-level approach that is more programmer friendly. Our scheme is based on soft graph coloring and highly adaptive heuristics generated by genetic programming. We have evaluated our scheme on an Analog Devices TigerSHARC TS-101 DSP and achieved speedups of up to 57% on 13 UTDSP benchmarks. 
id382#Novel Low-Complexity Polynomial Multiplication over Hybrid Fields for Efficient Implementation of Binary Ring-LWE Post-Quantum Cryptography#Post-quantum cryptography (PQC) refers to the cryptosystem that can resist the attacks launched from mature quantum computers in the not far future and has recently gained intensive attention from the research community as most of the existing public-key cryptosystems are vulnerable to attacks from quantum computers. Ring-Learning-with-Errors (Ring-LWE)-based scheme is an essential type of the lattice-based PQC due to its strong security proof and ease of implementation. As the latest variant of the Ring-LWE, the binary Ring-LWE (BRLWE)-based scheme possesses even smaller computational complexity and thus is more suitable for resource-constrained applications. However, the existing works have not well covered various aspects related to this new scheme, especially on the low-complexity hardware implementation. In this paper, we aim to present a novel implementation of the BRLWE-based scheme on the hardware platform with very low-complexity with this point of view. To carry out the specified work in a successful manner, we have proposed mainly four layers of coherent interdependent efforts: (i) we have provided the necessary algorithmic derivation process in detail to formulate the desired algorithm for the polynomial multiplication over hybrid fields, which is the major arithmetic component of the BRLWE scheme; (ii) we have presented the corresponding hardware architecture in a thorough format with sufficient description of the internal structures; (iii) we have also provided the complexity analysis and implementation-based comparison to demonstrate the superior performance of the proposed polynomial multiplication over the state-of-the-art design; (iv) finally, we have extended the proposed low-complexity polynomial multiplication to the major operational phase of the BRLWE scheme. We have shown that the proposed BRLWE structure involves significantly lower area-time complexities over the existing design, e.g., the proposed design has at least 66.01% less area-delay product (ADP) than the newly reported (Straix V device). Overall, the proposed design and implementation strategies are highly efficient, and the proposed BRLWE structure is desirable for many emerging applications. 
id384#Novel fault attack resistant architecture for elliptic curve cryptography#Hardware implementations of cryptosystems are susceptible to fault attacks. By analyzing the side channel information from implementation, the attacker can retrieve the secret information. Generally, in the hardware implementations, validations of results are reported at the end of the computation. If faults are injected at the input side of computation, all the computations performed afterward are wasteful and this is a potential situation which can leak the secret key information using side channel attacks. The current work proposes fault attack resistant implementation of an elliptic curve cryptosystem using a shared point validator unit, zero-one detector, and double coherence check by modified Montgomery Powering Ladder Algorithm. The architecture is robust to fault attacks along with power and area efficiency. 
id385#Adaptive fast scale estimation, with accurate online model update based on kernelized correlation filter#Despite the considerable advances that are emerged in correlation filter-based tracking, in fact, they may achieve excellent performance in robustness, speed, and accuracy; they still fail when dealing with large-scale alteration and show the inability to handle long-term tracking in complex scenarios, where the object undergoes partial occlusion, out-of-view, and deformation. In this paper, we propose a robust approach to address two important problems: the first one is scale estimation in kernelized correlation filter (KCF), and the second one is how to update the model in the process of tracking. We aim in this work to overcome the scale fixed size limitation of kernelized correlation filter-based tracking algorithms and improve the mechanism of model online training. Our approach learns a separate correlation filter to estimate the accurate target scale by finding the scale's candidate that maximizes the output response of the correlation filter mentioned above. Besides, we define a minimum rate of similarity for the online model update to avoid training with failure detections. Our approach is evaluated in terms of precision and accuracy, on a commonly used tracking benchmark with 100 challenging videos; the experimental results show that our proposed tracker outperforms the KCF algorithm and shows promising performance compared to state-of-the-art tracking methods. 
id386#A secure visual secret sharing (VSS) scheme with CNN-based image enhancement for underwater images#Nowadays, underwater images are being used to identify various important resources like objects, minerals, and valuable metals. Due to the wide availability of the Internet, we can transmit underwater images over a network. As underwater images contain important information, there is a need to transmit them securely over a network. Visual secret sharing (VSS) scheme is a cryptographic technique, which is used to transmit visual information over insecure networks. Recently proposed randomized VSS (RVSS) scheme recovers secret image (SI) with a self-similarity index (SSIM) of 60–80%. But, RVSS is suitable for general images, whereas underwater images are more complex than general images. In this paper, we propose a VSS scheme using super-resolution for sharing underwater images. Additionally, we have removed blocking artifacts from the reconstructed SI using convolution neural network (CNN)-based architecture. The proposed CNN-based architecture uses a residue image as a cue to improve the visual quality of the SI. The experimental results show that the proposed VSS scheme can reconstruct SI with almost 86–99% SSIM. 
id387#Mining guidelines for architecting robotics software#Context: The Robot Operating System (ROS) is the de-facto standard for robotics software. However, ROS-based systems are getting larger and more complex and could benefit from good software architecture practices. Goal: We aim at (i) unveiling the state-of-the-practice in terms of targeted quality attributes and architecture documentation in ROS-based systems, and (ii) providing empirically-grounded guidance to roboticists about how to properly architect ROS-based systems. Methods: We designed and conducted an observational study where we (i) built a dataset of 335 GitHub repositories containing real open-source ROS-based systems, and (ii) mined the repositories to extract and synthesize quantitative and qualitative findings about how roboticists are architecting ROS-based systems. Results: First, we extracted an empirically-grounded overview of the state of the practice for architecting and documenting ROS-based systems. Second, we synthesized a catalog of 47 architecting guidelines for ROS-based systems. Third, the extracted guidelines were validated by 119 roboticists working on real-world open-source ROS-based systems. Conclusion: Roboticists can use our architecting guidelines for applying good design principles to develop robots that meet quality requirements, and researchers can use our results as evidence-based indications about how real-world ROS systems are architected today, thus inspiring future research contributions. 
id388#Sustainability of project-based learning by incorporating transdisciplinary design in fabrication of hydraulic robot arm#Wider acceptance of project-based learning (PjBL) in the tertiary education industry has been obstructed by its resource-intensive nature. This paper introduces a transdisciplinary variant of PjBL for undergraduate engineering students through a multidisciplinary complex engineering problem requiring the design and fabrication of a hydraulic robot arm. The robotics-inspired transdis-ciplinary PjBL variant was first evaluated through student feedback using the Chi-square hypothesis test, which, at Chi-square (4, N = 101) = 129.12; p < 0.05, revealed a statistically significant difference in the proportion of the student feedback in favor of the PjBL for sustainability of transdisciplinary project-based learning. Furthermore, the students’ PjBL and PbBL scores were subjected to the Mann–Whitney U test which concluded the effectiveness of PjBL against PbBL with statistical signifi-cance, U(N = 101) = 192.00, z = −11.826, p < 0.05. The results indicate that the novel transdisciplinary project-based learning (PjBL) approach develops students’ practical engineering knowledge spanning multiple disciplines, thereby resulting in a sustainable concept of project-based learning. 
id389#Materialization of OWL Ontologies from Relational Databases: A Practical Approach#Providing both end-users and applications with a uniform way to query legacy databases through a high-level ontology that models both the business logic and the underlying data sources is the main concern in Ontology-based Data Access (OBDA). Our goal in this research is providing tools for performing OBDA with relational and non-relational data sources. Within the OBDA framework, in this work, we present a prototype tool that can access an H2 database, allowing the user to explicitly express mappings, and populating an ontology that can be saved for later querying. We report on the current functionality of our tool, which includes creating, loading, saving a global ontology populated with a database or a CSV file. For the latter, we devised a language for specifying the underlying schema of the CSV file. We argue that this language is better suited than current alternatives such as JSON. Also, the system allows the user to visually express mappings from the database to the ontology and the ability to create databases for testing the behavior of the system in the presence of increasing workloads. Our tests indicate that the system can handle a moderate workload of tables of tens of thousands of records but fails to handle tables of millions of records. 
id390#Natural instruction level parallelism-aware compiler for high-performance QueueCore processor architecture#This work presents a static method implemented in a compiler for extracting high instruction level parallelism for the 32-bit QueueCore, a queue computation-based processor. The instructions of a queue processor implicitly read and write their operands, making instructions short and the programs free of false dependencies. This characteristic allows the exploitation of maximum parallelism and improves code density. Compiling for the QueueCore requires a new approach since the concept of registers disappears. We propose a new efficient code generation algorithm for the QueueCore. For a set of numerical benchmark programs, our compiler extracts more parallelism than the optimizing compiler for an RISC machine by a factor of 1.38. Through the use of QueueCore's reduced instruction set, we are able to generate 20% and 26% denser code than two embedded RISC processors. 
id391#Lattice-based key agreement protocol under ring-LWE problem for IoT-enabled smart devices#Advances in communication technologies along with the availability of Internet and Internet of Things (IoT) devices enable users to acquire various services over the Internet. However, IoT devices are prone to attacks on the open communication channel. Many authenticated key agreement schemes have been introduced in the last decades to improve security, where most of the schemes are based on the classical number-theoretic assumptions. Unfortunately, Shor’s algorithm provides the mechanism to solve the existing number-theory-based problems such as discrete logarithm, integer factorization, etc. As a result, the hard problems based on number theory could be solved very efficiently on a quantum computer using Shor’s algorithm. Therefore, the design of a protocol is required that can resist all known attacks by quantum computers. To address the security issues raised by Shor’s algorithm, we propose a lattice-based key agreement protocol under ring learning with errors (RLWE). Security analysis of the proposed protocol is also presented, where both informal security and formal security analyses are followed. The analysis of security clearly indicates that the proposed scheme is provably secure under a random oracle model. In addition we study the performance of the proposed scheme, which shows the enhancement in terms of performance. 
id392#Coupled Control Systems: Periodic Orbit Generation with Application to Quadrupedal Locomotion#A robotic system can be viewed as a collection of lower-dimensional systems that are coupled via reaction forces (Lagrange multipliers) enforcing holonomic constraints. Inspired by this viewpoint, this letter presents a novel formulation for nonlinear control systems that are subject to coupling constraints via virtual 'coupling' inputs that abstractly play the role of Lagrange multipliers. The main contribution of this letter is a process - mirroring solving for Lagrange multipliers in robotic systems - wherein we isolate subsystems free of coupling constraints that provably encode the full-order dynamics of the coupled control system from which it was derived. This dimension reduction is leveraged in the formulation of a nonlinear optimization problem for the isolated subsystem that yields periodic orbits for the full-order coupled system. We consider the application of these ideas to robotic systems, which can be decomposed into subsystems. Specifically, we view a quadruped as a coupled control system consisting of two bipedal robots, wherein applying the framework developed allows for gaits (periodic orbits) to be generated for the individual biped yielding a gait for the full-order quadrupedal dynamics. This is demonstrated on a quadrupedal robot through simulation and walking experiments on rough terrains. 
id393#Data-dependency graph transformations for instruction scheduling#This paper presents a set of efficient graph transformations for local instruction scheduling. These transformations to the data-dependency graph prune redundant and inferior schedules from the solution space of the problem. Optimally scheduling the transformed problems using an enumerative scheduler is faster and the number of problems solved to optimality within a bounded time is increased. Furthermore, heuristic scheduling of the transformed problems often yields improved schedules for hard problems. The basic node-based transformation runs in O(ne) time, where n is the number of nodes and e is the number of edges in the graph. A generalized subgraph-based transformation runs in O(n 2 e) time. The transformations are implemented within the Gnu Compiler Collection (GCC) and are evaluated experimentally using the SPEC CPU2000 floating-point benchmarks targeted to various processor models. The results show that the transformations are fast and improve the results of both heuristic and optimal scheduling. 
id394#Program transformation for numerical precision#This article introduces a new program transformation in order to enhance the numerical accuracy of floating-point computations.We consider that a program would return an exact result if the computations were carried out using real numbers. In practice, roundoff errors due to the finite representation of values arise during the execution. These errors are closely related to the way formulas are evaluated. Indeed, mathematically equivalent formulas, obtained using laws like associativity, distributivity, etc., may lead to very different numerical results in the computer arithmetic. We propose a semantics-based transformation in order to optimize the numerical accuracy of programs. This transformation is expressed in the abstract interpretation framework and it aims at rewriting pieces of numerical codes in order to obtain results closer to what the computer would output if it used the exact arithmetic. 
id396#Pairwise Preferences-Based Optimization of a Path-Based Velocity Planner in Robotic Sealing Tasks#Production plants are being re-designed to implement human-centered solutions. Especially considering high added-value operations, robots are required to optimize their behavior to achieve a task quality at least comparable to the one obtained by the skilled operators. A manual programming and tuning of the manipulator is not an efficient solution, requiring to adopt towards automated strategies. Adding external sensors (e.g., cameras) increases the robotic cell complexity and it doesn't solve the issue since it is usually difficult to build explicit reward functions measuring the robot performance, while it is easier for the user to define a qualitative comparison between two experiments. According to these needs, in this letter, the recently-developed preferences-based optimization approach GLISp is employed and adapted to tune the novel developed path-based velocity planner. The implemented solution defines an intuitive human-centered procedure, capable of transferring (through pairwise preferences between experiments) the task knowledge from the operator to the manipulator. A Franka EMIKA panda robot has been employed as a test platform to perform a robotic sealing task (i.e., material deposition task), validating the proposed methodology. The proposed approach has been compared with a programming by demonstration approach, and with the manual tuning of the path-based velocity planner. Achieved results demonstrate the improved deposition quality obtained with the proposed optimized path-based velocity planner methodology in a limited number of experimental trials (20). 
id397#Wireless Powered Dielectric Elastomer Actuator#The need for cable connection with soft robotic systems suppresses the benefits granted by their softness and flexibility. Such systems can be untethered by equipping batteries or by relying on non-electrical actuation mechanisms. However, these approaches cannot simultaneously support long-term and intelligent operations. This research examines a proposed wireless soft actuator based on wireless power transfer (WPT) and dielectric elastomer actuator (DEA) technology, thereby realizing soft robomore diversified application and long-term locomotion. A compact conical DEA fabrication process is presented with 6 mm periodic linear output and design of a lightweight WPT receiver that weighs only 13 g integrated with a driver circuit. Evaluation results show that this system remotely powers the DEA and the intelligent peripheral circuits for system control. Furthermore, our design seamlessly bridges the WPT system, power-efficient in low-voltage output conditions, and the DEA, which requires high-voltage input (kV) for deformation, by leveraging high-voltage boost-converters. Experimentally obtained results demonstrate untethered DEA operation at 170 mm from the transmitter. Also, we demonstrated applying this DEA as a wireless pump. 
id398#Development of client-server technology for access to the database of algorithms on the VUE.jS platform in#In this paper, the creation of an information system is considered, the main task of which is to provide information on algorithms in a form accessible for training, namely a general description of the algorithm, examples of implementations in programming languages, links to sources with additional information, visualization of their work. In addition, there are tasks to develop such functions: comparing performance algorithms, step-by-step execution of the algorithm with reference to its visualization. The methodology for developing a client-server application is demonstrated, the architecture of the system as a whole is described, and its individual elements are described in detail: the client part is the Vue.js framework, as well as its add-ons: for rapid prototyping of the user interface (Vuetify), for managing the global state of the application (Vuex); the server part, which is a H2 relational database, as well as a server application written in the Java programming language using the Spring MVC framework, which involves the allocation of layers such as controllers responsible for redistributing requests from clients to services that have access to special repositories that, in turn, refer to the layer that implements the database interaction interface (DAO). The result was a prototype application in which the basic functions are implemented, divided into two roles: a regular user with the rights to view content, and an administrator who has the ability to both view materials, and create, edit and delete them; as well as directions for the further development of this information system, including for educational purposes.. Copyright 
id399#Data Encryption for Internet of Things Applications Based on Catalan Objects and Two Combinatorial Structures#This article presents a novel data encryption technique suitable for Internet of Things (IoT) applications. The cryptosystem is based on the application of a Catalan object (as a cryptographic key) that provides encryption based on combinatorial structures with noncrossing or nonnested matching. The experimental part of this article includes a comparative analysis of the proposed encryption method with the Catalan numbers and data encryption standard (DES) algorithm, which is performed with machine learning-based identification of the encryption method using ciphertext only. These tests showed that it is much more difficult to recognize ciphertext generated with the Catalan method than one made with the DES algorithm. System reliability depends on the quality of the key, therefore, statistical testing proposed by National Institute of Standards and Technology was also performed. Twelve standard tests, the approximate entropy measurement, and random digression complexity analysis are applied in order to evaluate the quality of the generated Catalan key. A proposal for applying this method in e-Health IoT is also given. Possibilities of applying this method in the IoT applications for smart cities data storage and processing are provided. 
id400#Blockchain for securing aerial communications: Potentials, solutions, and research directions#Most natural disasters are consequences of hurricanes, floods, volcano eruptions, and earthquakes, and can severely disturb traditional communications networks and interrupt infrastructure of physical interconnection. After a disaster, communication failures are one of the essential causes of sufferers. To deal with this problem, offering “connectivity from the sky” is a novel and creative development. Aerial communications have been examined through the assessment and the design of the stratospheric platform capable of providing numerous types of wireless services. Drones, low and high-altitude platforms, airships, aircraft, and unmanned aerial vehicles (UAVs) are regarded as applicants for organizing aerial communications supplementing the infrastructure of global interaction. Aerial communication devices are vulnerable to being physically hijacked, destroyed, or lost. Therefore, security is a crucial issue in aerial communication networks. The blockchain technology is a potential solution candidate to tackle this issue. Blockchain is a decentralized and disseminated ledger, guards the distributed information using methods of cryptography, for example, public-key encryption and hash functions. It can use for guaranteeing the reliability of the data stored and for enhancing the transparency and security of aerial communication networks. This paper presents a survey on the integration of Blockchain with Aerial Communications (BAC). First, we study aerial communication networks and their current security issues, blockchain and its advantages, the feasibility and opportunity of applying the blockchain to resolve the current security issue in aerial communication networks. Next, we discuss current related solutions for applying the blockchain to resolve the current security issue in aerial communication networks in detail. We classify the solutions, compare and analyze their advantages and disadvantages. Finally, we recommend some research directions for future investigations. 
id401#Open source and open hardware mobile robot for developing applications in education and research#Nowadays, additive manufacturing, rapid prototyping and assembly modules represent a market that has invaded the entire world, especially in developing countries where traditional manufacturing is more restricted. In robotics, it is pertinent to think that modular construction is essential, due to the complexity of geometry in each of the pieces and their manufacture. Taking into account the globalization of information and the worldwide reproduction of databases, facilitating access to CAD files to be reproduced in 3D printing promotes the easy construction of archived mechanical designs. A robotic architecture becomes a complex assembly by having multiple operating systems. The sensorics, mechanics, electronics and programming that it requires for navigation, collaboration, development, operation and even industrial manufacturing means that more and more elaborate embedded systems are used. In this work, a mobile robotics architecture was developed with a sensory system that allows free movement and navigation in closed loop inverse kinematics. This kind of robot uses navigation algorithms to take a trajectory in collaborative closed environments, that is, closed industrial environments where obstacles are normally immovable and corridors to move narrow, in addition to having mobile obstacles like humans. 
id402#An enhanced CNN-enabled learning method for promoting ship detection in maritime surveillance system#The accurate and real-time detection of moving ships has become an essential component in maritime video surveillance, leading to enhanced traffic safety and security. With the rapid development of artificial intelligence, it becomes feasible to develop intelligent techniques to promote ship detection results in maritime applications. In this work, we propose to develop an enhanced convolutional neural network (CNN) to improve ship detection under different weather conditions. To be specific, the learning and representation capacities of our network are promoted by redesigning the sizes of anchor boxes, predicting the localization uncertainties of bounding boxes, introducing the soft non-maximum suppression, and reconstructing a mixed loss function. In addition, a flexible data augmentation strategy with generating synthetically-degraded images is presented to enlarge the volume and diversity of original dataset to train learning-based ship detection methods. This strategy is capable of making our CNN-based detection results more reliable and robust under adverse weather conditions, e.g., rain, haze, and low illumination. Experimental results under different monitoring conditions demonstrate that our method significantly outperforms other competing methods (e.g., SSD, Faster R-CNN, YOLOv2 and YOLOv3) in terms of detection accuracy, robustness and efficiency. The ship detection results under poor imaging conditions have also been implemented to demonstrate the superior performance of our learning method. 
id403#Compiler testing via a theory of sound optimisations in the C11/C++11 memory model#Compilers sometimes generate correct sequential code but break the concurrency memory model of the programming language: these subtle compiler bugs are observable only when the miscom-piled functions interact with concurrent contexts, making them particularly hard to detect. In this work we design a strategy to reduce the hard problem of hunting concurrency compiler bugs to differential testing of sequential code and build a tool that puts this strategy to work. Our first contribution is a theory of sound optimisations in the C11/C++11 memory model, covering most of the optimisations we have observed in real compilers and validating the claim that common compiler optimisations are sound in the C11/C++11 memory model. Our second contribution is to show how, building on this theory, concurrency compiler bugs can be identified by comparing the memory trace of compiled code against a reference memory trace for the source code. Our tool identified several mistaken write introductions and other unexpected behaviours in the latest release of the gcc compiler. Copyright 
id404#A handy open-source application based on computer vision and machine learning algorithms to count and classify microplastics#Microplastics have recently been discovered as remarkable contaminants of all environmental matrices. Their quantification and characterisation require lengthy and laborious analytical procedures that make this aspect of microplastics research a critical issue. In light of this, in this work, we developed a Computer Vision and Machine-Learning-based system able to count and classify microplastics quickly and automatically in four morphology and size categories, avoiding manual steps. Firstly, an early machine learning algorithm was created to count and classify mi-croplastics. Secondly, a supervised (k-nearest neighbours) and an unsupervised classification were developed to determine microplastic quantities and properties and discover hidden information. The machine learning algorithm showed promising results regarding the counting process and classification in sizes; it needs further improvements in visual class classification. Similarly, the supervised classification demonstrated satisfactory results with accuracy always greater than 0.9. On the other hand, the unsupervised classification discovered the probable underestimation of some microplastic shape categories due to the sampling methodology used, resulting in a useful tool for bringing out non-detectable information by traditional research approaches adopted in microplastic studies. In conclusion, the proposed application offers a reliable automated approach for microplastic quantification based on counts of particles captured in a picture, size distribution, and morphology, with considerable prospects in method standardisation. 
id405#Cultural behaviors analysis in video sequences#In this paper, we investigate the cultural aspects of different populations from video sequences. For that, we proposed a model that considers a series of characteristics of the pedestrians and the crowd, such as distances and speeds and performs the mapping of these characteristics in personalities, emotions, and cultural aspects. The model called Big4GD consists of four dimensions of geometric characteristics and seeks to describe the behavior of pedestrians and groups in the crowd. We performed a study of group behavior in a controlled experiment and focused on differences in two attributes that vary across cultures: (walking speed and personal distance) in three countries (India, Brazil, and Germany). We use the Fundamental Diagram theory that determines the relationship between the density and speed of individuals. We use Computer Vision methods to detect and track individuals through video sequences by generating their positions and speeds as a function of time. With these data, we analyze emergent walking speeds and densities while considering the personal distance of each individual and the neighbor in front of him/her. Our results show that human behavior is more similar in highly dense populations, i.e., individuals behave like a mass when presented with limited free personal space. The opposite result is also relevant: cultural differences can be observed at low and moderate densities, and such assumptions can be applied to computational interfaces and simulations, games, and movies. Besides, we present GeoMind, a software we developed to detect a series of characteristics from pedestrians. We also performed a practical case-study using GeoMind focusing on event detection in video sequences. 
id406#Artificial intelligence aided analysis of flexible space in landscape environment#Artificial intelligence aided analysis of the flexible space in smart landscape design is studied in this paper. When recognizing scene data sets, the scene recognition method of convolutional neural network can get a better recognition effect, and the accuracy can be improved by more than the 13.5% compared with traditional methods. In our designed model, the listed aspects are considered. (1) The feature fusion method can comprehensively utilize multiple features of image to achieve feature complementarity. The machine learning is applied to then construct the pipeline. (2) Data enhancement can improve the recognition performance and generalization ability of the network which is applied to the model design. The simulation results have proven the accuracy. 
id407#Vision-based estimation of clothing insulation for building control: A case study of residential buildings#Efforts have been made to estimate clothing insulation in real time, an element of thermal comfort for occupants. Nevertheless, an effective method to estimate clothing insulation in real time is lacking. In addition, there has been little debate on how to apply clothing insulation to building control in practice. The purpose of this study is to propose a method for estimating clothing insulation using deep learning-based vision recognition, which has recently attracted attention and implement building control based on clothing insulation. The study also evaluates the significance of the method in effective building control. The results demonstrated that the proposed framework, CloNet, showed an accuracy of 94% for the validation image dataset and 86% for the actual built environment. In addition, we proved that the proposed vision-based estimation method is very fast and practical for estimating clothing insulation. The control experiment showed that the CloNet-based predicted mean vote (PMV) control changed the set temperature in response to changes in the subject's clothing. Compared to the traditional PMV control, the CloNet-based PMV control improved the thermal preference and thermal comfort vote. These results prove that clothing insulation estimation can be useful for building control. 
id408#Implementing cryptography in LoRa based communication devices for unmanned ground vehicle applications#In this study, a Long Range (LoRa) based bidirectional secured communication link for controlling and monitoring the robots from a remote location is proposed. The security features and structure of the LoRa is build upon the standard protocol called LoRa wide area networks (LoRaWAN). To take advantage of these features, LoRa devices/end modules need to be connected to a network server through a LoRa gateway. However, for certain military scenarios like war zones, terrorist attack sites, disaster sites etc., the availability of standard network cannot be guaranteed, and therefore, the security features available with LoRaWAN protocol cannot be guaranteed. To overcome this critical limitation, a LoRa based secured device for establishing a bidirectional communication link between the base station and the robots without relying on any network is proposed. To secure the exchanged data, a cryptographic protocol is developed, and confidentiality, authenticity and integrity of the transmitted data between the base station and the robot are ensured. The protocol is then implemented with a pair of LoRa devices and its functionality is tested from a remote location by controlling and monitoring a Pioneer-P3Dx robot from a distance of 1.2 miles. The test shows that the proposed cryptographic protocol can securely control and track the robot from a remote location. 
id409#Relative independence of upper limb position sense and reaching in children with hemiparetic perinatal stroke#Background: Studies using clinical measures have suggested that proprioceptive dysfunction is related to motor impairment of the upper extremity following adult stroke. We used robotic technology and clinical measures to assess the relationship between position sense and reaching with the hemiparetic upper limb in children with perinatal stroke. Methods: Prospective term-born children with magnetic resonance imaging-confirmed perinatal ischemic stroke and upper extremity deficits were recruited from a population-based cohort. Neurotypical controls were recruited from the community. Participants completed two tasks in the Kinarm robot: arm position-matching (three parameters: variability [Varxy], contraction/expansion [Areaxy], systematic spatial shift [Shiftxy]) and visually guided reaching (five parameters: posture speed [PS], reaction time [RT], initial direction error [IDE], speed maxima count [SMC], movement time [MT]). Additional clinical assessments of sensory (thumb localization test) and motor impairment (Assisting Hand Assessment, Chedoke-McMaster Stroke Assessment) were completed and compared to robotic measures. Results: Forty-eight children with stroke (26 arterial, 22 venous, mean age: 12.0 ± 4.0 years) and 145 controls (mean age: 12.8 ± 3.9 years) completed both tasks. Position-matching performance in children with stroke did not correlate with performance on the visually guided reaching task. Robotic sensory and motor measures correlated with only some clinical tests. For example, AHA scores correlated with reaction time (R = − 0.61, p &lt; 0.001), initial direction error (R = − 0.64, p &lt; 0.001), and movement time (R = − 0.62, p &lt; 0.001). Conclusions: Robotic technology can quantify complex, discrete aspects of upper limb sensory and motor function in hemiparetic children. Robot-measured deficits in position sense and reaching with the contralesional limb appear to be relatively independent of each other and correlations for both with clinical measures are modest. Knowledge of the relationship between sensory and motor impairment may inform future rehabilitation strategies and improve outcomes for children with hemiparetic cerebral palsy. 
id410#A pointer logic and certifying compiler#Proof-Carrying Code brings two big challenges to the research field of programming languages. One is to seek more expressive logics or type systems to specify or reason about the properties of low-level or high-level programs. The other is to study the technology of certifying compilation in which the compiler generates proofs for programs with annotations. This paper presents our progress in the above two aspects. A pointer logic was designed for PointerC (a C-like programming language) in our research. As an extension of Hoare logic, our pointer logic expresses the change of pointer information for each statement in its inference rules to support program verification. Meanwhile, based on the ideas from CAP (Certified Assembly Programming) and SCAP (Stack-based Certified Assembly Programming), a reasoning framework was built to verify the properties of object code in a Hoare style. And a certifying compiler prototype for PointerC was implemented based on this framework. The main contribution of this paper is the design of the pointer logic and the implementation of the certifying compiler prototype. In our certifying compiler, the source language contains rich pointer types and operations and also supports dynamic storage allocation and deallocation. 
id411#Investigating the effects of object-relational impedance mismatch on the efficiency of object-relational mapping frameworks#The object-relational impedance mismatch (ORIM) problem characterises differences between the object-oriented and relational approaches to data access. Queries generated by object-relational mapping (ORM) frameworks are designed to overcome ORIM difficulties and can cause performance concerns in environments which use object-oriented paradigms. The aim of this paper is twofold, first presenting a survey of database practitioners on the effectiveness of ORM tools followed by an experimental investigation into the extent of operational concerns through the comparison of ORM-generated query performance and SQL query performance with a benchmark data set. The results show there are perceived difficulties in tuning ORM tools and distrust around their effectiveness. Through experimental testing, these views are validated by demonstrating that ORMs exhibit performance issues to the detriment of the query and the overall scalability of the ORM-led approach. Future work on establishing a system to support the query optimiser when parsing and preparing ORM-generated queries is outlined. Copyright 
id412#Entrainment during Human Locomotion Using a Soft Wearable Ankle Robot#An entrainment study was conducted with a novel soft robotic ankle-foot orthosis (SR-AFO) consisting of a pair of flat fabric pneumatic artificial muscles (ff-PAM). Entrainment capabilities of a lighter soft robotic orthosis were compared with heavy rigid robotic counterparts reported previously. To measure the SR-AFO's capacity to manifest gait entrainment, periodic pneumatic plantarflexion perturbations equal to the calculated increase from the subject's preferred gait frequency were applied to the ankle. Two days of experiments were conducted. In the Day 1 experiment, perturbations were applied from the baseline to a 15% increase in the gait frequency with steps of 3% at a fixed treadmill speed of the subject's preferred walking speed. In the Day 2 experiment, in order to investigate the maximum entrainment capability with the SR-AFO, perturbations were applied from the baseline with steps of 5% with proportionally increasing walking speed until subjects failed to maintain phase locking for 50 or more consecutive steps. In the Day 1 experiment, all 10 subjects were entrained at the highest 15% condition. In the Day 2 experiments, the average basin of entrainment was 39.3 pm 9.2%. Importantly, phase locking was always observed in the push-off phase of the gait cycle in both days of experiments. The observed basin of entrainment with the SR-AFO was substantially higher than the previously reported value (+7%) with a heavy rigid ankle robot, confirming the potential of the SR-AFO to significantly extend the effectiveness of the entrainment paradigm in gait adaptation and rehabilitation. 
id413#A three-dimensional chaotic map and their applications to digital audio security#In nonlinear dynamical systems, chaotic behavior is an attractive characteristic that has been broadly examined and researched over the most recent few decades. The chaotic systems are widely used in artificial intelligence and cryptology. Especially, these are considered efficient for multimedia data security. This manuscript has introduced a novel three dimensional (3-D) chaotic systems. The proposed maps are investigated through bifurcation diagrams and phase plots. The resulting diagrams demonstrates the chaotic attractor characteristic and the dynamic behavior of the suggested maps. Furthermore, a lossless audio encryption scheme is introduced utilizing the proposed chaotic maps. In the suggested scheme, the suggested 3-D chaotic sequences are utilized to shuffle the audio data points to achieve the diffusion property. In the confusion module, initially, the sequence of the audio data is divided into 8-bit and 7-bit sequences. Subsequently, the separated sequences are then substituted with different good quality substitution boxes, which are generated through a Mobius transformation over Galois fields. The suggested encryption algorithm is applied to the different audio files of various sizes and characters. The experimental results have revealed that the proposed scheme is capable to secure all kinds of audio files. The security analysis shows that the suggested scheme is capable to withstand differential and statistical attacks. 
id414#Discriminative feature alignment: Improving transferability of unsupervised domain adaptation by Gaussian-guided latent alignment#In this paper, we focus on the unsupervised domain adaptation problem where an approximate inference model is to be learned from a labeled data domain and expected to generalize well to an unlabeled domain. The success of unsupervised domain adaptation largely relies on the cross-domain feature alignment. Previous work has attempted to directly align features by classifier-induced discrepancies. Nevertheless, a common feature space cannot always be learned via this direct feature alignment especially when large domain gaps exist. To solve this problem, we introduce a Gaussian-guided latent alignment approach to align the latent feature distributions of the two domains under the guidance of a prior. In such an indirect way, the distributions over the samples from the two domains will be constructed on a common feature space, i.e., the space of the prior, which promotes better feature alignment. To effectively align the target latent distribution with this prior distribution, we also propose a novel unpaired L1-distance by taking advantage of the formulation of the encoder-decoder. The extensive evaluations on nine benchmark datasets validate the superior knowledge transferability through outperforming state-of-the-art methods and the versatility of the proposed method by improving the existing work significantly. 
id415#An approach and software prototype for translation of natural language business rules into database structure#In the recent decades data has indeed become one of the most valuable assets for government institutions, private businesses, and individual persons. Nowadays almost any software, from social networks and dating mobile applications to large information systems and analytical services for enterprise management, accumulates, stores, and processes data to solve certain problems in their subject areas. Extremely large data volumes are organized in databases that are used as the baseline for almost all of modern software applications. As the most important components of software systems, databases should be carefully designed, since drawbacks at the stage of requirements elicitation may result in exponential growth of defects fixing costs at testing and maintenance phases. Therefore, this study proposes an approach and software tool to database schema generation from textual requirements also known in database design domain as business rules. This may help database designers to rapidly obtain usable database schemas in order to detect and fix defects as early as possible. Moreover, proposed solution may simplify the database design process, since database creation scripts are generated from business rules directly. Thus, instead of coding all the required statements, engineers are only need to check obtained schema and make certain adjustments to data types, unique attributes, or used naming style. This research considers relational model and relational databases, since they are most widely used nowadays. State-of-the-art analysis is made, proposed approach is described in details, software tool with its brief usage examples is described, conclusions are made, and further research directions are formulated. 
id416#Stylized robotic clay sculpting#This paper presents an interactive design system that allows the user to create and fabricate stylized sculptures in water-based clay, using a standard 6-axis robot arm. This system facilitates the materialization of abstract design intentions into clay, through the algorithmic formulation of sculpting styles, the optimal path planning of the sculpting toolpaths, and a subtractive robotic fabrication process using customized tools. Unlike other precision-driven fabrication technologies, the authors embrace artistic uncertainty by conducting manual and robotic sculpting experiments and incorporating prominent parameters that affect the fabrication quality. The versatility of the described approach is demonstrated by designing a series of sculpting styles over a wide range of 3D models and robotically fabricating them in clay. Additionally, the paper explores various strategies for designing stylized robotic sculpting patterns by generating toolpaths informed by different techniques. 
id417#Confidence score: The forgotten dimension of object detection performance evaluation#When deploying a model for object detection, a confidence score threshold is chosen to filter out false positives and ensure that a predicted bounding box has a certain minimum score. To achieve state-of-the-art performance on benchmark datasets, most neural networks use a rather low threshold as a high number of false positives is not penalized by standard evaluation metrics. However, in scenarios of Artificial Intelligence (AI) applications that require high confidence scores (e.g., due to legal requirements or consequences of incorrect detections are severe) or a certain level of model robustness is required, it is unclear which base model to use since they were mainly optimized for benchmark scores. In this paper, we propose a method to find the optimum performance point of a model as a basis for fairer comparison and deeper insights into the trade-offs caused by selecting a confidence score threshold. 
id418#Multi-scale vehicle and pedestrian detection algorithm based on attention mechanism [基于注意力机制的多尺度车辆行人检测算法]#In complex and dynamic traffic scenes, accurate and timely detection of dynamic vehicle and pedestrian information by driver-less cars is particularly important. However, problems such as rapid camera movement, large scale changes, target occlusion, and light changes are encountered in unmanned driving scenarios. To overcome these challenges, this paper proposes a multi-scale target detection algorithm based on attention mechanism. Based on the YOLOv3 network, multi-scale local area features were fused and stitched by adding an improved spatial pyramid pooling module, so that the network could learn target features more comprehensively. Next, a spatial pyramid was used to shorten the information fusion and construct the YOLOv3-SPP+-PAN network. Finally, an efficient attention mechanism-based target detector, SE-YOLOv3-SPP+-PAN, was designed. Numerical results from the simulated system indicate that the SE-YOLOv3-SPP+-PAN network proposed herein achieved an improvement of 2.2% in mean average precision over the YOLOv3 network while retaining superior real-time reasoning-speed performance. This proves that the proposed SE-YOLOv3-SPP+-PAN network is more efficient and accurate than YOLOv3 is, and thus, it is more suitable for target detection in complex intelligent driving scenarios. 
id419#Five-Card AND Computations in Committed Format Using Only Uniform Cyclic Shuffles#In card-based cryptography, designing AND protocols in committed format is a major research topic. The state-of-the-art AND protocol proposed by Koch, Walzer, and Härtel in ASIACRYPT 2015 uses only four cards, which is the minimum permissible number. The minimality of their protocol relies on somewhat complicated shuffles having non-uniform probabilities of possible outcomes. Restricting the allowed shuffles to uniform closed ones entails that, to the best of our knowledge, six cards are sufficient: the six-card AND protocol proposed by Mizuki and Sone in 2009 utilizes the random bisection cut, which is a uniform and cyclic (and hence, closed) shuffle. Thus, a question has arisen: “Can we improve upon this six-card protocol using only uniform closed shuffles?” In other words, the existence or otherwise of a five-card AND protocol in committed format using only uniform closed shuffles has been one of the most important open questions in this field. In this paper, we answer the question affirmatively by designing five-card committed-format AND protocols using only uniform cyclic shuffles. The shuffles that our protocols use are the random cut and random bisection cut, both of which are uniform cyclic shuffles and can be easily implemented by humans. 
id421#Learning from Demonstrations Using Signal Temporal Logic in Stochastic and Continuous Domains#Learning control policies that are safe, robust and interpretable are prominent challenges in developing robotic systems. Learning-from-demonstrations with formal logic is an arising paradigm in reinforcement learning to estimate rewards and extract robot control policies that seek to overcome these challenges. In this approach, we assume that mission-level specifications for the robotic system are expressed in a suitable temporal logic such as Signal Temporal Logic (STL). The main idea is to automatically infer rewards from user demonstrations (that could be suboptimal or incomplete) by evaluating and ranking them w.r.t. the given STL specifications. In contrast to existing work that focuses on deterministic environments and discrete state spaces, in this letter, we propose significant extensions that tackle stochastic environments and continuous state spaces. 
id422#Vision measurement of gear pitting based on DCGAN and U-Net#To quantitatively detect the gear pitting, this paper proposes a vision measurement method based on deep convolutional generative adversarial network (DCGAN) and fully convolutional segmentation network (U-Net) for measuring the area ratio of gear pitting. A machine vision system is designed to automatically collect the pitting images of all gear teeth obtained from gear fatigue tests, but the pitting images are not sufficient due to the high cost of gear fatigue test. To solve the problem of small sample, DCGAN is applied to expand pitting samples. By the expansive sample set, the edge detection algorithm and U-Net are respectively used to segment the tooth surface and pitting. The proposed approach is applied to measure the gear pitting, and a comprehensive evaluation index is designed to evaluate the performance of gear pitting detection. Experimental results show that the average relative error and the absolute error of pitting area ratio obtained by the proposed method are respectively 7.83% and 0.18%, which are much lower than those obtained by the conventional detection method without sample augmentation. Thus, the proposed method has satisfactory accuracy and precede the detection method without sample augmentation. 
id423#Towards enhanced nanoindentation by image recognition#Abstract: The Oliver–Pharr method is maybe the most established method to determine a material’s Young’s modulus and hardness. However, this method has a number of requirements that render it more challenging for hard and stiff materials. Contact area and frame stiffness have to be calibrated for every tip, and the surface contact has to be accurately identified. The frame stiffness calibration is particularly prone to inaccuracies since it is easily affected, e.g., by sample mounting. In this study, we introduce a method to identify Young’s modulus and hardness from nanoindentation without separate area function and frame stiffness calibrations and without surface contact identification. To this end, we employ automatic image recognition to determine the contact area that might be less than a square micrometer. We introduce the method and compare the results to those of the Oliver–Pharr method. Our approach will be demonstrated and evaluated for nanoindentation of Si, a hard and stiff material, which is challenging for the proposed method. Graphic Abstract: [Figure not available: see fulltext.]. 
id424#A pipelined division for fixed operation using user-defined floating point#A pipelined division arithmetic for fixed operation using user-defined floating point is proposed in this paper. Division operation is difficult in fixed operation, the dividend A is firstly converted into user-defined floating point, then the inverse is implemented in the invertor unit. B/A will complete with a fixed multiplication. Linear approximation theory and Newton-Raphson iteration are used in invertor unit. The major advanced of this arithmetic is that it easily combines with fixed operation, we can proposed user-defined floating point according to the range of dividend A and the accuracy can easily acquire. In this paper, invertor unit for A range to [0.5, 2) with 23 decimal bits is designed. It is compiled and implemented both on FPGA and Synopsys Design Compiler. 
id425#A robust impedance controller for improved safety in human-robot interaction#This paper presents a novel impedance controller modified with a switching strategy for the purpose of improving safety in human-robot interactions. Under normal operating conditions, an impedance controller is enabled when adequate tracking performance is maintained in the presence of bounded disturbances. However, if disturbances are greater than anticipated such that tracking performance is degraded, the proposed controller temporarily switches modes to a control strategy better apt to limit control inputs. With disturbances returning to the prescribed bounds, tracking performance will be restored and the impedance controller will resume for nominal operation. The control parameters are constrained by a few conditions necessary for smooth operation. First, a pair of equality constraints is required for the control signal to be continuous when switching control modes. Second, a Lyapunov analysis is performed to formulate an equality constraint on the control parameters to ensure only a single switch occurs when changing control modes to avert control chatter. Third, a matrix inequality constraint is necessary to ensure a robust positive invariant set is formed for when impedance control is active. Numerical simulations are provided to illustrate the controller and conditions. The simulation results successfully validate the presented theory, demonstrating how the constraints yield a continuous control signal, eliminate switching chatter, and permit robustness to disturbances. 
id426#Fast‐moving coin recognition using deep learning#Deep learning has made essential contributions to the development of visual object detection and recognition. Identifying fast-moving objects from the viewpoint of computer vision remains as a challenging problem. The best solution in deep learning that can well represent the characteristics of object motion is related to recurrent neural network (RNN), the best model for fast-moving object recognition is Long Short-Term Memory (LSTM) in RNN. Therefore, the combination of LSTM and CNN fully utilizes spatial and temporal features of moving objects. In this paper, our goal is to identify fast-moving coins from digital videos by using deep learning methods, especially based on the mixture of LSTM and CNN. By using the proposed method, we gain the attainment with high accuracy of fast-moving coin recognition which is superior to our human visual system. 
id427#Design and implementation of a heterogeneous relational database synchronization mechanism based on tree distribution architecture#In allusion to the database server of tree distribution in the network environment, this paper proposes a heterogeneous database synchronization mechanism based on the shadow table method. In this paper, the application environment of the isolation netgap is described, and the overall architecture of the synchronization system is introduced, and the implementation of the module such as change data capture, incremental file generation, data synchronization operation and exception handling is proposed. Finally, the database synchronization mechanism is verified by the application of the data acquisition system of an organization. 
id428#Optical Hash Function for High Speed and High Security Algorithm using Ring Resonator System#This work presents a novel security technique using the optical hash function to create a message digest algorithm in the wavelength domain. The optical devices used for high speed and high security algorithm handling comprised a PANDA ring resonator connected with an add/drop filter system. The PANDA ring resonator was introduced to access the dynamic behavior of bright-bright soliton collision within the modified add/drop filter. Outputs of the dynamic states formed key suppression as a high security application for optical cryptography. The add/drop filter was an essential device in the proposed design for optical hash function processing. Simulation outputs proved that the proposed technique obtained optical hash function in the wavelength domain for real time message digest creation. The wavelength of the data must be within 40% of the center wavelength of the system input signal. The integrity of the data was maintained by this highly secure process. 
id430#Visual recognition of gymnastic exercise sequences. Application to supervision and robot learning by demonstration#This work presents a novel software architecture to autonomously identify and evaluate the gymnastic activity that people are carrying out. It is composed of three different interconnected layers. The first corresponds to a Multilayer Perceptron (MLP) trained from a set of angular magnitudes derived from the information provided by the OpenPose library. This library works frame by frame, so some postures may be incorrectly detected due to eventual occlusions. The MLP layer makes it possible to accurately identify the posture a person is performing. A second layer, based on a Hidden Markov Model (HMM) and the Viterbi algorithm, filters the incorrect spurious postures. Thus, the accuracy of the algorithm is improved, leading to a precise sequence of postures. A third layer identifies the current exercise and evaluates whether the person is doing it at a correct speed. This layer uses an innovative Modified Levenshtein Distance (MLD), which considers not only the number of operations to transform a given sequence, but also the nature of the elements participating in the comparison. The system works in real time with little delay, thus recognizing sequences of arbitrary length and providing continuous feedback on the exercises being performed. An experiment carried out consisted in reproducing the output of the second layer on an autonomous Pepper robot that can be used in environments where physical exercise is performed, such as a residence for the elderly or others. It has reproduced different exercises previously executed by an instructor so that people can copy the robot. The article analyzes the current situation of the automated gymnastic activities recognition, presents the architecture, the different experiments carried out and the results obtained. The integration of the three components (MLP, HMM and MLD) results in a robust system that has allowed us to improve the results of previous works. 
id431#Curve25519 based lightweight end-to-end encryption in resource constrained autonomous 8-bit IoT devices#Robust encryption techniques require heavy computational capability and consume large amount of memory which are unaffordable for resource constrained IoT devices and Cyber-Physical Systems with an inclusion of general-purpose data manipulation tasks. Many encryption techniques have been introduced to address the inability of such devices, lacking in robust security provision at low cost. This article presents an encryption technique, implemented on a resource constrained IoT device (AVR ATmega2560) through utilizing fast execution and less memory consumption properties of curve25519 in a novel and efficient lightweight hash function. The hash function utilizes GMP library for multi-precision arithmetic calculations and pre-calculated curve points to devise a good cipher block using ECDH based key exchange protocols and large random prime number generator function. 
id432#Scoping monadic relational database queries#We present a novel method for ensuring that relational database queries in monadic embedded languages are well-scoped, even in the presence of arbitrarily nested joins and aggregates. Demonstrating our method, we present a simplified version of Selda, a monadic relational database query language embedded in Haskell, with full support for nested inner queries. To our knowledge, Selda is the first relational database query language to support fully general inner queries using a monadic interface. In the Haskell community, monads are the de facto standard interface to a wide range of libraries and EDSLs. They are well understood by researchers and practitioners alike, and they enjoy first class support by the standard libraries. Due to the difficulty of ensuring that inner queries are well-scoped, database interfaces in Haskell have previously either been forced to forego the benefits of monadic interfaces, or have had to do without the generality afforded by inner queries. 
id433#Optimus: Efficient realization of streaming applications on FPGAs#In this paper, we introduce Optimus: an optimizing synthesis compiler for streaming applications. Optimus compiles programs written in a high level streaming language to either software or hardware implementations. The compiler uses a hierarchical compilation strategy that separates concerns between macro- and micro-functional requirements. Macro-functional concerns address how components (modules) are assembled to implement larger more complex applications. Micro-functional issues deal with synthesis issues of the module internals. Optimus thus allows software developers who lack deep hardware design expertise to transparently leverage the advantages of hardware customization without crossing the semantic gap between high level languages and hardware description languages. Optimus generates streaming hardware that achieves on average 40x speedup over our baseline embedded processor for a fraction of the energy. Additionally, our results show that streaming-specific optimizations can further improve performance by 255% and reduce the area requirements by 16% in average. These designs are competitive with Handel-C implementations for some of the same benchmarks. Copyright 2008 ACM.
id434#NIST Lightweight Cryptography Standardization Process: Classification of Second Round Candidates, Open Challenges, and Recommendations#In January 2013, the National Institute of Standards and Technology (NIST) announced the CAESAR (Competition for Authenticated Encryption: Security, Applicability, and Robustness) contest to identify authenticated ciphers that are suitable for a wide range of applications. A total of 57 submissions made it into the first round of the competition out of which 6 were announced as winners in March 2019. In the process of the competition, NIST realized that most of the authenticated ciphers submitted were not suitable for resource-constrained devices used as end nodes in the Internet-of-Things (IoT) platform. For that matter, the NIST Lightweight Cryptography Standardization Process was set up to identify authenticated encryption and hashing algorithms for IoT devices. The call for submissions was initiated in 2018 and in April 2019, 56 submissions made it into the first round of the competition. In August 2019, 32 out of the 56 submissions were selected for the second round which is due to end in the year 2021. This work surveys the 32 authenticated encryption schemes that made it into the second round of the NIST lightweight cryptography standardization process. The paper presents an easy-to-understand comparative overview of the recommended parameters, primitives, mode of operation, features, security parameter, and hardware/software performance of the 32 candidate algorithms. The paper goes further by discussing the challenges of the Lightweight Cryptography Standardization Process and provides some suitable recommendations. Copyright
id436#HEAP - The autonomous walking excavator#The demand and the potential for automation in the construction sector is unmatched, particularly for increasing environmental sustainability, improving worker safety and reducing labor shortages. We have developed an autonomous walking excavator - based one of the most versatile machines found on construction sites - as one way to begin fulfilling this potential. This article describes the process of converting an off-the-shelf construction machine into an autonomous robotic system. First we outline the necessary sensing equipment for full autonomy and the novel actuation of the legs, and compare three different complementary actuation principles for the excavator's arm. Second, we solve the state estimation problem for a general wheeled-legged robot. Beside kinematic measurements, it includes GNSS-RTK, to absolutely reference the machine on a construction site. Third, we developed individual controllers for driving, chassis balancing and arm motions allowing for fully autonomous operation. Lastly, we highlight the machine's potential in four different real-world applications, e.g. autonomous trench digging, autonomous assembly of dry stone walls, autonomous forestry work and semi-autonomous teleoperation. On top, we also share some development insights and possible future research directions. 
id439#Single class detection-based deep learning approach for identification of road safety attributes#Automatic detection of road safety attributes is an important step in designing a reliable road safety system. Due to the outstanding performance over the handcraft feature extraction-based methods for detecting objects, deep learning can be used to develop a robust road safety system. However, there are many challenges in using deep learning models. Firstly, they require a large dataset for training. Secondly, a class imbalance is a common problem in deep learning models. Finally, when a new attribute is introduced to a deep learning model, the whole model must be re-trained using all training samples which requires a lot of time. In order to solve some of these problems, we propose a novel single class detection-based deep learning approach for the identification of safety attributes in roadside video data. The approach is based on fusion of multiple fully convolutional network (FCN) models. Each model is trained to detect a single attribute/class using two classes (single attribute vs all other attributes) datasets. The proposed approach was evaluated on data provided by the Department of Transport and Main Roads (DTMR). The proposed approach achieved high accuracy and a new attribute can be added to the system without retraining the whole system. 
id441#A survey on online learning for visual tracking#Visual object tracking has become one of the most active research topics in computer vision, which has been growing in commercial development as well as academic research. Many visual trackers have been proposed in the last two decades. Recent studies of computer vision for dynamic scenes include motion detection, object classification, environment modeling, tracking of moving objects, understanding of object behaviors, object identification, and data fusion from multiple sensors. This paper provides an in-depth overview of recent object tracking research. Object tracking tasks in realistic scenario often face challenging problems such as camera motion, occlusion, illumination effect, clutter, and similar appearance. A variety of tracker techniques have been published, which combine multiple techniques to solve multiple visual tracking sub-problems. This paper also reviews the latest research trend in object tracking based on convolutional neural networks, which is receiving growing attention. Finally, the paper discusses the future challenges and research directions for the object tracking problems that still need extensive studies in coming years. 
id442#A Human-Centered Dynamic Scheduling Architecture for Collaborative Application#In collaborative robotic applications, human and robot have to work together during a whole shift for executing a sequence of jobs. The performance of the human robot team can be enhanced by scheduling the right tasks to the human and the robot. The scheduling should consider the task execution constraints, the variability in the task execution by the human, and the job quality of the human. Therefore, it is necessary to dynamically schedule the assigned tasks. In this letter, we propose a two-layered architecture for task allocation and scheduling in a collaborative cell. Job quality is explicitly considered during the allocation of the tasks and over a sequence of jobs. The tasks are dynamically scheduled based on the real time monitoring of the human's activities. The effectiveness of the proposed architecture is experimentally validated. 
id443#Neuroevolution of a recurrent neural network for spatial and working memory in a simulated robotic environment#We evolved weights in a recurrent neural network (RNN) to replicate the behavior and neural activity observed in rats during a spatial and working memory task. The rat was simulated using a robot simulator to navigate a virtual maze. After evolving weights from sensory inputs to the RNN, within the RNN, and from the RNN to the robot's motors, the robot successfully navigated the space to reach four reward arms with minimal repeats before the timeout. Our current findings suggest that it is the RNN dynamics that are key to performance, and that performance is not dependent on any one sensory type, which suggests that neurons in the RNN are performing mixed selectivity and conjunctive coding. The RNN activity resembles spatial information and trajectory-dependent coding observed in the hippocampus. The evolved RNN exhibits navigation skills, spatial memory, and working memory. 
id444#SocialSDN: Design and Implementation of a Secure Internet Protocol Tunnel between Social Connections#End-to-end encrypted (E2EE) network services can be classified into 1) network services that provide native endto-end encryption and 2) non-encrypted services transported through secure tunnels. While the first solution of native E2EE applications lacks generality and standardization, the second option of secure tunnels shows itself to be a promising solution, yet the current state-of-the-art still possesses several drawbacks. Primarily, the current state-of-the-art for establishing a secure tunnel for arbitrary IP traffic between two or more users requires significant technical expertise. Secondly, due to side-channel effects, the current state-of-the-art for cryptographically protected network tunnels may leak sensitive information through traffic pattern analysis. Lastly, the current state-of-the-art for this type of networking lacks elegance and convenience and therefore users often settle for less secure non-E2EE services. In this paper, we present SocialSDN which utilizes concepts from social networking and software-defined networking to build a tool which addresses many of the issues holding back mass adoption of E2EE network services. 
id445#Emotion recognition model based on facial expressions#Face mining is characterized as the revelation of picture designs in a given congregation of pictures. It is an exertion that generally attracts upon information PC (Personal Computer) vision, picture handling, information mining, AI (Artificial Intelligence), database, and human-made reasoning. Facial acknowledgement breaks down and contemplates the examples from the images of the facial. Facial component extraction is a programmed acknowledgment of human faces by recognizing its highlights, for example, eyebrows, eyes, and lips. In this paper, we are assessing the execution of PCA (Priniciple Component Analysis), GMM (Gaussian Mixture Models), GLCM (Gray Level Co-Occurrence Matrix), and SVM (Support Vector Machines) to perceive seven distinctive outward appearances of two people, for example, angry, sad, happy, disgust, neutral, fear, and surprise in database. Our point is to talk about the best systems that work best for facial acknowledgement. The present investigation demonstrates the plausibility of outward appearance acknowledgement for viable applications like surveillance and human PC communication. [Figure not available: see fulltext.] 
id446#Optimizing code parallelization through a constraint network based approach#Increasing employment of chip multiprocessors in embedded computing platforms requires a fresh look at conventional code parallelization schemes. In particular, any compiler-based parallelization scheme for chip multiprocessors should account for the fact that interprocessor communication is cheaper than off-chip memory accesses in these architectures. Based on this observation, this paper proposes a constraint network based approach to code parallelization for chip multiprocessors. Constraint networks have proven to be a useful mechanism for modeling and solving computationally intensive tasks in artificial intelligence. They operate by expressing a problem as a set of variables, variable domains and constraints and define a search procedure that tries to satisfy the constraints (an acceptable subset of them) by assigning values to variables from their specified domains. This paper demonstrates that it is possible to use a constraint network based formulation for the problem of code parallelization for chip multiprocessors. Our experimental evaluation shows that not only a constraint network based approach is feasible for our problem but also highly desirable since it outperforms all other parallelization schemes tested in our experiments. Copyright 2006 ACM.
id448#Multimodal classification of parkinson’s disease in home environments with resiliency to missing modalities#Parkinson’s disease (PD) is a chronic neurodegenerative condition that affects a patient’s everyday life. Authors have proposed that a machine learning and sensor-based approach that contin-uously monitors patients in naturalistic settings can provide constant evaluation of PD and objectively analyse its progression. In this paper, we make progress toward such PD evaluation by presenting a multimodal deep learning approach for discriminating between people with PD and without PD. Specifically, our proposed architecture, named MCPD-Net, uses two data modalities, acquired from vision and accelerometer sensors in a home environment to train variational autoencoder (VAE) models. These are modality-specific VAEs that predict effective representations of human movements to be fused and given to a classification module. During our end-to-end training, we minimise the difference between the latent spaces corresponding to the two data modalities. This makes our method capable of dealing with missing modalities during inference. We show that our proposed multimodal method outperforms unimodal and other multimodal approaches by an average increase in F1-score of 0.25 and 0.09, respectively, on a data set with real patients. We also show that our method still outperforms other approaches by an average increase in F1-score of 0.17 when a modality is missing during inference, demonstrating the benefit of training on multiple modalities. 
id449#Simulation of quantum key distribution in a secure star topology optimization in quantum channel#The field of quantum cryptography is mostly theoretical therefore in this paper we represent its implementation by means of virtual scenarios. The central issue in cryptography is the secure transmission of the key between nodes. Thus, in this paper we establish a secure channel using Quantum Key Distribution (QKD) for the transfer of the key material between the nodes and help to identify an eavesdropper in the channel. A graphical representation of the quantum channel traffic at the ideal state and also during network disruption has been established. Due to the complex nature of quantum networks and high cost of establishment, a physical implementation of the same is not feasible. Hence a simulation has been implemented via the use of NS-3 (Network Simulator Version 3) which has QKDNetSim module built into it. Finally, our simulation indicates the presence of an intruder by virtue of various network implementations within the quantum channel. 
id450#An Improved Method of Nonmotorized Traffic Tracking and Classification to Acquire Traffic Parameters at Intersections#High computational cost and low tracking stability make it still a challenging task to acquire nonmotorized traffic parameters at intersections via vision-based method. In order to address the above issues, our study improves a cooperative tracking and classification method, and proposes a vision-based data collection system to monitor nonmotorized traffic at intersections. The system utilizes the combination of two tracking algorithms, Kernelized Correlation Filter and Kalman filter, to ensure the continuous tracking. Based on multivariate feature, K-means clustering and Support Vector Machine are implemented to classify nonmotorized traffic according to the motion and appearance feature respectively. As a result, the proposed system can acquire trajectories of pedestrians and cyclists and extract traffic parameters, including flow and velocity. Our method performs well in both efficiency and accuracy by fusing simple but effective algorithms and is robust in the complex scenario especially at large-scale intersections with limited training samples. The experimental results show that it can extract more trajectories with low computational lost. Moreover, the error of flow and velocity result is controlled within acceptable limits, which directly proves it feasible to collect field data in project applications. 
id451#Inference of Other’s Minds with Limited Information in Evolutionary Robotics#Theory of mind (ToM) is the ability to understand others’ mental states (e.g., intentions). Studies on human ToM show that the way we understand others’ mental states is very efficient, in the sense that observing only some portion of others’ behaviors can lead to successful performance. Recently, ToM has gained interest in robotics to build robots that can engage in complex social interactions. Although it has been shown that robots can infer others’ internal states, there has been limited focus on the data utilization of ToM mechanisms in robots. Here we show that robots can infer others’ intentions based on limited information by selectively and flexibly using behavioral cues similar to humans. To test such data utilization, we impaired certain parts of an actor robot’s behavioral information given to the observer, and compared the observer’s performance under each impairment condition. We found that although the observer’s performance was not perfect compared to when all information was available, it could infer the actor’s mind to a degree if the goal-relevant information was intact. These results demonstrate that, similar to humans, robots can learn to infer others’ mental states with limited information. 
id452#Practical pluggable types for Java#This paper introduces the Checker Framework, which supports adding pluggable type systems to the Java language in a backward-compatible way. A type system designer defines type qualifiers and their semantics, and a compiler plug-in enforces the semantics. Programmers can write the type qualifiers in their programs and use the plug-in to detect or prevent errors. The Checker Framework is useful both to programmers who wish to write error-free code, and to type system designers who wish to evaluate and deploy their type systems. The Checker Framework includes new Java syntax for expressing type qualifiers; declarative and procedural mechanisms for writing type-checking rules; and support for flow-sensitive local type qualifier inference and for polymorphism over types and qualifiers. The Checker Framework is well-integrated with the Java language and toolset. We have evaluated the Checker Framework by writing 5 checkers and running them on over 600K lines of existing code. The checkers found real errors, then confirmed the absence of further errors in the fixed code. The case studies also shed light on the type systems themselves.
id453#Improving the Accuracy of Forming a Digital Terrain Model Along a Railway Track#The article considers the problem of improving the accuracy of digital terrain models created for monitoring and diagnostics of the railway track and the surrounding area. A technical solution to this problem is presented, which includes a method for joint aerial photography and laser scanning, as well as a method for digital processing of the obtained data. The relevance of using this solution is due to the existence of zones of weak reception of signals from the global navigation satellite system, since in these zones the accuracy of constructing digital terrain models using currently used diagnostic spatial scanning systems is reduced. The technical solution is based on the method of digital processing of aerial photographs of the railway track. In this case, the rail track lines located at a normalized distance from each other are used as external orientation elements. The use of this method made it possible to increase the accuracy of determining the flight path of an aircraft over railway tracks and, as a result, the accuracy of calculating the coordinates of points on the earth’s surface. As a result, a digital terrain model that is suitable for diagnosing and monitoring the state of the roadbed of a railway track was created. In simulation modeling, it was found that the application of the proposed method allowed reducing the confidence interval of the distribution of the error in determining the coordinates of points on the terrain to 50% and increasing the accuracy of forming a digital terrain model. This promising technical solution for improving the accuracy of digital terrain models for railway track diagnostics is implemented using unmanned aerial vehicles that are part of a mobile diagnostic complex. The advantages of the proposed solution include high efficiency and availability of application. 
id454#A holistic cyber-physical security protocol for authenticating the provenance and integrity of structural health monitoring imagery data#Modern infrastructure systems, such as bridges, dams, power generation stations, and buildings increasingly have an intrinsic cyber-physical nature to them. Infrastructure now commonly, includes actuators, network connections, sensors, control systems, and computational resources. It is of increasing concern that modern infrastructure is vulnerable to cyber-attacks that can damage both the cyber and physical nature of the infrastructure. To date, the physical and cyber health of infrastructure has been considered separately. However, the increasing concerns associated with the cyber-physical security of infrastructure coupled with the emergence of 5G networks made using components that are not universally considered trustworthy, and the emergence of techniques for creating deepfakes and adversarial examples suggests the time has come to begin considering cyber health and structural health with a more holistic approach. In this work, a protocol is developed for ensuring the imagery data captured by a structural health monitoring system can be unambiguously attributed to legitimate sensors associated with the structural health monitoring system. A computer vision approach based on the idea of mutual information is then presented to detect damage in an image. This work presents the protocol for authenticating the provenance of imager data and demonstrates that this protocol does not have overly adverse effects when used with the mutual information-based technique for detecting damage in the resulting imagery data. 
id455#Parallelization of Machine Learning Applied to Call Graphs of Binaries for Malware Detection#Malicious applications have become increasingly numerous. This demands adaptive, learning-based techniques for constructing malware detection engines, instead of the traditional manual-based strategies. Prior work in learning-based malware detection engines primarily focuses on dynamic trace analysis and byte-level n-grams. Our approach in this paper differs in that we use compiler intermediate representations, i.e., the callgraph representation of binaries. Using graph-based program representations for learning provides structure of the program, which can be used to learn more advanced patterns. We use the Shortest Path Graph Kernel (SPGK) to identify similarities between call graphs extracted from binaries. The output similarity matrix is fed into a Support Vector Machine (SVM) algorithm to construct highly-Accurate models to predict whether a binary is malicious or not. However, SPGK is computationally expensive due to the size of the input graphs. Therefore, we evaluate different parallelization methods for CPUs and GPUS to speed up this kernel, allowing us to continuously construct up-To-date models in a timely manner. Our hybrid implementation, which leverages both CPU and GPU, yields the best performance, achieving up to a 14.2x improvement over our already optimized OpenMP version. We compared our generated graph-based models to previously state-of-The-Art feature vector 2-gram and 3-gram models on a dataset consisting of over 22,000 binaries. We show that our classification accuracy using graphs is over 19% higher than either n-gram model and gives a false positive rate (FPR) of less than 0.1%. We are also able to consider large call graphs and dataset sizes because of the reduced execution time of our parallelized SPGK implementation. 
id456#“Looking beneath the surface”: A visual-physical feature hybrid approach for unattended gauging of construction waste composition#There are various scenarios challenging human experts to judge the interior of something based on limited surface information. Likewise, at waste disposal facilities around the world, human inspectors are often challenged to gauge the composition of waste bulks to determine admissibility and chargeable levy. Manual approaches are laborious, hazardous, and prone to carelessness and fatigue, making unattended gauging of construction waste composition using simple surface information highly desired. This research attempts to contribute to automated waste composition gauging by harnessing a valuable dataset from Hong Kong. Firstly, visual features, called visual inert probability (VIP), characterizing inert and non-inert materials are extracted from 1127 photos of waste bulks using a fine-tuned convolutional neural network (CNN). Then, these visual features together with easy-to-obtain physical features (e.g., weight and depth) are fed to a tailor-made support vector machine (SVM) model to determine waste composition as measured by the proportions of inert and non-inert materials. The visual-physical feature hybrid model achieved a waste composition gauging accuracy of 94% in the experiments. This high performance implies that the model, with proper adaption and integration, could replace human inspectors to smooth the operation of the waste disposal facilities. 
id457#RETRO +: SQL to SPARQL improvement of the RETRO framework#Semantic web has recently gained a great attention due to its potential for providing a common framework that allows data sharing and reprocessed, relational databases are still the most used. To deal with the variety of modern database formats and query languages, we propose an approach to translate SQL queries into equivalent SPARQL queries for querying resource description framework (RDF) data. The main purpose is to give an extension of the Retro framework by adding a set of algorithms to improve its function. The algorithms in question deal with the conversion of the relevant SQL queries INSERT, UPDATE, DELETE and improve the SELECT query by adding the aggregate function, GROUP BY and ORDER BY in equivalent SPARQL queries and validate all translated queries on allegrograph. Copyright 
id458#Pose estimation of excavator manipulator based on monocular vision marker system#Excavation is one of the broadest activities in the construction industry, often affected by safety and productivity. To address these problems, it is necessary for construction sites to automatically monitor the poses of excavator manipulators in real time. Based on computer vision (CV) technology, an approach, through a monocular camera and marker, was proposed to estimate the pose parameters (including orientation and position) of the excavator manipulator. To simulate the pose estimation process, a measurement system was established with a common camera and marker. Through comprehensive experiments and error analysis, this approach showed that the maximum detectable depth of the system is greater than 11 m, the orientation error is less than 8.5◦, and the position error is less than 22 mm. A prototype of the system that proved the feasibility of the proposed method was tested. Furthermore, this study provides an alternative CV technology for monitoring construction machines. 
id459#Design and Implementation of a Secure QR Payment System Based on Visual Cryptography#In this paper, we will describe the design and implementation of a secure payment system based on QR codes. These QR codes have been extensively used in recent years since they speed up the payment process and provide users with ultimate convenience. However, as convenient as they may sound, QR-based online payment systems are vulnerable to different types of attacks. Therefore, transaction processing needs to be secure enough to protect the integrity and confidentiality of every payment process. Moreover, the online payment system must provide authenticity for both the sender and receiver of each transaction. In this paper, the security of the proposed QR-based system is provided using visual cryptography. The proposed system consists of a mobile application and a payment gateway server that implements visual cryptography. The application provides a simple and user-friendly interface for users to carry out payment transactions in user-friendly secure environment. 
id460#Secure two-party computations in ANSI C#The practical application of Secure Two-Party Computation is hindered by the difficulty to implement secure computation protocols. While recent work has proposed very simple programming languages which can be used to specify secure computations, it is still difficult for practitioners to use them, and cumbersome to translate existing source code into this format. Similarly, the manual construction of two-party computation protocols, in particular ones based on the approach of garbled circuits, is labor intensive and error-prone. The central contribution of the current paper is a tool which achieves Secure Two-Party Computation for ANSI C. Our work is based on a combination of model checking techniques and two-party computation based on garbled circuits. Our key insight is a nonstandard use of the bit-precise model checker CBMC which enables us to translate C programs into equivalent Boolean circuits. To this end, we modify the standard CBMC translation from programs into Boolean formulas whose variables correspond to the memory bits manipulated by the program. As CBMC attempts to minimize the size of the formulas, the circuits obtained by our tool chain are also size efficient; to improve the efficiency of the garbled circuit evaluation, we perform optimizations on the circuits. Experimental results with the new tool CBMC-GC demonstrate the practical usefulness of our approach. Copyright 
id461#Inductive learning of answer set programs for autonomous surgical task planning: Application to a training task for surgeons#The quality of robot-assisted surgery can be improved and the use of hospital resources can be optimized by enhancing autonomy and reliability in the robot’s operation. Logic programming is a good choice for task planning in robot-assisted surgery because it supports reliable reasoning with domain knowledge and increases transparency in the decision making. However, prior knowledge of the task and the domain is typically incomplete, and it often needs to be refined from executions of the surgical task(s) under consideration to avoid sub-optimal performance. In this paper, we investigate the applicability of inductive logic programming for learning previously unknown axioms governing domain dynamics. We do so under answer set semantics for a benchmark surgical training task, the ring transfer. We extend our previous work on learning the immediate preconditions of actions and constraints, to also learn axioms encoding arbitrary temporal delays between atoms that are effects of actions under the event calculus formalism. We propose a systematic approach for learning the specifications of a generic robotic task under the answer set semantics, allowing easy knowledge refinement with iterative learning. In the context of 1000 simulated scenarios, we demonstrate the significant improvement in performance obtained with the learned axioms compared with the hand-written ones; specifically, the learned axioms address some critical issues related to the plan computation time, which is promising for reliable real-time performance during surgery. 
id462#Evaluation of image partitioning strategies for preserving spatial information of cross-sectional micrographs in automated wood recognition of Fagaceae#Although wood cross sections contain spatiotemporal information regarding tree growth, computer vision-based wood identification studies have traditionally favored disordered image representations that do not take such information into account. This paper describes image partitioning strategies that preserve the spatial information of wood cross-sectional images. Three partitioning strategies are designed, namely grid partitioning based on spatial pyramid matching and its variants, radial and tangential partitioning, and their recognition performance is evaluated for the Fagaceae micrograph dataset. The grid and radial partitioning strategies achieve better recognition performance than the bag-of-features model that constitutes their underlying framework. Radial partitioning, which is a strategy for preserving spatial information from pith to bark, further improves the performance, especially for radial-porous species. The Pearson correlation and autocorrelation coefficients produced from radially partitioned sub-images have the potential to be used as auxiliaries in the construction of multi-feature datasets. The contribution of image partitioning strategies is found to be limited to species recognition and is unremarkable at the genus level. 
id463#A new model checking tool#In this paper we present a new CTL model checking tool used to prove whether a CTL model represented as a directed graph satisfies a set of specifications given in form of one or more temporal logic formulas. Our tool is implemented in client-server paradigm: CTL Designer, the client tool, allows an interactive construction of the Kripke model as a directed graph and the CTL Checker, the core of our tool, represents the server part and is published as Web service. The CTL Checker includes an algebraic compiler implemented with ANTLR (Another Tool for Language Recognition) support. The main function of the Web service is to parse a given formula, find the set of nodes in which the formula is satisfied and return result to the user. As test case for our tool, we choose a CTL Model for Login Page Controller. The model will check if logging is allowed or not depending on input data.
id464#A RDBMS-Based Bitcoin Analysis Method#Due to the proliferation of Bitcoin and Ethereum, over 1000 cryptocurrencies have appeared in the market. As hundreds of thousands of cryptocurrency transactions are taken place per day, cryptocurrency exchange, service operators, or government agencies have to observe user transaction activities for legal concerns or economic purposes. Cryptocurrencies generally use the blockchain structure where every transaction is connected in the linked list with the public key hash function. In order to examine specific transaction, address or a group of addresses, called a cluster, we need an efficient cryptocurrency data analyzer and scalable storage. Though a few studies on cryptocurrency analysis tools have been presented, they do not generally satisfy all the requirements. In this paper, we propose an extensible and user-friendly Bitcoin analysis software based on RDBMS. From extensive Bitcoin experiments, we demonstrate that RDBMS queries are useful to perform analysis of Bitcoin transaction, cluster and graph. In addition, we show that the indexed SQLite3 database provides quick response time and the extensible Bitcoin storage. This study contributes to a method of analyzing Bitcoin blockchain data using an easy-to-use RDBMS. 
id465#Hybrid Electric Vehicle Energy Management with Computer Vision and Deep Reinforcement Learning#Modern automotive systems have been equipped with a highly increasing number of onboard computer vision hardware and software, which are considered to be beneficial for achieving eco-driving. This article combines computer vision and deep reinforcement learning (DRL) to improve the fuel economy of hybrid electric vehicles. The proposed method is capable of autonomously learning the optimal control policy from visual inputs. The state-of-the-art convolutional neural networks-based object detection method is utilized to extract available visual information from onboard cameras. The detected visual information is used as a state input for a continuous DRL model to output energy management strategies. To evaluate the proposed method, we construct 100 km real city and highway driving cycles, in which visual information is incorporated. The results show that the DRL-based system with visual information consumes 4.3-8.8% less fuel compared with the one without visual information, and the proposed method achieves 96.5% fuel economy of the global optimum-dynamic programming. 
id466#High-Speed RLWE-Oriented Polynomial Multiplier Utilizing Karatsuba Algorithm#Lattice-based cryptography (LBC) is one of the promising post-quantum candidates which offers good security and performance. The most time consuming operations in LBC is the polynomial multiplication, which can be performed through widely explored algorithms like schoolbook polynomial multiplication algorithm (SPMA) and Number Theoretic Transform (NTT). However, Karatsuba algorithm with better complexity compared to SPMA, is not widely studied for FPGA implementation of LBC. In this brief, we proposed an optimized SPMA-Karatsuba (SK) architecture with novel technique to implement the negacyclic convolution. The proposed architecture is more than 2.09× faster in expense of 96.06% additional hardware resources compared to the state-of-the-art SPMA architecture. This shows that the combination of SPMA and Karatsuba algorithm can produce hardware architecture with higher speed yet maintain balanced area-time efficiency compared to SPMA-only architecture. This is especially useful for developing IoT edge nodes or gateway devices that require high speed but able to tolerate some additional hardware area. 
id467#Performance Analysis of Queries in RDBMS vs NoSQL#Due to the enormous amount of information getting stored in databases, an extra effort is needed to handle the information so that it is not lost in any way. To avoid the loss of information various methods were introduced and one such technique is introduction of NoSQL using MongoDB. In this paper, a comparison is made between structured Oracle and an unstructured MongoDB, with few identified operations in a controlled environment. The result obtained is then used to realize better performance of the structured or unstructured methods. 
id468#A deep learning approach to solve sudoku puzzle#Computer vision has been lionized in the recent trends of information technology world. Because of its enormous application and features, it has been gaining limelight for solving real world complex problems which are beyond human intelligence. Extending this artificial intelligence marvel to conventional life makes it useful and convenient. Sudoku has become a part of amusement in many lives which is ubiquitous in many articles news papers, magazine and apps. So imbibing computer vision techniques to solve the Sudoku puzzle is upshot of this paper. We have employed some of the prominent tools like Open CV, OCR, Tensorflow. The paper explains the methodology with which we have used these tools to solve the Sudoku puzzle. The methodology shown in the paper involves several stages such as image preprocessing and image extraction using OpenCV, OCRing the numerical data from the extracted image using Tesseract and finally feeding the numerical data extracted to the neural network(tensorflow) model to get the desired output. The main motivation of our research is to develop up a help device for Sudoku players, for example urging players to tackle the hard Sudoku puzzles or when looking for help. It is troublesome at various stages and simple to artful players, particularly for new players or individuals insufficient certainty or endurance. The input data, comes from the camera or as an image. 
id470#Deep learning approaches to semantic segmentation of fatigue cracking within cyclically loaded nickel superalloy#Improvements to synchrotron-based micro-computed tomography scanning capabilities have gifted researchers the ability to characterize 4D material thermomechanical responses more thoroughly than ever before. These advancements, however, have brought about new challenges in analyzing the resulting deluge of data. We report on a nickel-based superalloy specimen imaged 26 times in-situ during cyclic loading at Argonne National Laboratory Advanced Photon Source beamline 1ID, in order to monitor crack growth within the microstructure. Several deep learning approaches which utilize convolutional neural networks are implemented to segment crack features from reconstructed tomography scans. U-Net architecture implementations are found to be especially effective, achieving IoU=0.995±0.004 and Matthews correlation coefficient scores of ϕ=0.826±0.085. These advancements broaden possibilities for scientists seeking to automate segmentation analyses of similar large datasets. 
id471#Digitized data validation using dual color images with improved robustness and error correction facility#The proposed algorithm takes an initiative to justify strong ownership claims by blending visual cryptography with steganography, which is quite different from a conventional approach. The major consideration of the proposed protocol is to implement cryptographic technique in digitized document justifying Confidentiality, Integrity, Authenticity and Non-Repudiation, similar to cryptographic technique implementation in born digital document. In addition, the approach is less complex than a conventional system but without compromising the level of security. For the protection of a sensitive or copyright document, the owner cleans the digitized signature, generate two shares and fabricate one of the shares alongside the encrypted message digest of the signature in the pseudorandom positions of coiflet transformed blocks. Instead of using a single plane as cover, three planes of two color images are utilized to enhance the effect of robustness in hiding. The secret fabrication in frequency domain preserves excellent security as well as imperceptibility of the hidden data. The loss of signal due to white noise is properly adjusted to make the authenticated images resemble close to the original ones as depicted with the histogram and RGB analysis. Moreover, an intended receiver only will be able to verify the confidentiality of the document and the owner through self-defined appropriate techniques. Finally, the worthiness of the algorithm in digitized domain is established through exhaustive experimentation in terms of data hiding imperceptibility, robustness and data recovery aspects. 
id472#Semantics-preserving optimisation of mapping multi-column key constraints for RDB to RDF transformation#The relational database (RDB) to resource description framework (RDF) transformation is a major semantic information extraction method because most web data are managed by RDBs. Existing automatic RDB-to-RDF transformation methods generate RDF data without losing the semantics of original relational data. However, two major problems have been observed during the mapping of multi-column key constraints: repetitive data generation and semantic information loss. In this article, we propose an improved RDB-to-RDF transformation method that ensures mapping without the aforementioned problems. Optimised rules are defined to generate an accurate semantic data structure for a multi-column key constraint and to reduce repetitive constraint data. Experimental results show that the proposed method achieves better accuracy in transforming multi-column key constraints and generates compact semantic results without repetitive data. 
id473#Practical challenges of ILP-based SPM allocation optimizations#Scratchpad Memory (SPM) allocation is a well-known technique for compiler-based code optimizations. Integer-Linear Programming has been proven to be a powerful technique to determine which parts of a program should be moved to the SPM. Although the idea is quite straight-forward in theory, the technique features several challenges when being applied to modern embedded systems. In this paper, we aim to bring out the main issues and possible solutions which arise when trying to apply those optimizations to existing hardware platforms. 
id474#Surface color distribution analysis by computer vision compared to sensory testing: Vacuum fried fruits as a case study#Color is a main factor in the perception of food product quality. Food surfaces are often not homogenous at micro-, meso-, and macroscopic scales. This matrix can include a variety of colors that are subject to changes during food processing. These different colors can be analyzed to provides more information than the average color. The objective of this study was to compare color analysis techniques on their ability to differentiate samples, quantify heterogeneity, and flexibility. The included techniques are sensory testing, Hunterlab colorimeter, a commercial CVS (IRIS-Alphasoft), and the custom made CVS (Canon-CVS) in analyzing nine different vacuum fried fruits. Sensory testing was a straightforward method and able to describe color heterogeneity. However, the subjectivity of the panelist is a limitation. Hunterlab was easy and accurate to measure homogeneous samples with high differentiation, without the color distribution information. IRIS-Alphasoft was quick and easy for color distribution analysis, however the closed system is the limit. The Canon-CVS protocol was able to assess the color heterogeneity, able to discriminate samples and flexible. As a take home massage, objective color distribution analysis has a potential to unlock the limitation of traditional color analysis by providing more detailed color distribution information which is important with respect to overall product quality. 
id475#Occlusion image data generation system [遮挡图像数据生成系统]#To address the inadequacy of current datasets for systematic evaluating target detection algorithm under the occlusion problem and the difficulty in acquiring some data in reality, this paper proposes an occlusion image data generation system to generate images with occlusion and corresponding annotations and to build the occlusion image dataset, namely more than common object dataset (MOCOD). In terms of system architecture, a scene and global management module, a control module, and a data processing module were designed to generate and process data to build an occlusion image dataset. In terms of data generation, for opaque objects, pixel-level annotation was generated via post-processing with a stencil buffer; for translucent objects, the annotation was generated by sampling the 3D temporal space with ray marching. Finally, the occlusion level could be calculated based on the generated annotations. The experiment result indicates that our system could efficiently annotate instance-level data, with an average annotation speed of nearly 0.07 s. The images provided by our dataset have ten occlusion levels. In the case of MOCOD, the annotation is more accurate, occlusion level classification is more precise, and annotation speed is considerably faster, compared to those in the case of other datasets. Further, the annotation of translucent objects is introduced in MOCOD, which expands the occlusion types and can help evaluate the occlusion problem better. In this study, we focused on the occlusion problem, and herein, we propose an occlusion image data generation system to effectively build an occlusion image dataset, MOCOD; the accurate annotation in our dataset can help evaluate the bottleneck and performance of detection algorithms under the occlusion problem better.
id476#DSEC: A Stereo Event Camera Dataset for Driving Scenarios#Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high resolution, large scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms. 
id477#The alternative method to finish modular exponentiation and point multiplication processes#The aim of this paper is to propose the alternative algorithm to finish the process in public key cryptography. In general, the proposed method can be selected to finish both of modular exponentiation and point multiplication. Although this method is not the best method in all cases, it may be the most efficient method when the condition responds well to this approach. Assuming that the binary system of the exponent or the multiplier is considered and it is divided into groups, the binary system is in excellent condition when the number of groups is small. Each group is generated from a number of 0 that is adjacent to each other. The main idea behind the proposed method is to convert the exponent or the multiplier as the subtraction between two integers. For these integers, it is impossible that the bit which is equal to 1 will be assigned in the same position. The experiment is split into two sections. The first section is an experiment to examine the modular exponentiation. The results demonstrate that the cost of completing the modular multiplication is decreased if the number of groups is very small. In tables 7 - 9, four modular multiplications are required when there is one group, although number of bits which are equal to 0 in each table is different. The second component is the experiment to examine the point multiplication process in Elliptic Curves Cryptography. The findings demonstrate that if the number of groups is small, the costs to compute point additions are low. In tables 10 - 12, assigning one group is appeared, number of point addition is one when the multiplier of a point is an even number. However, three-point additions are required when the multiplier is an odd number. As a result, the proposed method is an alternative way that should be used when the number of groups is minimal in order to save the costs. 
id478#Nonlinear electromechanical modeling and robustness of soft robotic fish-like energy harvester: Insights and possible issues#This work investigates the possible integration of an energy harvester in a bioinspired fish-like aquatic unmanned vehicle. The defined fish-like system utilizes a reduced complexity prescribed motion as the representation for energy harvester to be subjected to. Nonlinear electromechanical modeling is performed by considering the geometric and piezoelectric nonlinearities. A convergence analysis is carried out in order to determine the required modes in the Galerkin discretization due to the presence of nonlinear interactions between the prescribed and relative motions. The utilization of higher-order modeling for the strain and material leads to the identification of impactful prescribed motions terms that can activate the nonlinearities in the system, results in more harmonics to consider, and leads to the presence parametric excitation terms. Considering a reduced-complex model by decreasing the value of the quadratic constraint envelope that the fish-like system would be forced with, the soft-robotic system behaves more with a base excitation characteristic. Small damping would allow this prescribed motion with reduced quadratic envelope forcing still induces a hardening behavior, but the other harmonics and parametric resonance seen are greatly reduced. Considering this reduced complexity system, the interaction between the prescribed and base excitations is also investigated to demonstrate that when the two excitations are of similar nature constructive and destructive build of the response waveform can occur when looking at near the first natural resonance. It is shown that the quenching phenomenon can take place which may result in a destructive response of the piezoelectric energy harvester. The results show that the robustness of the fish-like robot is directly dependent on the design parameters including the damping of the structure, importance of the undulatory motion, and activation of the resonances. 
id479#Risk assessment tools for industrial human-robot collaboration: Novel approaches and practical needs#According to the standard ISO 10218–2, industrial robot systems must be subjected to a risk assessment prior to commissioning. In current industrial practice, risk assessments are conducted on the basis of experience, expert knowledge, and simple tools such as checklists. However, the recent trend towards human-robot collaboration (HRC) significantly increases the complexity of robot systems and makes risk assessment more challenging. In response to this challenge, the scientific community has proposed various new tools and methods to support risk assessment of HRC applications. As of yet, only few of these novel approaches have found their way into industrial practice. In this paper, we review literature on novel approaches to HRC risk assessment. Furthermore, we evaluate interviews with professionals from the field of HRC to explore needs of industrial practitioners. We compare our findings from literature review and interviews, and discuss which challenges need to be addressed to successfully transfer novel approaches into industrial practice. 
id480#A stronger participant attack on the measurement-device-independent protocol for deterministic quantum secret sharing#Recently, a measurement-device-independent protocol for deterministic quantum secret sharing was proposed (Gao et al. in Sci Chin Phys Mech Astron 63(12):120311, 2020). Unfortunately, it was pointed out to be insecure against the participant attack (Yang et al. in Sci Chin Phys Mech Astron 64(6):260321, 2021). However, this participant attack strategy has an assumption that a dishonest agent has to reveal his single-photon state after other agents. Here, we give a more powerful participant attack strategy regardless of the announcement order. 
id481#Navigate-and-Seek: A Robotics Framework for People Localization in Agricultural Environments#The agricultural domain offers a working environment where many human laborers are nowadays employed to maintain or harvest crops, with huge potential for productivity gains through the introduction of robotic automation. Detecting and localizing humans reliably and accurately in such an environment, however, is a prerequisite to many services offered by fleets of mobile robots collaborating with human workers. Consequently, in this letter, we expand on the concept of a topological particle filter (TPF) to accurately and individually localize and track workers in a farm environment, integrating information from heterogeneous sensors and combining local active sensing (exploiting a robot's onboard sensing employing a Next-Best-Sense planning approach) and global localization (using affordable IoT GNSS devices). We validate the proposed approach in topologies created for the deployment of robotics fleets to support fruit pickers in a real farm environment. By combining multi-sensor observations on the topological level complemented by active perception through the NBS approach, we show that we can improve the accuracy of picker localization in comparison to prior work. 
id482#Multi-GRU based automated image captioning for smartphones [Akilli telefonlar için çok katmanli GRU tabanli otomatik görüntü altyazilama]#Image captioning is the description of an image with natural language expressions using computer vision and natural language processing fields. Recent advances in hardware and processing power in smartphones lead the development of many image captioning applications. In this study, a novel automatic image captioning system based on the encoder-decoder approach that can be applied in smartphones is proposed. While high-level visual information is extracted with the ResNet152V2 convolutional neural network in the encoder part, the proposed decoder transforms the extracted visual information into natural expressions of the images. The proposed decoder with the multilayer gated recurrent unit structure allows generating more meaningful captions using the most relevant visual information. The proposed system has been evaluated using different performance metrics on the MSCOCO dataset and it outperforms the state-ofthe-art approaches. The proposed system is also integrated with our custom-designed Android application, named IMECA, which generates captions in offline mode unlike similar applications. Thus, image captioning is intended to be practical for more people. 
id483#Compiler testing: a systematic literature analysis#Compilers are widely-used infrastructures in accelerating the software development, and expected to be trustworthy. In the literature, various testing technologies have been proposed to guarantee the quality of compilers. However, there remains an obstacle to comprehensively characterize and understand compiler testing. To overcome this obstacle, we propose a literature analysis framework to gain insights into the compiler testing area. First, we perform an extensive search to construct a dataset related to compiler testing papers. Then, we conduct a bibliometric analysis to analyze the productive authors, the influential papers, and the frequently tested compilers based on our dataset. Finally, we utilize association rules and collaboration networks to mine the authorships and the communities of interests among researchers and keywords. Some valuable results are reported. We find that the USA is the leading country that contains the most influential researchers and institutions. The most active keyword is “random testing”. We also find that most researchers have broad interests within small-scale collaborators in the compiler testing area. 
id484#Pixel-level fatigue crack segmentation in large-scale images of steel structures using an encoder–decoder network#Fatigue cracks are critical types of damage in steel structures due to repeated loads and distortion effects. Fatigue crack growth may lead to further structural failure and even induce collapse. Efficient and timely fatigue crack detection and segmentation can support condition assessment, asset maintenance, and management of existing structures and prevent the early permit post and improve life cycles. In current research and engineering practices, visual inspection is the most widely implemented approach for fatigue crack inspection. However, the inspection accuracy of this method highly relies on the subjective judgment of the inspectors. Furthermore, it needs large amounts of cost, time, and labor force. Non-destructive testing methods can provide accurate detection results, but the cost is very high. To overcome the limitations of current fatigue crack detection methods, this study presents a pixel-level fatigue crack segmentation framework for large-scale images with complicated backgrounds taken from steel structures by using an encoder-decoder network, which is modified from the U-net structure. To effectively train and test the images with large resolutions such as 4928 × 3264 pixels or larger, the large images were cropped into small images for training and testing. The final segmentation results of the original images are obtained by assembling the segment results in the small images. Additionally, image post-processing including opening and closing operations were implemented to reduce the noises in the segmentation maps. The proposed method achieved an acceptable accuracy of automatic fatigue crack segmentation in terms of average intersection over union (mIOU). A comparative study with an FCN model that implements ResNet34 as backbone indicates that the proposed method using U-net could give better fatigue crack segmentation performance with fewer training epochs and simpler model structure. Furthermore, this study also provides helpful considerations and recommendations for researchers and practitioners in civil infrastructure engineering to apply image-based fatigue crack detection. 
id485#Preserving mobile commerce IoT data using light weight SIMON block cipher cryptographic paradigm#Internet of Things (IoT) data security is one of the critical ideas in data security as the data to be transferred should be made secure. In the field of mobile commerce is the high effect that it will have on a few parts of the regular life and behavior of potential clients. In this present examination, we focused to build up the IoT security in the field of mobile commerce utilizing new cryptographic strategies. Fundamentally the sensitive data are classified from the entire dataset to enhance the accuracy utilizing Feed Forward Back Propagation Algorithm (FFBN). At that point the security of sensitive data is upgraded by utilizing Light Weight Cryptography (LWC) which encodes the input sensitive data called encryption. To enhance the data privacy and secrecy, an imaginative security model is proposed for example Lightweight SIMON block cipher. This will encrypt the data alongside optimal key selection; this LWC upgrades the m commerce data security level in cloud. For key optimization, a meta-heuristic algorithm called Crow Search Algorithm (CSA) is introduced. The proposed SIMON-CSA accomplishes least time to generate key value to decrypt the data. 
id486#Shrunken Social Brains? A Minimal Model of the Role of Social Interaction in Neural Complexity#The social brain hypothesis proposes that enlarged brains have evolved in response to the increasing cognitive demands that complex social life in larger groups places on primates and other mammals. However, this reasoning can be challenged by evidence that brain size has decreased in the evolutionary transitions from solitary to social larger groups in the case of Neolithic humans and some eusocial insects. Different hypotheses can be identified in the literature to explain this reduction in brain size. We evaluate some of them from the perspective of recent approaches to cognitive science, which support the idea that the basis of cognition can span over brain, body, and environment. Here we show through a minimal cognitive model using an evolutionary robotics methodology that the neural complexity, in terms of neural entropy and degrees of freedom of neural activity, of smaller-brained agents evolved in social interaction is comparable to the neural complexity of larger-brained agents evolved in solitary conditions. The nonlinear time series analysis of agents' neural activity reveals that the decoupled smaller neural network is intrinsically lower dimensional than the decoupled larger neural network. However, when smaller-brained agents are interacting, their actual neural complexity goes beyond its intrinsic limits achieving results comparable to those obtained by larger-brained solitary agents. This suggests that the smaller-brained agents are able to enhance their neural complexity through social interaction, thereby offsetting the reduced brain size. 
id487#Rapid screening of DON contamination in whole wheat meals by Vis/NIR spectroscopy and computer vision coupling technology#This work intends to develop an online experimental system for screening of deoxynivalenol (DON) contamination in whole wheat meals by visible/near-infrared (Vis/NIR) spectroscopy and computer vision coupling technology. Spectral and image information of samples with various DON levels was collected at speed of 0.15 m s−1 on a conveyor belt. The two-type data were then integrated and subjected to chemometric analysis. Discriminant analysis showed that samples could be classified by setting 1000 μg kg−1 as the cut-off value. The best correct classified rate obtained in prediction was 93.55% based on fusion of spectral and image features, with reduced prediction uncertainty as compared to single feature. However, quantification of DON by quantitative analysis was not successful due to poor model performance. These results indicate that, although not accurate enough to provide conclusive result, this coupling technology could be adopted for rapid screening of DON contamination in cereals and feeds during processing. 
id488#3D Printable Sensorized Soft Gelatin Hydrogel for Multi-Material Soft Structures#The ability to 3D print soft materials with integrated strain sensors enables significant flexibility for the design and fabrication of soft robots. Hydrogels provide an interesting alternative to traditional soft robot materials, allowing for more varied fabrication techniques. In this work, we investigate the 3D printing of a gelatin-glycerol hydrogel, where transglutaminase is used to catalyse the crosslinking of the hydrogel such that its material properties can be controlled for 3D printing. By including electron-conductive elements (aqueous carbon black) in the hydrogel we can create highly flexible and linear soft strain sensors. We present a first investigation into adapting a desktop 3D printer and optimizing its control parameters to fabricate sensorized 2D and 3D structures which can undergo >300% strain and show a response to strain which is highly linear and synchronous. To demonstrate the capabilities of this material and fabrication approach, we produce some example 2D and 3D structures and show their sensing capabilities. 
id489#AQUADA: Automated quantification of damages in composite wind turbine blades for LCOE reduction#Reliability and cost are two important driving factors in the development of wind energy. Automation and digitalization of operation and maintenance (O&M) procedures help to increase turbine reliability and reduce the levelized cost of energy (LCOE). Here, we demonstrate a novel method, coined as AQUADA, which may change the current labor-intensive and operation-interfering blade inspection by using thermography and computer vision. We experimentally show that structural damages below the surfaces can be detected and quantified remotely when wind turbine blades are subject to fatigue loads. The data acquisition and analysis are automatically done in one single step, which may shift the current inspection paradigm through more automated O&M procedures. The cost analysis shows that the AQUADA method has a potential to at least half the total inspection cost and reduce the LCOE by 1%–2% when applied to a baseline land-based wind farm consisting of twenty 2.45-MW turbines. All data and source codes are published for researchers to reproduce our results and facilitate further development of AQUADA towards more mature industrial applications. 
id490#Multidatabase Location Based Services (MLBS)#Can LBS be an added feature to many types of applications? It will add values to those applications and provides more services for the users of the system, such as police work, digital battlefield, emergency services, and tourism services, etc. For example, in the security field, security officers can use it to get the information they need in a particular place about a particular person. Using the technology of reading the license plate they can retrieve all the car data, the owner’s data, and the validity of car license and owner license, as well as whether the car owner is wanted by security or not. Connecting different databases can facilitate a lot of tasks in all areas by providing data access to authorized users. In this research, we propose a new framework for processing location-based services queries that access multidatabase system (traditional databases, geodatabase, and spatiotemporal databases). We view a user query as an initiator of a transactional workflow that accesses multidatabase system, and we proposed a technique for processing it. The query analyzer analyzes the query to identify the type of database required, records, and attributes constraints that will be retrieved. We implemented a simulation protype as a multiphase and multipart application to test the validity of the proposed framework. 
id491#Succinct blind Quantum computation using a random oracle#In the universal blind quantum computation problem, a client wants to make use of a single quantum server to evaluate C|0»where C is an arbitrary quantum circuit while keeping C secret. The client's goal is to use as few resources as possible. This problem, first raised by Broadbent, Fitzsimons and Kashefi [FOCS 2009], has become fundamental to the study of quantum cryptography, not only because of its own importance, but also because it provides a testbed for new techniques that can be later applied to related problems (for example, quantum computation verification). Known protocols on this problem are mainly either information-theoretically (IT) secure or based on trapdoor assumptions (public key encryptions). In this paper we study how the availability of symmetric-key primitives, modeled by a random oracle, changes the complexity of universal blind quantum computation. We give a new universal blind quantum computation protocol. Similar to previous works on IT-secure protocols (for example, Broadbent-Fitzsimons-Kashefi), our protocol can be divided into two phases. In the first phase the client prepares some quantum gadgets with relatively simple quantum gates and sends them to the server, and in the second phase the client is entirely classical - it does not even need quantum storage. Crucially, the protocol's first phase is succinct, that is, its complexity is independent of the circuit size. Given the security parameter ?, its complexity is only a fixed polynomial of ?, and can be used to evaluate any circuit (or several circuits) of size up to a subexponential of ?. In contrast, known schemes either require the client to perform quantum computations that scale with the size of the circuit, or require trapdoor assumptions. 
id492#A constraint programming approach for instruction assignment#A fundamental problem in compiler optimization, which has increased in importance due to the spread of multi-core architectures, is to find parallelism in sequential programs. Current processors can only be fully taken advantage of if workload is distributed over the available processors. In this paper we look at distributing instructions in a block of code over multi-cluster processors, the instruction assignment problem. The optimal assignment of instructions in blocks of code on multiple processors is known to be NP-complete. In this paper we present a constraint programming approach for scheduling instructions on multi-cluster systems that feature fast inter-processor communication. We employ a problemdecomposition technique to solve the problem in a hierarchicalmanner where an instance of the master problem solves multiple sub-problems to derive a solution. We found that our approach was able to achieve an improvement of 6-20 percent, on average, over the state-of-the-art techniques on superblocks from SPEC 2000 benchmarks. 
id493#CBMC-GC: An ANSI C compiler for secure two-party computations#Secure two-party computation (STC) is a computer security paradigm where two parties can jointly evaluate a program with sensitive input data, provided in parts from both parties. By the security guarantees of STC, neither party can learn any information on the other party's input while performing the STC task. For a long time thought to be impractical, until recently, STC has only been implemented with domain-specific languages or hand-crafted Boolean circuits for specific computations. Our open-source compiler CBMC-GC is the first ANSI C compiler for STC. It turns C programs into Boolean circuits that fit the requirements of garbled circuits, a generic STC approach based on circuits. Here, the size of the resulting circuits plays a crucial role since each STC step involves encryption and network transfer and is therefore extremely slow when compared to computations performed on modern hardware architectures. We report on newly implemented circuit optimization techniques that substantially reduce the circuit sizes compared to the original release of CBMC-GC. 
id494#Interference graphs for procedures in static single information form are interval graphs#Static Single Information (SSI) Form is a compiler intermediate representation that extends the more well-known Static Single Assignment (SSA) Form. In 2005, several research groups independently proved that interference graphs for procedures represented in SSA Form are chordal graphs. This paper performs a similar analysis concerning SSI Form, and proves that interference graphs are interval graphs. The primary consequences of this paper are threefold: (1) Linear scan register allocation for programs in SSI Form can be implemented in such a way that there are no lifetime holes, thereby sidestepping one of the drawbacks that plagued non-SSI implementations; (2) the k-colorable subgraph problem can be solved in polynomial-time for interval graphs, but remains NP-Complete for chordal graphs - -to date, no register allocation algorithms have been implemented that solve the k-colorable subgraph problem directly; and (3) liveness analysis converges after a single iteration for programs represented in SSI Form.
id495#Parametric higher-order abstract syntax for mechanized semantics#We present parametric higher-order abstract syntax (PHOAS), a new approach to formalizing the syntax of programming languages in computer proof assistants based on type theory. Like higher-order abstract syntax (HOAS), PHOAS uses the meta language's binding constructs to represent the object language's binding constructs. Unlike HOAS, PHOAS types are definable in general-purpose type theories that support traditional functional programming, like Coq's Calculus of Inductive Constructions. We walk through how Coq can be used to develop certified, executable program transformations over several statically-typed functional programming languages formalized with PHOAS; that is, each transformation has a machine-checked proof of type preservation and semantic preservation. Our examples include CPS translation and closure conversion for simply-typed lambda calculus, CPS translation for System F, and translation from a language with ML-style pattern matching to a simpler language with no variable-arity binding constructs. By avoiding the syntactic hassle associated with first-order representation techniques, we achieve a very high degree of proof automation. Copyright 
id496#Certified concurrent abstraction layers#Concurrent abstraction layers are ubiquitous in modern computer systems because of the pervasiveness of multithreaded programming and multicore hardware. Abstraction layers are used to hide the implementation details (e.g., fine-grained synchronization) and reduce the complex dependencies among components at different levels of abstraction. Despite their obvious importance, concurrent abstraction layers have not been treated formally. This severely limits the applicability of layer-based techniques and makes it difficult to scale verification across multiple concurrent layers. In this paper, we present CCAL - a fully mechanized programming toolkit developed under the CertiKOS project - for specifying, composing, compiling, and linking certified concurrent abstraction layers. CCAL consists of three technical novelties: a new game-theoretical, strategy-based compositional semantic model for concurrency (and its associated program verifiers), a set of formal linking theorems for composing multithreaded and multicore concurrent layers, and a new CompCertX compiler that supports certified thread-safe compilation and linking. The CCAL toolkit is implemented in Coq and supports layered concurrent programming in both C and assembly. It has been successfully applied to build a fully certified concurrent OS kernel with fine-grained locking. 
id497#Landslide susceptibility prediction based on image semantic segmentation#The visual characteristics of landslide susceptibility have not yet been fully explored. Professional or trained technicians have to take much time and effort to interpret remote sensing images and locate landslides accordingly. Although conventional machine learning methods based on hand-crafted features for landslide susceptibility prediction (LSP) have acquired remarkable performance, they have certain requirements for prior knowledge. Aiming to learn complex and inherent visual patterns of landslides through minimal manual intervention and achieve fine-grained prediction, in this paper, we define LSP as a semantic segmentation problem on optical remote sensing images. Six widely used semantic segmentation models including Fully Convolutional Network, U-Net, Pyramid Scene Parsing Network, Global Convolutional Network (GCN), DeepLab v3 and DeepLab v3+ are introduced and evaluated for LSP. As the lack of landslide datasets, an open labeled landslide dataset of remote sensing imagery is created for research. The results show that GCN and DeepLab v3 are more applicable for this problem scenario, and the best Mean Intersection-over-Union and Pixel Accuracy of models are 54.2% and 74.0% respectively, which could be further improved by more targeted network architectures. In conclusion, semantic segmentation methods are demonstrated to be effctive for predicting new potential landslides based on remote sensing images. 
id498#Video Abnormal Detection Combining FCN with LSTM [融合FCN和LSTM的视频异常事件检测]#In view of the shortcomings of the traditional video anomaly detection model, a network structure combining the fully convolutional neural (FCN) network and the long short-term memory (LSTM)network is proposed. The network can perform pixel-level prediction and can accurately locate abnormal areas. The network first uses the convolutional neural network to extract image features of different depths in video frames. Then, different image features are input to memory network to analyze semantic information on time series. Image features and semantic information are fused through residual structure. At the same time, the skip structure is used to integrate the fusion features in multi-mode and upsampling is conducted to obtain a prediction image with the same size as the original video frame. The proposed model is tested on the ped 2 subset of University of California, San Diego (UCSD) anomaly detection dataset and University of Minnesota System(UMN)crowd activity dataset. And both two datasets achieve good results. On the UCSD dataset, the equal error rate is as low as 6.6%, the area under curve reaches 98.2%, and the F1 score reaches 94.96%. On the UMN dataset, the equal error rate is as low as 7.1%, the area under curve reaches 93.7%, and the F1 score reaches 94.46%. 
id499#Efficient synthesis of out-of-core algorithms using a nonlinear optimization solver#We address the problem of efficient out-of-core code generation for a special class of imperfectly nested loops encoding tensor contractions arising in quantum chemistry computations. These loops operate on arrays too large to fit in physical memory. The problem involves determining optimal tiling of loops and placement of disk I/O statements. This entails a search in an explosively large parameter space. We formulate the problem as a nonlinear optimization problem and use a discrete constraint solver to generate optimized out-of-core code. The solution generated using the discrete constraint solver consistently outperforms other approaches by up to a factor of four. Measurements on sequential and parallel versions of the generated code demonstrate the effectiveness of the approach.
id500#A floating-point paradigm for high-level synthesis#The current difficulty to deliver complex digital integrated circuits (ICs) on time in the market has created the need for formal, fully automated and rapid methods for designing and developing such products. These methods exploit approaches such as compiler technology, High-level Synthesis (HLS) and Electronic System Level (ESL) flows to handle such complexity. Designing custom VLSI hardware units motivate for developing large arithmetic blocks such as floating-point units, using HLS tools. Here, the formal and automated CCC high-level synthesis tools are used in the floating-point paradigm. The CCC tools generate provably-correct floating-point blocks due to the formality of the method, in order to avoid time-consuming RTL and gate-level simulations. The floating-point operations are coded in the ADA programming language and competitive hardware performance is delivered from the rapid CCC compiler experiment, therefore the presented approach is proven useable. 
id501#Behavioral and neurophysiological effects of an intensified robot-assisted therapy in subacute stroke: a case control study#Background: Physical training is able to induce changes at neurophysiological and behavioral level associated with performance changes for the trained movements. The current study explores the effects of an additional intense robot-assisted upper extremity training on functional outcome and motor excitability in subacute stroke patients. Methods: Thirty moderately to severely affected patients &lt; 3 months after stroke received a conventional inpatient rehabilitation. Based on a case–control principle 15 patients were assigned to receive additional 45 min of robot-assisted therapy (Armeo®Spring) 5 times per week (n = 15, intervention group, IG). The Fugl-Meyer Assessment for the Upper Extremity (FMA-UE) was chosen as primary outcome parameter. Patients were tested before and after a 3-week treatment period as well as after a follow-up period of 2 weeks. Using transcranial magnetic stimulation motor evoked potentials (MEPs) and cortical silent periods were recorded from the deltoid muscle on both sides before and after the intervention period to study effects at neurophysiological level. Statistical analysis was performed with non-parametric tests. Correlation analysis was done with Spearman´s rank correlation co-efficient. Results: Both groups showed a significant improvement in FMA-UE from pre to post (IG: + 10.6 points, control group (CG): + 7.3 points) and from post to follow-up (IG: + 3.9 points, CG: + 3.3 points) without a significant difference between them. However, at neurophysiological level post-intervention MEP amplitudes were significantly larger in the IG but not in the CG. The observed MEP amplitudes changes were positively correlated with FMA-UE changes and with the total amount of robot-assisted therapy. Conclusion: The additional robot-assisted therapy induced stronger excitability increases in the intervention group. However, this effect did not transduce to motor performance improvements at behavioral level. Trial registration The trial was registered in German Clinical Trials Register. Clinical trial registration number: DRKS00015083. Registration date: September 4th, 2018. https://www.drks.de/drks_web/navigate.do?navigationId=trial.HTML&amp;TRIAL_ID=DRKS00015083. Registration was done retrospectively 
id502#An Efficient Small Traffic Sign Detection Method Based on YOLOv3#In recent years, target detection framework based on deep learning has made brilliant achievements. However, real-life traffic sign detection remains a great challenge for most of the state-of-the-art object detection methods. The existing deep learning models are inadequate to effectively extract the features of small traffic signs from large images in real-world conditions. In this paper, we address the small traffic sign detection challenge by proposing a novel small traffic sign detection method based on a highly efficient end-to-end deep network model. The proposed method features fast speed and high precision as it attaches three key insights to the established You Only Look Once (YOLOv3) architecture and other correlated algorithms. Besides, network pruning is appropriately introduced to minimize network redundancy and model size while keeping a competitive detection accuracy. Furtherly, four scale prediction branches are also adopted to significantly enrich the feature maps of multi-scales prediction. In our method, we adjust the loss function to balance the contribution of error source to the total loss. The effectiveness, and robustness of the network is further proved with experiments on Tsinghua-Tencent 100 K traffic sign dataset. The experimental results indicate that our proposed method has achieved better accuracy than that of the original YOLOv3 model. Compared with the schemes in relevant literatures our proposed method not only emerges performance superiors in detection recall and accuracy, but also achieves 1.9–2.7x improvement in detection speed. 
id503#High-performance area-efficient polynomial ring processor for CRYSTALS-Kyber on FPGAs#The quantum-resistant attribute is a new design criterion for cryptography algorithms in the era of quantum supremacy. Lattice-based cryptography is proved to be secure against quantum computing. CRYSTALS-Kyber is a lattice-based promising candidate in the post-quantum cryptography standardization process. This paper proposes a high-performance polynomial ring processor for the CRYSTALS-Kyber algorithm. The processor executes optimized polynomial ring arithmetic, which cuts off over 20%/50% on the times of modular multiplication/addition compared with the straightforward implementations. Besides, the forward and inverse Number Theoretic Transform (NTT) reuse the control logic with the help of an efficient configurable butterfly unit to minimize the area of the finite state machine. Further, the underlying dual-column sequential storage scheme breaks the bottleneck of memory accessing. To evaluate the performance, a fully pipelined architecture is implemented on a low-cost FPGA platform. Benefiting from these optimizations, the Kyber1024processor can perform NTT operation for a 4-dimensional polynomial vector in 17.1 μs, and it achieves speedup by a factor of 2.1 compared with the state-of-the-art implementation. 
id504#Performance Analysis in NoSQL Databases, Relational Databases and NoSQL Databases as a Service in the Cloud#Non-Relational Database Management Systems (NoSQL) arise as an alternative solution to problems not efficiently solved by traditional Database Management Systems (DBMS). NoSQL, unlike the relational model, does not respond to a Data Base type, but represents a set of Database types, with different implementations and characteristics to represent the information. This work represents the continuation of previous studies and aims to compare and analyse 4 local NoSQL Database engines, 2 NoSQL Database as a Service engines in the cloud and a Relational Database engine, using different schemas and under large data volume. 
id505#A multi-resolution deep feature framework for dynamic displacement measurement of bridges using vision-based tracking system#Structural displacement is an imperative indicator for safety evaluation and maintenance. To address the limitations of conventional displacement sensors, advanced non-contact vision-based trackers offer a promising alternative. Based on the reconstructed Efficient Convolution Operator (ECO), a more powerful multi-resolution deep feature framework is proposed to efficiently encode the informative representation. The fusion of the shallow convolutional and the deep convolutional features is discriminative while preserving spatial and structural information. Furthermore, the discrete feature map is transferred to the continuous spatial domain by introducing an interpolation operator to achieve accurate sub-pixel registration. A careful comparison of the results on the steel suspension bridge demonstrates the high accuracy of the multi-resolution deep feature tracker (MDFT) for displacement measurement. The performances in the time and frequency domain show decent agreement with the results acquired by the laser displacement sensor (LDS), which confirm the low-cost, target-free, high resolution, and non-contact measurement capacities. 
id506#Automated classification of fauna in seabed photographs: The impact of training and validation dataset size, with considerations for the class imbalance#Machine learning is rapidly developing as a tool for gathering data from imagery and may be useful in identifying (classifying) visible specimens in large numbers of seabed photographs. Application of an automated classification workflow requires manually identified specimens to be supplied for training and validating the model. These training and validation datasets are generally generated by partitioning the available manual identified specimens; typical ratios of training to validation dataset sizes are 75:25 or 80:20. However, this approach does not facilitate the desired scalability, which would require models to successfully classify specimens in hundreds of thousands to millions of images after training on a relatively small subset of manually identified specimens. A second problem is related to the ‘class imbalance’, where natural community structure means that fewer specimens of rare morphotypes are available for model training. We investigated the impact of independent variation of the training and validation dataset sizes on the performance of a convolutional neural network classifier on benthic invertebrates visible in a very large set of seabed photographs captured by an autonomous underwater vehicle at the Porcupine Abyssal Plain Sustained Observatory. We tested the impact of increasing training dataset size on specimen classification in a single validation dataset, and then tested the impact of increasing validation set size, evaluating ecological metrics in addition to computer vision metrics. Computer vision metrics (recall, precision, F1-score) indicated that classification improved with increasing training dataset size. In terms of ecological metrics, the number of morphotypes recorded increased, while diversity decreased with increasing training dataset size. Variation and bias in diversity metrics decreased with increasing training dataset size. Multivariate dispersion in apparent community composition was reduced, and bias from expert-derived data declined with increasing training dataset size. In contrast, classification success and resulting ecological metrics did not differ significantly with varying validation dataset sizes. Thus, the selection of an appropriate training dataset size is key to ensuring robust automated classifications of benthic invertebrates in seabed photographs, in terms of ecological results, and validation may be conducted on a comparatively small dataset with confidence that similar results will be obtained in a larger production dataset. In addition, our results suggest that automated classification of less common morphotypes may be feasible, providing that the overall training dataset size is sufficiently large. Thus, tactics for reducing class imbalance in the training dataset may produce improvements in the resulting ecological metrics. 
id507#CryptoGA: a cryptosystem based on genetic algorithm for cloud data security#Cloud Computing is referred to as a set of hardware and software that are being combined to deliver various services of computing. The cloud keeps the services for delivery of software, infrastructure, and platform over the Internet based on the user’s demand. In the IT industry, cloud computing plays an important role to access services anywhere in the world. With increasing demand and popularity of cloud computing, several types of threats and vulnerabilities are also increased. Data integrity and privacy are the key issues in cloud computing and are thoughtful as the data is stored in different geographical locations. Therefore, data integrity and privacy protection provisions are the most prominent factors of user’s concerns about the cloud computing environment. In this paper, a new model based on a genetic algorithm (GA) CryptoGA is proposed to cope with data integrity and privacy issues. GA is used to generate keys for encryption and decryption which are integrated with a cryptographic algorithm to ensure privacy and integrity of cloud data. Known and common parameters i.e. execution time, throughput, key size, and avalanche effect are considered for evaluation and comparison. Ten different datasets are used in experiments for testing and validation. Experimental results analysis show that the proposed model ensures the integrity and preserves the privacy of the user’s data against unauthorized parties. Moreover, the CryptoGA is robust and provides better performance on selected parameters as compared to state-of-the-art cryptographic algorithms i.e. DES, 3DES, RSA, Blowfish, and AES. 
id508#Development of BIM-integrated construction robot task planning and simulation system#A major challenge toward construction robotization is a lack of a system that generates detailed behaviors of robots as part of the construction process based on information contained in building information modeling (BIM) and construction schedules. This study extends BIM to incorporate robot task planning and generate detailed motions conducting construction tasks. A prototype was built upon robot operating system (ROS), focusing on generating robot task plans for indoor wall painting. The prototype includes a converter that generates a ROS-compliant world file from industry foundation classes (IFC) file and sub-processes that conduct localization, navigation, and motion planning. A case study was conducted to demonstrate the system's capability to simulate behaviors of a painting robot and evaluate the performance within the context of the construction-related tasks. The case study demonstrates the proposed BIM-leveraged robot task planning can integrate construction and robotics domains to plan operations of autonomous robots in construction projects. 
id509#Card-Based Cryptography Meets Formal Verification#Card-based cryptography provides simple and practicable protocols for performing secure multi-party computation with just a deck of cards. For the sake of simplicity, this is often done using cards with only two symbols, e.g., ♣ and ♡ . Within this paper, we also target the setting where all cards carry distinct symbols, catering for use-cases with commonly available standard decks and a weaker indistinguishability assumption. As of yet, the literature provides for only three protocols and no proofs for non-trivial lower bounds on the number of cards. As such complex proofs (handling very large combinatorial state spaces) tend to be involved and error-prone, we propose using formal verification for finding protocols and proving lower bounds. In this paper, we employ the technique of software bounded model checking (SBMC), which reduces the problem to a bounded state space, which is automatically searched exhaustively using a SAT solver as a backend. Our contribution is threefold: (a) we identify two protocols for converting between different bit encodings with overlapping bases, and then show them to be card-minimal. This completes the picture of tight lower bounds on the number of cards with respect to runtime behavior and shuffle properties of conversion protocols. For computing AND, we show that there is no protocol with finite runtime using four cards with distinguishable symbols and fixed output encoding, and give a four-card protocol with an expected finite runtime using only random cuts. (b) We provide a general translation of proofs for lower bounds to a bounded model checking framework for automatically finding card- and run-minimal (i.e., the protocol has a run of minimal length) protocols and to give additional confidence in lower bounds. We apply this to validate our method and, as an example, confirm our new AND protocol to have its shortest run for protocols using this number of cards. (c) We extend our method to also handle the case of decks on symbols ♣ and ♡ , where we show run-minimality for two AND protocols from the literature. 
id510#Scientific Inquiry in Middle Schools by combining Computational Thinking, Wet Lab Experiments, and Liquid Handling Robots#Computational thinking (CT) is necessary for Science, Technology, Engineering, and Mathematics (STEM) literacy, but it can be difficult for many students to develop and it is challenging to integrate into science curricula. Here, we present a five-session curriculum where sixth-grade students programmed a Liquid Handling Robot (LHR) to conduct a science experiment while engaging in CT. We used a mixed-methods approach to assess how the curricular integration of robotics and science experimentation advances students' CT skills and perceptions of computation in science. We identified growth in CT skills, specifically regarding Algorithmic Thinking. Students identified as key advantages of this approach the increased precision in experimental procedures, time-efficiency, and easier debugging. This course provides a proof of concept curriculum on how the implications for teaching and learning of CT can be assessed, and how CT and robotics can be brought to science classrooms, especially for chemistry and biology. 
id511#Multi-Radio Support on Asynchronous Processor Cores: A Design Methodology Approach for Cognitive Radios#It has only been very recently that commercial asynchronous processors on FPGAs have started to take shape, and much of the design details of the architecture prototypes are not publicly available. Programming description languages and CAD tools for asynchronous design are still maturing, and there are different languages like CSP, Tangram, OCCAM, Verilog+, etc., which are difficult to port to different asynchronous target architectures. The on-going research on multi-radio realization on asynchronous microprocessors (that do not run an operating system) focuses on a custom-instruction based hybrid optimality structure involving a combination of STAPL (Single Track Handshake Asynchronous Pulse Logic) circuits and QDI (Quasi- Delay Insensitive) circuits design styles which are fundamentally different in implementation character. Compiling different description languages dynamically run-time on reconfigurable asynchronous targets where the target architectures might themselves morph based on the processed data (an embodiment of multimedia information systems for ultra-low power and battery conserving constrains), is currently difficult to realize in an optimal manner. This work-in-progress paper attempts to describe a design proposal that extends the Microsoft Phoenix compiler framework to include asynchronous instruction set targets and aims at extending the functionality of asynchronous processors to support mobile computing and development of future multimedia information systems. 
id512#Development and implementation of technical decision for digital support of construction using photogrammetry methods#The SfM-algorithm (Structure from Motion) as a computer vision algorithm is commonly applied and has been extensively studied. The possibility of using the SfM-algorithm during the construction of the NPP (nuclear power plant) is considered in this study. The aim of the study is to analyze basic 3D-scanning methods to find a decision for digital support of the NPP construction. To achieve this aim, it is necessary to: • study basic 3D scanning methods in detail; • examine 3D scanning methods in practice; • identify specific features of 3D scanning methods; • create a methodology based on the specific features of 3D scanning methods. Special attention is given to the influence of specific features on the result. This study demonstrates the features of applying the SfM-algorithm to control the NPP construction. 3D-models of nuclear facilities were obtained by using SfM- and MVS-methods and then comparing these models with the project (with the reference) made it possible to find construction deviations. Important characteristics of the methodology are accuracy, usability and profitability. The technique for digital support of the NPP construction was found and tested. The obtained technique has already found its application in the NPP construction as a part of the «Proryv» project in Russia. The study enables a better understanding of scanning methods advantages and the scope of their application. 
id513#Proved generation of implementations from computationally secure protocol specifications#In order to obtain implementations of security protocols proved secure in the computational model, we previously proposed the following approach: we write a specification of the protocol in the input language of the computational protocol verifier CryptoVerif, prove it secure using CryptoVerif, then generate an OCaml implementation of the protocol from the CryptoVerif specification using a specific compiler that we have implemented. However, until now, this compiler was not proved correct, so we did not have real guarantees on the generated implementation. In this paper, we fill this gap. We prove that this compiler preserves the security properties proved by CryptoVerif: if an adversary has probability p of breaking a security property in the generated code, then there exists an adversary that breaks the property with the same probability p in the CryptoVerif specification. Therefore, if the protocol specification is proved secure in the computational model by CryptoVerif, then the generated implementation is also secure. 
id514#Exploring design and governance challenges in the development of privacy-preserving computation#Homomorphic encryption, secure multi-party computation, and differential privacy are part of an emerging class of Privacy Enhancing Technologies which share a common promise: to preserve privacy whilst also obtaining the benefts of computational analysis. Due to their relative novelty, complexity, and opacity, these technologies provoke a variety of novel questions for design and governance.We interviewed researchers, developers, industry leaders, policymakers, and designers involved in their deployment to explore motivations, expectations, perceived opportunities and barriers to adoption. This provided insight into several pertinent challenges facing the adoption of these technologies, including: how they might make a nebulous concept like privacy computationally tractable; how to make them more usable by developers; and how they could be explained and made accountable to stakeholders and wider society. We conclude with implications for the development, deployment, and responsible governance of these privacy-preserving computation techniques. 
id516#Video reasoning for conflict events through feature extraction#The rapid growth of multimedia data and the improvement of deep learning technology has allowed high-accuracy models to be trained for various fields. Video tools such as video classification, temporal action detection, and video summary are now available for the understanding of videos. In daily life, many social events start with a small conflict event. If conflicts and the subsequent dangers can be learned about from a video, we can prevent social incidents from occurring early on. This research presents a video and audio reasoning network that infers possible conflict events through video and audio features. To make the respective model more generalizable to other tasks, we have also added a predictive network to predict the risk of conflict events. We use multitasking to render the characteristics of movies and voices more generalizable to other similar tasks. We also propose several methods to integrate video features and audio features, improving the reasoning performance of the model. There’s a model we proposed is called the video and audio reasoning Network (VARN) which is more accurate than other models. Compared with RandomNet, it achieves a 2.9 times greater accuracy. 
id517#Exploring performance characteristics of the optane 3D xpoint storage technology#Intel's Optane solid-state nonvolatile storage device is constructed using their new 3D Xpoint technology. Although it is claimed that this technology can deliver substantial performance improvements compared to NAND-based storage systems, its performance characteristics have not been well studied. In this study, intensive experiments and measurements have been carried out to extract the intrinsic performance characteristics of the Optane SSD, including the basic I/O performance behavior, advanced interleaving technology, performance consistency under a highly intensive I/O workload, influence of unaligned request size, elimination of write-driven garbage collection, read disturb issues, and tail latency problem. The performance is compared to that of a conventional NAND SSD to indicate the performance difference of the Optane SSD in each scenario. In addition, by using TPC-H, a read-intensive benchmark, a database system's performance has been studied on our target storage devices to quantify the potential benefits of the Optane SSD to a real application. Finally, the performance impact of hybrid Optane and NAND SSD storage systems on a database application has been investigated. 
id518#Generative Adversarial Networks (GANs)#Generative Adversarial Networks (GANs) is a novel class of deep generative models that has recently gained significant attention. GANs learn complex and high-dimensional distributions implicitly over images, audio, and data. However, there exist major challenges in training of GANs, i.e., mode collapse, non-convergence, and instability, due to inappropriate design of network architectre, use of objective function, and selection of optimization algorithm. Recently, to address these challenges, several solutions for better design and optimization of GANs have been investigated based on techniques of re-engineered network architectures, new objective functions, and alternative optimization algorithms. To the best of our knowledge, there is no existing survey that has particularly focused on the broad and systematic developments of these solutions. In this study, we perform a comprehensive survey of the advancements in GANs design and optimization solutions proposed to handle GANs challenges. We first identify key research issues within each design and optimization technique and then propose a new taxonomy to structure solutions by key research issues. In accordance with the taxonomy, we provide a detailed discussion on different GANs variants proposed within each solution and their relationships. Finally, based on the insights gained, we present promising research directions in this rapidly growing field. 
id519#"Information system ""academic load distribution"""#The preparation of documentation for the needs of university department (curriculum, working curriculum, academic load distribution lecturers, and other documents and reporting forms) is a labour-consuming process that requires a lot of care and time. That is why the automatic generation of these documents is an urgent problem, the solution of which will significantly simplify the planning of educational process. To improve the efficiency of work on the preparation of documentation, the major part of which is based on the same source data, it is advisable to use information systems. The large amounts of data and the number of source documents, their frequent change or appearance of new documents suggest the development and expansion of the database structure over time. Therefore, there is an urgent need for the widespread introduction of information technology in the document management of university department. To solve this problem, the document management model of the department was developed. This model considers the relationship between lecturers, valid actions on documents and the status of individual documents. For the automated distribution of the academic load, a software module was developed. This module is developed in Java language. Based on the relational database management system MySQL, a department database of documents was created. This information system has been successfully tested at the Department of Management and Public Administration of the State University of Infrastructure and Technologies. Copyright 
id520#Queue programs characterization using performance bounds#The special characteristics and simplicity of queue computing offer an attractive model for building state of the art processors with low hardware complexity, low power consumption, small memory footprint, and high parallelism. This paper presents a performance bounds characterization of the queue computation model. Performance bounds gives an approximation of the upper limit performance that can be expected from a given system for a given application. Thus, this characterization provides useful information about the strengths of a queue-based computer system. This design space exploration approach is useful to estimate the performance of a computer system without having to build the actual hardware. We present a performance bounds model based on the Lower Bound Execution Time (LBET) that has been implemented in the queue compiler. Our infrastructure profiles applications and gathers basic block execution statistics to feed and solve the LBET equation to approximate its performance. Our results suggest that a queue processor is capable of achieving similar performance to a conventional high-performance superscalar register machine.
id521#Hybrid image encryption scheme for secure E-health systems#DNA computing is an area of natural computing inspired by molecular biology. It can be used to perform arithmetic and logic operations on information encoded as DNA strands. Due to the rapid development of network technology, E-health care services are helpful for all patients from anywhere at any time. Patients can use these services through a public channel. The security of patients’ privacy is therefore an essential issue in E-health care. In this paper, an encryption scheme is proposed for hiding patient information in a medical image using the Least Significant Bit algorithm, which involves hiding data in the least bit of image pixels. The image is then compressed with a key which is generated from a combination of six stages of chaotic maps and DNA encoding rules that result in encoding data into a DNA format. The proposed security system was applied to six different medical images, including X-ray images, CT images, and MRI images. The results have been evaluated using several evaluation metrics. The proposed scheme appears to be robust. 
id522#Analysis of disease data based on Neo4J graph database#As we all know, there are many diseases in the natural environment in which we humans live. A disease may show a variety of symptoms in patients, such as appendicitis, a variety of symptoms of appendicitis including abdominal pain, fever, gastrointestinal reactions and so on. At the same time, a symptom may also correspond to a variety of diseases. The relational database uses a large number of links to represent and query these complex and larger correspondences, which is very expensive. Many data relationships in the real world are graphical, and the graph database can better describe such data [1].The Neo4j graph database's data store has pointers to their neighbors, so it's easy to extend the newly discovered content. In addition, querying highly correlated data is very efficient for the Neo4j graph database. In the graphical database, the symptoms associated with each disease and the association between the disease and the disease can be clearly displayed to help people better judge the disease. This paper first introduces the data into the Neo4j graph database, and then introduces Neo4j's query language Cypher, so as to make a more intuitive image analysis and provide corresponding treatment suggestions for the disease data of related queries. 
id523#Transvesical robotic excision of a Müllerian duct remnant#Müllerian duct remnants are rare and found in patients with disorders of sexual development. Presenting symptoms vary and many parents opt for surgical management. Literature on robotic repair is limited to small series, single case reports and all were approached extravesically. We present our case of a unique transvesical approach. Perioperative parameters were favorable with no complications, suggesting robotic repair is a safe and effective treatment strategy for these unique patients. 
id524#AtoCC - Learning environment for teaching theory a of automata and formal languages#The learning environment AtoCC is presented to be of use in teaching abstract automata, formal languages, and some of its applications in compiler construction. From a teacher's perspective AtoCC aims to address a broad range of different learning activities forcing the students to actively interact with the subjects being taught.
id525#Robotics in Teaching-Learning: But in Nursing?#In this teaching-learning column, the use of robotic technology is explored as a possible adjunctive assistant to faculty. Given the advances in technology and the imposed restrictions of the recent COVID-19 pandemic, a discussion of using robotic technology in the teaching-learning of nursing seems timely and relevant. Questions to consider are explored. Then a concern and a possibility are presented for incorporating this advancing technology into teaching-learning from a humanbecoming perspective. Faculty are encouraged to enter into a deeper dialogue with colleagues in exploring options for incorporating robots with the caveat that the core of teaching-learning must remain the presence of teacher with student on the co-created never ending journey of the unfolding mystery in coming-to-know. 
id526#Intelligent smart home system using amazon alexa tools#The paper is devoted to developing an intelligent, innovative home system using voice assistant technologies - Amazon Alexa. This platform is designed to simplify human life and help to cope faster, more conveniently and safer with everyday tasks. The system constantly collects data and analyses user commands to create history files to generate behavioural scenarios using the neural network in the future. The system's primary purpose is to monitor, track, manage, and integrate with people's methods in their homes. The intelligent home system presented in this paper allows you to track all failures of integration systems. One of the critical factors of this information system is that the module is developed and implemented allows integration with previously incompatible hardware from Xiaomi and programmers “brains” of the voice assistant from Amazon. The first company to market a wide range of devices that can be used in the smart home of the 21st century, but unfortunately, Xiaomi does not have enough integration with voice assistants of global brands. It is one of the problems that this system is designed to solve. After all, by integrating with voice assistants, you can significantly simplify the process of controlling the house by turning on or off various appliances or systems, setting or configuring multiple means and methods of comfort, such as temperature or underfloor heating. The innovative home systems currently on the market are evolving extremely rapidly, exponentially. At present, even systems are known where, controlling the house remotely, being at the other end of the city, the user can start cooking in the kitchen, or analyse what products are missing in the refrigerator, or take a bath in advance. The main point in such a system is the convenience of the application, which is used to control a smart home. The more functions in such an application to manage the system, the more convenient it is, as practice shows. And here come the aid of voice assistants, where the user does not need to spend time flipping through the menu and looking for the correct functionality. All you need is to say the sentence clearly, the command to be executed. Then the system will trace the order into the code and, using protocols, send it to systems integrated with the house, such as security systems: cameras, motion sensors, etc. The research methods in this master's thesis are the analysis and comparison of voice assistants for integration with smart home and the study of transport protocols for data transmission between hardware devices. The primary purpose of this system was to create a “bridge” for the integration between hardware and software of two different companies from different parts of the world. This integration will open up great opportunities for a combination of devices and programs and make the process of creating an intelligent home more flexible and comfortable. The server part of the system is implemented using a development platform from Microsoft - .Net. It is the best choice because it is evolving extremely fast and provides many new features with each version. This technology is used by large business projects and small non-commercial projects. It is ideal for solving both large resource-intensive and time-consuming tasks and minor ones in which the main criterion is the speed of execution. NoSQL is chosen from the databases side because we need to store large amounts and arrays of data, and atomicity and trans-actionality are not as crucial as in systems that use relational databases. The client-side is built as simply as possible because, in this system, the critical factor is voice control, not the web interface or mobile application. The object of research is the mechanisms for creating and managing innovative home systems with the help of voice commands. The subject of study is creating, managing, and accumulating data from components and modules connected to smart home and based on them to develop scenarios of system behaviour. The research aims to present and analyse the functional capabilities of this system, the formation of methods, and its integration with the hardware of incompatible companies. The implemented system covers all the basic needs required to perform the functionality, as shown in the UML diagrams. 
id527#Calibrating feature maps for deep CNNs#Many performance improvement techniques calibrate the outputs of convolutional layers to improve the performance of convolutional neural networks, e.g., Squeeze-and-Excitation Networks (SENets). These techniques train the network to extract calibration weights from the input itself. However, these methods increase the complexity of the model in order to perform calibration. We propose an approach to calibrate the outputs of convolutional layers efficiently. Specifically, we propose an architectural block called Accuracy Booster, which calibrates the convolutional layer outputs channel-wise while introducing minimal extra parameters and computation. We experimentally show that our approach achieves higher performance than existing calibration methods over several datasets and architectures while introducing lesser parameters than them. We also generalize our proposed block to calibrate the channel, width, and height of the layer output in parallel. We empirically show that this type of composite calibration performs better than applying channel-wise calibration, spatial calibration, or both. We validate our approach on the CIFAR-10/100, CUB, ImageNet, and MS-COCO datasets for various tasks. The ResNet-50 architecture with the accuracy booster block performs comparably on the classification task to the ResNet-152 architecture, which has more than twice the number of parameters. We empirically show that our method generalizes well to other tasks such as object detection. We perform extensive ablation experiments to validate our approach. 
id528#LIGA: a cryptosystem based on the hardness of rank-metric list and interleaved decoding#We propose the new rank-metric code-based cryptosystem LIGA which is based on the hardness of list decoding and interleaved decoding of Gabidulin codes. LIGA is an improved variant of the Faure–Loidreau (FL) system, which was broken in a structural attack by Gaborit, Otmani, and Talé Kalachi (GOT, 2018). We keep the FL encryption and decryption algorithms, but modify the insecure key generation algorithm. Our crucial observation is that the GOT attack is equivalent to decoding an interleaved Gabidulin code. The new key generation algorithm constructs public keys for which all polynomial-time interleaved decoders fail—hence LIGA resists the GOT attack. We also prove that the public-key encryption version of LIGA is IND-CPA secure in the standard model and the key encapsulation mechanisms version is IND-CCA2 secure in the random oracle model, both under hardness assumptions of formally defined problems related to list decoding and interleaved decoding of Gabidulin codes. We propose and analyze various exponential-time attacks on these problems, calculate their work factors, and compare the resulting parameters to NIST proposals. The strengths of LIGA are short ciphertext sizes and (relatively) small key sizes. Further, LIGA guarantees correct decryption and has no decryption failure rate. It is not based on hiding the structure of a code. Since there are efficient and constant-time algorithms for encoding and decoding Gabidulin codes, timing attacks on the encryption and decryption algorithms can be easily prevented. 
id529#Team RuBot's experiences and lessons from the ARIAC#We share experiences and lessons learned in participating the annual Agile Robotics for Industrial Automation Competition (ARIAC). ARIAC is a simulation-based competition focusing on pushing the agility of robotic systems for handling industrial pick-and-place challenges. Team RuBot started competing from 2019, placing 2nd place in ARIAC 2019 and 3rd place in ARIAC 2020. The article also discusses the difficulties we faced during the contest and our strategies for tackling them. Video of system sketches: https://youtu.be/7H7YLeJz2zE. 
id530#Qrobot: A quantum computing approach in mobile robot order picking and batching problem solver optimization#This article aims to bring quantum computing to robotics. A quantum algorithm is developed to minimize the distance traveled in warehouses and distribution centers where order picking is applied. For this, a proof of concept is proposed through a Raspberry Pi 4, generating a quantum combinatorial optimization algorithm that saves the distance travelled and the batch of orders to be made. In case of computational need, the robot will be able to parallelize part of the operations in hybrid computing (quantum + classical), accessing CPUs and QPUs distributed in a public or private cloud. We developed a stable environment (ARM64) inside the robot (Raspberry) to run gradient operations and other quantum algorithms on IBMQ, Amazon Braket (D-Wave), and Pennylane locally or remotely. The proof of concept, when run in the above stated quantum environments, showed the execution time of our algorithm with different public access simulators on the market, computational results of our picking and batching algorithm, and analyze the quantum real-time execution. Our findings are that the behavior of the Amazon Braket D-Wave is better than Gate-based Quantum Computing over 20 qubits, and that AWS-Braket has better time performance than Qiskit or Pennylane. 
id531#WRGPruner: A new model pruning solution for tiny salient object detection#The model pruning is one of the predominant model compression tasks to decrease the demands in computing power and memory footprint. However, most existing pruning methods have overly broad application areas, which defects in a sub-optimal solution specifically to solve certain specified difficult problems in the tasks of salient object detection. In this paper, we propose a novel solution, dubbed as WRGPruner, based on the concept of salient energy level (SEL) for tiny salient object detection. The concept of SEL defines the level of assessing the distinguishing ability of parameters in the trained model between background and salient objects. To exploit the SEL, the WRGPruner is proposed, which considers three factors for model compression including the weight in the filter, the mathematical rank of the feature map matrix, and the gradient in the backward propagation. We mathematically prove the effectiveness of the WRGPruner for tiny salient objects. Besides, a tiny salient object dataset (TSOD) is constructed for evaluation. Extensive experiments show that WRGPruner reduces 60% of parameters with slight enhancement in terms of six accuracy metrics for VGG16 on TSOD. This demonstrates that the SEL is suitable for measure parameters and the effectiveness of WRGPruner. 
id532#Internal models of system F for decompilation#This paper considers Girard's internal coding of each term of System F by some term of a code type. This coding is the type-erasing coding definable already in the simply typed lambda-calculus using only abstraction on term variables. It is shown that there does not exist any decompiler for System F in System F, where the decompiler maps a term of System F to its code. An internal model of F is given by interpreting each type of F by some type equipped with maps between the type and the code type. This paper gives a decompiler- normalizer for this internal model in F, where the decompiler-normalizer maps any term of the internal model to the code of its normal form. It is also shown that for any model of F the composition of this internal model and the model produces another model of F whose equational theory is below untyped beta-eta-equality. 
id533#Hybrid Embedded-Systems-Based Approach to in-Driver Drunk Status Detection Using Image Processing and Sensor Networks#Car drivers under the influence of alcohol is one of the most common causes of road traffic accidents. To tackle this issue, an emerging, suitable alternative is the use of intelligent systems -traditionally based on either sensor networks or artificial vision- that are aimed to prevent starting the car when drunk status on the car driver is detected. In such vein, this paper introduces a system whose main objective is identifying a person having alcohol in the blood through supervised classification of sensor-generated and computer-vision-based data. To do so, some drunk-status criteria are considered, namely: the concentration of alcohol in the car environment, the facial temperature of the driver and the pupil width. Specifically, for data acquisition purposes, the proposed system incorporates a gas sensor, temperature sensor and a digital camera. Acquired data are analyzed into a two-stages machine learning system consisting of feature selection and supervised classification algorithms. Both acquisition and analysis stages are to be performed into a embedded system, and therefore all procedures and algorithms are designed to work at low-computational resources. As a remarkable outcome, due mainly to the incorporation of feature selection and relevance analysis stages, proposed approach reaches a classification performance of 98% while ensures adequate operation conditions for the embedded system. 
id534#Weight and volume estimation of poultry and products based on computer vision systems: a review#The appearance, size, and weight of poultry meat and eggs are essential for production economics and vital in the poultry sector. These external characteristics influence their market price and consumers' preference and choice. With technological developments, there is an increase in the application and importance of vision systems in the agricultural sector. Computer vision has become a promising tool in the real-time automation of poultry weighing and processing systems. Owing to its noninvasive and nonintrusive nature and its capacity to present a wide range of information, computer vision systems can be applied in the size, mass, volume determination, and sorting and grading of poultry products. This review article gives a detailed summary of the current advances in measuring poultry products' external characteristics based on computer vision systems. An overview of computer vision systems is discussed and summarized. A comprehensive presentation of the application of computer vision-based systems for assessing poultry meat and eggs was provided, that is, weight and volume estimation, sorting, and classification. Finally, the challenges and potential future trends in size, weight, and volume estimation of poultry products are reported. 
id536#A new efficient permutation-diffusion encryption algorithm based on a chaotic map#Nowadays, the internet is the main channel for private communication. The use of a public channel for private communications increases the importance of cryptography to protect the information. The most commonly shared archives are images, for which traditional algorithms are not suitable due to the file's large size and algorithms’ high computational cost. In this scenario, chaos theory surges as a promising alternative for image encryption due to intrinsic features of chaos, leading to the development of many chaotic ciphers for image encryption. However, many of them are either insecure or ineffective, motivating us to present a new chaotic cipher that satisfies both of these requirements. To ensure peak performance, a comparison of chaotic maps is made to select the one most conductible to a high throughout, while architecture measures such as the use of the permutation-diffusion architecture and a hash function are taken to ensure security. Extensive tests applied show that the cipher is secure against various forms of attacks such as statistical, differential, noise, occlusion and chosen-plaintext, while the throughput analysis shows that our cipher compares favorably to other recently presented chaotic ciphers in literature. 
id537#Bim and computer vision‐based framework for fire emergency evacuation considering local safety performance#Fire hazard in public buildings may result in serious casualties due to the difficulty of evacuation caused by intricate interior space and unpredictable development of fire situations. It is essential to provide safe and reliable indoor navigation for people trapped in the fire. Distinguished from the global shortest rescue route planning, a framework focusing on the local safety performance is proposed for emergency evacuation navigation. Sufficiently utilizing the information from Building Information Modeling (BIM), this framework automatically constructs geometry network model (GNM) through Industry Foundation Classes (IFC) and integrates computer vision for indoor positioning. Considering the available local egress time (ALET), a back propagation (BP) neural network is applied for adjusting the rescue route according to the fire situation, improving the local safety performance of evacuation. A campus building is taken as an example for proving the feasibility of the framework proposed. The result indicates that the rescue route generated by proposed framework is secure and reasonable. The proposed framework provides an idea for using real‐time images only to implement the automatic generation of rescue route when a fire hazard occurs, which is passive, cheap, and convenient. 
id538#Self-Supervised Single-Image Depth Estimation from Focus and Defocus Clues#Self-supervised depth estimation has recently demonstrated promising performance compared to the supervised methods on challenging indoor scenes. However, the majority of efforts mainly focus on exploiting photometric and geometric consistency via forward image warping and backward image warping, based on monocular videos or stereo image pairs. The influence of defocus blur to depth estimation is neglected, resulting in a limited performance for objects and scenes in out of focus. In this work, we propose the first framework for simultaneous depth estimation from a single image and image focal stacks using depth-from-defocus and depth-from-focus algorithms. The proposed network is able to learn optimal depth mapping from the information contained in the blur of a single image, generate a simulated image focal stack and all-in-focus image, and train a depth estimator from an image focal stack. In addition to the validation of our method on both synthetic NYUv2 dataset and real DSLR dataset, we also collect our own dataset using a DSLR camera and further verify on it. Experiments demonstrate that our system surpasses the state-of-the-art supervised depth estimation method over 4% in accuracy and achieves superb performance among the methods without direct supervision on the synthesized NYUv2 dataset, which has been rarely explored. 
id539#A verified compiler for an impure functional language#We present a verified compiler to an idealized assembly language from a small, untyped functional language with mutable references and exceptions. The compiler is programmed in the Coq proof assistant and has a proof of total correctness with respect to big-step operational semantics for the source and target languages. Compilation is staged and includes standard phases like translation to continuation-passing style and closure conversion, as well as a common subexpression elimination optimization. In this work, our focus has been on discovering and using techniques that make our proofs easy to engineer and maintain. While most programming language work with proof assistants uses very manual proof styles, all of our proofs are implemented as adaptive programs in Coq's tactic language, making it possible to reuse proofs unchanged as new language features are added. In this paper, we focus especially on phases of compilation that rearrange the structure of syntax with nested variable binders. That aspect has been a key challenge area in past compiler verification projects, with much more effort expended in the statement and proof of binder-related lemmas than is found in standard pencil-and-paper proofs. We show how to exploit the representation technique of parametric higher-order abstract syntax to avoid the need to prove any of the usual lemmas about binder manipulation, often leading to proofs that are actually shorter than their pencil-and-paper analogues. Our strategy is based on a new approach to encoding operational semantics which delegates all concerns about substitution to the meta language, without using features incompatible with general-purpose type theories like Coq's logic. Copyright 
id540#CORF: Coalescing Operand Register File for GPUs#The Register File (RF) in GPUs is a critical structure that maintains the state for thousands of threads that support the GPU processing model. The RF organization substantially affects the overall performance and the energy efficiency of a GPU. For example, the frequent accesses to the RF consume a substantial amount of the dynamic energy, and port contention due to limited ports on operand collectors and register file banks affect performance as register operations are serialized. We present CORF, a compiler-assisted Coalescing Operand Register File which performs register coalescing by combining reads to multiple registers required by a single instruction, into a single physical read. To enable register coalescing, CORF utilizes register packing to co-locate narrow-width operands in the same physical register. CORF uses compiler hints to identify which register pairs are commonly accessed together. CORF saves dynamic energy by reducing the number of physical register file accesses, and improves performance by combining read operations, as well as by reducing pressure on the register file. To increase the coalescing opportunities, we re-architect the physical register file to allow coalescing reads across different physical registers that reside in mutually exclusive sub-banks; we call this design CORF++. The compiler analysis for register allocation for CORF++ becomes a form of graph coloring called the bipartite edge frustration problem. CORF++ reduces the dynamic energy of the RF by 17%, and improves IPC by 9%. 
id541#BIM Big Data Storage in WebVRGIS#In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data. 
id542#Whole Database Retrieval Method of General Relational Database Based on Lucene#Aiming at the needs of full-database retrieval of multi-table massive data, a general-purpose, lightweight, non-invasive full-database retrieval method was proposed and implemented. The JDBC technology was used to complete the content extraction of the general database, and the mapping relationships of common database data types such as Mysql, Sqlserver, and Oracle in the Type class were summarized. Based on Lucene, The full-text indexing of the database content is implemented. From the result of experiment, it could be demonstrated that the proposed method has good performance and stability and is well appropriate for construction of index system for common relational database. 
id543#A vision-based approach for automatic progress tracking of floor paneling in offsite construction facilities#Offsite construction is an approach focused on moving construction tasks from traditional jobsites to manufacturing facilities. Improved productivity of construction tasks is paramount in terms of competitiveness and is achieved through the continuous improvement of operations and planning, which often relies on historical data obtained from previous projects. Despite being a common practice, current methods, such as time studies, are not able to capture the changing scenarios resulting from improvements to production. This paper presents a novel approach to automatically detect and track the progress of construction operations by applying a method that combines deep learning algorithms and finite state machines to existing footage captured by closed-circuit television (CCTV) security cameras. Applied in the context of floor panel manufacturing stations, the proposed method examines entire production days recorded by CCTV cameras, while providing the durations of each task, its required resources, and the task efficiency per panel with high accuracy. 
id544#Systematic crosstalk mitigation for superconducting qubits via frequency-aware compilation#One of the key challenges in current Noisy Intermediate-Scale Quantum (NISQ) computers is to control a quantum system with high-fidelity quantum gates. There are many reasons a quantum gate can go wrong - for superconducting transmon qubits in particular, one major source of gate error is the unwanted crosstalk between neighboring qubits due to a phenomenon called frequency crowding. We motivate a systematic approach for understanding and mitigating the crosstalk noise when executing near-term quantum programs on superconducting NISQ computers. We present a general software solution to alleviate frequency crowding by systematically tuning qubit frequencies according to input programs, trading parallelism for higher gate fidelity when necessary. The net result is that our work dramatically improves the crosstalk resilience of tunable-qubit, fixed-coupler hardware, matching or surpassing other more complex architectural designs such as tunable-coupler systems. On NISQ benchmarks, we improve worst-case program success rate by 13.3x on average, compared to existing traditional serialization strategies. 
id545#Reducing NoC energy consumption through compiler-directed channel voltage scaling#While scalable NoC (Network-on-Chip) based communication architectures have clear advantages over long point-to-point communication channels, their power consumption can be very high. In contrast to most of the existing hardware-based efforts on NoC power optimization, this paper proposes a compiler-directed approach where the compiler decides the appropriate voltage/frequency levels to be used for each communication channel in the NoC. Our approach builds and operates on a novel graph based representation of a parallel program and has been implemented within an optimizing compiler and tested using 12 embedded benchmarks. Our experiments indicate that the proposed approach behaves better - from both performance and power perspectives - than a hardware-based scheme and the energy savings it achieves are very close to the savings that could be obtained from an optimal, but hypothetical voltage/frequency scaling scheme. Copyright 
id546#Underutilised crops database for supporting agricultural diversification#Digital agriculture is driven by the interrelated needs to increase crop production, develop sustainable food systems and cope with the global change. Databases exist for several major crops but a barrier to increasing agrobiodiversity is the lack, and dispersed nature, of information for variously termed minor, orphan or underutilised crops. In this article, we describe an attempt to build a globally accessible database that can be used to store information for underutilised crops. A relational data model was adopted due to its robustness in terms of prototyping and building user interfaces for data governance and dissemination. We have reviewed the design of related agricultural databases, data standards and crop diversification priorities, to build a data model that encompasses major elements of the value chain of crops in the food system. Due to the importance of data accuracy, we added a metadata table that stores information about the sources of all data recorded in the database. We also built a web-based user interface for data management and access. The open-access user interface allows simple data sorting and filtering operations based on the user's needs. As an example of potential use, data were used to build an automated crop selection tool that could shortlist suitable crops based on location at global scale. Other use-cases are discussed including the development of metrics, indicators for the United Nations Sustainable Development Goals and developing bespoke diversification solutions. 
id547#Efficient compilation for queue size constrained queue processors#Queue computers use a FIFO data structure for data processing. The essential characteristics of a queue-based architecture excel at satisfying the demands of embedded systems, including compact instruction set, simple hardware logic, high parallelism, and low power consumption. The size of the queue is an important concern in the design of a realizable embedded queue processor. We introduce the relationship between parallelism, length of data dependency edges in data flow graphs and the queue utilization requirements. This paper presents a technique developed to make the compiler aware of the size of the queue register file and, thus, optimize the programs to effectively utilize the available hardware. The compiler examines the data flow graph of the programs and partitions it into clusters whenever it exceeds the queue limits of the target architecture. The presented algorithm deals with the two factors that affect the utilization of the queue, namely parallelism and the length of variables' reaching definitions. We analyze how the quality of the generated code is affected for SPEC CINT95 benchmark programs and different queue size configurations. Our results show that for reasonable queue sizes the compiler generates a code that is comparable to the code generated for infinite resources in terms of instruction count, static execution time, and instruction level parallelism. 
id548#Synthesis of SQL queries from narrations#Structured Query Language (SQL) remains a standard language used in Relational Database Management Systems (RDBMSs), and has found applications in healthcare (patient registries), businesses (inventories, trend analysis), military, and education, etc. Although, SQL statements are English-like, the process of writing SQL queries is often problematic for non-technical end-users. To address this problem, a tool called Narrations-2-SQL is developed to allow an end-user to specify a query in natural language. Narrations-2-SQL is a desktop application that uses a Jumping Finite Automaton (JFA)-a type of Finite Machine for translating natural language descriptions into SQL queries, execute the queries, and provide a feedback to a user. An experimental evaluation was performed on 204 crowdsourced queries in natural language from the XNorthwind DB. Our results show an accuracy of 88%. To get the users' perceptions of this study, we carried out a survey on 167 end-users. Majority of the participants found Narrations-2-SQL to be very helpful, and agreed that it could be useful in industry. If implemented on a large scale, the tool may be helpful to many end-users in different domains. 
id549#PVPP: A programmable vector packet processor#Recent work on simplifying data plane programming focuses on providing simple, high-level domain-specific languages (DSLs). These languages hide the complex and intricate details of the underlying switching substrate. Programmers write their data-plane programs in these languages which are then compiled to run on a given switch target, which further runs on a particular CPU architecture. However, the simplicity and the domain-specific nature of these DSLs and the lack of flexible switch interfaces that can be targeted by a DSL compiler restrict the ability to optimize generated code. In this work, we present our findings on how the complexity of interfaces on a software switch target available to a compiler can affect the performance of compiled data plane programs. For our experiment platform, we built a P4 compiler called the Programmable Vector Packet Processor (PVPP) targeting the existing Vector Packet Processor (VPP) software switch. P4 is a data plane DSL based on match-action tables, while VPP uses a packet processing node graph model. PVPP compiles a data plane program written in P4 to VPP's internal graph representation. VPP also exposes a sophisticated interface for PVPP to interact with the various features of the underlying architecture e.g., execution modes, memory types, and the batch I/O. Our evaluation shows that PVPP can efficiently exploit these features, resulting in the increased performance of the same data plane program. 
id551#Deep motion blur removal using noisy/blurry image pairs#Removing spatially variant motion blur from a blurry image is a challenging problem as image blur can be complicated and difficult to model accurately. Recent progress in deep neural networks suggests that kernel-free single image deblurring can be achieved, but questions about deblurring performance persist. To improve performance, we proposed a deep convolutional neural network to restore a sharp image from a noisy/blurry image pair captured in quick succession. Two neural network structures, Deblur Long Short-Term Memory (LSTM) and DeblurMerger, are presented to fuse the pair of images in either sequential or parallel manner. To boost the training, gradient loss, adversarial loss, and spectral normalization are leveraged. The training dataset that consists of pairs of noisy/blurry images and the corresponding ground truth sharp image is synthesized based on the benchmark dataset GOPRO. We evaluated the trained networks on a variety of synthetic datasets and real image pairs. The results demonstrate that the proposed approach outperforms the state-of-the-art methods both qualitatively and quantitatively. DeblurLSTM achieves the best debluring performance, while DeblurMerger achieves nearly the same result but with significantly less computation time. 
id552#A vision-based surface displacement/strain measurement technique based on robust edge-enhanced transform and algorithms for high spatial resolution#Measurement of two-dimensional surface displacement/strain distributions can be crucial in monitoring important structures. Computer vision techniques have the potentials for measuring surface displacements/strains. Conventional digital-image-correlation (DIC)-based computer vision techniques that have been applied in controlled conditions with artificially painted speckle patterns, however, have difficulties in robust measurement of structures' surface displacements against optical noises. Additionally, surface displacements obtained by DIC based on block-resolution template matching have limited accuracy because of low spatial resolutions and low level of smoothness. Therefore, a new computer vision technique SurfaceVision is proposed for accurate and robust surface displacement/strain measurement to tackle simulated field environmental conditions by incorporating multiple novel algorithms. First, a gradient-based edge-enhanced transform (EET) originally developed for one-point displacement tracking is extended for enabling robust surface displacement measurements against optical noises by manipulating gradient information rather than image intensities. Then, the improvement in the smoothness of surface displacements is enabled by incorporating EET with the iterative displacement optimization and the customized smart branching algorithms. Moreover, a novel pixel-resolution measurement algorithm is proposed for increasing the spatial resolution of surface displacements. Finally, an original intuitive strain conversion algorithm is developed for converting surface displacements into surface strains based on the principle similar to strain-gauge transducers. The performance of SurfaceVision is first validated in the numerical simulation and further demonstrated in the experiment of the four-point bending test. And a new method developed for predicting crack formations before they appear on structure surfaces, based on analyses of surface displacements/strains, is demonstrated. 
id553#Deep Learning for Visual Analytics of the Spread of COVID-19 Infection in Crowded Urban Environments#The novel SARS-CoV-2 coronavirus caused a global pandemic in 2020 with millions of diagnosed cases and a staggering number of deaths. As a preventive measure, many governments issued social distancing and shelter-in-place mandates to limit human contact and slow the rate of infection. The large extent and duration of the crisis is poised to transform the health sector and alter current practices in retail, business, manufacturing, and construction. While medical researchers are working on antidote and vaccine solutions, contact tracing and self-isolation are deemed effective methods to control community spread. This paper presents a visual analysis approach that uses convolutional neural networks (CNNs) to generate quantifiable metrics of contact tracing. In particular, the YOLO-v3 architecture was trained on an annotated video dataset containing pedestrians. Network pruning and non-maximum suppression were applied to optimize model performance, resulting in 69.41% average precision. The fully trained model was then tested on sample crosswalk video data from Xiamen, China, recorded during the COVID-19 pandemic, followed by projecting detected pedestrians onto an orthogonal map for contact tracing by tracking movement trajectories and simulating the spread of droplets among the healthy population. Results demonstrate that the proposed technique is capable of tracing and documenting infection sources, times, and locations. 
id555#A preliminary study of compiler transformations for graph applications on the emU system#Unlike dense linear algebra applications, graph applications typically suffer from poor performance because of 1) inefficient utilization of memory systems through random memory accesses to graph data, and 2) overhead of executing atomic operations. Hence, there is a rapid growth in improving both software and hardware platforms to address the above challenges. One such improvement in the hardware platform is a realization of the Emu system, a thread migratory and near-memory processor. In the Emu system, a thread responsible for computation on a datum is automatically migrated over to a node where the data resides without any intervention from the programmer. The idea of thread migrations is very well suited to graph applications as memory accesses of the applications are irregular. However, thread migrations can hurt the performance of graph applications if overhead from the migrations dominates benefits achieved through the migrations. In this preliminary study, we explore two high-level compiler optimizations, i.e., loop fusion and edge flipping, and one low-level compiler transformation leveraging hardware support for remote atomic updates to address overheads arising from thread migration, creation, synchronization, and atomic operations. We performed a preliminary evaluation of these compiler transformations by manually applying them on three graph applications over a set of RMAT graphs from Graph500.-Conductance, Bellman-Ford's algorithm for the single-source shortest path problem, and Triangle Counting. Our evaluation targeted a single node of the Emu hardware prototype, and has shown an overall geometric mean reduction of 22.08% in thread migrations. 
id556#A novel lightweight instruction scheduling algorithm for Just-In-Time compiler#In this paper, we present a lightweight algorithm of instruction scheduling to reduce the pipeline stalls on XScale. The algorithm is designed for and implemented in a J2ME Just-In-Time (JIT) compiler. It is not based on Directed Acyclic Graphs (DAGs) or expression trees, but a novel data structure namely extended dependency matrix (EDM). The algorithm has almost linear time complexity to one order of magnitude less of the code length in practice, and linear to the code length in the worst cases. It consumes only about 1 KB constant memory space. On the benchmarks we studied, it can eliminate up to 41% data dependency stalls at runtime. The algorithm is on average 2 times faster than a list scheduling implementation, in terms of compilation time. On all benchmarks we studied, the performance is more than 90% as efficient as that obtained using more time and memory consuming algorithms on average. 
id557#Semantic-driven watermarking of relational textual databases#In relational database watermarking, the semantic consistency between the original database and the distorted one is a challenging issue which is disregarded by most watermarking proposals, due to the well-known assumption for which a small amount of errors in the watermarked database is tolerable. We propose a semantic-driven watermarking approach of relational textual databases, which marks multi-word textual attributes, exploiting the synonym substitution technique for text watermarking together with notions in semantic similarity analysis, and dealing with the semantic perturbations provoked by the watermark embedding. We show the effectiveness of our approach through an experimental evaluation, highlighting the resulting capacity, robustness and imperceptibility watermarking requirements. We also prove the resilience of our approach with respect to the random synonym substitution attack. 
id558#(Hierarchical) Identity-Based Key-Encapsulation Mechanism with Leakage-Resilience [抗泄露的(分层)身份基密钥封装机制]#In the actual application, any adversary can obtain a certain amount of additional information on the internal secret states by performing various leakage attacks, such as cold boot attacks, side-channel attacks, etc. Thus, the security of the traditional cryptography system cannot keep their claimed security in the leakage setting, because we always assume that an adversary cannot capture the leakage on the internal secret states of participator in the traditional ideal security model, and the traditional security was proved in the above ideal security model. Since the hybrid encryption technology has the advantages of both symmetric and asymmetric encryption, the research on identity-based key-encapsulation mechanism (IB-KEM) has received extensive attention in recent years, which is the important underlying technology of identity-based hybrid encryption. To obtain the leakage resilience, a leakage-resilient IB-KEM with chosen ciphertext attacks (CCA) security was proposed. However, the previous scheme has shortcomings in computing, transmission and storage. To address the above shortcomings, a generic construction of CCA secure IB-KEM with leakage resilience is proposed in this paper, and the formal proof can be given from the underlying chosen-plaintext attacks (CPA) secure IB-KEM. In additional, to further show the practicability of the above generic construction, two instances of IB-KEM and hierarchical identity-based key-encapsulation mechanism (HIB-KEM) are proposed, the corresponding CPA security is proved based on the decisional bilinear Diffie-Hellman (DBDH) and the bilinear Diffie-Hellman exponent (BDHE) assumptions, respectively. In additional, based on our generic construction of IB-KEM, a leakage-resilient HIB-KEM with CCA security can be also proposed. Finally, in order to achieve the goal of resisting continuous leakage attacks, key update algorithms of each instance are also researched in this paper, because the previous conclusions shown that the continuous leakage-resilient problem can be obtained from the corresponding bounded leakage resilience by performing an additional key update algorithm. Analysis and comparison show that our construction of leakage-resilient IB-KEM with CCA security has certain advantages in computing, transmission and storage. 
id559#An Efficient and High-Speed Overlap-Free Karatsuba-Based Finite-Field Multiplier for FGPA Implementation#Cryptography systems have become inseparable parts of almost every communication device. Among cryptography algorithms, public-key cryptography, and in particular elliptic curve cryptography (ECC), has become the most dominant protocol at this time. In ECC systems, polynomial multiplication is considered to be the most slow and area consuming operation. This article proposes a novel hardware architecture for efficient field-programmable gate array (FPGA) implementation of Finite-field multipliers for ECC. Proposed hardware was implemented on different FPGA devices for various operand sizes, and performance parameters were determined. Comparing to state-of-The-Art works, the proposed method resulted in a lower combinational delay and area-delay product indicating the efficiency of design. 
id560#Secure storage and sharing of visitor images generated by smart entrance on public cloud#Visitor validation at entrance generates a large number of image files that need to be transmitted over to cloud for future reference. The image data needs to be protected by active and passive adversaries from performing cryptographic attacks on these data. The image data also needs to be authenticated before giving it for future use. Focusing on reliable and secure image sharing, the proposed method involves building a novel cloud platform, which aims to provide a secure storage in the public cloud. The main objective of this paper is to provide a new way of secure image data storage and transmission on cloud using cryptographic algorithms. To overcome the flaws in current system, a novel method using BigchainDB, which has advantages of blockchain technology and traditional database, is proposed for storing attributes of image. 
id561#Research development of small object traching based on deep learning [基于深度学习的小目标检测研究进展]#Object detection, one of the most fundamental and challenging tasks in computer vision, has achieved remarkable breakthroughs with the rapid development of deep learning methods. However, most of the current state-of-the-art algorithms are designed for medium or large objects with regular sizes or proportions. The performance of detecting small objects is still far from satisfactory due to the small size and weak features of target objects. In recent years, Small Object Traching (SOT), which is widely applied in outdoor remote shooting and aerospace remote sensing scenarios, has drawn significant attention and substantial approaches have emerged. However, few comprehensive reviews on this issue have been conducted. This paper summarizes the research progress of deep learning-based SOT methods in terms of problem definition, algorithm analysis, application introduction, and future prospects. We first present the definition of the SOT problem and illustrate its technical difficulties and challenges faced in practical applications. The main reasons for the low accuracy of small object detectors and the corresponding improvement methods from eight different perspectives are then analyzed, and the SOT research work in various technical aspects summarized in detail. The representative applications of SOT algorithms in several specific scenarios are introduced. Finally, we prospect the development trends and promising directions for future research. Hopefully, this survey can provide reference for the research work in this field. 
id562#A polynomial-time algorithm for memory space reduction#Reducing memory space requirement is important to many applications. For data-intensive applications, it may help avoid executing the program out-of-core. For high-performance computing, memory space reduction may improve the cache hit rate as well as performance. For embedded systems, it can reduce the memory requirement, the memory latency and the energy consumption. This paper investigates program transformations which a compiler can use to reduce the memory space required for storing program data. In particular, the paper uses integer programming to model the problem of combining loop shifting, loop fusion and array contraction to minimize the data memory required to execute a collection of multi-level loop nests. The integer programming problem is then reduced to an equivalent network flow problem which can be solved in polynomial time.
id563#A Climbing Robot for Steel Bridge Inspection#As an effort of automating the bridge inspection process, this paper presents a new development of an adaptable tank-like robot, which can climb on steel structures to collect data and perform inspection. While most current steel climbing mobile robots are designed to work on flat steel surface, our proposed tank-like robot design is capable of climbing on different steel structural shapes (e.g., cylinder, cube) by using reciprocating mechanism and magnetic roller-chains. The developed robot can pass through the joints and transition from one surface to the other (e.g., from flat to curving surfaces). A prototype robot integrating multiple sensors (hall-effects, IR, IMU, Eddy current and cameras), has been developed by coping with variety of strict concerns including tight dimension, effective adhesive and climbing adaptation. Rigorous analysis of robot kinematics, adhesive force, sliding and turn-over failure and motor power has been conducted to certify the stability of the proposed design. The theory calculations can serve as an useful framework for designing future steel climbing robots. The cameras and Eddy current sensor is integrated on Robot for visual and in-depth fatigue crack inspection of steel structures. Experimental results and field deployments on more than twenty steel bridges confirm the adhesive, climbing, inspection capability of the robot. Video of this deployment can be seen in this link: https://youtu.be/1Wl9Trd3EoM. 
id564#Can't see the wood for the trees? An assessment of street view- and satellite-derived greenness measures in relation to mental health#Greenness in the urban living environment is inconsistently associated with mental health. Satellite-derived measures of greenness may inadequately characterize how people encounter greenness visually on site, but systematic comparisons are lacking. We aimed 1) to compare associations between remotely sensed and street view (SV) greenness, and 2) to examine whether these greenness metrics are differently associated with mental health outcomes. We used cross-sectional depressive and anxiety symptoms data on adults in Amsterdam, the Netherlands. We employed a convolutional neural network to segment greenness in SV panoramas. Greenness was measured top-down by normalized difference vegetation indices (NDVI) from 1 m resolution orthophotos (OP) and 30 m resolution Landsat-8 (LS) imagery per postal code, and 100 and 300 m concentric and street-network buffers at the home address. Correlation analyses assessed associations across greenness measures. Covariate-adjusted regressions (e.g., noise, air pollution, deprivation) were conducted to assess associations between each greenness metric and mental health outcomes. Correlations between greenness metrics were significantly positive and moderately high. SV greenness was less sensitive across scales and residential contexts than OP and LS greenness. There was no statistically significant evidence that people with less urban residential greenness had higher depression or anxiety scores than those exposed to higher levels. Nor did different greenness measures, scales, or residential context definitions alter our null associations. This suggests that even though SV and remotely sensed measures capture different aspects of greenness, these differences across exposure metrics did not translate into an association with mental health outcomes. 
id565#Œuf: Minimizing the Coq extraction TCB#Verifying systems by implementing them in the programming language of a proof assistant (e.g., Gallina for Coq) lets us directly leverage the full power of the proof assistant for verifying the system. But, to execute such an implementation requires extraction, a large complicated process that is in the trusted computing base (TCB). This paper presents Œuf, a verified compiler from a subset of Gallina to assembly. Œuf's correctness theorem ensures that compilation preserves the semantics of the source Gallina program. We describe how Œuf's specification can be used as a foreign function interface to reason about the interaction between compiled Gallina programs and surrounding shim code. Additionally, Œuf maintains a small TCB for its front-end by reflecting Gallina programs to Œuf source and automatically ensuring equivalence using computational denotation. This design enabled us to implement some early compiler passes (e.g., lambda lifting) in the untrusted reflection and ensure their correctness via translation validation. To evaluate Œuf, we compile Appel's SHA256 specification from Gallina to x86 and write a shim for the generated code, yielding a verified sha256sum implementation with a small TCB. 
id566#Ontology learning from relational databases#An ontology is a formal, explicit specification of a shared conceptualization. Ontologies are used in many fields, such as software engineering, information extraction, semantic search, knowledge management, recommender systems, etcetera. Since manual ontology building is a very costly, time-consuming, and error-prone task, automating the process of ontology building, or in other words, learning ontology from existing resources, is a good option. Nowadays, a large amount of data on the web is stored in relational databases, but databases cannot be used directly in the semantic web. Hence, in this paper, we have proposed a new approach to automatically creating an OWL ontology from a relational database. We have defined a set of rules to analyze all database components and convert them to corresponding ontology components. The core contribution of our work is the set of rules which can analyze and extract ontology elements from stored procedures, user-defined functions, views, multiple inheritance, the specific representation of single inheritance, common attributes, and the constraints on tables and their columns. The proposed approach has been compared with existing approaches using three frameworks. 
id567#The MSK: a simple and robust image encryption method#This article proposes a new image encryption approach based on bitplane decomposition methods and chaotic maps. This approach does not depend on any additional images to initiate the encryption process. The encryption method involves a chaotic logistic map to create the initial security bitplanes. The proposed approach is flexible enough to choose any one of the available bitplane decomposition methods. Moreover, some different scrambling algorithms can be used that can efficiently scramble the bitplanes, instead of using the proposed scrambling algorithm. The proposed method can be implemented very easily and does not involve highly complex operations that makes the algorithm suitable for real time applications. The proposed method is simulated, tested, compared with some standard image encryption approaches and analyzed with the help of some standard cipher image evaluation parameters. Both visual and quantitative analysis of the obtained results are presented in detail. The results of the experiments are very promising and shows effective encryption performance on various types of images, that makes the proposed algorithm suitable for the real-life applications. The experimental results also demonstrate the strength of the proposed algorithm against different types of the cryptographic attacks. 
id568#Large-Scale Visual Data-Driven Probabilistic Risk Assessment of Utility Poles Regarding the Vulnerability of Power Distribution Infrastructure Systems#Inspecting and assessing existing utility poles has become increasingly important for reducing the vulnerability of power distribution infrastructure systems in disaster situations, which can enhance community resilience. Although vision-based systems have been applied to detect faults in power distribution infrastructures, little research currently exists on assessing component- and network-level failures of utility poles based on their geometric and environmental information. This paper aims to propose a new data-driven approach to support risk-informed decision-making for utility maintenance under extreme wind conditions. Large-scale open-source imagery from Google Street View is used to assess geometric properties of utility poles (i.e., leaning angle). Then the failure probability of utility poles is analyzed under varying conditions (e.g., age, leaning angle, and wind loads) in a three-dimensional virtual city model. The proposed method is tested through case studies in Texas to (1) validate an algorithm for estimating leaning angles of utility poles and (2) understand the progress of failures of leaning utility poles from a network perspective. The outcomes of the case studies demonstrate that the proposed method has the potential to leverage large-scale open-source visual data to assess the vulnerability of utility pole networks that may lead to cascading failures in power distribution infrastructure systems. Based on the proposed virtual environment, the method is expected to enable practitioners to facilitate risk-informed decision-making against disaster situations, which creates an opportunity for prioritizing maintenance tasks regarding power distribution infrastructures. 
id571#Compiler generator based on restarting automata#Restarting automata are a very strong theoretical model which can recognize much more than context-free languages in its most general variant. However, for its wider usage in real-world applications it is necessary to fill two gaps: to add semantics - instead of accepting we would like to get a meaning of the input - and to design a tool which for a given restarting automaton in human-readable format generates a program computing the meaning of an input text. The resulting tool is actually a compiler compiler. 
id572#Automatic equivalence checking of programs with uninterpreted functions and integer arithmetic#Proving equivalence of programs has several important applications, including algorithm recognition, regression checking, compiler optimization verification and validation, and information flow checking. Despite being a topic with so many important applications, program equivalence checking has seen little advances over the past decades due to its inherent (high) complexity. In this paper, we propose, to the best of our knowledge, the first semi-algorithm for the automatic verification of partial equivalence of two programs over the combined theory of uninterpreted function symbols and integer arithmetic (UF+IA). The proposed algorithm supports, in particular, programs with nested loops. The crux of the technique is a transformation of uninterpreted functions (UFs) applications into integer polynomials, which enables the precise summarization of loops with UF applications using recurrences. The equivalence checking algorithm then proceeds on loop-free, integer only programs. We implemented the proposed technique in CORK, a tool that automatically verifies the correctness of compiler optimizations, and we show that it can prove more optimizations correct than state-of-the-art techniques. 
id573#High-speed tracking based on multi-CF filters and attention mechanism#Recently, correlation filters and deep convolutional network show good performance for visual tracking. Many real-time and high accuracy tracking algorithms are realized; however, there are still some challenges to build a robust tracker. In this paper, we present a novel tracking framework named multi-attention filter (MAF) to solve some challenges for tracking like object drift in a long time, lack of training samples and fast motion. Our framework consists of two components, a basic CNN network to extract feature maps and a set of classifiers to distinguish between the target and the background. First, to solve the problem of object drift in a long time, a simple but effective evaluation mechanism is proposed to the framework, the evaluation mechanism checks the tracking results and corrects it when needed. In addition, the results from different classifiers are fused to predict the object location according to intersection over union. Second, to overcome the lack of training samples, MAF stores some positive and negative samples in two queues, one named long-term queue and another named short-term queue. Third, to deal with fast motion of the target, attention mechanisms including channel attention and location attention are added to the tracker. In our experiments on the popular benchmarks including OTB-2013 and OTB-2015. MFA achieves state of the art among trackers, and as a correlation filter framework, MAF is very flexible and has great rooms for improvement and generalization. 
id574#HP-VCS: A high-quality and printer-friendly visual cryptography scheme#Visual Cryptography Scheme (VCS) is a secret-sharing scheme which aims to encrypt a secret message into multiple shares and transmit them to participants over an untrusted communication channel. Although human vision can easily reveal the secret message by stacking a sufficient number of shares, this scheme reduces the visual quality of recovered images. This paper presents a novel high-quality and printer-friendly VCS. When providing high-quality recovery, this scheme keeps the size of the shares the same as the secret image. Experimental results show that, compared with previous work, the proposed scheme significantly improves the performance of recovered images. 
id575#Energy-aware compilation for network processors: Frameworks, techniques and trend#To feed the increasing need for higher network processing line-rate and easier network application development, compilation for Network Processors (NP) has been a hotspot in research. Bit-stream-oriented programming, multiple processing units and heterogeneous architectures all make the job of an NP compiler complicated. Meanwhile, energy efficiency has also become a heated issue while parallel NP system is becoming more powerful and power-hungry. This paper comprehensively reviews NP compilation techniques and energy-aware optimizations. It focuses on the classification in respect of NP compiler's parallel packet processing capability, and the energy-aware optimizations in all compilation stages. In the end, it concludes with the predicted research trends in the NP compilation area.
id576#A software methodology for compiling quantum programs#Quantum computers promise to transform our notions of computation by offering a completely new paradigm. To achieve scalable quantum computation, optimizing compilers and a corresponding software design flow will be essential. We present a software architecture for compiling quantum programs from a high-level language program to hardware-specific instructions. We describe the necessary layers of abstraction and their differences and similarities to classical layers of a computer-aided design flow. For each layer of the stack, we discuss the underlying methods for compilation and optimization. Our software methodology facilitates more rapid innovation among quantum algorithm designers, quantum hardware engineers, and experimentalists. It enables scalable compilation of complex quantum algorithms and can be targeted to any specific quantum hardware implementation. 
id577#A review on the attention mechanism of deep learning#Attention has arguably become one of the most important concepts in the deep learning field. It is inspired by the biological systems of humans that tend to focus on the distinctive parts when processing large amounts of information. With the development of deep neural networks, attention mechanism has been widely used in diverse application domains. This paper aims to give an overview of the state-of-the-art attention models proposed in recent years. Toward a better general understanding of attention mechanisms, we define a unified model that is suitable for most attention structures. Each step of the attention mechanism implemented in the model is described in detail. Furthermore, we classify existing attention models according to four criteria: the softness of attention, forms of input feature, input representation, and output representation. Besides, we summarize network architectures used in conjunction with the attention mechanism and describe some typical applications of attention mechanism. Finally, we discuss the interpretability that attention brings to deep learning and present its potential future trends. 
id578#Cryptanalysis and improvement of a two-factor user authentication scheme for smart home#Smart home is a latest technology in the field of Internet of Things (IoT) through which one can manage the smart devices at home using internet. This communication takes place via insecure channel which makes it vulnerable to many attacks. Hence, a secure authentication protocol is required to transfer sensitive data within smart home. Recently Shuai et al. have designed two factor user authentication protocol for smart home environment. In this paper, it has been discussed that this protocol is vulnerable to offline password guessing attack, insider attack, replay attack, gateway bypass attack and insecure session key agreement problem. To overcome these attacks, a user authentication protocol has been proposed. Its formal security analysis and verification is given using random oracle model and ProVerif tool, respectively. In performance analysis, the proposed scheme is compared with other related scheme to show that it is more reliable and secure than them. 
id579#Image-based size estimation of broccoli heads under varying degrees of occlusion#The growth and the harvestability of a broccoli crop is monitored by the size of the broccoli head. This size estimation is currently done by humans, and this is inconsistent and expensive. The goal of our work was to develop a software algorithm that can estimate the size of field-grown broccoli heads based on RGB-Depth (RGB-D) images. For the algorithm to be successful, the problem of occlusion must be solved, which is the partial visibility of the broccoli head due to overlapping leaves. This partial visibility causes sizing errors. In this research, we studied the use of deep-learning algorithms to deal with occlusions. We specifically applied the Occlusion Region-based Convolutional Neural Network (ORCNN) that segmented both the visible and the amodal region of the broccoli head (which is the visible and the occluded region combined). We hypothesised that ORCNN, with its amodal segmentation, can improve the size estimation of occluded broccoli heads. The ORCNN sizing method was compared with a Mask R–CNN sizing method that only used the visible broccoli region to estimate the size. The sizing performance of both methods was evaluated on a test set of 487 broccoli images with systematic levels of leaf occlusion. With a mean sizing error of 6.4 mm, ORCNN outperformed Mask R–CNN, which had a mean sizing error of 10.7 mm. Furthermore, ORCNN had a significantly lower absolute sizing error on 161 heavily occluded broccoli heads with an occlusion rate between 50% and 90%. Our software and data set are available on https://git.wur.nl/blok012/sizecnn. 
id580#Implicit force and position control to improve drilling quality in CFRP flexible robotic machining#Flexible machining systems are being widely applied in the aircraft and automotive industries to reduce costs and increase productivity. The workpieces for the corresponding applications are bigger and more varied than those for conventional applications. These workpieces can be made of carbon fiber reinforced plastic (CFRP) because it provides a high strength-to-weight ratio. Nevertheless, CFRP has poor machinability owing to its brittleness. To achieve flexible machining, industrial robots are being increasingly adopted, especially for drilling. However, they cannot meet industrial requirements such as machining precision, cost, and productivity. This is because industrial robots have a considerably lower stiffness than conventional computer numerical control machines given their structure consisting of serial links. Consequently, tasks such as drilling using industrial robots may present production defects. Drilling defects generally appear as burrs in the hole entrance and deviations of the desired hole center. In this study, the patterns and causes of such defects during robotic drilling were analyzed, and it was determined that the main defect was caused by the deviation of the tool tip due to the low robot stiffness. The defect was then corrected using implicit force and position control. Experimental results showed that when the defect was corrected, the maximum hole diameter error decreased by 16.68% when compared with a scenario in which no compensation for the defect was made. In addition, the maximum error decreased by 10.83% when compared with drilling that was guided by a pilot hole. 
id581#Robotic Computing on FPGAs#This book provides a thorough overview of the state-of-the-art field-programmable gate array (FPGA)-based robotic computing accelerator designs and summarizes their adopted optimized techniques. This book consists of ten chapters, delving into the details of how FPGAs have been utilized in robotic perception, localization, planning, and multi-robot collaboration tasks. In addition to individual robotic tasks, this book provides detailed descriptions of how FPGAs have been used in robotic products, including commercial autonomous vehicles and space exploration robots. 
id582#Interprocedural compiler optimization for partial run-time reconfiguration#"In this paper, we study the performance impact of dynamic hardware reconfigurations for current reconfigurable technology. As a testbed, we target the Xilinx Virtex II Pro, the Molen experimental platform and the MPEG2 encoder as the application. Our experiments show that slowdowns of up to a factor 1000 are observed when the configuration latency is not hidden by the compiler. In order to avoid the performance decrease, we propose an interprocedural optimization that minimizes the number of executed hardware configuration instructions taking into account constraints such as the ""FPGA-area placement conflicts"" between the available hardware configurations. The presented algorithm allows the anticipation of hardware configuration instructions up to the application's main procedure. The presented results show that our optimization produces a reduction of 3 to 5 order of magnitude of the number of executed hardware configuration instructions. Moreover, the optimization allows to exploit up to 97% of the maximal theoretical speedup achieved by the reconfigurable hardware execution. "
id584#A lightweight key management scheme for key-escrow-free ECC-based CP-ABE for IoT healthcare systems#The rapid increase in the implementation of Internet of Things (IoT) solutions in diverse healthcare sectors raises the IoT in the healthcare market. The real-time health monitoring to manage the chronic diseases is likely to drive the demand of IoT in healthcare. Moreover, the enhancement in communication technologies like real-time data transmission has improved the patients’ confidence in managing the chronic disease and medication dosage. Although IoT in healthcare makes a positive impact on patients and healthcare providers, however, it gets the challenges like data security and privacy. One means of sustaining security in IoT based healthcare system is through key management and at the same time an effective security mechanism for outsourced data to obtain fine-grained access control is the Ciphertext Policy-Attribute Based Encryption (CP-ABE). However, the overhead of complex decryption operations and key-escrow problem of CP-ABE hinders its applicability in IoT. Hence, in this work, we design a lightweight key management mechanism for the CP-ABE scheme using Elliptic Curve Cryptography (ECC). The novelty in this scheme is that irrespective of the secret key generation by the semi-trusted authority (honest, but curious to know the secret information), it is not capable to decrypt any message using these keys until and unless it has an additional private key of the receiver. For a constraint environment like IoT, the applicability of CP-ABE has two major issues: complex decryption operations and key-escrow problem. Hence, in this paper, a lightweight CP-ABE scheme for IoT based health care system using ECC has been designed. The proposed key management mechanism in the CP-ABE scheme is key-escrow free as well as significantly reduces the decryption overhead of the data receiver. The performance analysis shows that the proposed scheme is more effective as compared to the existing competing schemes. 
id585#Exploiting telerobotics for sensorimotor rehabilitation: a locomotor embodiment#Background: Manual treadmill training is used for rehabilitating locomotor impairments but can be physically demanding for trainers. This has been addressed by enlisting robots, but in doing so, the ability of trainers to use their experience and judgment to modulate locomotor assistance on the fly has been lost. This paper explores the feasibility of a telerobotics approach for locomotor training that allows patients to receive remote physical assistance from trainers. Methods: In the approach, a trainer holds a small robotic manipulandum that shadows the motion of a large robotic arm magnetically attached to a locomoting patient's leg. When the trainer deflects the manipulandum, the robotic arm applies a proportional force to the patient. An initial evaluation of the telerobotic system’s transparency (ability to follow the leg during unassisted locomotion) was performed with two unimpaired participants. Transparency was quantified by the magnitude of unwanted robot interaction forces. In a small six-session feasibility study, six individuals who had prior strokes telerobotically interacted with two trainers (separately), who assisted in altering a targeted gait feature: an increase in the affected leg’s swing length. Results: During unassisted walking, unwanted robot interaction forces averaged 3−4 N (swing–stance) for unimpaired individuals and 2−3 N for the patients who survived strokes. Transients averaging about 10 N were sometimes present at heel-strike/toe-off. For five of six patients, these forces increased with treadmill speed during stance (R2 =.99; p &lt; 0.001) and increased with patient height during swing (R2 =.71; p = 0.073). During assisted walking, the trainers applied 3.0 ± 2.8 N (mean ± standard deviation across patients) and 14.1 ± 3.4 N of force anteriorly and upwards, respectively. The patients exhibited a 20 ± 21% increase in unassisted swing length between Days 1−6 (p = 0.058). Conclusions: The results support the feasibility of locomotor assistance with a telerobotics approach. Simultaneous measurement of trainer manipulative actions, patient motor responses, and the forces associated with these interactions may prove useful for testing sensorimotor rehabilitation hypotheses. Further research with clinicians as operators and randomized controlled trials are needed before conclusions regarding efficacy can be made. 
id586#3D Perception with Slanted Stixels on GPU#This article presents a GPU-accelerated software design of the recently proposed model of Slanted Stixels, which represents the geometric and semantic information of a scene in a compact and accurate way. We reformulate the measurement depth model to reduce the computational complexity of the algorithm, relying on the confidence of the depth estimation and the identification of invalid values to handle outliers. The proposed massively parallel scheme and data layout for the irregular computation pattern that corresponds to a Dynamic Programming paradigm is described and carefully analyzed in performance terms. Performance is shown to scale gracefully on current generation embedded GPUs. We assess the proposed methods in terms of semantic and geometric accuracy as well as run-time performance on three publicly available benchmark datasets. Our approach achieves real-time performance with high accuracy for 2048 × 1024 image sizes and 4 × 4 Stixel resolution on the low-power embedded GPU of an NVIDIA Tegra Xavier. 
id587#Radar and video fusion to identify vehicle tracks#For a single sensor such as radar, video monitoring system cannot cope with such as illumination change, the movement of the branches swing, non-motor vehicles, goals, and other objects of shade, and the shortcomings of the sensor itself parameters change factors such as interference, lacking in vehicle trajectory identification accuracy, etc. In this paper, by studying the vehicle detection and tracking algorithm of the fusion of millimeter-wave radar sensor and camera vision sensor, a vehicle detection algorithm based on the fusion of radar and video is proposed. The radar sensor is the main one, and the video sensor is the auxiliary one. The advantages of the two are combined to track and identify the vehicle. Finally, the experimental results show that the algorithm can effectively deal with the non-motor vehicle interference, frequent blocking, illumination changes and other situations, and improve the accuracy of vehicle detection results. 
id588#File Security Using Hybrid Cryptography and Face Recognition#Currently, data security has been a major issue as data is usually transferred over the Internet between users. Some security flaw might leak sensitive data. For resolving this, we can use cryptography to secure data so that other unauthorized individuals can’t access the data stored in the file. Use of one cryptography algorithm might not provide higher level security for sensitive data. In this paper, we have proposed a new security system which uses three separate cryptography algorithms to provide better security for the data and facial recognition to verify the user who is currently using the system to provide better security. The file to be secured is split into three parts and is encrypted using AES, Blowfish and Twofish, respectively. File and user information are added to the database. The user of the system is verified using facial recognition during the authorization process in decryption phase. 
id589#Design patterns for teaching type checking in a compiler construction course#A course in compiler construction seeks to develop an understanding of well-defined fundamental theory and typically involves the production of a language processor. In a graduate degree in software engineering, the development of a compiler contributes significantly to the developer's comprehension of the practical application of theoretical concepts. Different formal notations are commonly used to define type systems, and some of them are used to teach the semantic analysis phase of language processing. In the traditional approach, attribute grammars are probably the most widely used ones. This paper shows how object-oriented design patterns represented in unified modeling language (UML) can be used to both teach type systems and develop the semantic analysis phase of a compiler. The main benefit of this approach is two-fold: better comprehension of theoretical concepts because of the use of notations known by the students (UML diagrams), and improvement of software engineering skills for the development of a complete language processor. 
id590#A City Monitoring System Based on Real-Time Communication Interaction Module and Intelligent Visual Information Collection System#With the rapid development of society, the improvement of material level and the current situation of the large-scale population flow in China, the awareness of security is becoming more and more important in people’s life. With the rapid development of image processing and computer vision technology, people try to analyze, process and understand the collected video image automatically without human intervention. The intelligent video monitoring system collects video signals of interested objects in a dynamic scene through a camera, and processes and analyzes image information by a computer. Only by establishing a reasonable and effective urban video monitoring management system can government departments find out problems in the first time. The traditional highway monitoring and commanding traffic scheduling system based on GIS, which can obtain road traffic information and conduct traffic scheduling by remote sensing, has the disadvantage of poor effect on traffic scheduling. In this paper, real-time communication technology and computer vision acquisition technology are used to build a city monitoring system. The experimental results show that this method has strong timeliness and good monitoring effect. Compared with the state-of-the-art methodologies, the proposed framework is efficient and accurate. 
id591#The impact of different tasks on evolved robot morphologies#A well-established fact in biology is that the environmental conditions have a paramount impact on the evolved life forms. In this paper we investigate this in an evolutionary robot system where morphologies and controllers evolve together. We evolve robots for two tasks independently and simultaneously and compare the outcomes. The results show that the robots evolved for multiple tasks simultaneously developed new morphologies that were not present in the robots evolved for single tasks independently. 
id592#CompCertO: Compiling certified open C components#Since the introduction of CompCert, researchers have been refining its language semantics and correctness theorem, and used them as components in software verification efforts. Meanwhile, artifacts ranging from CPU designs to network protocols have been successfully verified, and there is interest in making them interoperable to tackle end-to-end verification at an even larger scale. Recent work shows that a synthesis of game semantics, refinement-based methods, and abstraction layers has the potential to serve as a common theory of certified components. Integrating certified compilers to such a theory is a critical goal. However, none of the existing variants of CompCert meets the requirements we have identified for this task. CompCertO extends the correctness theorem of CompCert to characterize compiled program components directly in terms of their interaction with each other. Through a careful and compositional treatment of calling conventions, this is achieved with minimal effort. 
id593#Lifelong robotic visual-tactile perception learning#Lifelong machine learning can learn a sequence of consecutive robotic perception tasks via transferring previous experiences. However, 1) most existing lifelong learning based perception methods only take advantage of visual information for robotic tasks, while neglecting another important tactile sensing modality to capture discriminative material properties; 2) Meanwhile, they cannot explore the intrinsic relationships across different modalities and the common characterization among different tasks of each modality, due to the distinct divergence between heterogeneous feature distributions. To address above challenges, we propose a new Lifelong Visual-Tactile Learning (LVTL) model for continuous robotic visual-tactile perception tasks, which fully explores the latent correlations in both intra-modality and cross-modality aspects. Specifically, a modality-specific knowledge library is developed for each modality to explore common intra-modality representations across different tasks, while narrowing intra-modality mapping divergence between semantic and feature spaces via an auto-encoder mechanism. Moreover, a sparse constraint based modality-invariant space is constructed to capture underlying cross-modality correlations and identify the contributions of each modality for new coming visual-tactile tasks. We further propose a modality consistency regularizer to efficiently align the heterogeneous visual and tactile samples, which ensures the semantic consistency between different modality-specific knowledge libraries. After deriving an efficient model optimization strategy, we conduct extensive experiments on several representative datasets to demonstrate the superiority of our LVTL model. Evaluation experiments show that our proposed model significantly outperforms existing state-of-the-art methods with about 1.16%∼15.36% improvement under different lifelong visual-tactile perception scenarios. 
id594#Bidirectional interaction between visual and motor generative models using Predictive Coding and Active Inference#In this work, we build upon the Active Inference (AIF) and Predictive Coding (PC) frameworks to propose a neural architecture comprising a generative model for sensory prediction, and a distinct generative model for motor trajectories. We highlight how sequences of sensory predictions can act as rails guiding learning, control and online adaptation of motor trajectories. We furthermore inquire the effects of bidirectional interactions between the motor and the visual modules. The architecture is tested on the control of a simulated robotic arm learning to reproduce handwritten letters. 
id595#Cramming in core values#The general-purpose CPU is undergoing its first revolution by embracing multicores operating in parallel. Such cores resemble the GPUs (graphics processing units) that provide visual processing for PCs and game consoles, with a highly parallel structure making them more efficient for a range of algorithms manipulating vectors or rows of symbols than general purpose CPUs. The even greater challenge, though, lies within the whole software development cycle, from compilers up to high level languages and re-engineering of legacy applications, mulitcore computing.
id596#Towards Improvements on Multi-tenant RDBMS Migration in the Cloud Environment#In a multi-tenant environment, the companies referred to as tenants share a common application and Relational Database Management System (RDBMS) instances to store their data. However, with the rapid adoption of multi-tenant databases, the cloud provider faces two challenges: Tenants have irregular workload patterns. Also, tenants require a strict guarantee of their rental services’ quality and performance, known as a Service Level Agreement (SLA). In this research, a Multi-Tenant Database Management System (MT DBMS) is presented. A multi-tenant migration algorithm called MT-M is presented, which migrates the violated tenants on an elastic cluster of machines to mitigate the SLA Violations. Experiment results show that the proposed MT-M algorithm is ideal for the migration of the violated multi-tenant databases, reducing SLA violations’ total number compared to the previous migration algorithms. 
id598#On Evaluating Fault Resilient Encoding Schemes in Software#Cryptographic implementations are often vulnerable against physical attacks, fault injection analysis being among the most popular techniques. On par with development of attacks, the area of countermeasures is advancing rapidly, utilizing both hardware- and software-based approaches. When it comes to software encoding countermeasures for fault protection and their evaluation, there are very few proposals so far, mostly focusing on single operations rather than cipher as a whole. In this paper we propose an evaluation framework that can be used for analyzing the effectivity of software encoding countermeasures against fault attacks. We first formalize the encoding schemes in software, helping us to define what properties are required when designing a fault protection. Based on these findings, we develop an evaluation metric that can be used universally to determine the robustness of a software encoding scheme against bit flip faults and instruction skips. We provide a way to select a code according to user criteria and also a dynamic code analysis method to estimate the level of protection of assembly implementations using encoding schemes. Finally, we verify our findings by implementing a block cipher PRESENT, protected by encoding scheme based on anticodes, and provide a detailed evaluation of this implementation using different codes. 
id599#A Secure Remote Clinical Sensor Network Approach for Privacy Enhancement#Health being the biggest concern of the patients all around the globe, e-healthcare Remote Clinical Sensor Network assists in gathering the crucial body information of individual boundaries using sensors. There are chances forlots of issues like absence of vigor, diminished framework's adequacy, bogus caution, since it is remote in nature. The security and protection assurance of the information gathered has been a major concern factor to worry about. Hence, we presented fused secure and fuzzy combination framework focusing on these issues to accomplish productive secure transmission and information combination. The proposed technique is on thought of providing a secured key and encoded by AES i.e., Advanced Encryption Standard mechanism. It handles loose information for less utilization of energy and therefore increasing the lifetime of the network. And thus the experimental results ensure the energy utilization, security and the proficiency of the put forth strategy. 
id600#Improvement of Optical Flow Estimation by Using the Hampel Filter for Low-End Embedded Systems#Owing to the recent advances in the field of deep-learning-based approaches, state-of-the-art performance has been achieved for optical flow estimation. However, non-deep-learning-based improvement in the optical flow estimation performance is still required because many platforms, such as small UGVs and drones, involve constraints that make mounting GPUs difficult. Thus, in this study, we do not apply deep learning methods; rather, we improve the accuracy of the optical flow pipeline by optimizing only its fundamental parameters. The optical flow estimation performance is influenced by the number of coarse-to-fine estimation pyramids, the filter applied to each pyramid level, the window size, and the graduated nonconvexity (GNC) step number. No significant differences from the previous research were achieved by optimizing the parameters heuristically because they have already been optimized. Therefore, we decided to change the filter applied in each pyramid, which is the most important factor in determining the optical flow estimation performance. As a result of verification, the optical flow with the median filter did not show good optical flow estimation performance due to oversmoothing of the image boundary, and the optical flow with the weighted median filter proposed to overcome this drawback could not well address the complexity of the equation and deal with the large computation cost. The proposed Hampel filter shows better performance by minimizing the loss of the original image in the image smoothing process and reduced computational complexity compared to the weighted median filter. The principle of the Hampel filter is similar to that of the median filter, except that if the reference pixel is statistically close to the pixel median in the window, it works by preserving the original pixel value. This is usually used for filtering outliers in a 1D signal but has never been applied for 2D image filters. This study expanded the application of the existing Hampel filter to include 2D images. Compared to the conventional median filter, this filter demonstrates relatively good performance, as it can considerably reduce the loss in the image data and improve the excessive edge smoothing effect such that it can better identify motion. The performance-related improvements achieved by this research were verified using the KITTI Vision Benchmark Suite, and the simulation results demonstrate that the Hampel filter can replace the existing filter and that it has strengths related to multi-object detection, background recognition, and the identification of moving objects at the edges of images. 
id601#A data dependence test based on the projection of paths over shape graphs#We propose a data dependence detection test based on a new conflict analysis algorithm for C codes which make intensive use of recursive data structures dynamically allocated in the heap. This algorithm requires two pieces of information from the code section under analysis (a loop or a recursive function): (i) abstract shape graphs that represent the state of the heap at the code section; and (ii) path expressions that collect the traversing information for each statement. Our algorithm projects the path expressions on the shape graphs and checks over the graphs to ascertain whether one of the sites reached by a write statement matches one of the sites reached by another statement on a different loop iteration (or on a different call instance in a recursive function), in which case a conflict between the two statements is reported. Although our algorithm presents exponential complexity, we have found that in practice the parameters that dominate the computational cost have very low values, and to the best of our knowledge, all the other related studies involve higher costs. In fact, our experimental results show reductions in the data dependence analysis times of one or two orders of magnitude in some of the studied benchmarks when compared to a previous data dependence algorithm. Thanks to the information on uncovered data dependences, we have manually parallelized these codes, achieving speedups of 2.19 to 3.99 in four cores. 
id602#Classifier aided training for semantic segmentation#Semantic segmentation is a prominent problem in scene understanding expressed as a dense labeling task with deep learning models being one of the main methods to solve it. Traditional training algorithms for semantic segmentation models produce less than satisfactory results when not combined with post-processing techniques such as CRFs. In this paper, we propose a method to train segmentation models using an approach which utilizes classification information in the training process of the segmentation network. Our method employs the use of classification network that detects the presence of classes in the segmented output. These class scores are then used to train the segmentation model. This method is motivated by the fact that by conditioning the training of the segmentation model with these scores, higher order features can be captured. Our experiments show significantly improved performance of the segmentation model on the CamVid and CityScapes datasets with no additional post processing. 
id603#Vision-based tire deformation and vehicle-bridge contact force measurement#Accurate measurement of vehicle-bridge contact force is the promise of rapid vehicle-induced bridge impact testing. This paper proposed a cost-effective vertical tire force measurement method based on computer vision and the long and short-term memory (LSTM) neural network. The main contribution of this paper includes two aspects: (1) Circle Hough transform was adopted to monitor tire deformation and vehicle speed from videos of the tire sidewall. Besides, the perspective transformation was employed to eliminate vehicle-induced camera motion; (2) Using tire deformation, vehicle speed, and tire pressure as the input, vertical tire force is estimated through the LSTM neural network. Finally, a detachable vehicle-mounted computer vision system was developed. Experiments, including laboratory and field tests, were conducted. The estimated vertical tire forces show good agreement with the reference values, demonstrating the potential for applying the proposed method to rapid vehicle-induced bridge impact testing. 
id604#Vision-based control for trajectory tracking of four-bar linkage#It is well-known that mechanical sensors suffer failure in hostile environments. Therefore, in such applications, vision control is a suitable solution. In this paper, we propose a new approach for design and implementation of the control system for the speed of the coupler point in a four-bar linkage. To measure the coupler point position a computational vision system is implemented; the vision system sends the controller the precise position of the desired point for a wide range of crank rotational speeds. A proportional integral derivative control system is designed and implemented in a microprocessor. Stability analysis for the controlled system is performed via the Lyapunov stability theory. Performance of the system is validated experimentally in a prototype of a planar mechanism obtaining an average square error of measurement less than 0.03% and regulation of operating point less than 1% for different speed references. 
id605#Visual analysis of plant growth using transfer learning [Öǧrenme transferi ile bitki gelişiminin görsel analizi]#Attention for monitoring growth of plants has been escalating in recent years, due to concerns on climate change and its impacts on the ecosystem. Understanding the growth characteristics of plants is vital for management of resources and optimization of crop yield in agricultural sector as well. Each plant population exhibits varying seasonal growth and reproduction manner with respect to the local environmental parameters. Using computational power of advancing technology is becoming unavoidable for the optimal usage of resources and human labor. In this paper, growth stages of plants are analyzed using recently advancing technologies of transfer learning. Deep learning is utilized for visual analysis of a variety of plants at different growing stages. The performance of a particular deep learning architecture is investigated for phenology recognition of agricultural plants at roughly three phenological stages, namely early, middle and late phenological stages. 
id607#R2R-CSES: proactive security data process using random round crypto security encryption standard in cloud environment#Cloud service provide the information security to protect the data using broad set of privacy policies and controls. Various security mechanisms concentrates the reality of key leakage issues to deal the security policy. The problem arise due to the leakage of data from attackers which leads the time factor of creating vulnerabilities and information loss. So information security in the cloud needs higher security crypto policy standards to make effective privacy protection. The cloud security service provider’s intent the proactive security standard to make the higher security in cryptographic encryption techniques. To propose a standard two-phase implementation of Round-key and random key based crypto security encryption standard (R2R-CSES) for improving security system in a cloud environment. R2R provides the high-level implementation of security protection using key verification policy. This service makes the random encryption using proactive security encryption with round random crypto policy holds key applicable standards with cloud authenticator policy. The projection of our results estimates the advanced encryption standard with key substitution make effective security to prevent vulnerabilities. The optimization produces least time complexity with service access to provide the security mechanism in high standard. 
id608#The Pi-ADL.NET project: An inclusive approach to ADL compiler design#This paper describes results and observations pertaining to the development of a compiler utility for an Architecture Description Language π-ADL, for the NET platform. Architecture Description Languages or ADLs, are special purpose high level languages especially construed to define software architectures. π-ADL, a recent addition to this class of languages, is formally based on the π-Calculus, a process oriented formal method. The compiler for π-ADL, named π-ADL.NET, is designed with the view of bringing the architecture driven software design approach to the NET platform. The process oriented nature and a robust set of parallelism constructs of π-ADL make the π-ADL.NET project a novel application of compiler techniques in the context of the NET platform, with many valuable lessons learnt. This paper presents the π-ADL.NET effort from a compiler design perspective, and describes the inclusive approach driving the design that facilitates the representation of strong behavioral semantics in architecture descriptions. The subjects of parallel process modeling, communication and constructed data types are covered. The paper also documents the motivation, vision and future possibilities for this line of work. A detailed comparison with related work is also presented.
id609#A Motion Estimation Filter for Inertial Measurement Unit with On-Board Ferromagnetic Materials#Magnetic distortions due to existing appliances and on-board objects with ferromagnetic materials cause serious bias and deviations in motion estimation by inertial measurement sensors with magnetometers. This problem requires a proper sensor fusion to do motion tracking with a minimal angular error. This letter presents a design of a complementary filter that compensates the strong magnetic effects of on-board ferromagnetic materials. Not only the attached permanent magnets may have serious biases on the magnetometer axes but also there is a magnetic distortion due to soft ferromagnetic materials, i.e., steel. After defining the signals of the inertial/magnetic sensors, the process and measurement models are described and a Kalman filter is constructed. The designed filter can be used for motion tracking in environments with magnetic distortions, and in robot actuators with magnetic parts. The performance of the proposed filter is verified under experiment and compared with conventional filters. Finally, we raise a question about whether the attachment of permanent magnets to inertial measurement sensors can serve as a magnetic shield improving the motion estimation. &copy; 2021 IEEE. 
id610#A fast and fully distributed method for region-based image segmentation: Fast distributed region-based image segmentation#Distributed and parallel computing techniques allow fast image processing, namely when these techniques are applied at the low and the medium level of a vision system. In this paper, a collective and distributed method for image segmentation is introduced and evaluated. The method is modeled as a multi-agent system, where the agents aim to collectively produce a region-based segmentation. Each agent starts searching for an acceptable region seed by randomly jumping within the image. Next, it performs a region growing around its position. Thus, several agents find themselves within the same homogeneous region and are organized in a graph where two agents are connected if they are within the same region. So, a unifying of the labels in a same region is collaboratively performed by the agents themselves. The proposed method was experimented on real range images from the ABW dataset and the Object Segmentation Database (OSD) one, and the obtained results were compared to those of some well-referenced methods from the literature. The evaluation results show that the proposed method provides fast and accurate image segmentation, allowing it to be deployed for real-time vision systems. 
id611#Bridge-surface panoramic-image generation for automated bridge-inspection using deepmatching#Visual inspection is important for the efficient maintenance of bridge structures and has recently been supplemented with the use of image-processing techniques that can localize and quantify damages using images captured from bridges. A series of overlapping bridge images can be combined for constructing a panoramic bridge-surface image in which the locations and sizes of the damages can be noted. Despite the excellent performance of image-processing techniques, generating panoramic images from a series of bridge-surface images is challenging as bridge-surface images may not possess distinct patterns or patterns that can act as reference feature points for stitching adjacent images. To address this issue, this paper presents a general method for stitching bridge-surface images using Deepmatching, which determines a pixel-wise correspondence between an image pair in comparison with conventional feature-wise matching methods. To employ Deepmatching for panoramic-image generation, (1) image matching pair search using 2D Delaunay triangulation, (2) parametric model for optimal image stitching were developed, and (3) field validation was conducted in this study. First, possible image matching pairs are organized using the two-dimensional Delaunay triangulation, and then Deepmatching is used to determine the matching points between possible image pairs. The developed parametric model refines the valid image matching pair, which is used for obtaining optimal global homographies for panoramic-image generation. For the validation of the proposed method, a lab-scale experiment on a flat concrete wall and a field experiment on a concrete bridge were conducted. The experimental validation demonstrates that the proposed method successfully identifies dense matching points between image pairs and generates a panoramic image while minimizing the occurrence of ghosting and drift. 
id612#Reconfigurable magnetic soft robots with multimodal locomotion#Magnetic soft robots have recently attracted increasing attention owing to their remarkable advantages and potential applications, but the reconfigurable magnetization inside soft-composite materials remains challenging, otherwise they can only achieve fixed magnetic response under external magnetic fields with limited morphological features and motion modes. To tackle this issue, a direct magnetization method based on pulsed high magnetic field focusing is developed, where the local magnetization distribution can be programmed flexibly and reconfigurably after fabrication within milliseconds and millimeter-scale resolution. Based on this method, reconfigurable magnetic soft robots are developed with superior merits, including a fishing soft robot to resist the dynamic drag, an inchworm-inspired crawling robot with a speed more than 1 body/s, and a six-arm rolling soft robot to transport objects in complex environments. The underlying locomotion mechanisms of these soft robots are also well discussed. The developed magnetic soft robots, together with the proposed magnetization method, are expected to pave avenues for the future development of mass production and controllable magnetization of soft robots for practical applications. 
id613#A framework for monitoring multiple databases in industries using OPC UA#Database management and monitoring is an inseparable part of any industry. A uniform scheme of monitoring relational databases without explicit user access to database servers is not much explored outside the database environment. In this paper, we present an information distribution scheme related to databases using Open Platform Communication Unified Architecture (OPC UA) servers to clients when multiple databases are involved in a factory. The aim is for external, but relevant clients, to be able to monitor this information mesh independent of explicit access to user schemas. A methodology to dispense data from, as well as check changes in databases using SQL queries and events is outlined and implemented using OPC UA servers. The structure can be used as a remote viewing application for multiple databases in one address space of an OPC UA server. 
id615#On the algebraic structure of Ep(m) and applications to cryptography#In this paper we show that the Z/ pmZ-module structure of the ring Ep(m) is isomorphic to a Z/ pmZ-submodule of the matrix ring over Z/ pmZ. Using this intrinsic structure of Ep(m), solving a linear system over Ep(m) becomes computationally equivalent to solving a linear system over Z/ pmZ. As an application we break the protocol based on the Diffie–Hellman decomposition problem and ElGamal decomposition problem over Ep(m). Our algorithm terminates in a provable running time of O(m6) Z/ pmZ-operations. 
id616#Efficient mining of directly follow relation from event data in relational database [关系数据库中事件日志的紧邻关系高效挖掘方法]#As the most popular media for large-scale data, relational databases record amount of event logs during the operation of information system. However, traditional process mining techniques mainly deal with event logs in XES format, and every mining task needs to manually export the latest log file from the database. The whole process is very tedious, and it cannot make full use of the powerful data processing capacity of relational database. For these problems, the process mining strategy and algorithm for relational log were studied. To improve the process mining efficiency for relational logs, an approximate linear mining algorithm with high universal was proposed based on the fast sorting capability of relational database for large-scale event logs stored in relational databases, which had less invasive to the business database. The algorithm had been implemented on ProM platform, and the experiment based on large-scale event log showed the high efficiency of the proposed method. 
id617#Automated Vision-Based Microsurgical Skill Analysis in Neurosurgery Using Deep Learning: Development and Preclinical Validation#Background/Objective: Technical skill acquisition is an essential component of neurosurgical training. Educational theory suggests that optimal learning and improvement in performance depends on the provision of objective feedback. Therefore, the aim of this study was to develop a vision-based framework based on a novel representation of surgical tool motion and interactions capable of automated and objective assessment of microsurgical skill. Methods: Videos were obtained from 1 expert, 6 intermediate, and 12 novice surgeons performing arachnoid dissection in a validated clinical model using a standard operating microscope. A mask region convolutional neural network framework was used to segment the tools present within the operative field in a recorded video frame. Tool motion analysis was achieved using novel triangulation metrics. Performance of the framework in classifying skill levels was evaluated using the area under the curve and accuracy. Objective measures of classifying the surgeons' skill level were also compared using the Mann–Whitney U test, and a value of P &lt; 0.05 was considered statistically significant. Results: The area under the curve was 0.977 and the accuracy was 84.21%. A number of differences were found, which included experts having a lower median dissector velocity (P = 0.0004; 190.38 ms–1 vs. 116.38 ms–1), and a smaller inter-tool tip distance (median 46.78 vs. 75.92; P = 0.0002) compared with novices. Conclusions: Automated and objective analysis of microsurgery is feasible using a mask region convolutional neural network, and a novel tool motion and interaction representation. This may support technical skills training and assessment in neurosurgery. 
id618#Application of cloud computing technology in metal exploration#With the development of economy, the progress of China's industry is increasingly obvious, so the demand for metals is also increasing. However, most of the metal resources on the surface of the earth are very scarce at present. Therefore, in order to meet the demand of economic development, entrepreneurs began to find better ways to mine the deep metal resources on the earth's surface. Starting from metal exploration, this paper focuses on the application of cloud computing technology in metal exploration. First, this paper introduces two key technologies of cloud computing, namely MapReduce programming model and HBase non-relational database. MapReduce programming model can be applied to parallel computing of a large number of data sets and scheduling processing when multiple tasks are running together, HBase non-relational database B-database is a database that can store data by columns, and can handle more complex unstructured data. Secondly, the experiment in this paper is divided into two steps. The first step is to build the cloud platform, the second step is to input the database. For the construction of the cloud platform, this paper uses four CentOS system computers to build the operation environment of Hadoop + HBase fully distributed mode. The final research results can show that the efficiency of metal exploration in 2016, 2017, 2018 and 2019 has increased by 3.4%,4.2%,5.1%,5.9%. 
id619#Design and implementation of a queue compiler#Queue processors are a viable alternative for high performance embedded computing and parallel processing. We present the design and implementation of a compiler for a queue-based processor. Instructions of a queue processor implicitly reference their operands making the programs free of false dependencies. Compiling for a queue machine differs from traditional compilation methods for register machines. The queue compiler is responsible for scheduling the program in level-order manner to expose natural parallelism and calculating instructions relative offset values to access their operands. This paper describes the phases and data structures used in the queue compiler to compile C programs into assembly code for the QueueCore, an embedded queue processor. Experimental results demonstrate that our compiler produces good code in terms of parallelism and code size when compared to code produced by a traditional compiler for a RISC processor. 
id620#Can computer vision be used for anthropometry? A feasibility study of a smart mobile application#Anthropometry is a method for measuring physical characteristics of the human body, particularly dealing with measures of size and shape of the body. These measurements can be performed using a tape measure, but new devices and software solutions already been employed in digital anthropometry. However, such tools do not enable the anthropometric evaluation to be performed automatically. In this paper, we present the NLMeasurer application for anthropometry, a mobile tool based on computer vision for identifying anatomical reference points (ARPs) and assessing the size of body segments. To evaluate the performance of the NLMeasurer, four participants were photographed and their images processed. The anthropometric measures calculated by the application, using different settings, were compared with those obtained using a tape measure. Results indicate no statistically significant difference (p > 0.05) between the methods, except in one configuration. This initial experiment was promising to reveal the feasibility of using NLMeasurer for anthropometry. 
id621#QKD-Enhanced Cybersecurity Protocols#Security of QKD is guaranteed by the quantum mechanics laws rather than unproven assumptions employed in computational cryptography. Unfortunately, the secret-key rates are way too low and transmission distances are limited. The post-quantum cryptography (PQC) is proposed as an alternative to QKD. However, the PQC protocols are based on conjecture that there are no polynomial time algorithms to break the PQC protocols. To overcome key challenges of both post-quantum cryptography and QKD, we propose to use the QKD only in initialization stage to set-up corresponding cybersecurity protocols. The proposed concept is applied to both computational security and PQC protocols. The proposed QKD-enhanced cybersecurity protocols are tolerant to attacks initiated by quantum computers. 
id623#Edurovs: A low cost and sustainable remotely operated vehicles educational program#EDUROV is an educational underwater robot proposal from the researchers of the Oceanic Platform of Canary Islands (PLOCAN) and the Computer Vision and Robotics research group of the University of Girona (VICOROB), launched in January 2012 with the support of the Spanish Foundation for Science and Technology (FECyT). This program has evolved in the last decade in order to make it more sustainable, allowing the teleoperation of underwater vehicles from anywhere in the world. EDUROVs have passed through several phases, beginning with a basic electronics robot, followed by the incorporation of open-source electronic prototyping platforms and finally reaching the current state of teleoperation. Results based on 1–5 Likert scale questions show that both students and teachers consider the program useful to introduce technical and scientific concepts. It is concluded that the use of low-cost materials and tools that are easy to obtain, following education on sustainability approaches, also makes them possible for use in high schools, and science teachers can easily carry out the activity. Moreover, the possibility of remote teleoperation of underwater vehicles, together with the collaboration among groups of students in different locations that are in contact through these online tools, allows one to motivate students to work on the project from a different perspective. 
id624#Linnea: Compiling linear algebra expressions to high-performance code#Linear algebra expressions appear in fields as diverse as computational biology, signal processing, communication technology, finite element methods, and control theory. Libraries such as BLAS and LAPACK provide highly optimized building blocks for just about any linear algebra computation; thus, a linear algebra expression can be evaluated efficiently by breaking it down into those building blocks. However, this is a challenging problem, requiring knowledge in high-performance computing, compilers, and numerical linear algebra. In this paper we give an overview of existing solutions, and introduce Linnea, a compiler that solves this problem. As shown through a set of test cases, Linnea's results are comparable with those obtained by human experts. 
id625#Relative Position Estimation in Multi-Agent Systems Using Attitude-Coupled Range Measurements#The ability to accurately estimate the position of robotic agents relative to one another, in possibly GPS-denied environments, is crucial to execute collaborative tasks. Inter-agent range measurements are available at a low cost, due to technologies such as ultra-wideband radio. However, the task of three-dimensional relative position estimation using range measurements in multi-agent systems suffers from unobservabilities. This letter presents a sufficient condition for the observability of the relative positions, and satisfies the condition using a simple framework with only range measurements, an accelerometer, a rate gyro, and a magnetometer. The framework has been tested in simulation and in experiments, where 40-50 cm positioning accuracy is achieved using inexpensive off-the-shelf hardware. &copy; 2021 IEEE. 
id626#Online defect detection and automatic grading of carrots using computer vision combined with deep learning methods#The demand for smart automatic system in postharvest technology, particularly in the postharvest of carrot production is high. In this paper, an automatic carrot grading system was developed based on computer vision and deep learning, which can automatically inspect surface quality of carrots and grade washed carrots. Specifically, based on ShuffleNet and transfer learning, a lightweight deep learning model (CDDNet) was constructed to detect surface defects of carrots. Carrot grading methods were also proposed based on minimum bounding rectangle (MBR) fitting and convex polygon approximation. Experimental results showed that the detection accuracy of the proposed CDDNet was 99.82% for binary classification (normal and defective) and 93.01% for multi-class classification (normal, bad spot, abnormity, fibrous root), and demonstrated good performance both in time efficiency and detection accuracy. The grading accuracy of MBR fitting and convex polygon approximation was 92.8% and 95.1% respectively. This research provides a practical method for online defect detection and carrot grading, and has great application potential in commercial packing lines. 
id627#High-level synthesis, cryptography, and side-channel countermeasures: A comprehensive evaluation#Side-channel attacks pose a severe threat to both software and hardware cryptographic implementations. Current literature presents various countermeasures against these kinds of attacks, based on approaches such as hiding or masking, implemented either in software, or on register–transfer level or gate level in hardware. However, emerging trends in hardware design lean towards a system-level approach, allowing for faster, less error-prone, design process, an efficient hardware/software co-design, or sophisticated validation, verification, and (co)simulation strategies. In this paper, we propose a Boolean masking scheme suitable for high-level synthesis of substitution–permutation network-based encryption. We implement both unprotected and protected PRESENT, AES/Rijndael and Serpent encryption in C language, utilizing the concept of dynamic logic reconfiguration, synthesize it for Xilinx FPGA, and we compare our results regarding time and area utilization. We evaluate the effectiveness of proposed countermeasures using both specific and non-specific t-test leakage assessment methodology. We discuss the leakage assessment results, and we identify and discuss the related limitations of the system-level approach and the high-level synthesis. 
id628#ValueRank: Keyword search of object summaries considering values#The Relational ranking method applies authority-based ranking in relational dataset that can be modeled as graphs considering also their tuples’ values. Authority directions from tuples that contain the given keywords and transfer to their corresponding neighboring nodes in accordance with their values and semantic connections. From our previous work, ObjectRank extends to ValueRank that also takes into account the value of tuples in authority transfer flows. In a maked difference from ObjectRank, which only considers authority flows through relationships, it is only valid in the bibliographic databases e.g. DBLP dataset, ValueRank facilitates the estimation of importance for any databases, e.g. trading databases, etc. A relational keyword search paradigm Object Summary (denote as OS) is proposed recently, given a set of keywords, a group of Object Summaries as its query result. An OS is a multilevel-tree data structure, in which node (namely the tuple with keywords) is OS’s root node, and the surrounding nodes are the summary of all data on the graph. But, some of these trees have a very large in total number of tuples, size-l OSs are the OS snippets, have also been investigated using ValueRank.We evaluated the real bibliographical dataset and Microsoft business databases to verify of our proposed approach. Copyright 
id630#An adaptive framework of real-time continuous gait phase variable estimation for lower-limb wearable robots#Phase-variable-based approaches are emerging in the control of lower-limb wearable robots, such as exoskeletons and prosthetic legs. However, real-time smooth estimation of the gait phase within each gait cycle remains an open problem. This paper presents a novel method for real-time continuous gait phase estimation during walking. The proposed framework consists of three subsystems: real-time kinematic data collection, gait phase variable estimation, and online adaptation of individual kinematics through backward data segmentation of completed gait strides. It is worth noting that we introduce an online learning mechanism for extracting and learning gait features from previous strides, in contrast with offline parameter tuning. The proposed basic gait model is initialized by human average data and is incrementally refined as a function of the individual gait features over different walking speeds. This provides a framework for long-term personalized control. Furthermore, the phase variable is constructed through the thigh angle measured by an inertial measurement unit. The resulting simple sensor system improves the usability of the proposed technique in wearable robotics. Validation experiments with seven healthy subjects, including treadmill walking and free level-ground walking, were conducted to evaluate the performance of the proposed method. In treadmill validation, the root-mean-square error (RMSE) of the phase estimator was 4.14 ± 1.68% for steady speeds, while it was 6.77 ± 2.29% for unsteady-speed walking. In level-ground validation, the average RMSE of the phase estimator was 4.59 ± 1.76%. Preliminary experiments were also conducted using a single-joint hip exoskeleton to demonstrate the usability of our method in lower-limb wearable robots. 
id631#FLUX: From SQL to GQL query translation tool#With the influx of Web 3.0 the focus in Big Data Analytics has shifted towards modelling highly interconnected data and analysing relationships between them. Graph databases befit the requirements of Big Data Analytics yet organizations still depend on relational databases. A major roadblock in the industry wide adoption of graph databases is that a standard query language is still in its inception stage hence withholding interoperability between the two technologies. In this research we propose a tool FLUX for translating relational database queries to graph database queries. 
id632#A list-machine benchmark for mechanized metatheory#We propose a benchmark to compare theorem-proving systems on their ability to express proofs of compiler correctness. In contrast to the first POPLmark, we emphasize the connection of proofs to compiler implementations, and we point out that much can be done without binders or alpha-conversion. We propose specific criteria for evaluating the utility of mechanized metatheory systems; we have constructed solutions in both Coq and Twelf metatheory, and we draw conclusions about those two systems in particular. 
id634#Targeted muscle effort distribution with exercise robots: Trajectory and resistance effects#The objective of this work is to relate muscle effort distributions to the trajectory and resistance settings of a robotic exercise and rehabilitation machine. Muscular effort distribution, representing the participation of each muscle in the training activity, was measured with electromyography sensors (EMG) and defined as the individual activation divided by the total muscle group activation. A four degrees-of-freedom robot and its impedance control system are used to create advanced exercise protocols whereby the user is asked to follow a path against the machine's neutral path and resistance. In this work, the robot establishes a zero-effort circular path, and the subject is asked to follow an elliptical trajectory. The control system produces a user-defined stiffness between the deviations from the neutral path and the torque applied by the subject. The trajectory and resistance settings used in the experiments were the orientation of the ellipse and a stiffness parameter. Multiple combinations of these parameters were used to measure their effects on the muscle effort distribution. An artificial neural network (ANN) used part of the data for training the model. Then, the accuracy of the model was evaluated using the rest of the data. The results show how the precision of the model is lost over time. These outcomes show the complexity of the muscle dynamics for long-term estimations suggesting the existence of time-varying dynamics possibly associated with fatigue. 
id635#An approach to multiple security system development using database schemas#Information security is a key issue in an Enterprise Information System (EIS) development. It is important characteristic of the entire EIS and all EIS’s information subsystems. Information security effectiveness affects adequacy of enterprise decision making at all management levels and especially depends on database security. So, it is a good practice to develop a unified relational database for several subsystems of EIS. This paper discusses an approach to multiple security system development for several subsystems using one or several schemas of the unified database. The key peculiarity of the approach is an ability to evaluate “similarity” of database security systems. The “similar” database security systems should be united into the common security system, otherwise they must be separated. The “similarity” is calculated as weighted correlation between sets of user roles permissions defined as functional on sets of database tables, data operations and user roles. The proposed approach was tested on a production database of University Management Information System that allowed optimizing of its data access control through several database schemas. Also, the approach allows automation of determining the feasibility of creating new database schemas in the further development of the EIS. 
id636#Value range analysis based on abstract interpretation and generalized monotone data flow framework#Safe and accurate value range analysis is crucial for compiler optimization. Based on abstract interpretation and generalized monotone data flow framework, a complete framework for value range analysis is proposed in this paper. Different from other value range analysis methods, this framework includes complete definitions, analysis and correctness proofs. Compared with general theory about abstract interpretation, the method focuses on value range analysis, so the analysis and the proof of the analysis is straightforward.
id638#Design and application of assisted instruction software for the Compiler Construction Principles with java#The Compiler Construction Principles course is a core foundation stone in a computer science curriculum. The course features a combination of strong theoretical and practical. In order to help students understand the integral structure of compiler and master the complex principles and methods of compiler more easily, an assisted instruction software for Compiler Construction Principle with java to improve teaching is developed in this paper, which is based on the standard grammar of four Items of commixture arithmetic. The software can demo the whole process of the algorithm implementation in detail and print out the date and operation in the process of the algorithm implementation step by step. So we can show the complex theory and algorithm clearly and intuitively by the data, in which way the reliability and validity of teaching can be highly improved. 
id639#ASFNet: Adaptive multiscale segmentation fusion network for real-time semantic segmentation#Recently, the development of deep learning has facilitated continuous progress in the field of computer vision. Pixel-level semantic segmentation serves as a fundamental task in computer vision. It achieves significant results by connecting wider and deeper backbone networks and building fine-grained segmentation heads. However, applications such as self-driving cars are more critical to the computational speed of the algorithms. The trade-off between accuracy and real-time performance of existing algorithms is still a challenging task. To address this challenge, this article proposes an adaptive multiscale segmentation fusion network to fuse multiscale contextual, which designs an adaptive multiscale segmentation fusion module based on an attention mechanism. Using segmentation fusion instead of feature fusion, the multiscale segmentation results are aggregated to obtain more precise segmentation results. The final results achieved 70.9% mIoU of accuracy in the Cityspace test set, processing images at 61 FPS when the input is 1024 × 2048. In addition, when adjusting the input size to 512 × 1024, the images are processed at 185 FPS. 
id640#Automated extraction and evaluation of fracture trace maps from rock tunnel face images via deep learning#This paper proposes an image-based method for automated rock fracture segmentation and fracture trace quantification. It is integrated using a CNN-based model named FraSegNet, a skeleton extraction algorithm, and a chain code-based polyline approximation algorithm. A rock tunnel fracture database with a total of 3,000 images of rock tunnel faces is established and selected to train and test the FraSegNet model. A comparison study is further conducted and shows that the FraSegNet model shows advanced performance in pixel-level fracture trace map extraction and noise reduction compared to other deep learning approaches and traditional image edge detection algorithms. Next, the skeletons of the predicted fracture trace maps are extracted and the corresponding polyline for each fracture skeleton is thus obtained and output as a text file composed of key nodes coordinates. The fracture trace characteristics (trace length, dip angle, density, and intensity) are acquired using node-based files. The quantitative evaluation of the proposed method illustrates that it can extract trace occurrences effectively and accurately. A case study of three full scale tunnel sections demonstrates the proposed method to be an efficient approach for acquiring and evaluating 2D fracture occurrences of under-construction rock tunnel faces. 
id642#Two approaches for clustering algorithms with relational-based data#It is well known that relational databases still play an important role for many companies around the world. For this reason, the use of data mining methods to discover knowledge in large relational databases has become an interesting research issue. In the context of unsupervised data mining, for instance, the conventional clustering algorithms cannot handle the particularities of the relational databases in an efficient way. There are some clustering algorithms for relational datasets proposed in the literature. However, most of these methods apply complex and/or specific procedures to handle the relational nature of data, or the relational-based methods do not capture the relational nature in an efficient way. Aiming to contribute to this important topic, in this paper, we will present two simple and generic approaches to handle relational-based data for clustering algorithms. One of them treats the relational data through the use of a hierarchical structure, while the second approach applies a weight structure based on relationship and attribute information. In presenting these two approaches, we aim to tackle relational-based dataset in a simple and efficient way, improving the efficiency of corporations that handle relational-based in the unsupervised data mining context. In order to evaluate the effectiveness of the presented approaches, a comparative analysis will be conducted, comparing the proposed approaches with some existing approaches and with a baseline approach. In all analyzed approaches, we will use two well-known types of clustering algorithms (agglomerative hierarchical and K-means). In order to perform this analysis, we will use two internal and one external clusters as validity measures. 
id643#Type theoretical databases#We show how the display-map category of finite (symmetric) simplicial complexes can be seen as representing the totality of database schemas and instances in a single mathematical structure. We give a sound interpretation of a certain dependent type theory in this model and show how it allows for the syntactic specification of schemas and instances and the manipulation of the same with the usual type-theoretic operations. 
id644#A Guide to Annotation of Neurosurgical Intraoperative Video for Machine Learning Analysis and Computer Vision#Objective: Computer vision (CV) is a subset of artificial intelligence that performs computations on image or video data, permitting the quantitative analysis of visual information. Common CV tasks that may be relevant to surgeons include image classification, object detection and tracking, and extraction of higher order features. Despite the potential applications of CV to intraoperative video, however, few surgeons describe the use of CV. A primary roadblock in implementing CV is the lack of a clear workflow to create an intraoperative video dataset to which CV can be applied. We report general principles for creating usable surgical video datasets and the result of their applications. Methods: Video annotations from cadaveric endoscopic endonasal skull base simulations (n = 20 trials of 1–5 minutes, size = 8 GB) were reviewed by 2 researcher-annotators. An internal, retrospective analysis of workflow for development of the intraoperative video annotations was performed to identify guiding practices. Results: Approximately 34,000 frames of surgical video were annotated. Key considerations in developing annotation workflows include 1) overcoming software and personnel constraints; 2) ensuring adequate storage and access infrastructure; 3) optimization and standardization of annotation protocol; and 4) operationalizing annotated data. Potential tools for use include CVAT (Computer Vision Annotation Tool) and Vott: open-sourced annotation software allowing for local video storage, easy setup, and the use of interpolation. Conclusions: CV techniques can be applied to surgical video, but challenges for novice users may limit adoption. We outline principles in annotation workflow that can mitigate initial challenges groups may have when converting raw video into useable, annotated datasets. 
id645#INSSEPT: An open-source relational database of seismic performance estimation to aid with early design of buildings#Performance-based earthquake engineering (PBEE) assessments are data-, effort-, and time-intensive, usually requiring a detailed structural model and limiting their integration with early design. Decades of research have produced an abundance of PBEE assessments for different structural systems and building taxonomies. The results of these PBEE studies can be assimilated to approximately represent the seismic design space for new structures and to identify possibly optimal systems with low effort. This article introduces an open-source relational database, Inventory of Seismic Structural Evaluations, Performance Functions and Taxonomies for Buildings (INSSEPT) that contains PBEE assessment of 222 buildings from literature and is freely available to the public in a natural hazard repository. INSSEPT is organized to provide a curated building taxonomy and PBEE data to readily serve as a resource for early design or PBEE-derived regional seismic risk analysis. 
id646#An efficient quantum compiler that reduces T count#Before executing a quantum algorithm, one must first decompose the algorithm into machine-level instructions compatible with the architecture of the quantum computer, a process known as quantum compiling. There are many different quantum circuit decompositions for the same algorithm but it is desirable to compile leaner circuits. A fundamentally important cost metric is the T count - the number of T gates in a circuit. For the single qubit case, optimal compiling is essentially a solved problem. However, multi-qubit compiling is a harder problem with optimal algorithms requiring classical runtime exponential in the number of qubits. Here, we present and compare several efficient quantum compilers for multi-qubit Clifford + T circuits. We implemented our compilers in C++ and benchmarked them on random circuits, from which we determine that our TODD compiler yields the lowest T counts on average. We also benchmarked TODD on a library of reversible logic circuits that appear in quantum algorithms and found that it reduced the T count for 97% of the circuits with an average T-count saving of 20% when compared against the best of all previous circuit decompositions. 
id648#An intelligent and cost-effective remote underwater video device for fish size monitoring#Monitoring the size of key indicator species of fish is important to understand ecosystem functions, anthropogenic stress, and population dynamics. Standard methodologies gather data using underwater cameras, but are biased due to the use of baits, limited deployment time, and short field of view. Furthermore, they require experts to analyse long videos to search for species of interest, which is time consuming and expensive. This paper describes the Underwater Detector of Moving Object Size (UDMOS), a cost-effective computer vision system that records events of large fishes passing in front of a camera, using minimalistic hardware and power consumption. UDMOS can be deployed underwater, as an unbaited system, and is also offered as a free-to-use Web Service for batch video-processing. It embeds three different alternative large-object detection algorithms based on deep learning, unsupervised modelling, and motion detection, and can work both in shallow and deep waters with infrared or visible light. 
id649#Secure Cryptographic Unit as Root-of-Trust for IoT Era#The Internet of Things (IoT) implicates an infrastructure that creates new value by connecting everything with communication networks, and its construction is rapidly progressing in anticipation of its great potential. Enhancing the security of IoT is an essential requirement for supporting IoT. For ensuring IoT security, it is desirable to create a situation that even a terminal component device with many restrictions in computing power and energy capacity can easily verify other devices and data and communicate securely by the use of public key cryptography. To concretely achieve the big goal of penetrating public key cryptographic technology to most IoT end devices, we elaborated the secure cryptographic unit (SCU) built in a low-end microcontroller chip. The SCU comprises a hardware cryptographic engine and a built-in access controlling functionality consisting of a software gate and hardware gate. This paper describes the outline of our SCU construction technology s research and development and prospects. 
id650#LabDER - Relational database virtual learning environment#This paper describes a virtual learning environment for use in introductory database disciplines that aligns with the professor’s teaching plan and aims to automatically evaluate students’ responses to questions, which may be multiple-choice or discursive or may involve entity relationship diagrams (ERDs) or SQL. The main advantage of LabDER over previous automatic evaluation approaches for ERD and SQL is that it accepts multiple responses and encourages students to develop the best solution through semantic feedback based on compiler theories, software engineering metrics and supervised machine learning. This approach considers the distance of the student’s response from the model response and provides semantic feedback via a blend of compiler and various other metrics, while predicting the student’s grade using a machine learning algorithm. A case study was designed to confirm the approach and 15,158 students’ responses were automatically evaluated. As a result, semantic feedback provided student self-learning through suggestions on the database concepts involved in each solution, which generated a considerable increase in student participation as well as an increase in their average grades. In future work, we will investigate how to include other database topics that can be automatically evaluated, such as query performance and relational algebra. Copyright 
id651#Learning from Textual Data in Database Systems#Relational database systems hold massive amounts of text, valuable for many machine learning (ML) tasks. Since ML techniques depend on numerical input representations, pre-trained word embeddings are increasingly utilized to convert text values into meaningful numbers. However, a naïve one-to-one mapping of each word in a database to a word embedding vector misses incorporating rich context information given by the database schema. Thus, we propose a novel relational retrofitting framework Retro to learn numerical representations of text values in databases, capturing the rich information encoded by pre-trained word embedding models as well as context information provided by tabular and foreign key relations in the database. We defined relation retrofitting as an optimization problem, present an efficient algorithm solving it, and investigate the influence of various hyperparameters. Further, we develop simple feed-forward and complex graph convolutional neural network architectures to operate on those representations. Our evaluation shows that the proposed embeddings and models are ready-to-use for many ML tasks, such as text classification, imputation, and link prediction, and even outperform state-of-the-art techniques. 
id652#A survey and performance evaluation of deep learning methods for small object detection#In computer vision, significant advances have been made on object detection with the rapid development of deep convolutional neural networks (CNN). This paper provides a comprehensive review of recently developed deep learning methods for small object detection. We summarize challenges and solutions of small object detection, and present major deep learning techniques, including fusing feature maps, adding context information, balancing foreground-background examples, and creating sufficient positive examples. We discuss related techniques developed in four research areas, including generic object detection, face detection, object detection in aerial imagery, and segmentation. In addition, this paper compares the performances of several leading deep learning methods for small object detection, including YOLOv3, Faster R-CNN, and SSD, based on three large benchmark datasets of small objects. Our experimental results show that while the detection accuracy on small objects by these deep learning methods was low, less than 0.4, Faster R-CNN performed the best, while YOLOv3 was a close second. 
id654#SAGES consensus recommendations on an annotation framework for surgical video#Background: The growing interest in analysis of surgical video through machine learning has led to increased research efforts; however, common methods of annotating video data are lacking. There is a need to establish recommendations on the annotation of surgical video data to enable assessment of algorithms and multi-institutional collaboration. Methods: Four working groups were formed from a pool of participants that included clinicians, engineers, and data scientists. The working groups were focused on four themes: (1) temporal models, (2) actions and tasks, (3) tissue characteristics and general anatomy, and (4) software and data structure. A modified Delphi process was utilized to create a consensus survey based on suggested recommendations from each of the working groups. Results: After three Delphi rounds, consensus was reached on recommendations for annotation within each of these domains. A hierarchy for annotation of temporal events in surgery was established. Conclusions: While additional work remains to achieve accepted standards for video annotation in surgery, the consensus recommendations on a general framework for annotation presented here lay the foundation for standardization. This type of framework is critical to enabling diverse datasets, performance benchmarks, and collaboration. 
id657#Near-road air quality modelling that incorporates input variability and model uncertainty#Dispersion modelling is an effective tool to estimate traffic-related fine particulate matter (PM2.5) concentrations in near-road environments. However, many sources of uncertainty and variability are associated with the process of near-road dispersion modelling, which renders a single-number estimate of concentration a poor indicator of near-road air quality. In this study, we propose an integrated traffic-emission-dispersion modelling chain that incorporates several major sources of uncertainty. Our approach generates PM2.5 probability distributions capturing the uncertainty in emissions and meteorological conditions. Traffic PM2.5 emissions from 7 a.m. to 6 p.m. were estimated at 3400 ± 117 g. Modelled PM2.5 levels were validated against measurements along a major arterial road in Toronto, Canada. We observe large overlapping areas between modelled and measured PM2.5 distributions at all locations along the road, indicating a high likelihood that the model can reproduce measured concentrations. A policy scenario expressing the impact of reductions in truck emissions revealed that a 30% reduction in near-road PM2.5 concentrations can be achieved by upgrading close to 55% of the current trucks circulating along the corridor. A speed limit reduction of 10 km/h could lead to statistically significant increases in PM2.5 concentrations at twelve out of the eighteen locations. 
id658#Verified compilation on a verified processor#Developing technology for building verified stacks, i.e., computer systems with comprehensive proofs of correctness, is one way the science of programming languages furthers the computing discipline. While there have been successful projects verifying complex, realistic system components, including compilers (software) and processors (hardware), to date these verification efforts have not been compatible to the point of enabling a single end-to-end correctness theorem about running a verified compiler on a verified processor. In this paper we show how to extend the trustworthy development methodology of the CakeML project, including its verified compiler, with a connection to verified hardware. Our hardware target is Silver, a verified proof-of-concept processor that we introduce here. The result is an approach to producing verified stacks that scales to proving correctness, at the hardware level, of the execution of realistic software including compilers and proof checkers. Alongside our hardware-level theorems, we demonstrate feasibility by hosting and running our verified artefacts on an FPGA board. 
id659#Frontalization and adaptive exponential ensemble rule for deep-learning-based facial expression recognition system#Automatic facial expression recognition (FER) is an important technique in human–computer interfaces and surveillance systems. It classifies the input facial image into one of the basic expressions (anger, sadness, surprise, happiness, disgust, fear, and neutral). There are two types of FER algorithms: feature-based and convolutional neural network (CNN)-based algorithms. The CNN is a powerful classifier, however, without proper auxiliary techniques, its performance may be limited. In this study, we improve the CNN-based FER system by utilizing face frontalization and the hierarchical architecture. The frontalization algorithm aligns the face by in-plane or out-of-plane, rotation, landmark point matching, and removing background noise. The proposed adaptive exponentially weighted average ensemble rule can determine the optimal weight according to the accuracy of classifiers to improve robustness. Experiments on several popular databases are performed and the results show that the proposed system has a very high accuracy and outperforms state-of-the-art FER systems. 
id660#Evolving gaits for damage control in a hexapod robot#Autonomous robots are increasingly used in remote and hazardous environments, where damage to sensory-actuator systems cannot be easily repaired. Such robots must therefore have controllers that continue to function effectively given unexpected malfunctions and damage to robot morphology. This study applies the Intelligent Trial and Error (IT&E) algorithm to adapt hexapod robot control to various leg failures and demonstrates the IT&E map-size parameter as a critical parameter in influencing IT&E adaptive task performance. We evaluate robot adaptation for multiple leg failures on two different map-sizes in simulation and validate evolved controllers on a physical hexapod robot. Results demonstrate a trade-off between adapted gait speed and adaptation duration, dependent on adaptation task complexity (leg damage incurred), where map-size is crucial for generating behavioural diversity required for adaptation. 
id661#IBM Functional Genomics Platform, A Cloud-Based Platform for Studying Microbial Life at Scale#The rapid growth in biological sequence data is revolutionizing our understanding of genotypic diversity and challenging conventional approaches to informatics. Due to the increasing available genomic data, traditional bioinformatic tools require substantial computational time and the creation of ever-larger indices each time a researcher seeks to gain insight from the data. To address this, we pre-computed important relationships between biological entities spanning the Central Dogma of Molecular Biology and captured this information in a relational database. The database can be queried across hundreds of millions of entities and returns results in a fraction of the time required by traditional methods. We describe IBM Functional Genomics Platform, a comprehensive database relating genotype to phenotype for bacterial life. Continually updated, the platform contains data derived from 200,000 curated, self-consistently assembled genomes. The database stores functional data for over 68 million genes, 52 million proteins, and 239 million domains with associated biological activity annotations from Gene Ontology, KEGG, MetaCyc, and Reactome. It maps the connections between each biological entity including the originating genome, gene, protein, and protein domain. We describe the data selection, the pipeline to create and update, and the developer tools. IEEE
id662#A generic watermarking model for object relational databases#Object-Relational Databases have been implemented largely and thus, there is urgent need to protect them from potential misuse. Digital watermarking is considered as technological protection measure that deters the attempts to plagiarize or distort databases. Prior work focused primarily on protection of Relational Databases by digital watermarking. However, Object-Relational Databases seem to have escaped the attention of researchers. In this research work, we propose the framework for a novel, generic watermarking technique for ORDBs with a view to incorporate Ownership and Integrity protection. The software developed on the basis of this model is called WORD – Watermarking Object Relational Databases. To the best of our knowledge, this is for the first time that a complete model for watermarking ORDBs has been developed. To deal with changing security aspects with time, strategy pattern is adopted while implementing the watermarking model WORD. The use of strategy pattern designs incorporates flexibility and scalability in our model. Robustness and integrity of proposed system has been proved theoretically as well as practically by various experiments. 
id663#A register allocation algorithm based on regions priority#Compiling speed and code quality are two key factors to evaluate the performance of register allocation. Modern Just In Time (JIT) compilers desire the best code in least time. Neither traditional graph coloring algorithm nor linear scan algorithm can satisfy the condition perfectly. A regions priority algorithm is proposed in this paper, which does not pursue the perfect register allocation in theory but takes greedy design method. The program is divided by loop region, on which physical register allocation is implemented by dealing lifetime length priority queue and spill weight priority queue. Then the allocation extends to the whole program. This algorithm creates high quality code in linear time and is applied to the compiler which compiles PTX instructions in SSA to traditional multicore platform. The experiment based on this compiler verifies the effectiveness of the algorithm.
id664#Optimising pheromone communication in a UAV swarm#Communications in multi-robot systems is a key factor, especially when aiming for real-world applications. In this article we address the optimisation of the communications in a swarm of unmanned aerial vehicles for surveillance applications. More precisely a genetic algorithm is introduced to optimize the exchange of pheromone maps used in the CACOC (Chaotic Ant Colony Optimisation for Coverage) mobility model which enhance the vehicles' routes in order to achieve unpredictable trajectories as well as maximise area coverage. 
id665#An improved approach of register allocation via graph coloring#"Register allocation is an important part of optimizing compiler. The algorithm of register allocation via graph coloring is implemented by Chaitin and his colleagues firstly and improved by Briggs and others. By abstracting register allocation to graph coloring, the allocation process is simplified. As the physical register number is limited, coloring of the interference graph can't succeed for every node. The uncolored nodes must be spilled. There is an assumption that almost all the allocation method obeys: when a register is allocated to a variable v, it can't be used by others before v quit even if v is not used for a long time. This may causes a waste of register resource. The authors relax this restriction under certain conditions and make some improvement. In this method, one register can be mapped to two or more interfered ""living"" live ranges at the same time if they satisfy some requirements. An operation named merge is defined which can arrange two interfered nodes occupy the same register with some cost. Thus, the resource of register can be used more effectively and the cost of memory access can be reduced greatly. "
id666#Origami-Inspired Robot That Swims via Jet Propulsion#Underwater swimmers present unique opportunities for using bodily reconfiguration for self propulsion. Origami-inspired designs are low-cost, fast to fabricate, robust, and can be used to create compliant mechanisms useful in energy efficient underwater locomotion. In this paper, we demonstrate an origami-inspired robot that can change its body shape to ingest and expel water, creating a jet that propels it forward similarly to cephalopods. We use the magic ball origami pattern, which can transform between ellipsoidal (low volume) and spherical (high volume) shapes. A custom actuation mechanism contracts the robot to take in fluid, and the inherent mechanics of the magic ball returns the robot to its natural shape upon release. We describe the design and control of this robot and verify its locomotion in a water tank. The resulting robot is able to move forward at 6.7 cm/s (0.2 body lengths/s), with a cost of transport of 2.0. 
id667#Automatic application of power analysis countermeasures#We introduce a compiler that automatically inserts software countermeasures to protect cryptographic algorithms against power-based side-channel attacks. The compiler first estimates which instruction instances leak the most information through side-channels. This information is obtained either by dynamic analysis, evaluating an information theoretic metric over the power traces acquired during the execution of the input program, or by static analysis. As information leakage implies a loss of security, the compiler then identifies (groups of) instruction instances to protect with a software countermeasure such as random precharging or Boolean masking. As software protection incurs significant overhead in terms of cryptosystem runtime and memory usage, the compiler protects the minimum number of instruction instances to achieve a desired level of security. The compiler is evaluated on two block ciphers, AES and Clefia; our experiments demonstrate that the compiler can automatically identify and protect the most important instruction instances. To date, these software countermeasures have been inserted manually by security experts, who are not necessarily the main cryptosystem developers. Our compiler offers significant productivity gains for cryptosystem developers who wish to protect their implementations from side-channel attacks. 
id668#EEG based arm movement intention recognition towards enhanced safety in symbiotic Human-Robot Collaboration#Consumer markets demonstrate an observable trend towards mass customization. Assembly processes are required to adapt in order to meet the requirements of increased product complexity and constant variant updates. A concept to meet challenges within this trend, is a close collaboration between human workers and robots. Currently, in order to protect human operators, there are barriers and restrictions in place which prevent close collaboration. This is due to safety systems being mostly reactive, rather than anticipating motions or intentions. There are probabilistic models, which aim to overcome these limitations, yet predicting human behavior remains highly complex. Thus, it would be desirable to physically measure movement intentions in advance. A novel approach is presented of how upper-limb movement intentions can be measured with a mobile electroencephalogram (EEG). The human brain constantly analyses and evaluates motor movements up to 0.5 s before their execution. A safety system could therefore be enhanced to have an early warning of an upcoming movement. In order to classify the EEG-signals as fast as possible and to minimize fine-tuning efforts, a novel data processing methodology is introduced. This includes TimeSeriesKMeans labelling of movement intentions, which is then used to train a Long Short-Term Memory Recurrent Neural Network (LSTM-RNN). The results suggested high detection accuracies and potential time gains of up to 513 ms to be achieved in a semi-online system. Thus, the time advantages included in a simulation demonstrated the potential to increase a system's reaction time and therefore improve the safety and the fluency of Human-Robot Collaboration. 
id669#Analysis of Forces Involved in the Perching Maneuver of Flapping-Wing Aerial Systems and Development of an Ultra-Lightweight Perching System#Trying to optimize the design of aerial robotics systems, this work presents an optimized low-weight landing system for flapping-wing aerial robots. The design, based on the use of low-sized neodymium magnets, intends to provide that these aerial robots have the capability of landing in restricted areas by using the presented solution. This capacity will increase the application range of these robots. A study of this situation has been done to analyze the perching maneuver forces and evaluate the system. The solution presented is low-weight, low-sized, and also relatively inexpensive. Therefore, this solution may apply to most ornithopter robots. Design, analysis of the implied forces, development and experimental validation of the idea are presented in this work, demonstrating that the developed solution can overcome the ornithopter's payload limitation providing an efficient and reliable solution. 
id670#Efficient mapping of irregular C++ applications to integrated GPUs#There is growing interest in using GPUs to accelerate general- purpose computation since they offer the potential of mas- sive parallelism with reduced energy consumption. This in- Terest has been encouraged by the ubiquity of integrated processors that combine a GPU and CPU on the same die, lowering the cost of offoading work to the GPU. However, while the majority of effort has focused on GPU accelera- Tion of regular applications, relatively little is known about the behavior of irregular applications on GPUs. These ap- plications are expected to perform poorly on GPUs without major software engineering effort. We present a compiler framework with support for C++ features that enables GPU acceleration of a wide range of C++ applications with min- imal changes. This framework, Concord, includes a low- cost, software SVM implementation that permits seamless sharing of pointer-containing data structures between the CPU and GPU. It also includes compiler optimizations to improve irregular application performance on GPUs. Us- ing Concord, we ran nine irregular C++ programs on two computer systems containing Intel 4th Generation Core pro- cessors. One system is an Ultrabook with an integrated HD Graphics 5000 GPU, and the other system is a desktop with an integrated HD Graphics 4600 GPU. The nine appli- cations are pointer-intensive and operate on irregular data structures such as trees and graphs; they include face detec- Tion, BTree, single-source shortest path, soft-body physics simulation, and breadth-first search. Our results show that Concord acceleration using the GPU improves energy effi-ciency by up to 6.04 × on the Ultrabook and 3.52× on the desktop over multicore-CPU execution. Copyright 
id671#A novel robot co-worker system for paint factories without the need of existing robotic infrastructure#This paper presents a human–robot co-working system to be applied to industrial tasks such as the production line of a paint factory. The aim is to optimize the picking task with respect to manual operation in a paint factory. The use of an agile autonomous robot co-worker reduces the time in the picking process of materials, and the reduction of the exposure time to raw materials of the worker improves the human safety. Moreover, the process supervision is also improved thanks to a better traceability of the whole process. The whole system consists of a manufacturing process management system, an autonomous navigation system, and a people detection and tracking system. The localization module does not require the installation of reflectors or visual markers for robot operation, significantly simplifying the system deployment in a factory. The robot is able to respond to changing environmental conditions such as people, moving forklifts or unmapped static obstacles like pallets or boxes. The system is not tied to specific manufacturing orders. It is fully integrated with the manufacturing process management system and it can process all possible orders as long as their components are placed into the warehouse. Real experiments to validate the system have been performed in a paint factory by a real holonomic platform and a worker. The results are promising from the evaluation of performance indicators such as exposure time of the worker to raw materials, automation of the process, robust and safe navigation, and the assessment of the end-user. 
id673#Fuzzy querying with SQL: Fuzzy view-based approach#Nowadays several works have been proposed that allow users to perform fuzzy queries on relational databases. But most of these systems based on an additional software layer to translate a fuzzy query and a supplementary layer of a classic database management system (DBMS) to evaluate fuzzy predicates, which induces an important overhead. They are not also easy to implement by a non-expert user. Here we have proposed a simple and intelligent approach to extend the SQL language to allow us to write flexible conditions in our queries without the need for translation. The main idea is to use a view to manipulate the satisfaction degrees related to user-defined fuzzy predicates, instead of calculating them at runtime employing user functions embedded in the query. Consequently, the response time of executing a fuzzy query statement will be reduced. This approach allows us to easily integrate most fuzzy request characters such as fuzzy modifiers, fuzzy quantifiers, fuzzy joins, etc. Moreover, we present a user-friendly interface to make it easy to use fuzzy linguistic values in all clauses of a select statement. The main contribution of this paper is to accelerate the execution of fuzzy query statements. 
id674#Adaptively secure lattice-based revocable IBE in the QROM: compact parameters, tight security, and anonymity#Revocable identity-based encryption (RIBE) is an extension of IBE that satisfies a key revocation mechanism to manage a number of users dynamically and efficiently. To resist quantum attacks, two adaptively secure lattice-based RIBE schemes are known in the (quantum) random oracle model ((Q)ROM). Wang et al.’s scheme that is secure in the ROM has large secret keys depending on the depth of a binary tree and its security reduction is not tight. Ma and Lin’s scheme that is secure in the QROM has large ciphertexts depending on the length of identities and is not anonymous. In this paper, we propose an adaptively secure lattice-based RIBE scheme that is secure in the QROM. Our scheme has compact parameters, where the ciphertext-size is smaller than Wang et al.’s scheme and the secret key size is the same as Ma and Lin’s scheme. Moreover, our scheme is anonymous and its security reduction is completely tight. We design the proposed scheme by modifying Ma–Lin’s scheme instantiated by the Gentry–Peikert–Vaikuntanathan (GPV) IBE. We can obtain the advantages of our scheme by making use of Katsumata et al.’s proof technique of the GPV IBE in the QROM. 
id675#Training spiking neural networks with a multi-agent evolutionary robotics framework#We demonstrate the training of Spiking Neural Networks (SNN) in a novel multi-agent Evolutionary Robotics (ER) framework inspired by competitive evolutionary environments in nature. The topology of a SNN along with morphological parameters of the bot it controls in the ER environment is together treated as a phenotype. Rules of the framework select certain bots and their SNNs for reproduction and others for elimination based on their efficacy in capturing food in a competitive environment. While the bots and their SNNs are not explicitly trained to survive or reproduce using loss functions, these drives emerge implicitly as they evolve to hunt food and survive. Their efficiency in capturing food exhibits the evolutionary signature of punctuated equilibrium. We use this signature to compare the performances of two evolutionary inheritance algorithms on the phenotypes, Mutation and Crossover with Mutation, using ensembles of 100 experiments for each algorithm. We find that Crossover with Mutation promotes 40% faster learning in the SNN than mere Mutation with a statistically significant margin. 
id676#Trout-like multifunctional piezoelectric robotic fish and energy harvester#This work presents our experimental studies on a trout-inspired multifunctional robotic fish as an underwater swimmer and energy harvester. Fiber-based flexible piezoelectric composites with interdigitated electrodes, specifically macro-fiber composite (MFC) structures, strike a balance between the deformation and actuation force capabilities to generate hydrodynamic propulsion without requiring additional mechanisms for motion amplification. A pair of MFC laminates bracketing a passive fin functions like artificial muscle when driven out of phase to expand and contract on each side to create bending. The trout-like robotic fish design explored in this work was tested for both unconstrained swimming in a quiescent water tank and under imposed flow in a water tunnel to estimate the maximum swimming speed, which exceeded 0.25 m s-1, i.e., 0.8 body lengths per second. Hydrodynamic thrust characterization was also performed in a quiescent water setting, revealing that the fin can easily produce tens of mN of thrust, similar to its biological counterpart for comparable swimming speeds. Overall, the prototype presented here generates thrust levels higher than other smart material-based concepts (such as soft polymeric material-based actuators which provide large deformation but low force), while offering simple design, geometric scalability, and silent operation unlike motor-based robotic fish (which often use bulky actuators and complex mechanisms). Additionally, energy harvesting experiments were performed to convert flow-induced vibrations in the wake of a cylindrical bluff body (for different diameters) in a water tunnel. The shed vortex frequency range for a set of bluff body diameters covered the first vibration mode of the tail, yielding an average electrical power of 120 μW at resonance for a flow speed around 0.3 m s-1 and a bluff body diameter of 28.6 mm. Such low-power electricity can find applications to power small sensors of the robotic fish in scenarios such as ecological monitoring, among others. 
id677#Data-Driven Safety-Critical Control: Synthesizing Control Barrier Functions with Koopman Operators#Control barrier functions (CBFs) are a powerful tool to guarantee safety of autonomous systems, yet they rely on the computation of control invariant sets, which is notoriously difficult. A backup strategy employs an implicit control invariant set computed by forward integrating the system dynamics. However, this integration is prohibitively expensive for high dimensional systems, and inaccurate in the presence of unmodelled dynamics. We propose to learn discrete-time Koopman operators of the closed-loop dynamics under a backup strategy. This approach replaces forward integration by a simple matrix multiplication, which can mostly be computed offline. We also derive an error bound on the unmodeled dynamics in order to robustify the CBF controller. Our approach extends to multi-agent systems, and we demonstrate the method on collision avoidance for wheeled robots and quadrotors. 
id678#Orchard mapping with deep learning semantic segmentation#This study aimed to propose an approach for orchard trees segmentation using aerial images based on a deep learning convolutional neural network variant, namely the U-net network. The purpose was the automated detection and localization of the canopy of orchard trees under various conditions (i.e., different seasons, different tree ages, different levels of weed coverage). The implemented dataset was composed of images from three different walnut orchards. The achieved variability of the dataset resulted in obtaining images that fell under seven different use cases. The best-trained model achieved 91%, 90%, and 87% accuracy for training, validation, and testing, respectively. The trained model was also tested on never-before-seen orthomosaic images or orchards based on two methods (oversampling and undersampling) in order to tackle issues with out-of-the-field boundary transparent pixels from the image. Even though the training dataset did not contain orthomosaic images, it achieved performance levels that reached up to 99%, demonstrating the robustness of the proposed approach. 
id679#EMT: Elegantly Measured Tanner for Key-Value Store on SSD#With the emerging of big data era, NoSQL key-value database is considered as a promising candidate for replacing relational database management system (RDBMS). As cost per GB of flash memory is getting closer to HDD, high performance solid state drive (SSD) is regarded as the best substitute of HDD. However, there are still some issues that need to be taken into consideration. In KV-FTL, Chen et al. have reported that applying key-value store to SSD with conventional FTL would incur internal fragmentation and further cause the degradation of device lifespan. Although they proposed KVFTL to deal with the above issues, their work gave rise to the read amplification problem. In this paper, we investigate the root cause of the read amplification problem and propose EMT-FTL to mitigate internal fragmentation and read amplification with acceptable memory overhead. Different from KVFTL that slices a variable-sized value into multiple partitions (up to 20) of different sizes and manages them as a linked chain, EMT-FTL slices a value into 16 KiB full partitions with its remaining bytes treated as a fragment partition and appended to the fragment buffer. For a 64 GiB SSD with 16 KiB pages, the overall memory usage of EMT-FTL is only 8.81% of KVFTL. The experiments showed that EMT-FTL achieved almost the optimal space utilization as KVFTL did under most of traces and averagely improved the get performance by 78.57% (compared with KVFTL) under the traces with request sizes ranging from 1 byte to 16 KiB. IEEE
id680#Hardware-accelerated regular expression matching with overlap handling on IBM PowerEN™ processor#Programmable hardware accelerators for regular expression (regex) matching are evolving into increasingly complex stream processors, which involve multiple state machines that operate in parallel, and specialized post-processors that can process instructions dispatched by the state machines. To improve the speed and the storage-efficiency, complex regexs are decomposed into simpler sub expressions, where each sub expression can fire one or more instructions. Although the impact of regex decompositions on the storage efficiency is well-known, little has been done to address the correctness and completeness. We show that regex decompositions can resultin false positives if overlaps between sub expressions are not taken into account. We describe formal methods to recognize various types of sub expression overlaps that can arise in regex decompositions. We also describe efficient post-processing techniques to eliminate the associated false positives. To enable efficient mapping of the decomposed regexs to the post-processors, we propose integer programming based register allocation methods. Our methods pack narrow variables to reduce the register and instruction usage, and take advantage of multi-register reset instructions to reduce the number of instructions that must be executed in parallel. Experiments on regex sets obtained from open-source and proprietary network intrusion detection systems demonstrate orders of magnitude improvement in the storage efficiency over state-of-the-art. 
id681#Using type analysis in compiler to mitigate integer-overflow-to-buffer- overflow threat#One of the top two causes of software vulnerabilities in operating systems is the integer overflow. A typical integer overflow vulnerability is the Integer Overflow to Buffer Overflow (IO2BO for short) vulnerability. IO2BO is an underestimated threat. Many programmers have not realized the existence of IO2BO and its harm. Even for those who are aware of IO2BO, locating and fixing IO2BO vulnerabilities are still tedious and error-prone. Automatically identifying and fixing this kind of vulnerability are critical for software security. In this article, we present the design and implementation of IntPatch, a compiler extension for automatically fixing IO2BO vulnerabilities in C/C++ programs at compile time. IntPatch utilizes classic type theory and a dataflow analysis framework to identify potential IO2BO vulnerabilities, and then uses backward slicing to find out related vulnerable arithmetic operations, and finally instruments programs with runtime checks. Moreover, IntPatch provides an interface for programmers who want to check integer overflows manually. We evaluated IntPatch on a few real-world applications. It caught all 46 previously known IO2BO vulnerabilities in our test suite and found 21 new bugs. Applications patched by IntPatch have negligible runtime performance losses which are on average 1%. 
id682#Research on a novel construction of probabilistic visual cryptography scheme (k,n,0,1,1)−PVCS for threshold access structures#In this paper, we propose new algorithms to construct a probabilistic visual cryptography scheme (k,n,0,1,1)−PVCS for threshold access structures based on the deterministic visual cryptography schemes (DVCS) and classify the probabilistic schemes into online and offline ones according to the reference mode of distribution column vectors. First in consideration of the relationship between DVCS and PVCS we propose a new method for expanded DVCS based on a matrix with binary vectors of Vnk as column vectors so as to construct PVCS without pixel expansion. Then, using the extended DVCS we suggest PVCS construction method which does not require storing of the distribution column vectors and delineate the relationship between our online PVCS and visual cryptograms of random grids (VCRG). We also propose a new infinite tree in order to generate binary vectors of Vnk. 
id683#Distributed Data Storage and Fusion for Collective Perception in Resource-Limited Mobile Robot Swarms#In this letter, we propose an approach to the distributed storage and fusion of data for collective perception in resource-limited robot swarms. We demonstrate our approach in a distributed semantic classification scenario. We consider a team of mobile robots, in which each robot runs a pre-trained classifier of known accuracy to annotate objects in the environment. We provide two main contributions: (i) a decentralized, shared data structure for efficient storage and retrieval of the semantic annotations, specifically designed for low-resource mobile robots; and (ii) a voting-based, decentralized algorithm to reduce the variance of the calculated annotations in presence of imperfect classification. We discuss theory and implementation of both contributions, and perform an extensive set of realistic simulated experiments to evaluate the performance of our approach. 
id685#Robot enhanced stroke therapy optimizes rehabilitation (RESTORE): a pilot study#Background: Robotic rehabilitation after stroke provides the potential to increase and carefully control dosage of therapy. Only a small number of studies, however, have examined robotic therapy in the first few weeks post-stroke. In this study we designed robotic upper extremity therapy tasks for the bilateral Kinarm Exoskeleton Lab and piloted them in individuals with subacute stroke. Pilot testing was focused mainly on the feasibility of implementing these new tasks, although we recorded a number of standardized outcome measures before and after training. Methods: Our team developed 9 robotic therapy tasks to incorporate feedback, intensity, challenge, and subject engagement as well as addressing both unimanual and bimanual arm activities. Subacute stroke participants were assigned to a robotic therapy (N = 9) or control group (N = 10) in a matched-group manner. The robotic therapy group completed 1-h of robotic therapy per day for 10 days in addition to standard therapy. The control group participated only in standard of care therapy. Clinical and robotic assessments were completed prior to and following the intervention. Clinical assessments included the Fugl-Meyer Assessment of Upper Extremity (FMA UE), Action Research Arm Test (ARAT) and Functional Independence Measure (FIM). Robotic assessments of upper limb sensorimotor function included a Visually Guided Reaching task and an Arm Position Matching task, among others. Paired sample t-tests were used to compare initial and final robotic therapy scores as well as pre- and post-clinical and robotic assessments. Results: Participants with subacute stroke (39.8 days post-stroke) completed the pilot study. Minimal adverse events occurred during the intervention and adding 1 h of robotic therapy was feasible. Clinical and robotic scores did not significantly differ between groups at baseline. Scores on the FMA UE, ARAT, FIM, and Visually Guided Reaching improved significantly in the robotic therapy group following completion of the robotic intervention. However, only FIM and Arm Position Match improved over the same time in the control group. Conclusions: The Kinarm therapy tasks have the potential to improve outcomes in subacute stroke. Future studies are necessary to quantify the benefits of this robot-based therapy in a larger cohort. Trial registration: ClinicalTrials.gov, NCT04201613, Registered 17 December 2019—Retrospectively Registered, https://clinicaltrials.gov/ct2/show/NCT04201613. 
id686#REDBUL: An Online System for Reverse Engineering of Relational Databases#The paper presents an online system named REDBUL, which is aimed at reverse engineering of relational databases. REDBUL enables database designers to automatically extract the schema from an existing relational database, and visualize it in a web browser, whereby the extracted schema is represented by the standard UML class diagram. Currently, two relational database management systems are supported by the REDBUL system, MS SQL and MySQL, while the paper illustrates reverse schema engineering for a MySQL database. 
id688#Recent developments of content-based image retrieval (CBIR)#With the development of Internet technology and the popularity of digital devices, Content-Based Image Retrieval (CBIR) has been quickly developed and applied in various fields related to computer vision and artificial intelligence. Currently, it is possible to retrieve related images effectively and efficiently from a large scale database with an input image. In the past ten years, great efforts have been made for new theories and models of CBIR and many effective CBIR algorithms have been established. In this paper, we present a survey on the fast developments and applications of CBIR theories and algorithms during the period from 2009 to 2019. We mainly review the technological developments from the viewpoint of image representation and database search. We further summarize the practical applications of CBIR in the fields of fashion image retrieval, person re-identification, e-commerce product retrieval, remote sensing image retrieval and trademark image retrieval. Finally, we discuss the future research directions of CBIR with the challenge of big data and the utilization of deep learning techniques. 
id689#Formal compiler construction in a logical framework#The task of designing and implementing a compiler can be a difficult and error-prone process. In this paper, we present a new approach based on the use of higher-order abstract syntax and term rewriting in a logical framework. All program transformations, from parsing to code generation, are cleanly isolated and specified as term rewrites. This has several advantages. The correctness of the compiler depends solely on a small set of rewrite rules that are written in the language of formal mathematics. In addition, the logical framework guarantees the preservation of scoping, and it automates many frequently-occurring tasks including substitution and rewriting strategies. As we show, compiler development in a logical framework can be easier than in a general-purpose language like ML, in part because of automation, and also because the framework provides extensive support for examination, validation, and debugging of the compiler transformations. The paper is organized around a case study, using the MetaPRL logical framework to compile an ML-like language to Intel x86 assembly. We also present a scoped formalization of x86 assembly in which all registers are immutable. 
id691#Updatable Block-Level Message-Locked Encryption#Deduplication is widely used for reducing the storage requirement for storage service providers. Nevertheless, it is unclear how to support deduplication of encrypted data securely until the study of Bellare et al. on message-locked encryption (MLE, Eurocrypt 2013). While updating (shared) files is natural, existing MLE solutions do not allow efficient update of encrypted files stored remotely. Even modifying a single bit requires the expensive way of downloading and decrypting a large ciphertext (then re-uploading). This paper initiates the study of updatable block-level MLE, a new primitive in incremental cryptography and cloud cryptography. Our proposed provably-secure construction is updatable with computation cost logarithmic in the file size. It naturally supports block-level deduplication. It also supports proof-of-ownership which protects storage providers from being abused as a free content distribution network. Our experiments show its practical performance relative to the original MLE and existing non-updatable block-level MLE. 
id692#Hierarchical Data Model Choosing in the Information Systems Design in Relational DBMS#Working with hierarchical data is one of the typical tasks in the industrial information systems development. As a repository of this data, relational databases are usually used. The performance and operability of the designed application depend on the correct choice of storage structure and hierarchical data processing mechanisms. To select the most suitable structure, it is necessary to formulate requirements for operations with nodes, on the basis of which a comparative assessment of the various mechanisms operation will be performed. In the future, taking into account the results and specific features, models can be chosen in favor of a particular technology. 
id693#Constraint-Based Software Diversification for Efficient Mitigation of Code-Reuse Attacks#Modern software deployment process produces software that is uniform, and hence vulnerable to large-scale code-reuse attacks. Compiler-based diversification improves the resilience and security of software systems by automatically generating different assembly code versions of a given program. Existing techniques are efficient but do not have a precise control over the quality of the generated code variants. This paper introduces Diversity by Construction (DivCon), a constraint-based compiler approach to software diversification. Unlike previous approaches, DivCon allows users to control and adjust the conflicting goals of diversity and code quality. A key enabler is the use of Large Neighborhood Search (LNS) to generate highly diverse assembly code efficiently. Experiments using two popular compiler benchmark suites confirm that there is a trade-off between quality of each assembly code version and diversity of the entire pool of versions. Our results show that DivCon allows users to trade between these two properties by generating diverse assembly code for a range of quality bounds. In particular, the experiments show that DivCon is able to mitigate code-reuse attacks effectively while delivering near-optimal code (optimality gap). For constraint programming researchers and practitioners, this paper demonstrates that LNS is a valuable technique for finding diverse solutions. For security researchers and software engineers, DivCon extends the scope of compiler-based diversification to performance-critical and resource-constrained applications. 
id694#Speck-R: An ultra light-weight cryptographic scheme for Internet of Things#Lightweight cryptography (LWC) is an interesting research area in the field of information security. Some limitations like: increased components usage, time consumption, power consumption and memory requirement mandate the need for lightweight cryptography. One of the proposed algorithms in this field is Speck which was designed by the National Security Agency (NSA) in June 2013. In this paper, we propose a new ultra-lightweight cryptographic algorithm based on Speck known as Speck-R. Speck-R is a hybrid cipher, combining ARX architecture with a dynamic substitution layer. The novelty in this paper resides in adding a key-dynamic substitution layer that changes according to a dynamic key. With this modification, the number of rounds can be reduced from 26 (in Speck) to 7 (in Speck-R). Thus, the main contribution of this paper consists in reducing the execution time of Speck by at least 18% on limited devices to reach a reduction of 77% while keeping a high level of security. To backbone Speck-R’s security, different security and statistical tests are exerted on Speck-R. In addition, a real hardware implementation on three different famous IoT devices is also presented where Speck-R outperformed Speck in terms of execution times. Finally, extensive tests show that Speck-R possesses the necessary criteria to be considered as a good cipher scheme that is suitable for lightweight devices. 
id695#Evolutionary Tabu Inverted Ant Cellular Automata with Elitist Inertia for swarm robotics as surrogate method in surveillance task using e-Puck architecture#The area of swarm robotics has grown widely in recent years, precisely because its formulation is based on the use of various techniques, ranging from computer networks to controllers. We can employ different types of techniques to carry out the control of a robots team. In this work we will focus on creating techniques based on bio-inspired computing. Within this theme, we will be focused on using cellular automata with synchronous and asynchronous rules and ant colonies optimization. Additionally, we will consider greedy approaches to select the next robot's state cell and a local Tabu search with a queue of robot movement restrictions. Thus, we have a surrogate model capable of providing the team robot navigation in the surveillance task. We developed two different controllers, a simpler first, based on a precursor model and a second optimized model, based on the previous controller refinement. At the end, we used a genetic algorithm, which received the surrogate model as input for the improvement of our proposed models parameters. In addition, a survey with the evolution of surveillance models using cellular automata in a systematic review of literature will be shown. Experiments were performed to demonstrate the degree of robot team coverage by different environments. We accomplished statistical analysis with the intention of presenting different sizes of robot teams and amounts of pheromone deposited into the environment. In the end, we fulfilled experiments using the empirical simulation methodology of a robots team using the Webots simulator with e-Puck architecture. The results were promising, the robot team performed this task efficiently and the system is highly scalable. 
id698#Predicting soft robot's locomotion fitness#Organisms with different body morphology and movement dynamics have distinct abilities to move through the environment. Despite such truism, there is a lack of general principles that predict which shapes and dynamics make the organisms more fit to move. Studying a minimal yet embodied soft robot model under the influence of gravity, we find three features that predict robot locomotion fitness: (1) A larger body is better. (2) Two-point contact with the ground is better than one-point contact. (3) Out-of-phase oscillating body parts increase locomotion fitness. These design principles can guide the selection rules for evolutionary algorithms to obtain robots with higher locomotion fitness. 
id701#On the Codd semantics of SQL nulls#Theoretical models used in database research often have subtle differences with those occurring in practice. One particular mismatch that is usually neglected concerns the use of marked nulls to represent missing values in theoretical models of incompleteness, while in an SQL database these are all denoted by the same syntactic (Figure presented.) object. It is commonly argued that results obtained in the model with marked nulls carry over to SQL, because SQL nulls can be interpreted as Codd nulls, which are simply marked nulls that do not repeat. This argument, however, does not take into account that even simple queries may produce answers where distinct occurrences of (Figure presented.) do in fact denote the same unknown value. For such queries, interpreting SQL nulls as Codd nulls would incorrectly change the semantics of query answers. To use results about Codd nulls for real-life SQL queries, we need to understand which queries preserve the Codd interpretation of SQL nulls. We show, however, that the class of relational algebra queries preserving Codd interpretation is not recursively enumerable, which necessitates looking for sufficient conditions for such preservation. Those can be obtained by exploiting the information provided by NOT NULL constraints on the database schema. We devise mild syntactic restrictions on queries that guarantee preservation, do not limit the full expressiveness of queries on databases without nulls, and can be checked efficiently. 
id702#A Side-Channel-Resistant Implementation of SABER#The candidates for the NIST Post-Quantum Cryptography standardization have undergone extensive studies on efficiency and theoretical security, but research on their side-channel security is largely lacking. This remains a considerable obstacle for their real-world deployment, where side-channel security can be a critical requirement. This work describes a side-channel-resistant instance of Saber, one of the lattice-based candidates, using masking as a countermeasure. Saber proves to be very efficient to masking due to two specific design choices: power-of-two moduli and limited noise sampling of learning with rounding. A major challenge in masking lattice-based cryptosystems is the integration of bit-wise operations with arithmetic masking, requiring algorithms to securely convert between masked representations. The described design includes a novel primitive for masked logical shifting on arithmetic shares and adapts an existing masked binomial sampler for Saber. An implementation is provided for an ARM Cortex-M4 microcontroller, and its side-channel resistance is experimentally demonstrated. The masked implementation features a 2.5x overhead factor, significantly lower than the 5.7x previously reported for a masked variant of NewHope. Masked key decapsulation requires less than 3,000,000 cycles on the Cortex-M4 and consumes less than 12kB of dynamic memory, making it suitable for deployment in embedded platforms. 
id704#Defining and evaluating heuristics for the compilation of constraint networks#Several branching heuristics for compiling in a top-down fashion finite-domain constraint networks into multi-valued decision diagrams (MDD) or decomposable multi-valued decision graphs (MDDG) are empirically evaluated, using the cn2mddg compiler. This MDDG compiler has been enriched with various additional branching rules. These rules can be gathered into two families, the one consisting of heuristics for the satisfaction problem (which are suited to compiling networks into MDD representations) and the family of heuristics favoring decompositions (which are relevant when the MDDG language is targeted). Our empirical investigation on a large dataset shows the value of decomposability (targeting MDDG allows for compiling many more instances and leads to much smaller compiled representations). The well-known (Dom/Wdeg) heuristics appears as the best choice for compiling networks into MDD. When MDDG is the target, a new rule, based on a dynamic, yet parsimonious use of hypergraph partitioning for the decomposition purpose turns out to be the best option. As expected, the best heuristics for the satisfaction problem perform better than the best heuristics favoring decompositions when MDD is targeted, and the converse is the case when MDDG is targeted. 
id705#QUARC: An optimized DSL framework using LLVM#We describe aspects of the implementation of QUARC, a framework layered on C++ used for a domain specific language for Lattice Quantum Chromodynamics. It is built on top of Clang/LLVM to leverage long term support and performance portability. QUARC implements a general array extension to C++ with implicit data parallelism. A notable innovation is the method for using templates to capture and encode the high-level abstractions and to communicate these abstractions transparently to LLVM through an unmodified Clang. Another notable feature is a general array transformation mechanism used to improve memory hierarchy performance and maximize opportunities for vectorization. This reshapes and transposes arrays of structures containing nested complex arrays into arrays of structures of arrays.We discuss an example for which QUARC generated code has performance competitive with the very best hand-optimized libraries. 
id706#Feel-Good Requirements: Neurophysiological and Psychological Design Criteria of Affective Touch for (Assistive) Robots#Previous research has shown the value of the sense of embodiment, i.e., being able to integrate objects into one's bodily self-representation, and its connection to (assistive) robotics. Especially, tactile interfaces seem essential to integrate assistive robots into one's body model. Beyond functional feedback, such as tactile force sensing, the human sense of touch comprises specialized nerves for affective signals, which transmit positive sensations during slow and low-force tactile stimulations. Since these signals are extremely relevant for body experience as well as social and emotional contacts but scarcely considered in recent assistive devices, this review provides a requirement analysis to consider affective touch in engineering design. By analyzing quantitative and qualitative information from engineering, cognitive psychology, and neuroscienctific research, requirements are gathered and structured. The resulting requirements comprise technical data such as desired motion or force/torque patterns and an evaluation of potential stimulation modalities as well as their relations to overall user experience, e.g., pleasantness and realism of the sensations. This review systematically considers the very specific characteristics of affective touch and the corresponding parts of the neural system to define design goals and criteria. Based on the analysis, design recommendations for interfaces mediating affective touch are derived. This includes a consideration of biological principles and human perception thresholds which are complemented by an analysis of technical possibilities. Finally, we outline which psychological factors can be satisfied by the mediation of affective touch to increase acceptance of assistive devices and outline demands for further research and development. 
id707#Show, tell and summarise: learning to generate and summarise radiology findings from medical images#Radiology plays a vital role in health care by viewing the human body for diagnosis, monitoring, and treatment of medical problems. In radiology practice, radiologists routinely examine medical images such as chest X-rays and describe their findings in the form of radiology reports. However, this task of reading medical images and summarising its insights is time consuming, tedious, and error-prone, which often represents a bottleneck in the clinical diagnosis process. A computer-aided diagnosis system which can automatically generate radiology reports from medical images can be of great significance in reducing workload, reducing diagnostic errors, speeding up clinical workflow, and helping to alleviate any shortage of radiologists. Existing research in radiology report generation focuses on generating the concatenation of the findings and impression sections. Also, existing work ignores important differences between normal and abnormal radiology reports. The text of normal and abnormal reports differs in style and it is difficult for a single model to learn both the text style and learn to transition from findings to impression. To alleviate these challenges, we propose a Show, Tell and Summarise model that first generates findings from chest X-rays and then summarises them to provide impression section. The proposed work generates the findings and impression sections separately, overcoming the limitation of previous research. Also, we use separate models for generating normal and abnormal radiology reports which provide true insight of model’s performance. Experimental results on the publicly available IU-CXR dataset show the effectiveness of our proposed model. Finally, we highlight limitations in the radiology report generation research and present recommendations for future work. 
id708#Self-supervised monocular depth estimation with occlusion mask and edge awareness#Depth estimation is one of the basic and important tasks in 3D vision. Recently, many works have been done in self-supervised depth estimation based on geometric consistency between frames. However, these research works still have difficulties in ill-posed areas, such as occlusion areas and texture-less areas. This work proposed a novel self-supervised monocular depth estimation method based on occlusion mask and edge awareness to overcome these difficulties. The occlusion mask divides the image into two classes, making the training of the network more reasonable. The edge awareness loss function is designed based on the edge obtained by the traditional method, so that the method has strong robustness to various lighting conditions. Furthermore, we evaluated the proposed method on the KITTI datasets. The occlusion mask and edge awareness are both beneficial to find corresponding points in ill-posed areas. 
id709#Hyperion: A Robust Drone-based Target Tracking System#Recent advances in mobile computing and embedded systems have had a transformative impact in the field of Unmanned Aircraft Systems (UASs). These advances have unlocked a great number of new capabilities ranging from realtime machine learning for detection and tracking of targets, and autonomous intelligent control for mission-critical trajectory planning. This paper introduces Hyperion, a robust detect, track and follow algorithm for UASs, leveraging and integrating various computer vision techniques and a combination of two proportional integral derivative (PID) controllers for following a moving vehicle. The proposed system is evaluated under real settings using off-the-shelf hardware and an elaborated comparison is made with a variety of state-of-the-art trackers available in the OpenCV library. 
id710#Implementing OpenMP for clusters on top of MPI#llc is a language designed to extend OpenMP to distributed memory systems. Work in progress on the implementation of a compiler that translates llc code and targets distributed memory platforms is presented. Our approach generates code for communications directly on top of MPI, We present computational results for two different benchmark applications on a PC-cluster platform. The results reflect similar performances for the He compiled version and an ad-hoc MPI implementation, even for applications with fine-grain parallelism. 
id711#Towards semantic integration of machine vision systems to aid manufacturing event understanding#A manufacturing paradigm shift from conventional control pyramids to decentralized, service-oriented, and cyber-physical systems (CPSs) is taking place in today’s 4th industrial revolution. Generally accepted roles and implementation recipes of cyber systems are expected to be standardized in the future of manufacturing industry. The authors intend to develop a novel CPS-enabled control architecture that accommodates: (1) intelligent information systems involving domain knowledge, empirical model, and simulation; (2) fast and secured industrial communication networks; (3) cognitive automation by rapid signal analytics and machine learning (ML) based feature extraction; (4) interoperability between machine and human. Semantic integration of process indicators is fundamental to the success of such implementation. This work proposes an automated semantic integration of data-intensive process signals that is deployable to industrial signal-based control loops. The proposed system rapidly infers manufacturing events from image-based data feeds, and hence triggers process control signals. Two image inference approaches are implemented: cloud-based ML model query and edge-end object shape detection. Depending on use cases and task requirements, these two approaches can be designated with different event detection tasks to provide a comprehensive system self-awareness. Coupled with conventional industrial sensor signals, machine vision system can rapidly understand manufacturing scenes, and feed extracted semantic information to a manufacturing ontology developed by either expert or ML-enabled cyber systems. Moreover, extracted signals are interpreted by Programmable Logical Controllers (PLCs) and field devices for cognitive automation towards fully autonomous industrial systems. 
id712#Automatic Recognition and Augmentation of Attended Objects in Real-Time using Eye Tracking and a Head-mounted Display#Scanning and processing visual stimuli in a scene is essential for the human brain to make situation-Aware decisions. Adding the ability to observe the scanning behavior and scene processing to intelligent mobile user interfaces can facilitate a new class of cognition-Aware user interfaces. As a first step in this direction, we implement an augmented reality (AR) system that classifies objects at the user's point of regard, detects visual attention to them, and augments the real objects with virtual labels that stick to the objects in real-Time. We use a head-mounted AR device (Microsoft HoloLens 2) with integrated eye tracking capabilities and a front-facing camera for implementing our prototype. 
id713#Recognition of visual-related non-driving activities using a dual-camera monitoring system#For a Level 3 automated vehicle, according to the SAE International Automation Levels definition (J3016), the identification of non-driving activities (NDAs) that the driver is engaging with is of great importance in the design of an intelligent take-over interface. Much of the existing literature focuses on the driver take-over strategy with associated Human-Machine Interaction design. This paper proposes a dual-camera based framework to identify and track NDAs that require visual attention. This is achieved by mapping the driver's gaze using a nonlinear system identification approach, on the object scene, recognised by a deep learning algorithm. A novel gaze-based region of interest (ROI) selection module is introduced and contributes about a 30% improvement in average success rate and about a 60% reduction in average processing time compared to the results without this module. This framework has been successfully demonstrated to identify five types of NDA required visual attention with an average success rate of 86.18%. The outcome of this research could be applicable to the identification of other NDAs and the tracking of NDAs within a certain time window could potentially be used to evaluate the driver's attention level for both automated and human-driving vehicles. 
id715#Taxonomy, state-of-the-art, challenges and applications of visual understanding: A review#Since the dawn of Humanity, to communicate both abstract and concrete ideas, visualization through visual imagery has been an effective way. With the advancement of scientific technologies, vision has been imparted to machines like humans do. Computer vision give ability to machines, to receive and analyze visual data on its own, and then make decisions about it, hence computer vision is more than machine learning applied. So, visualization of computer models to learn without being explicitly programmed using machine learning algorithms is called Visual learning. This work aims to review the state-of-the-art in computer vision by highlighting the contributions, challenges and applications. We first provide an overview of important visual learning approaches and their recent developments, and then describes their applications in diverse vision tasks, such as image classification, object detection, object recognition, visual saliency detection, semantic and instance segmentation, human pose estimation and image retrieval. Hardware constraints are also highlighted for better understanding of model selection. Finally, some important challenges, trends and outlooks are also discussed for better design and training of learning modules, along with several directions that may be further explored in the future. 
id716#An encrypted multitone modulation method for physical layer security based on chaotic cryptography#This paper proposes an encrypted multitone modulation method to achieve low probability of being deciphered for physical layer security (PLS) of communication based on chaotic cryptography. The main objective of the proposed method is to map random, unpredictable, many-to-one relations between a multitone group (MTG) and baseband data. After the baseband data are mapped to several types of MTGs within the available frequency band with the mapping relation, the transmitter first determines the MTG type according to the data to be sent, and then the definite MTG is selected according to the MTG index sequence. Obviously, under this modulation scheme, the mapping relationship and MTG index sequence determine the distribution of frequency points. Inspired by the characteristics of this modulation, the long-term statistical undifferentiated features of different types of MTGs are obtained by encrypting the mapping relationships, and then the index sequence is designed to realize a roughly equal probability of being used for each MTG type. To accurately decipher the baseband data information, the cooperative receiver needs to master the mapping relationship and related secret keys. After establishing the communication system model under this modulation mode, the anti-non-cooperative reception deciphering ability of the system and its performance are analyzed theoretically. The reliability and concealment of the proposed method are experimentally verified and compared with the reliability and security of the orthogonal frequency-division multiplexing- (OFDM) quadrature phase-shift keying (QPSK) and differential frequency hopping (DFH) modulation methods under the same number of subcarriers and channel environments. 
id717#An improved minimal path selection approach with new strategies for pavement crack segmentation#Intelligent pavement detection technology provides a new research idea for identifying pavement cracks. To solve the problem that traditional minimal cost path selection (MPS) is computationally intensive and inefficient in crack segmentation, this study proposes an improved minimal cost path algorithm (IMPS). The contribution of IMPS is reflected in its more efficient path planning than MPS and better choice of source and destination of each path by using new strategies. First, a grayscale pavement image is divided into nonoverlapping cells with a certain size. Next, the darkest points in each cell are selected as candidate points and adjacent candidate points are connected based on IMPS. Then pseudocracks are removed based on postprocessing steps before cracks are well segmented. IMPS is compared with other crack segmentation algorithms to prove its reliability. The efficiency of IMPS is increased by approximately 56% than MPS. The accuracy of IMPS is higher than that of MPS and other methods, with Mean Intersection over Union reaching 73.99%. 
id718#Extended visual cryptography-based copyright protection scheme for multiple images and owners using LBP–SURF descriptors#Existing visual cryptography (VC)-based copyright protection schemes (Amiri and Mohaddam in Multimed Tools Appl 75(14):8527–8543, 2016; Liu and Wu in IET Inf Secur 5(2):121–128, 2011) for multiple images provide meaningless shares to the owners. These shares create a suspicion that some secret information is shared. Also, these schemes require the share of every owner to prove the copyright. If any of the ownership share is not available, the copyright of these owners cannot be verified. This makes the usage of schemes restricted. To address these issues, an extended visual cryptography-based copyright protection scheme is proposed for multiple images with multiple owners. This scheme provides meaningful ownership shares to the owners, and their copyright can be verified by using a qualified set of owner shares. In this scheme, three types of shares are used, i.e., master share, ownership share and key share. The proposed scheme ensures robustness against different geometrical attacks, especially the rotation attack, as LBP and SURF together represent the host image efficiently. There is no restriction on watermark size, as SURF gives a flexibility to select any number of feature points. Usage of LBP ensures no false positive cases. Each of the ownership shares is created using the master share and the watermark. The ownership share is used to create a key share which is stored with the Trusting Authority (TA). To prove the copyright of multiple images, the ownership images and key share are superimposed to retrieve the watermark. The experimental results show that the scheme clearly verifies the copyright of digital images and is robust against several image processing attacks while having high imperceptibility. Comparisons with the existing copyright protection schemes show better performance of the proposed scheme. 
id721#Feature extraction techniques in facial expression recognition#Facial expression recognition became a very important and very challenging aera within past few years in computer science vision field. It (FER) a very hot and active topic for research. FER play interesting role in aera of computer vision processing. This paper reviews the different algorithms which is used in facial expression recognition. usually, Facial expression recognition has 3 Majour stages first one is preprocessing I.e., face detection second is Feature extraction or selection (the aim of this paper) and last one is classification. In this paper study we review some current work done for Facial expression recognition technique. we know that feature extraction method is very important step for image recognition so in this paper we see that after the preprocessing the images how feature extraction done, we see some technique like local binary pattern (LBP), Gabor filter etc. And also, this paper gives an evaluation of Majour characteristic feature extraction and based on that their performance in terms of accuracy and them limitations. arrangement of this FER paper is following in order: section I contain the introduction section II contains a sample process of facial expression recognition section III contains the different technique of feature extraction and section IV contains the review discussion and last section V contain the conclusion.to making these algorithms efficient and more effective feature extraction technique played an important role. 
id722#Real Time Fire detection and Localization in Video sequences using Deep Learning framework for Smart Building#This work presents autonomous electrical fire-detection and localization using computer vision based techniques. The proposed work uses YOLO v2 to extract the electrical fire features more effectively than other conventional and machine learning approaches. This working model is tested on commercial and residential building as well as indoor and outdoor environments. This framework has achieved high detection accuracy and low false alarm rate. Besides, the proposed frame work can be used for early real-time electrical fire detection in surveillance videos and we present experimental results for electrical fire localization in CCTV footage using the deep learning architecture proposed in this work. 
id723#Certified concurrent abstraction layers#Concurrent abstraction layers are ubiquitous in modern computer systems because of the pervasiveness of multithreaded programming and multicore hardware. Abstraction layers are used to hide the implementation details (e.g., fine-grained synchronization) and reduce the complex dependencies among components at different levels of abstraction. Despite their obvious importance, concurrent abstraction layers have not been treated formally. This severely limits the applicability of layer-based techniques and makes it difficult to scale verification across multiple concurrent layers. In this paper, we present CCAL - -a fully mechanized programming toolkit developed under the CertiKOS project - -for specifying, composing, compiling, and linking certified concurrent abstraction layers. CCAL consists of three technical novelties: a new game-theoretical, strategy-based compositional semantic model for concurrency (and its associated program verifiers), a set of formal linking theorems for composing multithreaded and multicore concurrent layers, and a new CompCertX compiler that supports certified thread-safe compilation and linking. The CCAL toolkit is implemented in Coq and supports layered concurrent programming in both C and assembly. It has been successfully applied to build a fully certified concurrent OS kernel with fine-grained locking. 
id724#Towards Enhancing Database Education: Natural Language Generation Meets Query Execution Plans#The database systems course is offered as part of an undergraduate computer science degree program in many major universities. A key learning goal of learners taking such a course is to understand how sql queries are processed in a rdbms in practice. Since aquery execution plan (qep ) describes the execution steps of a query, learners can acquire the understanding by perusing the qep s generated by a rdbms. Unfortunately, in practice, it is often daunting for a learner to comprehend these qep s containing vendor-specific implementation details, hindering her learning process. In this paper, we present a novel, end-to-end,generic system called lantern that generates a natural language description of a qep to facilitate understanding of the query execution steps. It takes as input an sql query and its qep, and generates a natural language description of the execution strategy deployed by the underlying rdbms. Specifically, it deploys adeclarative framework called pool that enablessubject matter experts to efficiently create and maintain natural language descriptions of physical operators used in qep s. Arule-based framework called rule-lantern is proposed that exploits pool to generate natural language descriptions of qep s. Despite the high accuracy of rule-lantern, our engagement with learners reveal that, consistent with existing psychology theories, perusing such rule-based descriptions lead toboredom due to repetitive statements across different qep s. To address this issue, we present a noveldeep learning-based language generation framework called neural -lantern that infuses language variability in the generated description by exploiting a set ofparaphrasing tools andword embedding. Our experimental study with real learners shows the effectiveness of lantern in facilitating comprehension of qep s. 
id725#Brain–computer interface robotics for hand rehabilitation after stroke: a systematic review#Background: Hand rehabilitation is core to helping stroke survivors regain activities of daily living. Recent studies have suggested that the use of electroencephalography-based brain-computer interfaces (BCI) can promote this process. Here, we report the first systematic examination of the literature on the use of BCI-robot systems for the rehabilitation of fine motor skills associated with hand movement and profile these systems from a technical and clinical perspective. Methods: A search for January 2010–October 2019 articles using Ovid MEDLINE, Embase, PEDro, PsycINFO, IEEE Xplore and Cochrane Library databases was performed. The selection criteria included BCI-hand robotic systems for rehabilitation at different stages of development involving tests on healthy participants or people who have had a stroke. Data fields include those related to study design, participant characteristics, technical specifications of the system, and clinical outcome measures. Results: 30 studies were identified as eligible for qualitative review and among these, 11 studies involved testing a BCI-hand robot on chronic and subacute stroke patients. Statistically significant improvements in motor assessment scores relative to controls were observed for three BCI-hand robot interventions. The degree of robot control for the majority of studies was limited to triggering the device to perform grasping or pinching movements using motor imagery. Most employed a combination of kinaesthetic and visual response via the robotic device and display screen, respectively, to match feedback to motor imagery. Conclusion: 19 out of 30 studies on BCI-robotic systems for hand rehabilitation report systems at prototype or pre-clinical stages of development. We identified large heterogeneity in reporting and emphasise the need to develop a standard protocol for assessing technical and clinical outcomes so that the necessary evidence base on efficiency and efficacy can be developed. 
id726#Facilitating learning by practice and examples: A tool for learning table normalization#We describe the design and evaluation of a web-based tool to help students learn database normalization, which is an important topic in relational database design. Compared with existing systems, our tool has the advantage of allowing a user to practice with as many examples as he/she likes, at all possible levels of difculty, showing the detailed steps for each solution, and allowing users to upload existing examples and saving their own examples for later reference. Our user survey and observation indicate that the tool is well liked by those who used it, and the tool has impacted them positively in their learning. 
id727#Real-time railroad track components inspection based on the improved YOLOv4 framework#According to the Federal Railroad Administration (FRA) database, track component failure is one of the major factors causing train accidents. To improve railroad safety and reduce accident occurrence, tracks need to be regularly inspected. Many computer-aided track inspection methods have been introduced over the past decades, however, inspecting missing or broken track components still heavily relies on manual inspections. To address those issues, this study proposes a real-time and cost-effective computer vision-based framework to inspect track components quickly and efficiently. The cutting-edge convolutional neural network, YOLOv4 is improved trained, and evaluated based on the images in a public track components image database. Compared with other one-stage object detection models, the customized YOLOv4-hybrid model can achieve 94.4 mean average precision (mAP) and 78.7 frames per second (FPS), which outperforms other models in terms of both accuracy and processing speed. It paves the way for developing portable and high-speed track inspection tools to reduce track inspection cost and improve track safety. 
id728#Machine learning paradigm for structural health monitoring#Structural health diagnosis and prognosis is the goal of structural health monitoring. Vibration-based structural health monitoring methodology has been extensively investigated. However, the conventional vibration–based methods find it difficult to detect damages of actual structures because of a high incompleteness in the monitoring information (the number of sensors is much fewer with respect to the number of degrees of freedom of a structure), intense uncertainties in the structural conditions and monitoring systems, and coupled effects of damage and environmental actions on modal parameters. It is a truth that the performance and conditions of a structure must be embedded in the monitoring data (vehicles, wind, etc.; acceleration, displacement, cable force, strain, images, videos, etc.). Therefore, there is a need to develop completely novel structural health diagnosis and prognosis methodology based on the various monitoring data. Machine learning provides the advanced mathematical frameworks and algorithms that can help discover and model the performance and conditions of a structure through deep mining of monitoring data. Thus, machine learning takes an opportunity to establish novel machine learning paradigm for structural health diagnosis and prognosis theory termed the machine learning paradigm for structural health monitoring. This article sheds light on principles for machine learning paradigm for structural health monitoring with some examples and reviews the existing challenges and open questions in this field. 
id729#A novel cryptosystem based on DNA cryptography and randomly generated mealy machine#Nowadays, the amount of data produced and stored in computing devices is increasing at an alarming rate. Tremendous amounts of critical and sensitive data are transmitted between all these devices. Thus, it is very imperative to guarantee the security of all these indispensable data. Cryptography is a commonly used technique to ensure data security. The fundamental objective of cryptography is to transmit data from the sender to the receiver in the most secure way, so that an attacker is unable to extract the original data content. This paper proposes a novel cryptosystem based on Deoxyribonucleic Acid (DNA) cryptography and finite automata theory. The system is made of three entities, namely a key pair generator, a sender and a receiver. The sender generates a 256-bit DNA based secret key based on the attributes of the receiver, and this key is used for data encryption. Then, a randomly generated Mealy machine is used for coding the DNA sequence, which makes the ciphertext more secure. The proposed scheme can protect the system against numerous security attacks, such as brute force attack, known plaintext attack, differential cryptanalysis attack, cipher text only attack, man-in-the-middle attack and phishing attack. The results and discussions show that the proposed scheme is efficient and secure than the existing schemes. 
id730#Improved YOLOv3 Model for miniature camera detection#The abuse of miniature cameras has severely violated information security and privacy. Several unsolved challenges such as small or multiple targets detection as well as complex background environments in terms of active laser detection still exist, resulting in difficulties in its practical application. In this paper, an active laser detection system is proposed to obtain high-intensity cat-eye reflection images. An improved YOLOv3 model, YOLOv3-4L, was introduced to detect the actual position of the target. In the YOLOv3-4L model, each image was resized to 608×608 to preserve image details. The scales of prediction were increased from three to four, and an additional feature map was used to extract more details. YOLOv3-4L exhibited excellent performance in detecting small targets. The experimental results show that the mean average precision achieved by YOLOv3-4L was 90.37%, compared to the 85.41%,87.81%, and 88.97% achieved by traditional YOLOv3, Faster R-CNN, and Single Shot Multi-box Detector respectively. The speed of the YOLOv3-4L model meets the requirements of real-time target detection. 
id731#A portable three-component displacement measurement technique using an unmanned aerial vehicle (UAV) and computer vision: A proof of concept#This study proposes a new remote sensing technique to measure three-component (3C) dynamic displacement of three-dimensional (3D) structures. A sensing system with a UAV platform and contact-free sensors (e.g., optical and infrared (IR) cameras) is employed to provide a portable and convenient alternative to conventional approaches that require sensor installation on a structure. The original contributions of this study include (1) integrating both optical and IR cameras with a UAV platform to measure dynamic structural response, and (2) developing new data post-processing algorithms (based on target identification, Direct Linear Transformation, and active stereo vision) to simultaneously extract the 3C displacement of a 3D structure from optical and IR videos, which presents a unique advantage compared to the existing UAV-based displacement measurement techniques that allow the measurements in only one or two directions using optical cameras or laser sensors. The efficacy of the proposed technique is validated through laboratory experiments. 
id732#Biodiversity in evolved voxel-based soft robots#In many natural environments, there are different forms of living creatures that successfully accomplish the same task while being diverse in shape and behavior. This biodiversity is what made life capable of adapting to disrupting changes. Being able to reproduce biodiversity in non-biological agents, while still optimizing them for a particular task, might increase their applicability to scenarios where human response to unexpected changes is not possible. In this work, we focus on Voxel-based Soft Robots (VSRs), a form of robots that grants great freedom in the design of both body and controller and is hence promising in terms of biodiversity. We use evolutionary computation for optimizing, at the same time, body and controller of VSRs for the task of locomotion. We investigate experimentally whether two key factors - -evolutionary algorithm (EA) and representation - -impact the emergence of biodiversity and if this occurs at the expense of effectiveness. We devise a way for measuring biodiversity, systematically characterizing the robots shape and behavior, and apply it to the VSRs evolved with three EAs and two representations. The experimental results suggest that the representation matters more than the EA and that there is not a clear trade-off between diversity and effectiveness. 
id733#Visual Structure Constraint for Transductive Zero-Shot Learning in the Wild#To recognize objects of the unseen classes, most existing Zero-Shot Learning(ZSL) methods first learn a compatible projection function between the common semantic space and the visual space based on the data of source seen classes, then directly apply it to the target unseen classes. However, for data in the wild, distributions between the source and target domain might not match well, thus causing the well-known domain shift problem. Based on the observation that visual features of test instances can be separated into different clusters, we propose a new visual structure constraint on class centers for transductive ZSL, to improve the generality of the projection function (i.e.alleviate the above domain shift problem). Specifically, three different strategies (symmetric Chamfer-distance, Bipartite matching distance, and Wasserstein distance) are adopted to align the projected unseen semantic centers and visual cluster centers of test instances. We also propose two new training strategies to handle the data in the wild, where many unrelated images in the test dataset may exist. This realistic setting has never been considered in previous methods. Extensive experiments demonstrate that the proposed visual structure constraint brings substantial performance gain consistently and the new training strategies make it generalize well for data in the wild. The source code is available at https://github.com/raywzy/VSC. 
id734#Coping with Incomplete Data: Recent Advances#Handling incomplete data in a correct manner is a notoriously hard problem in databases. Theoretical approaches rely on the computationally hard notion of certain answers, while practical solutions rely on ad hoc query evaluation techniques based on three-valued logic. Can we find a middle ground, and produce correct answers efficiently? The paper surveys results of the last few years motivated by this question. We re-examine the notion of certainty itself, and show that it is much more varied than previously thought. We identify cases when certain answers can be computed efficiently and, short of that, provide deterministic and probabilistic approximation schemes for them. We look at the role of three-valued logic as used in SQL query evaluation, and discuss the correctness of the choice, as well as the necessity of such a logic for producing query answers. 
id735#Electromechanics of planar HASEL actuators#Soft actuators promise to expand the capabilities of conventional robots, allowing them to navigate unstructured terrain and to safely interact with humans. HASEL (hydraulically amplified, self-healing, electrostatic) actuators are a class of soft actuators that feature direct electrical activation via Maxwell stress, electrical self-healing, and fast actuation. Planar HASEL actuators, a subset of HASELs that expand in-plane upon activation, comprise a stretchable dielectric shell that is coated with compliant electrodes and filled with a liquid dielectric. While planar HASEL actuators have demonstrated strong experimental performance, the details of the underlying electromechanics are yet to be explored. In experiments, two mechanisms of deformation are observed: elastic stretching and electrohydraulic “zipping”. This letter analyzes these mechanisms using two examples: a circular planar HASEL actuator that stretches equibiaxially, and a linear actuator that both stretches and zips. We use an energy minimization approach to derive nonlinear electromechanical models for their quasistatic actuation behavior. The analysis shows that the actuation behavior of circular HASEL actuators is similar to that of dielectric elastomer actuators (DEAs), and reveals how the added liquid layer in planar HASELs reduces their stiffness, allowing them to achieve greater strains than DEAs of the same dimensions. For the linear actuator, the model displays how the actuator only stretches until it reaches a critical voltage at which it starts to zip, drastically increasing strain. This work lays the foundation for the theoretical analysis of planar HASEL actuators, which consist of stretchable materials. 
id736#The design and implementation of the DVS based dynamic compiler for power reduction#Recent years, as the wide deployment of embedded and mobile devices, reducing the power consumption in order to extend the battery life becomes a major factor that a designer must consider when designing a new architecture. DVS is regarded as one of the most effective power reduction techniques. This paper focuses on run-time compiler driven DVS for power reduction, especially two key design issues including DVS analysis model and DVS decision algorithm. Based on the design framework presented in this work, we also implement a run-time DVS compiler which is fine-grained, adaptive to the program's running environment without changing its behavior. The obtained system is deployed in a real hardware platform. Experimental results, based on some benchmarks, show that with average 5% performance loss, the benchmarks benefit with 26% dynamic power savings and the energy delay product (EDP) improvement is 22%. 
id737#Workspace Boundary Estimation for Soft Manipulators Using a Continuation Approach#Considering a soft manipulator configuration, controlled via installed bounded actuators, this paper addresses the end-effector workspace estimation problem for such a soft robot. For this, the Discrete Cosserat method is adopted to deduce the mathematical model of soft manipulators, based on which a continuation method that accounts for simple and multiple bifurcation points to solution curves is developed to map its workspace boundaries. Difficulties encountered in calculating tangents at simple and multiple bifurcation points are studied, and an efficient solution is provided. Numerical simulations applied to planar and spatial soft manipulator configurations are presented to emphasize the validity of the proposed methodology. 
id738#Semi-supervised anomaly detection for visual quality inspection#In this paper a semi-supervised method for the detection of anomalies in both texture- and object-based product images is presented. The method exploits a pre-trained Convolutional Neural Network (CNN) autoencoder that is blended with a statistical-based transformation of the neural network embedding layer in order to remove anomalies from the input image. The “cleaned” version of the input image is then compared with the input image itself in order to spatially localize the anomalies. The method does not require a specific training of the CNN to be applied to a new class of product, but it requires a very fast domain adaptation based on only “anomaly-free” examples. Experiments conducted on a publicly available dataset made of fifteen texture- and object-based classes show that overall performance is better than the state of the art of about 4%. In the case of texture-based classes the proposed method outperforms the state of the art of about 13%. In the case of object-based classes, the proposed method reaches overall the same performance of the state of the art. In this case, apart from 3 cases, that is “bottle”, “transistor” and “metal nut”, the proposed method performs better than the state of the art in 7 object classes out of 10. 
id739#Comparative Study of Robotics Curricula#Contribution: The information described in this study provides a starting point for discussing an effective robotics curriculum offered by any engineering university or institute. Background: Robotics is a multidisciplinary field that includes mechanical engineering, electrical and electronic engineering, and computer science. Several universities have established departments of robotics to teach their students robotics education; however, a comprehensive curriculum to deliver robotics education has not yet been fully developed. Research Questions: What are the significant component courses offered by existing departments of robotics? Methodology: This article investigates component courses among departments of robotics through a title-based aggregation of existing courses and textual analyses. Findings: From a title-based aggregation of robotics curricula from 19 departments established by 2018, control engineering, programming, technical drawing/design, and electronic engineering/electronic circuits were found to be the core courses. From textual analyses of the curricula, robotics departments' course titles were similar to those of mechanical engineering departments. 
id740#Automatic visual attention detection for mobile eye tracking using pre-trained computer vision models and human gaze#Processing visual stimuli in a scene is essential for the human brain to make situation-aware decisions. These stimuli, which are prevalent subjects of diagnostic eye tracking studies, are commonly encoded as rectangular areas of interest (AOIs) per frame. Because it is a tedious manual annotation task, the automatic detection and annotation of visual attention to AOIs can accelerate and objectify eye tracking research, in particular for mobile eye tracking with egocentric video feeds. In this work, we implement two methods to automatically detect visual attention to AOIs using pre-trained deep learning models for image classification and object detection. Furthermore, we develop an evaluation framework based on the VISUS dataset and well-known performance metrics from the field of activity recognition. We systematically evaluate our methods within this framework, discuss potentials and limitations, and propose ways to improve the performance of future automatic visual attention detection methods. 
id741#Facesight: Enabling hand-to-face gesture interaction on ar glasses with a downward-facing camera vision#We present FaceSight, a computer vision-based hand-to-face gesture sensing technique for AR glasses. FaceSight fxes an infrared camera onto the bridge of AR glasses to provide extra sensing capability of the lower face and hand behaviors.We obtained 21 hand-to-face gestures and demonstrated the potential interaction benefts through fve AR applications. We designed and implemented an algorithm pipeline that segments facial regions, detects hand-face contact (f1 score: 98.36%), and trains convolutional neural network (CNN) models to classify the hand-to-face gestures. The input features include gesture recognition, nose deformation estimation, and continuous fngertip movement. Our algorithm achieves classifcation accuracy of all gestures at 83.06%, proved by the data of 10 users. Due to the compact form factor and rich gestures, we recognize FaceSight as a practical solution to augment input capability of AR glasses in the future. 
id742#Miniature Tangible Cube: Concept and Design of Target-Object-Oriented User Interface for Dual-Arm Telemanipulation#In recent years, there has been a great deal of research on teleoperation of robots, and many end-effector-oriented control systems have been proposed, but these systems have difficulties in performing manipulation tasks with physical contacts between the target object, the robot, and the environment, such as dual-arm manipulation, because these systems can only express the end-effectors' pose and force commands. In this study, we propose Target-object-oriented User Interface (TOOUI) that focuses on the target object rather than the end effectors of the robot, and develop a tangible device called Miniature Tangible Cube as a TOOUI. Through this device, the pilot can express the target orientation and contact state of the target object, and the robot can be teleoperated to do dual-arm manipulation tasks with geometric and physical constraints. For dual-arm manipulation tasks, we also implement a dual-arm telemanipulation system with the Miniature Tangible Cube using the motion planner based on state machine. In the experiments, we evaluated our system with the Miniature Tangible Cube by conducting dual-arm manipulation tasks with the dual-arm robot PR2 in the real world. 
id743#A new approach newton-raphson load flow analysis in power system networks with STATCOM#Load flow analysis is an important tool used by engineers to ensure stable operation of the power system, it is also used in load forecasting, planning, and economic scheduling. In this paper, we propose an improved solution of load flow analysis in the power systems incorporating STATCOM using the Newton-Raphson method. A software based on MATLAB Compiler Runtime (MCR) based on the proposed method is developed to analyze the power system load flow with and without STATCOM. The software has been tested on IEEE 30-bus, and IEEE 57-bus test system. The results have proved the correctness of the proposed method, reliability of the software and high computation speed. 
id744#Alternative Approaches to Data Storing and Processing#Most data processing applications store data using a Database Management System (DBMS). The most widely used DBEs belong to relational. In these engines, data is stored in a number of tables. One of the main relational DBMS interface technologies is Structured Query Language (SQL), but there are alternative approaches, like Query By Example (QBE). Since relational databases have poor performance on some types of data, there have appeared other approaches. They in unite called No-SQL databases, although these approaches are very different from each other. Graph-based and document-based DBMS belongs to them. In this paper, we describe common implementations of relational and No-SQL DBMS and compare their capabilities using a practical example. 
id745#Computer vision and photosensor based hybrid control strategy for a two-axis solar tracker - Daylighting application#This paper deals with a computer vision and photosensor based two-axis solar tracking system for an active daylighting system. The real-time image processing was performed by using a Raspberry Pi 4 controller, and processed data were used as an input of an ATmega128 microcontroller to track the sun's path. An active daylighting system requires a higher concentration of sunlight with an acceptable accuracy in different weather conditions. Alone, an image based or photosensor based control strategy cannot fulfill both requirements. Successful integration of two different feedback mechanisms could overcome the tracking difficulties. The system was composed of Raspberry Pi 4 and ATmega128 controllers, a camera, electronic circuits, and stepper motors. The proposed control system could distinguish the objects (e.g., sun or clouds) in front of the camera, and process the data required to run the solar tracker. The integration of extra photosensors and the camera was able to avoid the cloud disturbance. To investigate the efficacy of the solar tracker, it was integrated with the optical fiber cable to transmit harvested daylight for an indoor illumination. 
id746#A research on an improved Unet-based concrete crack detection algorithm#Crack is an important indicator for evaluating the damage level of concrete structures. However, traditional crack detection algorithms have complex implementation and weak generalization. The existing crack detection algorithms based on deep learning are mostly window-level algorithms with low pixel precision. In this article, the CrackUnet model based on deep learning is proposed to solve the above problems. First, crack images collected from the lab, earthquake sites, and the Internet are resized, labeled manually, and augmented to make a dataset (1200 subimages with 256 × 256 × 3 resolutions in total). Then, an improved Unet-based method called CrackUnet is proposed for automated pixel-level crack detection. A new loss function named generalized dice loss is adopted to detect cracks more accurately. How the size of the dataset and the depth of the model affect the training time, detecting accuracy, and speed is researched. The proposed methods are evaluated on the test dataset and a previously published dataset. The highest results can reach 91.45%, 88.67%, and 90.04% on test dataset and 98.72%, 92.84%, and 95.44% on CrackForest Dataset for precision, recall, and F1 score, respectively. By comparing the detecting accuracy, the training time, and the information of datasets, CrackUnet model outperform than other methods. Furthermore, six images with complicated noise are used to investigate the robustness and generalization of CrackUnet models. 
id748#Automated generation of polyhedral process networks from affine nested-loop programs with dynamic loop bounds#The Process Networks (PNs) is a suitable parallel model of computation (MoC) used to specify embedded streaming applications in a parallel form facilitating the efficient mapping onto embedded parallel execution platforms. Unfortunately, specifying an application using a parallel MoC is a very difficult and highly error-prone task. To overcome the associated difficulties, we have developed the pn compiler, which derives specific Polyhedral Process Networks (PPN) parallel specifications from sequential static affine nested loop programs (SANLPs). However, there are many applications, for example, multimedia applications (MPEG coders/decoders, smart cameras, etc.) that have adaptive and dynamic behavior which cannot be expressed as SANLPs. Therefore, in order to handle dynamic multimedia applications, in this article we address the important question whether we can relax some of the restrictions of the SANLPs while keeping the ability to perform compile-time analysis and to derive PPNs. Achieving this would significantly extend the range of applications that can be parallelized in an automated way. The main contribution of this article is a first approach for automated translation of affine nested loop programs with dynamic loop bounds into input-output equivalent Polyhedral Process Networks. In addition, we present a method for analyzing the execution overhead introduced in the PPNs derived from programs with dynamic loop bounds. The presented automated translation approach has been evaluated by deriving a PPN parallel specification from a real-life application called Low Speed Obstacle Detection (LSOD) used in the smart cameras domain. By executing the derived PPN, we have obtained results which indicate that the approach we present in this article facilitates efficient parallel implementations of sequential nested loop programs with dynamic loop bounds. That is, our approach reveals the possible parallelism available in such applications, which allows for the utilization of multiple cores in an efficient way. 
id749#The Effect of Robotic Assisted Gait Training With Lokomat® on Balance Control After Stroke: Systematic Review and Meta-Analysis#Introduction: Disturbances of balance control are common after stroke, affecting the quality of gait and increasing the risk of falls. Because balance and gait disorders may persist also in the chronic stage, reducing individual independence and participation, they represent primary goals of neurorehabilitation programs. For this purpose, in recent years, numerous technological devices have been developed, among which one of the most widespread is the Lokomat®, an actuated exoskeleton that guide the patient's limbs, simulating a symmetrical bilateral gait. Preliminary evidence suggests that beyond gait parameters, robotic assisted gait training may also improve balance. Therefore, the aim of this systematic review was to summarize evidence about the effectiveness of Lokomat® in improving balance in stroke patients. Methods: Randomized controlled trials published between January 1989 and August 2020, comparing Lokomat® training to conventional therapy for stroke patients, were retrieved from seven electronic databases. Balance, assessed by means of validated clinical scales, was considered as outcome measure. The Physiotherapy Evidence Database (PEDro) scale was used to evaluate the methodological quality of the studies. The study protocol was registered on PROSPERO (no. CRD42020197531). Results: After the removal of the duplicates, according to the inclusion criteria, 13 studies were selected, involving 445 subacute or chronic stroke patients. Eleven papers contributed to three meta-analyses. Favorable results for recovery of balance in stroke survivors treated with Lokomat® were shown using Timed Up and Go (pooled mean difference = −3.40, 95% CI −4.35 to −2.44; p < 0.00001) and Rivermead Mobility Index as outcome measures (pooled mean difference = 0.40, 95% CI 0.26–0.55; p < 0.00001). Inconclusive results were found when balance was measured by means of the Berg Balance Scale (pooled mean difference = 0.17, 95% CI −0.26 to 0.60; p = 0.44). Conclusions: Overall, most studies have shown beneficial effects of Lokomat® on balance recovery for stroke survivors, at least comparable to conventional physical therapy. However, due to the limited number of studies and their high heterogeneity, further research is needed to draw more solid and definitive conclusions. 
id750#The State of Lifelong Learning in Service Robots:: Current Bottlenecks in Object Perception and Manipulation#Service robots are appearing more and more in our daily life. The development of service robots combines multiple fields of research, from object perception to object manipulation. The state-of-the-art continues to improve to make a proper coupling between object perception and manipulation. This coupling is necessary for service robots not only to perform various tasks in a reasonable amount of time but also to continually adapt to new environments and safely interact with non-expert human users. Nowadays, robots are able to recognize various objects, and quickly plan a collision-free trajectory to grasp a target object in predefined settings. Besides, in most of the cases, there is a reliance on large amounts of training data. Therefore, the knowledge of such robots is fixed after the training phase, and any changes in the environment require complicated, time-consuming, and expensive robot re-programming by human experts. Therefore, these approaches are still too rigid for real-life applications in unstructured environments, where a significant portion of the environment is unknown and cannot be directly sensed or controlled. In such environments, no matter how extensive the training data used for batch learning, a robot will always face new objects. Therefore, apart from batch learning, the robot should be able to continually learn about new object categories and grasp affordances from very few training examples on-site. Moreover, apart from robot self-learning, non-expert users could interactively guide the process of experience acquisition by teaching new concepts, or by correcting insufficient or erroneous concepts. In this way, the robot will constantly learn how to help humans in everyday tasks by gaining more and more experiences without the need for re-programming. In this paper, we review a set of previously published works and discuss advances in service robots from object perception to complex object manipulation and shed light on the current challenges and bottlenecks. 
id751#Combination of piezoelectric and triboelectric devices for robotic self-powered sensors#Sensors are an important part of the organization required for robots to perceive the exter-nal environment. Self-powered sensors can be used to implement energy-saving strategies in robots and reduce their power consumption, owing to their low-power consumption characteristics. The tri-boelectric nanogenerator (TENG) and piezoelectric transducer (PE) are important implementations of self-powered sensors. Hybrid sensors combine the advantages of the PE and TENG to achieve higher sensitivity, wider measurement range, and better output characteristics. This paper summarizes the principles and research status of pressure sensors, displacement sensors, and three-dimensional (3D) acceleration sensors based on the self-powered TENG, PE, and hybrid sensors. Additionally, the basic working principles of the PE and TENG are introduced, and the challenges and problems in the development of PE, TENG, and hybrid sensors in the robotics field are discussed with regard to the principles of the self-powered pressure sensors, displacement sensors, and 3D acceleration sensors applied to robots. 
id753#The data storage system for SHINE#Shanghai HIgh repetition rate XFEL aNd Extreme light facility (SHINE) is a quasi-continuous wave hard X-ray free electron laser facility, which is currently under construction. The data storage system holds large volumes of data generated instantaneously during the operation. A combination architecture of relational database, non-relational database and the distributed file system is designed as the centralized data storage system serving to multiple application demands for SHINE. The storage of high-speed stream data such as waveform and image data is studied and relevant storage schemas are tested and compared. The optimized schema is identified finally. 
id754#A survey of recent work on video summarization: approaches and techniques#The volume of video data generated has seen an exponential growth over the years and video summarization has emerged as a process that can facilitate efficient storage, quick browsing, indexing, fast retrieval and quick sharing of the content. In view of the vast literature available on different aspects of video summarization approaches and techniques, a need has arisen to summarize and organize various recent research findings, future research focus and trends, challenges, performance measures and evaluation and datasets for testing and validations. This paper investigates into the existing video summarization frameworks and presents a comprehensive view of the existing approaches and techniques. It highlights the recent advances in the techniques and discusses the paradigm shift that has occurred over the last two decades in the area, leading to considerable improvement. Attempts are made to consolidate the most significant findings right from the basic summarization structure to the classification of summarization techniques and noteworthy contributions in the area. Additionally, the existing datasets categorized domain-wise for the purpose of video summarization and evaluation are enumerated. The present study would be helpful in: assimilating important research findings and data for ready reference, identifying groundwork and exploring potential directions for further research. 
id755#A smartly simple way for joint crowd counting and localization#A growing number of state-of-the-art crowd counting methods employ the regression model. Such a model learns a person-density map first and its integral is further calculated to obtain the final count. However, this learned density map is uninterpretable and could deviate largely from the true person distribution even when the final count is accurate. In comparison, we present a conceptually interpretable and technically simple classification model for crowd counting, which consists of three novel modules: Deep Integrated Module (DIM), Scale Adaptive Module (SAM), and Interval Aware Module (IAM). Different from the traditional density map, the proposed pedestrian-aware density map (PADM) in our model can reveal the true people density, and meanwhile tackle the rarely-explored crowd localization task simultaneously. The proposed joint crowd counting and localization method does not require extra pretraining or fine-tuning for individual components of the network, and we train our model end-to-end in a single step. Without bells and whistles but a few lines of code, our simple yet effective method achieves better performances on both crowd counting and localization tasks when compared with state-of-the-art methods. The code is available online. 
id756#An ontological approach to organizing an active seismology web environment#At present, an approach to constructing knowledge-based information systems in which access to data is organized on the basis of ontologies is being actively developed. This approach makes it possible to combine a relational base for data storage with an ontology representing a conceptual system of the subject area. This paper proposes a technology for organizing a web environment for research in the field of active seismology, which allows, on an ontology basis, to present the contents of a data source to the user at a conceptual level and to address user requests to several heterogeneous data sources. High efficiency of executing requests for access and analysis of experimental data is provided by a relational database management system. Copyright 
id758#A deep dive of autoencoder models on low-contrast aquatic images#Public aquariums and similar institutions often use video as a method to monitor the be-havior, health, and status of aquatic organisms in their environments. These video footages take up a sizeable amount of space and require the use of autoencoders to reduce their file size for efficient storage. The autoencoder neural network is an emerging technique which uses the extracted latent space from an input source to reduce the image size for storage, and then reconstructs the source within an acceptable loss range for use. To meet an aquarium’s practical needs, the autoencoder must have easily maintainable codes, low power consumption, be easily adoptable, and not require a substantial amount of memory use or processing power. Conventional configurations of autoen-coders often provide results that perform beyond an aquarium’s needs at the cost of being too complex for their architecture to handle, while few take low-contrast sources into consideration. Thus, in this instance, “keeping it simple” would be the ideal approach to the autoencoder’s model design. This paper proposes a practical approach catered to an aquarium’s specific needs through the con-figuration of autoencoder parameters. It first explores the differences between the two of the most widely applied autoencoder approaches, Multilayer Perceptron (MLP) and Convolution Neural Networks (CNN), to identify the most appropriate approach. The paper concludes that while both approaches (with proper configurations and image preprocessing) can reduce the dimensionality and reduce visual noise of the low-contrast images gathered from aquatic video footage, the CNN approach is more suitable for an aquarium’s architecture. As an unexpected finding of the experi-ments conducted, the paper also discovered that by manipulating the formula for the MLP ap-proach, the autoencoder could generate a denoised differential image that contains sharper and more desirable visual information to an aquarium’s operation. Lastly, the paper has found that proper image preprocessing prior to the application of the autoencoder led to better model convergence and prediction results, as demonstrated both visually and numerically in the experiment. The paper concludes that by combining the denoising effect of MLP, CNN’s ability to manage memory consumption, and proper image preprocessing, the specific practical needs of an aquarium can be adeptly fulfilled. 
id759#Data block and tuple identification using master index#Relational databases are still very often used as a data storage, even for the sensor oriented data. Each data tuple is logically stored in the table referenced by relationships between individual tables. From the physical point of view, data are stored in the data files delimited by the tablespaces. Files are block‐oriented. When retrieving data, particular blocks must be identified and transferred into the memory for the evaluation and processing. This paper deals with storage principles and proposes own methods for effective data block location and identification if no suitable index for the query is present in the system. Thanks to that, the performance of the whole system is optimized, and the processing time and costs are significantly lowered. The proposed solution is based on the master index, which points just to the blocks with relevant data. Thus, no sequential block scanning is necessary for consuming many system resources. The paper analyzes the impact of block size, which can have a significant impact on sensor oriented data, as well. 
id760#Weakly-supervised hand part segmentation from depth images#Existing learning-based methods require a large number of labeled data to produce accurate part segmentation labels. However, acquiring ground truth labels is costly, giving rise to a need for methods that either require fewer labels or can utilize other currently available labels as a form of weak supervision for training. In this paper, in order to mitigate the burden of labeled-data acquisition, we propose a data-driven method for hand part segmentation on depth maps without any need for extra effort to obtain segmentation labels. The proposed method uses the labels already provided by public datasets in terms of major 3D hand joint locations to learn to estimate the hand shape and pose given a depth map. Given the pose and shape of a hand, the corresponding 3D hand mesh is generated using a deformable hand model and then rendered to a color image using a texture based on Linear Blend Skinning (LBS) weights of the hand model. The segmentation labels are then computed from the rendered color image. Since segmentation labels are not provided with current public datasets, we manually annotate a subset of the NYU dataset to perform quantitative evaluation of our method and show that a mIoU of 42% can be achieved with a model trained without using segmentation-based labels. Both qualitative and quantitative results confirm the effectiveness of our method. The code is publicly available for research purposes at: https://git.io/JmCBS. 
id761#A high quality compiler tool for application-specific instruction-set processors with library and parallel supports#Developing a high quality compiler tool for application-specific instruction-set processors (ASIPs) including DSP for multimedia application is challenging. The specialization in ASIPs often calls for extensions at the high-level languages to allow the designers to exploit the specialized capabilities. This in turn requires the frontend of the compiler to handle the new syntax and carry the intentions of the designers across to the compiler backend implementations. The backend implementations also require extra efforts for optimized uses of the specialize features of ASIPs. Meanwhile, because of the diversity of the application, it is necessary to make full use of the compiler to complete supports and to make up some shortages of ASIP processors, the corresponding library functions are increased to support of certain operations, such as floating point arithmetic that may not be supported in ASIPs. With the development of the embedded parallelism, the advanced ASIP compilers need the support of parallelism for future application. This paper describes the High-performance C Compiler (HCC) and its specific implementation for an industrial ASIP and its family processors. HCC is a C language compiler extended and retargeted from GCC. A compiler extension framework is proposed processing programming syntax extensions of standard ANSI C for the ASIPs. With target-specific implementation, the adding optimized arithmetic functions library and chips definition file (CDF) as well as the header files for corresponding ASIPs, HCC compiler could be enhanced for the processing capabilities of target processors. Finally, this paper describes a new compiler static allocation and scheduling scheme for loop parallelization based on the OpenMP specification to improve the load imbalance. We have conducted analysis and extensive experiments to verify the correctness and effectiveness of the HCC compiler with the presented ideas. The results show that HCC compiler has a stable performance with excellent codes quality and it has been used in market. 
id762#Detection of follicular regions in actin-stained whole slide images of the human lymph node by shock filter#Human lymph nodes play a central part of immune defense against infection agents and tumor cells. Lymphoid follicles are compartments of the lymph node which are spherical, mainly filled with B cells. B cells are cellular components of the adaptive immune systems. In the course of a specific immune response, lymphoid follicles pass different morphological differentiation stages. The morphology and the spatial distribution of lymphoid follicles can be sometimes associated to a particular causative agent and development stage of a disease. We report our new approach for the automatic detection of follicular regions in histological whole slide images of tissue sections immuno-stained with actin. The method is divided in two phases: (1) shock filter-based detection of transition points and (2) segmentation of follicular regions. Follicular regions in 10 whole slide images were manually annotated by visual inspection, and sample surveys were conducted by an expert pathologist. The results of our method were validated by comparing with the manual annotation. On average, we could achieve a Zijbendos similarity index of 0.71, with a standard deviation of 0.07. 
id763#SQL2X: Learning SQL, NoSQL, and MapReduce via Translation#A key challenge in designing a database course is how to introduce students to the great variety of data models, query languages, databases, and data processing systems available now. To address this challenge, we propose SQL2X, a novel SQL-centric learning model that teaches students SQL, NoSQL, and MapReduce via translation. For example, translating SQL queries into MapReduce programs to gain insights on how aggregation and join are performed in parallel in MapReduce, and translating SQL queries into REST requests to Firebase to help understand the differences between the query capability of SQL and NoSQL databases. We have applied the model to a graduate database course in our applied data science program. The evaluation and feedback from the students with diverse background indicate the effectiveness of the model in developing students' modeling, querying, and analytical skills over diverse data systems. 
id764#Removing backward go-to statements from Ada programs - Possibilities and problems#We here apply a recent mathematical result to the design of a process for removing backward go-to statements from Ada programs. Such statements are often used by programmers whose only reasons to use Ada are contractual. The mathematical result involves directed graphs, such as the flow graphs of programs, and the first step in our algorithm involves constructing the flow graph of an Ada program. In the second step, this flow graph is rearranged according to our mathematical result, which specifies a linear order for every flow graph, together with a structure for it, of loops within loops, in which no loops overlap. In the third step, a new Ada program is produced from the rearranged flow graph. A problem arises here, within the syntax of Ada, which does not allow branching into a sequence of statements. We show how this problem must be addressed, even though addressing it can involve removing existing structure from the Ada program which we are processing. Copyright 2008 ACM.
id765#A novel data fusion strategy based on multiple intelligent sensory technologies and its application in the quality evaluation of Jinhua dry-cured hams#Multiple intelligent sensory technologies including the electronic nose (E-nose), electronic tongue (E-tongue), and computer vision system have been developed for mimicking the functioning of olfaction, taste, and vision. The traditional data fusion relies on the data-level and feature-level fusion strategies, which may result in the unsatisfactory pattern recognition performance. In this study, a novel distance-probability classification (DPC) method was proposed for the pattern recognition of multiple intelligent sensory technologies. The proposed method was tested by data of multiple intelligent sensory technologies obtained from Jinhua dry-cured hams with different aging time. For qualitative classification, the DPC method can classify different hams with an accuracy rate of 100 %. To further use the fused data, the back propagation neural network (BPNN) models were built to predict the aging time and simultaneously predict 12 sensory attributes. The BPNN models exhibited satisfying performance on predicting aging time (R2 &gt; 0.972) and sensory attributes (R2 &gt; 0.935). This study suggests that data fusion of multiple intelligent sensory technologies is a promising method for food evaluation. 
id766#High-Performance Constant-Time Discrete Gaussian Sampling#Discrete Gaussian distribution plays an essential role in lattice cryptography whereas naive implementations suffer from timing attacks. Unfortunately, conversion to secure constant-time variant incurs severe deterioration in performance. In Knuth-Yao sampling, we demonstrate several properties of the discrete distribution generation tree involving structural features and finite node height. Accordingly we propose a generic method independent of standard deviations, which focuses on minimizing the Boolean expressions for the mapping from input bit strings to output sample values, along with an in-depth efficiency analysis. Two optimization techniques are devised to further propel the minimization by replacing and adjusting nodes. To strike the balance of computational overhead and closeness to optimum, heuristic strategies are introduced. Finally, performance evaluation is conducted both in software and hardware. Running on a 3.4GHz Intel Core i7-6700 processor, our method improves sampling rate by up to 29.5 percent compared to the latest technique. Targeting hardware FPGA devices, our approach can be 2.7 times faster and achieves 57.3 percent resource reduction than the original constant-time Knuth-Yao sampling. Compared to the Cumulative Distribution Table algorithm with fixed step binary search, our sampler can be at least 12.6 times faster and gains 79/61 percent better area-time product than its counterpart without/with BRAM. 
id767#PARA: A one-meter reach, two-kg payload, three-DoF open source robotic arm with customizable end effector#This article presents a robotic arm that is more cost effective than existing models on the market while still maintaining sufficient torque and speed capabilities. Most industrial-grade high power and precision arms are often very expensive, while conversely, more low-cost arms targeted toward the education and hobby sectors are inadequate in power and robustness. The Creative Machines Lab's three degree of freedom Printed Articulated Robotic Arm (PARA) can lift a 2 kg payload at a reach of 940 mm, while under a no-load case, it has exhibited a precision of about ±2.6 mm at an end effector speed of 250 mm/s. It costs about $3400 to build, an order of magnitude lower than market models with similar functionalities. This project is also meant to serve as a demonstration of the usage of 3D printed parts as practical tools in industry. 
id768#Schema Optimisation Instead of (Local) Normalisation#Classical normalisation theory has a number of lacunas although it is commonly and widely accepted and it is the basis for database theory since the 80ies. Most textbooks and monographs still follow this approach despite the good number of open problems. Today, modern object-relational DBMS offer far better capabilities than the systems that have been built in the past based on the strict relational paradigm. Constraint maintenance has been oriented on transformation of structures to structures that are free of functional dependencies beside key constraints. The maintenance of coherence constraints such as two-type inclusion constraints has been neglected although this maintenance might be the most expensive one. In reality normalisation is local optimisation that exclusively considers functional dependency maintenance. We thus need a different normalisation approach. This paper develops an approach towards optimisation of schemata and global normalisation. This approach results in a denormalisation and object-relational database schemata. 
id769#Autonomous 3D geometry reconstruction through robot-manipulated optical sensors#Many industrial sectors face increasing production demands and the need to reduce costs, without compromising the quality. The use of robotics and automation has grown significantly in recent years, but versatile robotic manipulators are still not commonly used in small factories. Beside of the investments required to enable efficient and profitable use of robot technology, the efforts needed to program robots are only economically viable in case of large lot sizes. Generating robot programs for specific manufacturing tasks still relies on programming trajectory waypoints by hand. The use of virtual simulation software and the availability of the specimen digital models can facilitate robot programming. Nevertheless, in many cases, the virtual models are not available or there are excessive differences between virtual and real setups, leading to inaccurate robot programs and time-consuming manual corrections. Previous works have demonstrated the use of robot-manipulated optical sensors to map the geometry of samples. However, the use of simple user-defined robot paths, which are not optimized for a specific part geometry, typically causes some areas of the samples to not be mapped with the required level of accuracy or to not be sampled at all by the optical sensor. This work presents an autonomous framework to enable adaptive surface mapping, without any previous knowledge of the part geometry being transferred to the system. The novelty of this work lies in enabling the capability of mapping a part surface at the required level of sampling density, whilst minimizing the number of necessary view poses. Its development has also led to an efficient method of point cloud down-sampling and merging. The article gives an overview of the related work in the field, a detailed description of the proposed framework and a proof of its functionality through both simulated and experimental evidences. 
id770#Efficient development of vision-based dense three-dimensional displacement measurement algorithms using physics-based graphics models#This research investigates a framework for the efficient development of vision-based dense three-dimensional displacement measurement algorithms to support reliable structural health monitoring of civil structures. The framework exploits the use of a photo-realistic synthetic model, termed a physics-based graphics model, to simulate the entire process of vision-based measurement. At the same time, the synthetic environment is used to evaluate the performance of different post-processing algorithms quantitatively for a given measurement scenario, such as camera selection and camera placement. The effectiveness of the framework is demonstrated by optimizing the algorithms for the three-dimensional displacement measurement of a 14-bay laboratory truss structure. The vision-based dense three-dimensional displacement estimation algorithms optimized in this study consist of four steps: (1) camera parameter estimation, (2) camera motion estimation and compensation, (3) vision-based two-dimensional tracking, and (4) projection of two-dimensional tracking results to three-dimensional space. The algorithms use the knowledge from the finite element model to facilitate the implementation and maximize the measurement outcome, that is, model-informed approach. To test and evaluate the model-informed approach, synthetic videos are rendered for two measurement scenarios, that is, using a Digital Single Lens Reflex camera mounted on a tripod and using an Unmanned Aerial Vehicle camera. Then, the performance of the model-informed approach is evaluated by comparing the estimated displacement with the ground truth values. Based on the performance evaluation, an algorithm with the highest expected performance is selected for each of the two measurement scenarios. Finally, the selected algorithm is tested in a laboratory experiment. In contrast to the existing literature that investigates fixed individual measurement scenarios, the proposed framework can be used to test different measurement scenarios and estimate the outcome of each scenario before performing actual tests, facilitating the implementation of vision-based measurement for the structural health monitoring of civil structures. 
id772#Exponential synchronization of fractional-order complex chaotic systems and its application#In this article, exponential synchronization between fractional order chaotic systems has been studied by using exponential stability theorem. The stability analysis has been done with help of an existing lemma, which is given for Lyapunov function for fractional order system. The fractional order complex chaotic systems viz., Lorenz and Lu systems are considered to illustrate the exponential synchronization. The numerical simulation results are presented through graphical plots to verify the effectiveness and reliability of exponential synchronization. The application in communication through digital cryptography is also discussed between the sender (transmitter) and receiver using the exponential synchronization. A well secured key system of a message is obtained in a systematic way. 
id773#Automated analysis and construction of feature models in a relational database using web forms#The Feature-Oriented Domain Analysis (FODA) approach introduced the feature model abstraction to represent software product lines. However, these abstractions are often difficult for mainstream developers to specify and maintain because most tools rely on specialized theories, notations, or technologies. To address this issue, we propose a design that uses mainstream Web technologies to enable users to construct syntactically and semantically correct feature models and mainstream relational database technologies to encode the models as directed acyclic graphs. The Web interface and relational database designs can form parts of a comprehensive, interactive environment that enables mainstream developers to specify, store, and update feature models and use them to configure members of product families. 
id774#Energy-Efficient and Adversarially Robust Machine Learning with Selective Dynamic Band Filtering#The popularity of neural networks is increasing day by day. Traditional machine learning solutions, such as image recognition, object detection, are being replaced by deep learning solutions because of their vigorous performance in computer vision. Despite their superior performance in these applications, neural networks are prone to adversarial attacks. The adversarial attack is the process of using adversarial samples as an input to the neural network which causes the network to misclassify, eventually degrading overall performance. Thus, it becomes very important to maintain their robustness by identifying, analyzing, and eliminating the cause of their vulnerability. In this paper, we introduce a technique to determine the most sensitive frequency band of input samples and filter the noise from this band to shield the network against adversarial attacks. First, we decompose the input sample into four different frequency components and then, identify the sensitive component by measuring the change in behavior of the pre-Trained network on normal frequency band and that on frequency band with added noise (frequency band of an adversary). Next, we exploit this vulnerable component to assist the network in tackling the adversaries through noise filtering. Thereby, enhancing the neural networks? performance and defending against the adversarial attack. The low-frequency component was the most vulnerable and mitigating the noise from this band significantly improved the accuracy of Convolutional Neural Networks (CNN) along with that of state-of-Art networks against adversarial attacks such as Fast Gradient Sign Method (FGSM), DeepFool (DF), and other techniques. The proposed technique showed performance enhancement from 85% to 95% classification accuracy for ResNet50. 
id775#Unsupervised Machine Learning Via Transfer Learning and k-Means Clustering to Classify Materials Image Data#Unsupervised machine learning offers significant opportunities for extracting knowledge from unlabeled datasets and for achieving maximum machine learning performance. This paper demonstrates how to construct, use, and evaluate a high-performance unsupervised machine learning system for classifying images in a popular microstructural dataset. The Northeastern University Steel Surface Defects Database includes micrographs of six different defects observed on hot-rolled steel in a format that is convenient for training and evaluating models for image classification. We use the VGG16 convolutional neural network pre-trained on the ImageNet dataset of natural images to extract feature representations for each micrograph. After applying principal component analysis to extract signal from the feature descriptors, we use k-means clustering to classify the images without needing labeled training data. The approach achieves 99.4% ± 0.16% accuracy, and the resulting model can be used to classify new images without retraining. This approach demonstrates an improvement in both performance and utility compared to a previous study. A sensitivity analysis is conducted to better understand the influence of each step on the classification performance. The results provide insight toward applying unsupervised machine learning techniques to problems of interest in materials science. 
id776#Refactoring as a Technique for Transformation IMS-Queries into SQL-Queries#Refactoring is defined as special techniques for restructuring an existing body of a code and its internal structure without changing its external functionality. In other words one can say that refactoring is recoding of the source code. Refactoring approach can be used for complex transformation of the application programs from PL/1 or COBOL (with invocations to IMS databases) to C or Java with invocations to some modern database like Oracle or MYSQL. Since an enormous lines of PL/1 code are working now (some of them with poor documentation) and are needed to be transformed on modern platforms therefore automated translation should be done (as intensive as possible). Some ideas about decision of such transformation will be done in future. 
id778#Extended XOR-based visual cryptography schemes by integer linear program#XOR-based visual cryptography scheme (XVCS) is a promising branch of VCS since superior visual quality of recovered secret image is provided. To assist the shadow management, meaningful shadow is preferred. However, existing extended XVCSs (EXVCSs) that generate meaningful shadows are confined to the (n,n) threshold. They are not practical for real-world applications. In this paper, we introduce EXVCS for (k,n) threshold and general access structure (GAS) by using integer linear program (ILP) technique. First of all, an approach for implementing the (k,n)-XVCS is given. We associate the (k,n)-XVCS with a minimizing problem, where the constraints in this problem are derived from the contrast and security conditions in (k,n)-XVCS. The ILP technique is adopted to solve this problem for obtaining the best solutions for constructing the base matrices for (k,n)-XVCS. Further, the construction is extended to (k,n)-EXVCS, in which the shadows with meaningful contents are constructed. To realize complicated sharing strategies, a construction of EXVCS for GAS is described as well. The base matrices for XVCS and EXVCS are demonstrated, as well as the pixel expansion and contrast. Comparisons among related methods are illustrated to show the advantages of the proposed schemes. 
id779#Benchmarking of deep learning irradiance forecasting models from sky images – An in-depth analysis#A number of industrial applications, such as smart grids, power plant operation, hybrid system management or energy trading, could benefit from improved short-term solar forecasting, addressing the intermittent energy production from solar panels. However, current approaches to modelling the cloud cover dynamics from sky images still lack precision regarding the spatial configuration of clouds, their temporal dynamics and physical interactions with solar radiation. Benefiting from a growing number of large datasets, data driven methods are being developed to address these limitations with promising results. In this study, we compare four commonly used deep learning architectures trained to forecast solar irradiance from sequences of hemispherical sky images and exogenous variables. To assess the relative performance of each model, we used the forecast skill metric based on the smart persistence model, as well as ramp and time distortion metrics. The results show that encoding spatiotemporal aspects of the sequence of sky images greatly improved the predictions with 10 min ahead forecast skill reaching 20.4% on the test year. However, based on the experimental data, we conclude that, with a common setup, deep learning models tend to behave just as a ‘very smart persistence model’, temporally aligned with the persistence model while mitigating its most penalising errors. Thus, despite being captured by the sky cameras, models often miss fundamental events causing large irradiance changes such as clouds obscuring the sun. We hope that our work will contribute to a shift of this approach to irradiance forecasting, from reactive to anticipatory. 
id780#Using the Linked Data for Building of the Production Capacity Planning System of the Aircraft Factory#The basic principles of data consolidation of the production capacities planning system of the large industrial enterprise are formulated in this article. The article describes an example of data consolidation process of two relational databases (RDBs). The proposed approach involves using of ontological engineering methods for extracting metadata (ontologies) from RDB schemas. The research contains an analysis of approaches to the consolidation of RDBs at different levels. The merging of extracted metadata is used to organize the data consolidation process of several RDBs. The difference between the traditional and the proposed data consolidation algorithms is shown, their advantages and disadvantages are considered. The formalization of the integrating data model as system of extracted metadata of RDB schemas is described. Steps for integrating data model building in the process of ontology merging is presented. An example of the integrating data model building as settings for data consolidation process confirms the possibility of practical use of the proposed approach in the data consolidation process. 
id781#3-D maximum likelihood estimation sample consensus for correspondence grouping in 3-D plant point cloud#Computer vision based plant phenomics can be used to monitor the health and the growth of plants. This letter presents the extension of 2-D maximum likelihood matching to 3-D maximum likelihood estimation sample consensus (MLEASAC) and provides a comparative evaluation of some popular 3-D correspondence grouping algorithms. We test these algorithms on 3-D point clouds of plants along with two standard benchmarks addressing shape retrieval and point cloud registration scenarios. The performance of the correspondence grouping algorithms is evaluated in terms of precision and recall. The results show that of all the evaluated algorithms, 3-D random sample consensus (RANSAC) and MLEASAC perform the best, with MLEASAC being slightly more efficient while being computationally less intense than RANSAC. 
id782#Optimization of a fuzzy controller for autonomous robot navigation using a new competitive multi-metaheuristic model#This paper describes a proposed competitive multi-metaheuristic optimization model for the optimal design of membership function parameters of a fuzzy system that controls the navigation of an autonomous mobile robot following a desired trajectory. The main contribution is the new competitive multi-metaheuristic optimization model, which is formed by an architecture consisting of the firefly algorithm, wind-driven optimization, drone squadron optimization, and stochastic fractal search. The proposed method has enable obtaining good results in the optimization of fuzzy controllers, which outperform other metaheuristics in the literature. In the competition, each of the methods is tested until finding the best one in generating an effective parameter vector for the optimization of fuzzy controller membership functions. The main contribution of this article is the use of four metaheuristics working in a competitive way, to find the best vector of data generated with the optimal values to successfully adjust the membership functions of the fuzzy controller. The resulting winning method proves in this way that it is the best among the chosen algorithms to solve this kind of control optimization problem. 
id783#Parallel implementation of Nussbaumer algorithm and number theoretic transform on a GPU platform: application to qTESLA#Among the popular post-quantum schemes, lattice-based cryptosystems have received renewed interest since there are relatively simple, highly parallelizable and provably secure under a worst-case hardness assumption. However, polynomial multiplication over rings is the most time-consuming operation in most of the lattice-based cryptosystems. To further improve the performance of lattice-based cryptosystems for large scale usage, polynomial multiplication must be implemented in parallel. The polynomial multiplication can be performed using either number theoretic transform (NTT) or Nussbaumer algorithm. However, Nussbaumer algorithm is inherently serial. Meanwhile, the efficient implementation of NTT using various indexing methods on GPU platform remains unknown.In this paper, we explore the best combination of various indexing methods to implement NTT on GPU platform and the efficient way to parallelize the Nussbaumer algorithm. Our results suggest that the combination of Gentleman–Sande and Cooley–Tukey (GS-CT) indexing methods produced the best performance on RTX2060 GPU (i.e. 422,638 polynomial multiplications per second). A technique to parallelize Nussbaumer algorithm by reducing the non-coalesced global memory access to half is produced. To the best of our knowledge, this is the first GPU implementation of Nussbaumer algorithm and it outperforms the best aforementioned NTT (GS-CT) implementation by 14.5%. For illustration purpose, the proposed GPU implementation techniques are applied to qTESLA, a state-of-the-art lattice based signature scheme. We emphasize that the proposed implementation techniques are not specific to any cryptosystem; they can be easily adapted to any other lattice-based cryptosystems. 
id784#Automatic identification and quantification of dense microcracks in high-performance fiber-reinforced cementitious composites through deep learning-based computer vision#High-performance fiber-reinforced cementitious composites (HPFRCCs) feature high mechanical strengths, crack resistance, and durability. Under excessive loading, HPFRCCs demonstrate dense microcracks that are difficult to identify using existing methods. This study presents a computer vision method for identification, quantification, and visualization of microcracks in HPFRCCs based on deep learning. The presented method integrates multiple deep learning models and computer vision techniques in a hierarchical architecture. The crack pattern (e.g., number, width, and spacing of cracks) are automatically determined from pictures without human intervention. This study shows that the presented method achieves an accuracy of 0.992 for crack detection and an accuracy finer than 50 μm (R2 &gt; 0.984) for quantification of crack width when deep learning models are trained using only 200 pictures of HPFRCCs and 200 pictures of conventional concrete with incorporation of data augmentation. The presented method is expected to be also applicable to other materials featuring complex cracks. 
id785#Finding candidate keys and 3NF via strategic port graph rewriting#We present new algorithms to compute candidate keys and third normal form design of a relational database schema, using strategic port graph rewriting. More precisely, we define port graph rewriting rules and strategies that implement a candidate key definition and Ullman's algorithm to decompose a relation schema into lossless 3NF schemata. We show the correctness of the resulting database schema by proving soundness, completeness and termination of our strategic graph programs. These rules and strategies provide a declarative and visual description of the algorithms, and permit a fine-grained analysis of the computation steps involved in the normalisation process. The algorithms have been implemented in Porgy, a visual, interactive modelling tool. 
id786#An Approach for Generating SQL Query Using Natural Language Processing#Today’s databases of corporations are so huge, that they can only be approached by experienced programmers. Accessing data from a database usually needs notable skills such as knowledge of SQL; however, the most of us who interact with databases every day don’t have that background. Hence it’s an increase demand for non-technical user to be able to redeem data from databases without having to list SQL queries. And this problem is solved by using approach of Natural Language Processing. This research work presents an approach for querying system for natural language processing. Hence it will dramatically simplify the process of handling with large data and making data available for everyone. 
id787#Coupled electro-chemo-elasticity: Application to modeling the actuation response of ionic polymer–metal composites#We have formulated a large deformation thermodynamically-consistent electro-chemo-elasticity theory for modeling the actuation response of ionic polymer–metal composites. Our theory accounts for the simultaneous evolution of the electric potential, the concentration of the mobile hydrated cations, and the deformation of the host ionic polymer matrix. We have numerically implemented our theory in an implicit finite element program. We use this simulation capability to first calibrate the material parameters in the theory for a Nafion-based ionic polymer–metal composite, and then validate our theory by comparing predictions from the theory against other experimental data available in the literature. We also demonstrate the utility of our simulation capability by conducting full three-dimensional simulations of two soft-robotics applications with some geometric complexity: (i) a biomimetic fin, and (ii) a micro-gripper. 
id788#A navigation and augmented reality system for visually impaired people#In recent years, we have assisted with an impressive advance in augmented reality systems and computer vision algorithms, based on image processing and artificial intelligence. Thanks to these technologies, mainstream smartphones are able to estimate their own motion in 3D space with high accuracy. In this paper, we exploit such technologies to support the autonomous mobility of people with visual disabilities, identifying pre-defined virtual paths and providing context information, reducing the distance between the digital and real worlds. In particular, we present ARIANNA+, an extension of ARIANNA, a system explicitly designed for visually impaired people for indoor and outdoor localization and navigation. While ARIANNA is based on the assumption that landmarks, such as QR codes, and physical paths (composed of colored tapes, painted lines, or tactile pavings) are deployed in the environment and recognized by the camera of a common smartphone, ARIANNA+ eliminates the need for any physical support thanks to the ARKit library, which we exploit to build a completely virtual path. Moreover, ARIANNA+ adds the possibility for the users to have enhanced interactions with the surrounding environment, through convolutional neural networks (CNNs) trained to recognize objects or buildings and enabling the possibility of accessing contents associated with them. By using a common smartphone as a mediation instrument with the environment, ARIANNA+ leverages augmented reality and machine learning for enhancing physical accessibility. The proposed system allows visually impaired people to easily navigate in indoor and outdoor scenarios simply by loading a previously recorded virtual path and providing automatic guidance along the route, through haptic, speech, and sound feedback. 
id789#Exploring structured information systems: A keyword search approach#Google search, and other search engines (e.g., Bing), provide a convenient way to find web pages that contain different keywords. Each web page is composed of loosely-structured paragraphs of text. On the other hand, a vast amount of information systems data are stored as structured data (e.g., in relational databases). In a structured database, the information about a particular entity, or the relation between different entities might be scattered across many rows and columns in different tables. These pieces of information must be stitched together to form the final answer to a user's query. For a large database, there might be millions of answers to a search query. Thus, finding the right answer is a challenging task. In this work, we present an intuitive search system over structured information systems. This search system empower non-technical users to explore structured information systems and turn hidden information into useful insights. 
id790#Does the Use of Intraoperative Technology Yield Superior Patient Outcomes Following Total Knee Arthroplasty?#Introduction: There is debate regarding whether the use of computer-assisted technology, such as navigation and robotics, has any benefit on outcomes or patient-reported outcome measures (PROMs) following total knee arthroplasty (TKA). This study aims to report on the association between intraoperative use of technology and outcomes in patients who underwent primary TKA. Methods: We retrospectively reviewed 7096 patients who underwent primary TKA from 2016-2020. Patients were stratified depending on the technology utilized: navigation, robotics, or no technology. Patient demographics, clinical data, Forgotten Joint Score-12 (FJS), and Knee injury and Osteoarthritis Outcome Score for Joint Replacement (KOOS, JR) were collected at various time points up to 1-year follow-up. Demographic differences were assessed with chi-square and ANOVA. Clinical data and PROMs were compared using univariate ANCOVA, controlling for demographic differences. Results: A total of 287(4%) navigation, 367(5%) robotics, and 6442(91%) manual cases were included. Surgical-time significantly differed between the three groups (113.33 vs 117.44 vs 102.11; P < .001). Discharge disposition significantly differed between the three groups (P < .001), with more manual TKA patients discharged to a skilled nursing facility (12% vs 8% vs 15%; P < .001) than those who had technology utilized. FJS scores did not statistically differ at three-months (P = .067) and one-year (P = .221). We found significant statistical differences in three-month KOOS, JR scores (59.48 vs 60.10 vs 63.64; P = .001); however, one-year scores did not statistically differ between all groups (P = .320). Conclusion: This study demonstrates shorter operative-time in cases with no utilization of technology and clinically similar PROMs associated with TKAs performed between all modalities. While the use of technology may aid surgeons, it has not currently translated to better short-term outcomes. Level III Evidence: Retrospective Cohort. 
id791#Automated Implant Resizing for Single-Stage Cranioplasty#Patient-specific customized cranial implants (CCIs) are designed to fill the bony voids in the cranial and craniofacial skeleton. The current clinical approach during single-stage cranioplasty involves a surgeon modifying an oversized CCI to fit a patient's skull defect. The manual process, however, can be imprecise and time-consuming. This article presents an automated surgical workflow with a robotic workstation for intraoperative CCI modification that provides higher resizing accuracy compared to the manual approach. We proposed a 2-scan method for intraoperative patient-to-CT registration using reattachable fiducial markers to address the registration issue caused by the clinical draping requirement. First, the draped defected skull was 3D scanned and registered to the CT space using our proposed 2-scan registration method. Next, our algorithm generates a robot cutting toolpath based on the 3D defect model. The robot then performs automatic 3D scanning to localize the implant and resizes the implant to match the cranial defect. We evaluated the implant resizing accuracy of the proposed paradigm against the resizing accuracy of the manual approach by an expert surgeon on two plastic skulls and two cadavers. The evaluation results showed that our system was able to decrease the bone gap distance by more than 60% and 30% on plastic skulls and cadavers respectively compared to the manual approach, indicating lower risk of post-surgical complication and better aesthetic restoration. 
id792#Human tracking of track and field athletes based on FPGA and computer vision#An essential issue that sports science faces in understanding the game's flow is to analyze the game's situation. The use of information technology can help to achieve this goal. Computing speed, the size of the system, consider the accuracy of the complex data analysis. From the practical application of views, technical problems can be divided into three significant respects. This article aims to accelerate image recognition and object tracking; propose a one-dimensional data pipeline architecture on a Field-Programmable Gate Array (FPGA). It is met by a small circuit considering calculation and the spatiotemporal data dependency between two high-velocity flows. The volleyball game has been selected as the target application. FPGA design, pre-processing, color filtering, digitization, noise reduction, including such as template matching. Design and implementation by training stations of the Spartan-6 FPGA Xilinx Atlys of the Spartan-6 FPGA LX45 'were evaluated. Computing power will reach per 100 frames at a resolution of SVGA 800 × 600 pixels. Design, excellent with the scalability, can more easily improve the performance in the case of using a large FPGA. The proposed system consists of the Atlys substrate and Atlys VmodCAM stereo camera board. The average accuracy rate before the game situation and the game with 87.1 percent and 65.7 percent. Since the streaming input data, you can improve the precision by taking into account the previous and the next frame. And 90.4 percent will be able to improve to 72.2 percent by using a moving average filter and template matching. 
id793#Optimal Soft Composites for Under-Actuated Soft Robots#Material properties and composite structures play key roles in tailoring the performance of soft robots. Unfortunately, current design and fabrication approaches limit achievable complexity and functionality in these two categories and hinder soft robot performance. Here, an approach that allows design and direct fabrication of novel soft composite structures is presented. The process uses computational topology optimization to determine the required 3D composite structure of soft hyper-elastic bodies. The direct fabrication of the soft composite structures using an all-in-one fabrication workflow with resilient silicone polymers enables precise tailoring of mechanical properties. By applying this approach to the design and fabrication of an underwater batoid-inspired soft robot, significant swimming performance improvements is demonstrated. An optimized composite prototype displays 50% faster swimming speeds, 28% faster turning rates, and 55% smaller turning radii than un-optimized benchmark prototypes. 
id794#Specification of the Schema of Spreadsheets for the Materialization of Ontologies from Integrated Data Sources#In Ontology-Based Data Access (OBDA), a knowledge base known as an ontology models both the problem domain and the underlying data sources. We are concerned with providing with tools for performing OBDA with relational and non-relational data sources. We developed an OBDA tool that is able to access H2 databases, CSV files and Excel spreadsheets allowing the user to explicitly formulate mappings, and populating an ontology that can be saved for later querying. In this paper, we present a language for specifying the schema of the data in a spreadsheet data application, which then can be used to access the contents of a set of Excel books with the ultimate goal of materializing its data as an OWL/RDF ontology. We characterize the syntax and semantics of the language, present a prototypical implementation and report on the performance tests showing that our implementation can handle a workload of Excel tables of the order of ten thousand records. We also show a case study in which the ontology of an idealized university library can be defined using the our tool integrating both relational and spreadsheet data. 
id795#Biomechanical motion planning for a wearable robotic forearm#Supernumerary robotic devices in the form of wearable arms enhance a user's reachable workspace and provide them with additional capabilities. However, the user may experience considerable force and moment loads on their body due to the robot's motion. In this letter, we present a simulation and trajectory planning framework that aims to minimize the load on a user's muscles while operating a Wearable Robotic Forearm (WRF). Using a high-fidelity model of the human arm, we construct a term for biomechanical costs that is subsequently added to the overall cost function for a motion planner. For evaluation, the planner is initialized with shortest paths linearly interpolated between ten start and goal state pairs in the configuration space, as well as with paths optimized for reaction moments using a local search. We find that the biomechanical planner coupled with locally-optimized initialization reduces mean human muscle fiber forces by up to 23.47% compared to the linearly interpolated trajectories. 
id796#Modeling and detection of heat haze in computer vision based displacement measurement#Computer vision has become widely applied for structural displacement monitoring. However, heat haze is one of the major challenges. Image distortions caused by heat haze in hot weather can result in displacement errors. Therefore, a comprehensive study of properties of heat haze-induced distortions and displacement errors is conducted. Firstly, an image distortion estimation method is proposed for estimating heat haze-induced image distortions. Secondly, displacement errors due to heat haze are analyzed. A heat haze error model is formulated to describe the properties of heat haze errors, and the explicit effect of the environmental factor of temperature on the heat haze error model. Thirdly, a heat haze detection method is proposed to enable detection of heat haze's influence on vision-based displacement sensors by extracting features from distortion measurements and applying a classification algorithm. Field tests in hot weather and experiments with dark heaters for introducing heat haze are conducted for validations. 
id797#Battery-Free Camera Occupancy Detection System#Occupancy detection systems are commonly equipped with high-quality cameras and a processor with high computational power to run detection algorithms. This paper presents a human occupancy detection system that uses battery-free cameras and a deep learning model implemented on a low-cost hub to detect human presence. Our low-resolution camera harvests energy from ambient light and transmits data to the hub using backscatter communication. We implement the state-of-the-art YOLOv5 network detection algorithm that offers high detection accuracy and fast inferencing speed on a Raspberry Pi 4 Model B. We achieve an inferencing speed of ∼ 100ms per image and an overall detection accuracy of >90% with only 2GB CPU RAM on the Raspberry Pi. In the experimental results, we also demonstrate that the detection is robust to noise, illuminance, occlusion, and angle of depression. 
id798#Automated Pixel-Wise Pavement Crack Detection by Classification-Segmentation Networks#Pavement crack detection on pixel-levels is a high-profile application of computer vision and semantic segmentation. In this paper, a two-step convolutional neural network (CNN) method is proposed to detect crack-pixels from pavement pictures and to reduce time consumption. The method contains two main parts: CNN-1 for patch classification and CNN-2 for semantic segmentation. The first part chooses regions with a high probability to contain cracks and sends them to CNN-2 to get pixel-wise detection results. The CNN-2 cancels down-sampling to ensure the size of a feature map is fixed, so it is an end-to-end network. The proposed method and CrackNet-II are trained and tested on the same datasets, and the results show that compared with the pure-segmentation network, the two-step CNN method reduces the processing-time dramatically while the loss of accuracy is small. 
id800#On the prediction of reduction goals: A deterministic approach#In LR parsing, a reduction goal is known at reduction time, but the goal can be sometimes found before that time. We recently proposed an algorithm generating pre-determinable reduction goals during LR parsing. On the other hand, the method requires a parser constructor to select a goal among a set of pre-determinable reduction goals. The non-deterministic set approach is hence inadequate to be used in an automatic parser generating system. This paper presents a deterministic approach such that a unique predicted goal is generated. The crux of the approach lies in preserving all the prediction information. We prove that all the predicted goals are orderable by the number of parsing actions required before a reduction occurs from the current parsing configuration. Based on the property, we propose the strategy choosing the nearest goal from the current parsing configuration as a way to impose determinism. This paper lastly shows the usefulness of the proposed approach in error handing applications of pre-determinable reduction goals. 
id801#High-Speed Modular Multiplier for Lattice-Based Cryptosystems#Thanks to the inherent post-quantum resistant properties, lattice-based cryptography has gained increasing attention in various cryptographic applications recently. To facilitate the practical deployment, efficient hardware architectures are demanded to accelerate the operations and reduce the computational resources, especially for the polynomial multiplication, which is the bottleneck of lattice-based cryptosystems. In this brief, we present a novel high-speed modular multiplier architecture for polynomial multiplication. The proposed architecture employs a divide and conquer strategy and exploits a special modulus to increase the parallelism and speed up the calculation, while enabling wider applications across various cryptosystems. The experimental results show that our design achieves around 27% and 39% reduction on the area consumption and delay, respectively, compared to prior works. 
id802#Innovative astronomical applications with a new-generation relational database#Database technology has been developing to exploit the next-generation hardware in the era of big data processing. At the same time, astronomical data size has been steadily increasing, and astronomical source catalogs obtained from largescale surveys with a wide-field camera, such as Subaru/Hyper Suprime-Cam (HSC), are a good test bench for evaluating the new database technology with a large data set. Such archive systems often employ a highly versatile relational database management system (RDBMS), but reducing the time required for data transaction and complex analysis has come to an important challenge. To tackle this difficulty, we aim to develop astronomical applications with a new catalog database using a next-generation RDBMS technology, where the query engine is designed to efficiently use computing infrastructures for processing big data. Demonstrations with science applications are essential to evaluate the new database. We verify query performance with the current HSC source catalog. For application to huge astronomical catalog databases, we are pursuing and verifying the capabilities of new database technologies. It will, in turn, enable fast ad hoc search and efficient detection of a wide range of variable events with the technology. Our pilot tests using typical astronomical queries on a cluster system shows significant improvements in response times with the aid of distributed query engines. We report performance of the test database for typical astronomical queries, and discuss optimizing the schema based on query workloads. 
id803#Mapping and Conversion between Relational and Graph Databases Models: A Systematic Literature Review#Given the current mass of information and, considering that such information is increasingly related, \the use of a graph model to represent Storing these can make it easier to identify information that would be hard to see when using a relational model. The purpose of this study is to characterize the existing techniques about the mapping and conversion process between relational and graph-oriented database models. For this, a systematic literature review was performed in the Scopus and IEEE Xplore databases. We validated 11 articles that were included in the period from 1 January 2013 to 31 May 2019. The results showed that most studies try to perform the mapping and migration process with different algorithms and data structures and each one has a point of failure, such as data loss, test execution distributed environments and other Relational DBMSs. The contribution of this research is to situate the state of the art of the conversion process between relational and graph-oriented databases, highlighting the positive and negative points of the existing techniques, with the objective of developing algorithms that combine the best of each technique, improving the existing failures. 
id804#A Simulated Environment for Traversability Estimation Experiments in Field Robotics Applications#We present an environment for simulated experiments in field robotics, and especially in experiments on estimating the traversability of foliage and other objects that appear as obstacles but that can be overcome by the robot without circumventing them. The simulated environment is developed in the Unity real-time development platform, integrated with the ROS middleware. In the preliminary experiments presented here, we demonstrate that our environment is able to simulate the sensory input needed in order to train supervised traversability estimation models. 
id805#Influence of Visual and Haptic Feedback on the Detection of Threshold Forces in a Surgical Grasping Task#Feedback from sensory modalities is crucial for the precise grasping of tissues during minimally invasive robotic surgery. The aims of the study are to determine the influence of visual and haptic feedback on the detection of threshold forces and to evaluate the applicability of the sensory integration model to a surgical grasping task. A sensorized surgical grasper and a fingertip haptic force feedback device were used. Three types of stimuli were presented (i.e. visual-alone, haptic-alone, and bimodal visual and haptic stimuli). Threshold forces of 100 mN and 87.5 mN were detected for visual and haptic feedback, respectively. When bimodal feedback was provided, the participants detected a threshold force of 75 mN. The threshold force for the bimodal condition was 28.6% lower than the visual-alone feedback and 15.4% lower than the haptic-alone feedback stimuli. Our bimodal condition results showed that there was a 13.1% difference between the experimental result and the predicted value from the sensory integration model. The threshold force discrimination was strongly influenced by the haptic force feedback. It is likely that the tissue stiffness can be more intuitively perceived through the direct force stimulation of the fingertip than just by visual observation alone. Cues like small deformations or changes in the grasping angles of the surgical tool are more difficult to interpret visually as compared to the haptic modality. 
id806#Think big, start small: a good initiative to design green query optimizers#Recently scientists, politicians, students, associations and actors are sounded the alarm to save our planet. The slogan of Greta Thunberg “Our house is on fire” urges any person to act on the climate. As researchers in the field of databases, one of the most active research communities, we are compelled to propose little and big steps to save our planet. It should be noticed that DBMSs are one of the main energy consumers, as responsible to store and efficiently process data. In data stores, research on energy consumption has been mainly focused on some specific types of stores: data centers, database clusters, known as big infrastructures. These stores are computer warehouses dedicated to store and process in a parallel manner a large amount of data. They include different servers and network infrastructures. Energy consumption in traditional DBMSs got less attention compared to data centers, and at the same time, they are widely used in the actual applications. In DB-Engine (https://db-engines.com/en/ranking) ranking DBMSs according to their popularity, traditional DBMSs (Oracle, MySQL, SQL Server, PostgreSQL, DB2) are the top 5 of the most popular systems. This motivates us to integrate energy consumption in the components of these DBMSs. Query optimizers are one of the energy consumer’s components. The actual studies were focused on integrating energy in query optimization in the mono-core processor architecture. Recently, thanks to multi-core, these studies have to be revisited. In this paper, we propose a new approach to integrate the energy dimension into query optimizers in the multi-core processor architecture. Firstly, we present a rich state of the art on energy consumption in the context of traditional databases. Secondly, a crossing from sequential query processing mode to parallel mode is given. Thirdly, we propose a cost model capturing energy in a multicore architecture. Its parameter values are obtained by using non-linear regression and neural network techniques. Finally, our cost model is integrated into the query optimizer in PostgreSQL on which several experiments were conducted showing the efficiency and effectiveness of our proposal. 
id807#ColMap: A memory-efficient occupancy grid mapping framework#In order to possess a significant degree of autonomy, a robot must be able to perceive its environment and store a representation of that environment for use in tasks such as localisation, navigation, collision avoidance, and higher decision making. It must do this subject to constraints on memory and processing power typical of the embedded computer systems commonly found on small robotic devices. These constraints are particularly important for flying robots (i.e. unmanned aerial vehicles), for which weight must be minimised. The challenge of storing a detailed map of a large area on a small embedded computer has led to the development of many algorithms that exploit the sparsity of typical maps to create a more memory-efficient representation. In this paper, we demonstrate that the verticality of both natural and man-made structures can be exploited to create a framework that can store occupancy grid maps efficiently, without causing additional computational burden. The new framework achieves an order-of-magnitude reduction in memory footprint relative to widely-used occupancy grid mapping software, while also achieving a slight speed-up in map insertion and access times. We also make available LIDAR scans taken from a hexacopter of an indoor flight arena that can be used to assist in evaluating future mapping and SLAM developments. 
id808#LAKAF: Lightweight authentication and key agreement framework for smart grid network#Combination of sustainable resources and expanded developments in vitality utilization have made new challenges for the traditional grid system. To confront these challenges, the Internet of Things (IoT) have changed the traditional grid system into a modernized electrical system which is called smart grid. In smart grid networking, security and privacy are the major concerns. Recently, various authentication and key agreement protocols have been presented in smart grid network with different privacy and security features. Although, some scheme suffer from different security and privacy challenges, some of them required much high computation and communication costs. To solve these challenges, we propose a lightweight authentication and key agreement protocol for smart grid which is free from key escrow issues and provides more security and privacy features. We providea formal security analysis of the proposed scheme which is based on the random oracle model that indicates correctness of the proposed protocol. Further, we provide security verification of the proposed protocol by using AVISPA software tool. Also, we demonstrates the informal security of the proposed scheme. At last, we show the better efficiency of the proposed protocol in terms of communication and computation cost compare to others protocols in smart grid network. 
id809#Vacant parking slot detection and tracking during driving and parking with a standalone around view monitor#Due to distortion, limitations of vision, and occlusion, most of the existing vacant parking slot detection methods with a standalone around view monitor (AVM) are prone to miss some parking slots and incorrectly identify whether the parking slot is vacant. To overcome this problem, we propose a complete method for vacant parking slot detection and tracking during driving and parking. Considering the different conditions of driving and parking, two different deep convolutional neural networks (DCNNs) are used to detect parking slots, of which the vacant parking slot detection network (VPS-Net) is used to detect vacant parking slots during driving, and the directional marking point detection network (DMPR-PS) is used to detect the directional marking points of the target parking slot during parking. Furthermore, in the driving process, we design a new matching rule and tracking management rule based on the Kernelized Correlation Filter (KCF) to track the parking slots, and fuse classification results of multiple frames to determine the occupancy status. In the parking process, since the parking slot is easily blocked by the vehicle, we design another new tracker to track the directional marking points and infer the complete parking slot using tracking results and prior geometric information. To evaluate the proposed method, a labeled video sequence dataset is established. Experiments show that the proposed method has improved the accuracy and continuity of vacant parking slots detection and positioning whether in the driving process or parking process. 
id810#Embedded Center Prediction Module of Yolov3 Occlusion Human Detection Network [嵌入中心点预测模块的Yolov3遮挡人员检测网络]#To solve the occlusion problem in the current human detection task in actual monitoring scenarios, an improved Yolov3 detection network was proposed. First, in view of the problem that the detected target posture of the existing human detection algorithms is that of single, mostly outdoor, upright pedestrians, a multi-scene human detection dataset(MHDD)containing 16832 samples was self-built for training and testing the network, which included 12090 samples in the training set and 4742 samples in the test set. Then, to improve the detection effect of the network in the case of occlusion, the center prediction module(CPM)was designed and embedded into the three-scale output feature map of the original Yolov3 network. This module first determined the center position of the target as the pre-extracted center point, and then the location and size of the target were accurately regressed on it. Finally, in the accurate regression of the candidate boxes, the GIoU(generalized intersection over union)was used to construct the loss function for optimization, and the regression accuracy was improved by accurately constructing the position relationship between the candidate boxes and real target boxes, which also reduced the fluctuation of the loss function under different scale targets. The experimental results show that the detection accuracy of the detection network on the test set after optimizing the network structure and the loss function is increased by 2.92%, and the missed detection rate is decreased by 2.94%. The network achieves a good detection effect for the occlusion situation in actual monitoring scenarios, and it has good robustness for the detection results of multi-pose human targets. At the same time, the detection speed reaches 28 frames per second, ensuring real-time detection. In addition, the missed detection rate of the network on the Caltech pedestrian database is 6.02%, which also achieves better results than those of the traditional detection networks, further confirming the superiority of the network in pedestrian detection tasks. 
id811#Theoretical and experimental investigation study of data driven work envelope modelling for 3D printed soft pneumatic actuators#In the last decade, soft robotics is considered one of the most widely researched fields in robotics, as it has many advantages and more versatile use than rigid robotics. Soft robots are flexible, which enable them to metaphorically complex designs, enabling them to imitate the movement of living things. In this article, the use of regression models with finite element analysis (FEA) data is compared with neural network (NN) models trained on visual feedback data. The effect of the soft pneumatic actuator (SPA) air pillow inclination angle (β) under positive and vacuum pressure on the actuator work envelope is modelled from the perspective of design and control. The mathematical model is developed by two successive one-dimensional regression models, in which the influence of 21 different designs of positive pressure and vacuum pressure on the working range is studied. A very good matching experimental results acquired using image processing feedback for three selected SPAs at β = 0°, 10°, and 20° have been validated with FEA results. Also, the NN model is developed based on the experimental data of the three SPAs to predict the effect of any β angle on the work envelope under positive and vacuum pressure. Finally, the two models (mathematical and NN models) ability to predict the behaviour of SPA is tested and validated by running random β that is not involved in the main study that β = 13.5°, and 20.5°. The results show great similarity and apply to a wide range of β. 
id812#Workflow for off-site bridge inspection using automatic damage detection-case study of the pahtajokk bridge#For the inspection of structures, particularly bridges, it is becoming common to replace humans with autonomous systems that use unmanned aerial vehicles (UAV). In this paper, a framework for autonomous bridge inspection using a UAV is proposed with a four-step workflow: (a) data acquisition with an efficient UAV flight path, (b) computer vision comprising training, testing and validation of convolutional neural networks (ConvNets), (c) point cloud generation using intelligent hierarchical dense structure from motion (DSfM), and (d) damage quantification. This workflow starts with planning the most efficient flight path that allows for capturing of the minimum number of images required to achieve the maximum accuracy for the desired defect size, then followed by bridge and damage recognition. Three types of autonomous detection are used: masking the background of the images, detecting areas of potential damage, and pixel-wise damage segmentation. Detection of bridge components by masking extraneous parts of the image, such as vegetation, sky, roads or rivers, can improve the 3D reconstruction in the feature detection and matching stages. In addition, detecting damaged areas involves the UAV capturing close-range images of these critical regions, and damage segmentation facilitates damage quantification using 2D images. By application of DSfM, a denser and more accurate point cloud can be generated for these detected areas, and aligned to the overall point cloud to create a digital model of the bridge. Then, this generated point cloud is evaluated in terms of outlier noise, and surface deviation. Finally, damage that has been detected is quantified and verified, based on the point cloud generated using the Terrestrial Laser Scanning (TLS) method. The results indicate this workflow for autonomous bridge inspection has potential. 
id813#Communication set generation for a special case of irregular parallel applications#Irregular computing significantly influences the performance of large scale parallel applications. How to generate local memory access sequence and communication set efficiently for irregular array reference is an important issue in compiling a data-parallel language into a single program multiple data (SPMD) code for distributed-memory machines. By far, many researches have focused on the problem of communication set generation under regular array references in parallel loops. However, little researches give attentions to generating communication set for irregular array accesses in loop nests. In general case of irregular accesses, Inspector/Executor model is adopted to scan over array elements at Inspector phase of run time such that communication set can be constructed. This paper proposes an approach to derive an algebraic solution of communication set enumeration at compile time for the situation of irregular array references in nested loops. And this paper introduces integer lattice into alignment and cyclic(k)-distribution for global-to-local and local-to-global index translations. Then it also presents the algorithm for the corresponding SPMD code generation. When the parameters of alignments and cyclic(k) and array references are known, the SPMD code can then be completely derived at compile time such that no inspector-like run-time code will be needed to construct enumeration of the communication set.
id814#Optimizing subroutines with optional parameters in F90 via function cloning#Optional parameters are a feature of Fortran90. The objective of this feature is to allow the F90 programmer to declare a subroutine using a number of arguments and specify a subset of these parameters as optional, which in turn, allows the programmer to omit any of these optional parameters during invocation of the subroutine. The body of such a subroutine uses an F90 intrinsic function called present(), to test -whether an optional parameter has meaningful values at run-time. The optional parameter can be written/read if and only if present() returns a true value. In this work, we describe a mechanism by which such subroutines which use optional parameters can be efficiently optimized using function cloning or specialization. Function Cloning is a well-known optimization technique that creates multiple copies of a function f(), in order to exploit better optimization opportunities. We will show how cloning/specialization can be utilized such that all calls to present() can be totally eliminated in the body of such subroutines. Cloning will also result in optimization opportunities not exploitable in the original source.
id816#A case-base fuzzification process: diabetes diagnosis case study#Medical case-based reasoning (CBR) systems require the handling of vague or imprecise data. The fuzzy set theory is particularly suitable for this purpose. This paper proposes a case-base preparation framework for CBR systems, which converts the electronic health record medical data into fuzzy CBR knowledge. It generates fuzzy case-base knowledge by suggesting a standard crisp entity–relationship data model for CBR case-base. The resulting data model is fuzzified using a proposed relational data model fuzzification methodology. The performances of this methodology and its resulting fuzzy case-base structure are evaluated. Diabetes diagnosis is used as a case study. A set of 60 real diabetic cases is used in the study. A fuzzy CBR system is implemented to check the diagnoses accuracy. It combines the resulting fuzzy case-base with a proposed fuzzy similarity measure. Experimental results indicate that the proposed fuzzy CBR method is superior to traditional CBR and other machine-learning methods. Our fuzzy CBR achieves an accuracy of 95%, a precision of 96%, a recall 97.96%, an f-measure of 96.97%, a specificity of 81.82%, and good robustness for dealing with vagueness. The resulting fuzzy case-base relational database enhances the representation of case-base knowledge, the performance of retrieval algorithms, and the querying capabilities of CBR systems. 
id817#ScaffCC: Scalable compilation and analysis of quantum programs#Abstract We present ScaffCC, a scalable compilation and analysis framework based on LLVM (Lattner and Adve, 2004), which can be used for compiling quantum computing applications at the logical level. Drawing upon mature compiler technologies, we discuss similarities and differences between compilation of classical and quantum programs, and adapt our methods to optimizing the compilation time and output for the quantum case. Our work also integrates a reversible-logic synthesis tool in the compiler to facilitate coding of quantum circuits. Lastly, we present some useful quantum program analysis scenarios and discuss their implications, specifically with an elaborate discussion of timing analysis for critical path estimation. Our work focuses on bridging the gap between high-level quantum algorithm specifications and low-level physical implementations, while providing good scalability to larger and more interesting problems. 
id818#Towards a comprehensive and robust micromanipulation system with force-sensing and vr capabilities#In this modern world, with the increase of complexity of many technologies, especially in the micro and nanoscale, the field of robotic manipulation has tremendously grown. Microrobots and other complex microscale systems are often to laborious to fabricate using standard microfabrication techniques, therefore there is a trend towards fabricating them in parts then assembling them together, mainly using micromanipulation tools. Here, a comprehensive and robust micromanipulation platform is presented, in which four micromanipulators can be used simultaneously to perform complex tasks, providing the user with an intuitive environment. The system utilizes a vision-based force sensor to aid with manipulation tasks and it provides a safe environment for biomanipulation. Lastly, virtual reality (VR) was incorporated into the system, allowing the user to control the probes from a more intuitive standpoint and providing an immersive platform for the future of micromanipulation. 
id819#The security of quantum noise stream cipher against collective attacks#Quantum noise stream cipher (QNSC), where signal states are masked by intrinsic quantum noise to directly encrypt data, provides a physical layer security. This paper firstly discusses the security of the QNSC system under the assumption that eavesdropper is restricted to collective attacks. The maximum security capacity for the encrypted data, the running key, and the entire system is derived, respectively. Our simulation results allow a positive information capacity when data transmission distance is no more than 300 km for the number of bases Mb= 31 and any mean photon number α2 by assuming state-of-the-art technology. We find that the security of whole system depends on the security of running key for shorter distances. However, the security of whole system is related to the security of data for longer distances. Furthermore, it is important to improve security of data in QNSC system for longer distances. Besides, this new method proposes a concrete solution for physical encryption systems and paves the way for a wider implementation of the QNSC system. 
id820#Scalable and Efficient Hardware Architectures for Authenticated Encryption in IoT Applications#Internet of Things (IoT) is a key enabling technology, wherein sensors are placed ubiquitously to collect and exchange information with their surrounding nodes. Due to the inherent interconnectivity, IoT devices are vulnerable to cybersecurity attacks. To mitigate these vulnerabilities, cryptographic primitives can be employed, but they require significant computation, which restricts their adoption in IoT. Moreover, IoT systems have diverse requirements, ranging from high-throughput (TP) to the area constrained. This makes it hard to deploy appropriate security measures in a systematic manner. To address these issues, three generic implementation strategies (unrolled, round-based, and serialized) are proposed for developing highly efficient hardware architectures. They are applicable to all authenticated encryption schemes and are lightweight and fast, compared to conventional public key encryption. In this article, Ascon is implemented as an example based on those three strategies: 1) the unrolled architecture achieves TP of 766.9 Mb/s (Ascon-128) and 1389.2 Mb/s (Ascon-128a), which are suitable for high-throughput IoT applications; 2) the round-based architecture achieves 0.153 (Ascon-128) and 0.244 (Ascon-128a) TP-to-area ratio, which are, respectively, 73.8% and 40.2% better than state-of-the-art results; and 3) a novel serialized implementation technique is proposed wherein the substitution-box (S-box) is processed in multiple-bit-per-cycle, in contrast to the conventional one-bit-per-cycle approach. The TP of the two-bits-per-clock-cycle implementation is increased by 230.8% with only 36.8% additional hardware area. The proposed strategies allow us to scale the number of rounds (round-based) and bits-per-clock-cycle (serialized) to meet differing requirements in TP and area which are demonstrated for smart city IoT applications. 
id823#Blindfolded Evaluation of Random Forests with Multi-Key Homomorphic Encryption#Decision tree and its generalization of random forests are a simple yet powerful machine learning model for many classification and regression problems. Recent works propose how to privately evaluate a decision tree in a two-party setting where the feature vector of the client or the decision tree model (such as the threshold values of its nodes) is kept secret from another party. However, these works cannot be extended trivially to support the outsourcing setting where a third-party who should not have access to the model or the query. Furthermore, their use of an interactive comparison protocol does not support branching program, hence requires interactions with the client to determine the comparison result before resuming the evaluation task. In this paper, we propose the first secure protocol for collaborative evaluation of random forests contributed by multiple owners. They outsource evaluation tasks to a third-party evaluator. Upon receiving the client's encrypted inputs, the cloud evaluates obliviously on individually encrypted random forest models and calculates the aggregated result. The system is based on our new secure comparison protocol, secure counting protocol, and a multi-key somewhat homomorphic encryption on top of symmetric-key encryption. This allows us to reduce communication overheads while achieving round complexity lower than existing work. 
id825#Design and Implementation of Novel BRISI Lightweight Cipher for Resource Constrained Devices#The Internet of Things (IoT) and cyber-physical systems (CPS) has grown exponentially over the recent years, has motivated the development and deployment of the low resource devices for a wide range of applications in the IoT. Many such resource constrained devices are deployed to match the heterogeneous application requirements of IoT and CPS systems, wherein privacy and security have emerged, as the most difficult challenges, as the constrained devices are not been designed to have security features. This paper presents a lightweight cipher, based on ARX (Addition-Modulo, Rotation and XOR) operations, Fiestel structure, an amalgamation of BRIGHT and SIMON structure, hence the name BRISI. The cipher encrypts 32-bit plaintext using 64-bit key. The software implementation is performed using MATLAB tool and it fulfils the Avalanche criterion, Key-sensitivity, correlation coefficient, entropy and histogram. The proposed design is simulated using Xilinx Vivado and is implemented on Nexys-4 DDR Artix-7 and Basys-3 Artix-7 FPGA family and is evaluated for (LUT and register) power and timing 
id826#Design and implementation of a maxi-sized mobile robot (Karo) for rescue missions#Rescue robots are expected to carry out reconnaissance and dexterity operations in unknown environments comprising unstructured obstacles. Although a wide variety of designs and implementations have been presented within the field of rescue robotics, embedding all mobility, dexterity, and reconnaissance capabilities in a single robot remains a challenging problem. This paper explains the design and implementation of Karo, a mobile robot that exhibits a high degree of mobility at the side of maintaining required dexterity and exploration capabilities for urban search and rescue (USAR) missions. We first elicit the system requirements of a standard rescue robot from the frameworks of Rescue Robot League (RRL) of RoboCup and then, propose the conceptual design of Karo by drafting a locomotion and manipulation system. Considering that, this work presents comprehensive design processes along with detail mechanical design of the robot’s platform and its 7-DOF manipulator. Further, we present the design and implementation of the command and control system by discussing the robot’s power system, sensors, and hardware systems. In conjunction with this, we elucidate the way that Karo’s software system and human–robot interface are implemented and employed. Furthermore, we undertake extensive evaluations of Karo’s field performance to investigate whether the principal objective of this work has been satisfied. We demonstrate that Karo has effectively accomplished assigned standardized rescue operations by evaluating all aspects of its capabilities in both RRL’s test suites and training suites of a fire department. Finally, the comprehensiveness of Karo’s capabilities has been verified by drawing quantitative comparisons between Karo’s performance and other leading robots participating in RRL. 
id827#POET: A scripting language for applying parameterized source-to-source program transformations#We present POET, a scripting language designed for applying advanced program transformations to code in arbitrary programming languages as well as building ad hoc translators between these languages. We have used POET to support a large number of compiler optimizations, including loop interchange, parallelization, blocking, fusion/fission, strength reduction, scalar replacement, SSE vectorization, among others, and to fully support the code generation of several domain-specific languages, including automatic tester/timer generation, and automatically translating a finite-state-machine- based behavior modeling language into C++/ Java code. This paper presents key design and implementation decisions of the POET language and show how to use various language features to significantly reduce the difficulty of supporting programmable compiler optimization for high performance computing and supporting ad hoc code generation for various domain-specific languages. 
id828#Formal verification of a C-like memory model and its uses for verifying program transformations#This article presents the formal verification, using the Coq proof assistant, of a memory model for low-level imperative languages such as C and compiler intermediate languages. Beyond giving semantics to pointer-based programs, this model supports reasoning over transformations of such programs. We show how the properties of the memory model are used to prove semantic preservation for three passes of the Compcert verified compiler. 
id829#Programming a topological quantum computer#Topological quantum computing has recently proven itself to be a powerful computational model when constructing viable architectures for large scale computation. The topological model is constructed from the foundation of a error correction code, required to correct for inevitable hardware faults that will exist for a large scale quantum device. It is also a measurement based model of quantum computation, meaning that the quantum hardware is responsible \emph{only} for the construction of a large, computationally universal quantum state. This quantum state is then strategically consumed, allowing for the realisation of a fully error corrected quantum algorithm. The number of physical qubits needed by the quantum hardware and the amount of time required to implement an algorithm is dictated by the manner in which this universal quantum state is consumed. In this paper we examine the problem of algorithmic optimisation in the topological lattice and introduce the required elements that will be needed when designing a classical software package to compile and implement a large scale algorithm on a topological quantum computer. 
id831#Optical designs for realization of a set of schemes for quantum cryptography#Several quantum cryptographic schemes have been proposed and realized experimentally in the past. However, even with an advancement in quantum technology and escalated interest in the designing of direct secure quantum communication schemes there are not many experimental implementations of these cryptographic schemes. In this paper, we have provided a set of optical circuits for such quantum cryptographic schemes, which have not yet been realized experimentally by modifying some of our theoretically proposed secure communication schemes. Specifically, we have proposed optical designs for the implementation of two single photon and one entangled state based controlled quantum dialogue schemes and subsequently reduced our optical designs to yield simpler designs for realizing other secure quantum communication tasks, i.e., controlled deterministic secure quantum communication, quantum dialogue, quantum secure direct communication, quantum key agreement, and quantum key distribution. We have further proposed an optical design for an entanglement swapping based deterministic secure quantum communication and its controlled counterpart. Finally, a brief discussion on security of the schemes, hacking strategies against different optical elements and corresponding countermeasures is also presented. 
id833#Recent Developments and Methods of Cloud Data Security in Post-Quantum Perspective#Cloud computing has changed the paradigm of using computing resources. It has shifted from traditional storage and computing to Internet based computing leveraging economy of scale, cost saving, elimination of data redundancy, scalability, availability and regulatory compliance. With these, cloud also brings plenty of security issues. As security is not a one-time solution, there have been efforts to investigate and provide countermeasures. In the wake of emerging quantum computers, the aim of post-quantum cryptography is to develop cryptography schemes that are secure against both classical computers and quantum computers. Since cloud is widely used across the globe for outsourcing data, it is essential to strive at providing betterment of security schemes from time to time. This paper reviews recent development, methods of cloud data security in post-quantum perspectives. It provides useful insights pertaining to the security schemes used to safeguard data dynamics associated with cloud computing. The findings of this paper gives directions for further research in pursuit of more secure cloud data storage and retrieval. 
id834#The use of data mining and artificial intelligence technology in art colors and graph and images of computer vision under 6G internet of things communication#To further optimize the existing methods in the field of computer vision and improve the intelligence of image data mining technology, the relevant feedback technology is combined with traditional image data mining technology, and an image data mining technology based on the relevant feedback K-Nearest-Neighbor (K-NN) algorithm is designed and further optimized. The focus is the actual feature extraction test for image color and shape. The test results reveal that for retrieval of K = 1, K = 2, K = 3 images, both positive feedback K-NN algorithm and negative feedback K-NN algorithm can effectively improve the accuracy of image data mining. Among them, negative feedback K-NN algorithm has the highest accuracy for image shape feature extraction. When there are K = 3 images, the accuracy of image data mining can reach 78.3%. Then, the image mining research is conducted on multiple databases. In a total of four databases, the accuracy of image retrieval increases with the increase of feedback times. At the same time, using the optimized KNN algorithm can greatly improve the accuracy of image feature extraction, and the highest accuracy can reach 99.3%. The research content can provide a scientific reference for the follow-up study of KNN algorithm. 
id835#New card-based copy protocols using only random cuts#"In card-based cryptography, a commitment to a Boolean value is usually represented by two face-down cards of different colors or numbers, whose order specifies the one-bit value (namely, 0 or 1). One of the most important primitives in card-based cryptography is a ""copy protocol,""which is supposed to make two identical copies of a given commitment. In the literature, there are several copy protocols, which can be categorized by kinds of shuffles they use; this paper focuses on those using only the so-called random cut, which is the simplest shuffle, and we propose two copy protocols that are more efficient than the existing ones. Specifically, we first work on a standard deck of cards and design a six-card copy protocol using three random cuts (on average). Since the previous protocol needs 5.5 random cuts, our protocol improves upon it. Next, we shift our attention to the case of a two-colored deck of cards, and construct a six-card copy protocol using three random cuts (on average). Because the previous protocol requires eight cards, our protocol uses two cards fewer than the previous one (although it uses one more shuffle). In addition, going back to the standard-deck setting, we provide a four-card XOR protocol using only one random cut for the first time. "
id836#ALAM: Anonymous Lightweight Authentication Mechanism for SDN-Enabled Smart Homes#The smart connected devices are the first choice of cybercriminals for spreading spy wares and different security attacks. The current security standards and protocols for Internet of Things (IoT) have failed in providing security to these devices. In addition, IoT market giants are producing nonsecure smart products in order to grab the open market. Furthermore, low resources of IoT devices, limits the traditional host-based protection solutions like anti-virus, IDS, IPS, etc. To overcome the resource constraintness and security barriers of smart devices, a network-level security architecture based on lightweight cryptographic parameters is required. Software-defined networking (SDN) is a new networking paradigm to overcome the control, management, and security issues in traditional networking. The SDN controller handles all the computation and complexities at the network level, rather than smart devices. In this research, we first present a new privacy-preserving security architecture for SDN-based smart homes. Subsequently, an anonymous lightweight authentication mechanism (ALAM) is designed based on the proposed security architecture core foundations. Furthermore, the security characteristics of the proposed protocol are formally analyzed using Burrows-Abadi-Needham (BAN) logic and ProVerif, followed by informal security analysis. Finally, performance evaluation and comparative analysis of the scheme is carried out. 
id837#SeqNet: Learning Descriptors for Sequence-Based Hierarchical Place Recognition#Visual Place Recognition (VPR) is the task of matching current visual imagery from a camera to images stored in a reference map of the environment. While initial VPR systems used simple direct image methods or hand-crafted visual features, recent work has focused on learning more powerful visual features and further improving performance through either some form of sequential matcher / filter or a hierarchical matching process. In both cases the performance of the initial single-image based system is still far from perfect, putting significant pressure on the sequence matching or (in the case of hierarchical systems) pose refinement stages. In this paper we present a novel hybrid system that creates a high performance initial match hypothesis generator using short learnt sequential descriptors, which enable selective control sequential score aggregation using single image learnt descriptors. Sequential descriptors are generated using a temporal convolutional network dubbed SeqNet, encoding short image sequences using 1-D convolutions, which are then matched against the corresponding temporal descriptors from the reference dataset to provide an ordered list of place match hypotheses. We then perform selective sequential score aggregation using shortlisted single image learnt descriptors from a separate pipeline to produce an overall place match hypothesis. Comprehensive experiments on challenging benchmark datasets demonstrate the proposed method outperforming recent state-of-the-art methods using the same amount of sequential information. Source code and supplementary material can be found online.11[Online]. Available: https://github.com/oravus/seqNet. 
id838#An Archetype Query Language interpreter into MongoDB: Managing NoSQL standardized Electronic Health Record extracts systems#The fast development of today's healthcare and the need to extract new medical knowledge from exponentially-growing volumes of standardized Electronic Health Records data, as required by studies in Precision Medicine, brings up a challenge that may probably only be addressed using NoSQL DBMSs, due to the non-optimal performance of traditional relational DBMSs on standardized data; and these database systems operated by semantic archetype-based query languages, because of the expected generalized extension of standardized EHR systems. An AQL into MongoDB interpreter has been developed to its first version. It translates system-independent AQL queries posed on ISO/EN 13606 standardized EHR extracts into the NoSQL MongoDB query language. The new interpreter has the advantages of both the archetype-based system-independent AQL queries and the dual-model-based standardized EHR extracts stored on document-centric NoSQL DBMSs, such as MongoDB. AQL queries are independent of applications, programming languages and system environments due to the use of the dual model, but EHR extracts featuring this model are best persisted on document-based NoSQL databases. Consequently, the interpreter allows us to query standardized EHR extracts semantically, and also affording optimal performance. 
id839#Lattice-based Key-sharing Schemes: A Survey#Public-key cryptography is an indispensable component used in almost all of our present-day digital infrastructure. However, most if not all of it is predominantly built upon hardness guarantees of number theoretic problems that can be broken by large-scale quantum computers in the future. Sensing the imminent threat from continued advances in quantum computing, NIST has recently initiated a global-level standardization process for quantum resistant public-key cryptographic primitives such as public-key encryption, digital signatures, and key encapsulation mechanisms. While the process received proposals from various categories of post-quantum cryptography, lattice-based cryptography features most prominently among all the submissions. Lattice-based cryptography offers a very attractive alternative to traditional public-key cryptography mainly due to the variety of lattice-based schemes offering varying flavors of security and efficiency guarantees. In this article, we survey the evolution of lattice-based key-sharing schemes (public-key encryption and key encapsulation schemes) and cover various aspects ranging from theoretical security guarantees, general algorithmic frameworks, practical implementation aspects, and physical attack security, with special focus on lattice-based key-sharing schemes competing in the NIST's standardization process. 
id840#Mostly C, challenges in LEGO® RCX code generation#In this paper, we describe the challenges of generating code for the LEGO® RCX microcomputer controller. The compiler improves on the widely used NQC (not-quite C) compiler with the addition of new data types, procedures, unions, and a math library. Copyright 2006 ACM.
id841#A multimodal biometric authentication scheme based on feature fusion for improving security in cloud environment#In recent days, due to the advent of advanced technologies such as cloud computing, accessing data can be done anywhere at any time. Meanwhile, ensuring the data security is highly significant. Authentication plays a major role in preserving security via different access control mechanisms. As a recent trend, the biological information of the individual user is considered as verification scheme for the authentication process. Traits such as fingerprint, iris, ear or palm print are widely used to develop the authentication systems from its patterns. But, to increase the complexity of the user authentication and to ensure high security, more than a trait is combined together. In this paper, a multimodal authentication system is proposed by fusing the feature points of fingerprint, iris and palm print traits. Each trait has undergone the following procedures of image processing techniques such as pre-processing, normalization and feature extraction. From the extracted features, a unique secret key is generated by fusing the traits in two stages. False Acceptance Rate (FAR) and False Rejection Rate (FRR) metrics are used to measure the robustness of the system. This performance of the model is evaluated using three standard symmetric cryptographic algorithms such as AES, DES and Blowfish. This proposed model provides better security and access control over data in cloud environment. 
id842#x-only point addition formula and faster compressed SIKE#The optimization of the main key compression bottlenecks of the supersingular isogeny key encapsulation mechanism (SIKE) has been a target of research in the last few years. Significant improvements were introduced in the recent works of Costello et al. (EUROCRYPT’2017) and Zanon et al. (PQCrypto’2018; IEEE ToC’2018). The combination of the techniques in Zanon et al. (PQCrypto’2018; IEEE ToC’2018) reduced the running time of binary torsion basis generation in decompression by a factor of 29 compared to previous work. On the other hand, generating such a basis still takes almost a million cycles on an Intel Core i5-6267U Skylake. In this paper, we continue the work of Zanon et al. (IEEE ToC’2018) and introduce a technique that drops the complexity of binary torsion basis generation by a factor log p in the number of underlying field multiplications. In particular, our experimental results show that a basis can be generated in about 1300 cycles, attaining an improvement by a factor more than 600. Although this result eliminates one of the key compression bottlenecks, many other bottlenecks remain. In addition, we give further improvements for the ternary torsion generation with significant impact on the related decompression procedure. Moreover, a new trade-off between ciphertext sizes versus decapsulation speed and storage is introduced and achieves a 1.7 times faster decapsulation. 
id843#Quantitative Evaluation of the Thickness of the Available Manipulation Volume Inside the Knee Joint Capsule for Minimally Invasive Robotic Unicondylar Knee Arthroplasty#Objective: Developing robotic tools that introduce substantial changes in the surgical workflow is challenging because quantitative requirements are missing. Experiments on cadavers can provide valuable information to derive workspace requirements, tool size, and surgical workflow. This work aimed to quantify the volume inside the knee joint available for manipulation of minimally invasive robotic surgical tools. In particular, we aim to develop a novel procedure for minimally invasive unicompartmental knee arthroplasty (UKA) using a robotic laser-cutting tool. Methods: Contrast solution was injected into nine cadaveric knees and computed tomography scans were performed to evaluate the tool manipulation volume inside the knee joints. The volume and distribution of the contrast solution inside the knee joints were analyzed with respect to the femur, tibia, and the anatomical locations that need to be reached by a laser-cutting tool to perform bone resection for a standard UKA implant. Results: Quantitative information was determined about the tool manipulation volume inside these nine knee joints and its distribution around the cutting lines required for a standard implant. Conclusion: Based on the volume distribution, we could suggest a possible workflow for minimally invasive UKA, which provides a large manipulation volume, and deducted that for the proposed workflow, an instrument with a thickness of 5-8 mm should be feasible. Significance: We present quantitative information on the three-dimensional distribution of the maximally available volume inside the knee joint. Such quantitative information lays the basis for developing surgical tools that introduce substantial changes in the surgical workflow. 
id844#Cryptographic Design of PriCloud, a Privacy-Preserving Decentralized Storage with Remuneration#Over the last years, demand for file hosting has sky-rocketed due to cost reductions and availability of services. However, centralized providers have a negative impact on the privacy of their users, since they are able to read and collect various data about their users and even link it to their identity via their payments. On the other hand, decentralized storage solutions like GNUnet suffer from a lack of participation by providers, since there is no feasible business model. We propose PriCloud, a decentralized storage system which allows users to pay their storage providers without sacrificing their privacy by employing anonymous storage smart contracts and private payments on a blockchain. We are able to provide privacy to the users and storage providers, and unlinkability between users and files. Our system offers decentralized file storage including strong privacy guarantees and built-in remuneration for storage providers. 
id845#Trajectory Prediction in Autonomous Driving with a Lane Heading Auxiliary Loss#Predicting a vehicle's trajectory is an essential ability for autonomous vehicles navigating through complex urban traffic scenes. Bird's-eye-view roadmap information provides valuable information for making trajectory predictions, and while state-of-the-art models extract this information via image convolution, auxiliary loss functions can augment patterns inferred from deep learning by further encoding common knowledge of social and legal driving behaviors. Since human driving behavior is inherently multimodal, models which allow for multimodal output tend to outperform single-prediction models on standard metrics. We propose a loss function which enhances such models by enforcing expected driving rules on all predicted modes. Our contribution to trajectory prediction is twofold; we propose a new metric which addresses failure cases of the off-road rate metric by penalizing trajectories that oppose the ascribed heading (flow direction) of a driving lane, and we show this metric to be differentiable and therefore suitable as an auxiliary loss function. We then use this auxiliary loss to extend the the standard multiple trajectory prediction (MTP) and MultiPath models, achieving improved results on the nuScenes prediction benchmark by predicting trajectories which better conform to the lane-following rules of the road. &copy; 2021 IEEE. 
id846#Online Integration of SQL and No-SQL Databases using RestAPIs: A Case on 2 furniture e-Commerce Sites#Database technology is one of the key elements in every internet-based system. As technology develops and the need for fast and large data exchange arises, a type of NoSQL or unstructured database emerges. SQL and relational databases have table forms, while NoSQL has a format of document-oriented stores. Integration issue came as a challenge when the two different databases were used on the same software. The challenge is increasingly complicated when the database used becomes the resource of online websites. In this study, we present a case which based on 2 online e-Commerce furniture websites that. These both online websites will merge their product data into 1 new database. This research combines two different databases using Web services to retrieve data in each database and a synchronization system for the process of checking and storing data into a new database. All merging components are stored on cloud storage services. Every batching process in the synchronization system is done online and automatically uses Cron. This research provides the results of data integration testing on web services and measurement of the synchronization system execution time. After testing 2 web services used, all passed the data integration test. Then measurements are made on the synchronization system by looking at the displayed execution time. The results of measurements using 100 to 1000 data and different conditions, the execution time obtained from 4 to 5 seconds for each data in the measurement increases to 500 data. The results of this study indicate that the integration of data from 2 different databases using web services and synchronization systems is tolerable and suitable for the amount of data under 10000 data with the condition of the database already filled. 
id848#MatchingRef: Matching Variable Names in a Reference Page to Help Introductory CS Students Fix Compiler Errors#Debugging compiler errors is essential to programming and can be challenging for novice programmers. In introductory computer science courses, challenging errors can discourage students. One reason these errors are difficult to resolve is that most online help systems do not match a student's code. For example, online reference pages use different variable names, identifiers, and method names compared with a student's particular code. To utilize existing resources, students must wade through other people's code (which is often too advanced for novices to comprehend). This is time-consuming and does not provide novices with solutions. To address this problem, we developed MatchingRef -a reference system that helps novices resolve compiler errors. It is a web-based reference guide that catalogs common Processing/Java compiler errors. MatchingRef integrates with the Processing programming environment to provide users with explanations and that match users' particular code. It includes a list of strategies to fix each error, accompanied by one or more concrete examples. Importantly, the key feature that distinguishes MatchingRef from prior reference systems (e.g., Decaf [1], HelpMeOut [4], CodeWrite [2]) is that MatchingRef examples are all dynamically generated using variable, method, and class names from users' programming environments. There are four guiding design principles behind MatchingRef. First is readability, which is listed as a criterion that good error messages should exhibit [5]. The description from Java documentation is usually technical and causes confusion for novice programmers. Hence, we provide explanations that are less cryptic and more familiar in wording. Second is learning by examples. Examples have been shown to be effective way to learn and fix programming errors, such as in online forum like Stack Overflow [7]. Therefore, we include examples in each suggestion so that users can have a concrete idea on how to fix the problems. Third is familiarity, a common design principle across domains [3]. MatchingRef matches elements in the fixing examples with the original code of the users. Finally, our system is intended to reduce cognitive load by customizing the pages to users' code to show the relevant information. According to Cognitive Load Theory (CLT), humans have a finite ability to efficiently process input [6]. As MatchingRef provide users with matching error messages and matching names, their brain only needs to process useful information such as ways to fix the errors or roots of the errors. Users can avoid reading some information in the page multiple times such as variable names or class names because they are already familiar with them. We conducted a within-subjects pilot study (n = 4) to evaluate whether MatchingRef improved novices' performance and comprehension while debugging compiler errors. Participants interacted with two designs of our system, one with matching variable names in the examples and one without this feature. Otherwise, the systems were identical. The participants were students in introductory computer science courses that use Processing. Due to the small sample size, we could not detect significant differences in task completion time. Although the conclusions we can draw are limited, we are encouraged to evaluate MatchingRef in a more formal study. 
id849#Trajectory tracking control of quadcopters under tunnel effects#There are many potential applications to utilise aerial robots in hazardous tunnel-like environments. For example, aiding human operators with inspections of small railway culverts or mineral mappings of mining tunnels. Nevertheless, such confined environments pose many challenges for quadcopters to navigate through. Suspended dust particles, poor lighting conditions and featureless/excessive features in the surroundings make localisation difficult. Furthermore, the fluid interactions between the rotors’ downwash and the surfaces of the surroundings create aerodynamic disturbances, which threaten the quadcopter's stability and increase its risk of collision in the restricted confined space, not to mention the longitudinal wind gusts. This paper presents our findings on the characteristics of these aerodynamic disturbances, the Tunnel Effects for quadcopters, in a 1.5m(W) x 1.5m(H) square cross section tunnel through a series of experiments. A semi-autonomous system is proposed with self-stabilisation in the vertical and lateral axes while a pilot provides commands in heading and the longitudinal direction of the tunnel for performing required tasks such as tunnel wall inspections. We propose a cross-sectional localisation scheme using Hough Scan Matching with a simple kinematic Kalman filter for providing reliable vertical and lateral position information. An integral backstepping (IBS) controller is designed and implemented to enable quadcopters to robustly fly in tunnel-like confined environments. The proposed system is tested in simulated tunnel environments and a real railway tunnel with various reference trajectories, and the IBS controller has shown superior tracking performance in comparison with a PID controller despite of the existence of the Tunnel Effects. 
id850#A True Random Number Generator Based on Ionic Liquid Modulated Memristors#The memristor-based neuromorphic computing application, which is highly flexible and capable of handling large amounts of parallel information, is one of the major breakthroughs in the past decade. It sheds light on future high-density storage, ultrafast logic computing systems, advanced artificial intelligence, etc. To explore the applications based on memristive devices, a memristive device using an indium-gallium-zinc oxide (IGZO)-TiO2 bilayer film as the functional layer is demonstrated. Its memristive behavior can be modulated by diethylmethyl(2-methoxyethyl)ammonium bis(trifluoromethylsulfonyl)imide (DEME-TFSI) ionic liquid (IL). Further, a true random number generator was designed on the basis of eight memristive units. This work demonstrates that ionic liquid regulated memristors can not only be used for data storage but also open up potential applications for cryptography. 
id851#Taijignn: A new cycle-consistent generative neural network for high-quality bidirectional transformation between rgb and multispectral domains#Since multispectral images (MSIs) and RGB images (RGBs) have significantly different definitions and severely imbalanced information entropies, the spectrum transformation between them, especially reconstructing MSIs from RGBs, is a big challenge. We propose a new approach, the Taiji Generative Neural Network (TaijiGNN), to address the above-mentioned problems. TaijiGNN consists of two generators, G_MSI, and G_RGB. These two generators establish two cycles by connecting one generator’s output with the other’s input. One cycle translates the RGBs into the MSIs and converts the MSIs back to the RGBs. The other cycle does the reverse. The cycles can turn the problem of comparing two different domain images into comparing the same domain images. In the same domain, there are neither different domain definition problems nor severely underconstrained challenges, such as reconstructing MSIs from RGBs. Moreover, according to several investigations and validations, we effectively designed a multilayer perceptron neural network (MLP) to substitute the convolutional neural network (CNN) when implementing the generators to make them simple and high performance. Furthermore, we cut off the two traditional CycleGAN’s identity losses to fit the spectral image translation. We also added two consistent losses of comparing paired images to improve the two generators’ training effectiveness. In addition, during the training process, similar to the ancient Chinese philosophy Taiji’s polarity Yang and polarity Yin, the two generators update their neural network parameters by interacting with and complementing each other until they all converge and the system reaches a dynamic balance. Furthermore, several qualitative and quantitative experiments were conducted on the two classical datasets, CAVE and ICVL, to evaluate the performance of our proposed approach. Promising results were obtained with a well-designed simplistic MLP requiring a minimal amount of training data. Specifically, in the CAVE dataset, to achieve comparable state-of-the-art results, we only need half of the dataset for training; for the ICVL dataset, we used only one-fifth of the dataset to train the model, but obtained state-of-the-art results. 
id853#Quantifying the Impact of EER Modeling on Relational Database Success: An Experimental Investigation#Despite the widespread idea in literature that the inclusion of EER modeling in the design process of a relational database is beneficial for the success of that database, almost no quantitative cost-benefit analyses of EER modeling exist today to support this statement. In order to fill this need, an empirical study is performed in which the success of a relational database of which the design process contains an EER modeling phase is compared to the success of a relational database in which only the minimally needed design effort was put. Hereby, database success is treated as originally proposed by the DeLone and McLean Information Systems Success Model, by specifically focusing on the information quality and system quality of both databases. To this end, respectively, the total amount of time that is needed by an end user to complete a set of tasks by using the database, and the total execution cost that is needed by the database system before a correct solution to each task is submitted, is analyzed. Moreover, the work accounts for the possible moderation of the technical competence of an end user in the relationship between EER modeling and the success of the eventual relational database. Preliminary results indicate that the inclusion of EER modeling in relational database design significantly highers the perceived information quality and system quality of that database. Moreover, there is statistical evidence that this result is independent of the competence profile of that user. 
id854#A new one-dimensional chaotic map and its application in a novel permutation-less image encryption scheme#In this paper, we propose a new real one-dimensional cosine fractional (1-DCF) chaotic map. Several chaos-theory analysis tests demonstrate that the proposed map has many good cryptography properties, such as a highly chaotic behavior, a large chaotic range, an infinite number of unstable fixed points, and a widely superior sensitivity to the initial conditions than most of the low-dimensional chaotic maps. Regarding these attractive features, we use the 1-DCF map to design a novel fast image encryption scheme for real-time image processing. Unlike most of the existing encryption schemes, we adopt a permutation-less architecture to increase the encryption speed. Regardless of the permutation phase absence, a high-security level is obtained by using a substitution process with a high sensitivity to the plain image. Moreover, we replace the natural row-order encryption with a more secure random-like encryption order generated from the secret key. Experimentation and simulations show that the new scheme is better than many recently proposed encryption schemes in both security and rapidity. 
id855#Seeing through events: Real-time moving object sonification for visually impaired people using event-based camera#Scene sonification is a powerful technique to help Visually Impaired People (VIP) understand their surroundings. Existing methods usually perform sonification on the entire images of the surrounding scene acquired by a standard camera or on the priori static obstacles acquired by image processing algorithms on the RGB image of the surrounding scene. However, if all the information in the scene are delivered to VIP simultaneously, it will cause information redundancy. In fact, biological vision is more sensitive to moving objects in the scene than static objects, which is also the original intention of the event-based camera. In this paper, we propose a real-time sonification framework to help VIP understand the moving objects in the scene. First, we capture the events in the scene using an event-based camera and cluster them into multiple moving objects without relying on any prior knowledge. Then, sonification based on MIDI is enabled on these objects synchronously. Finally, we conduct comprehensive experiments on the scene video with sonification audio attended by 20 VIP and 20 Sighted People (SP). The results show that our method allows both participants to clearly distinguish the number, size, motion speed, and motion trajectories of multiple objects. The results show that our method is more comfortable to hear than existing methods in terms of aesthetics. 
id856#Task Space Consensus of Heterogeneous Robots with Time-Delays and without Velocity Measurements#This letter proposes a novel distributed control scheme for heterogeneous robot networks (kinematically and dynamically different), modeled as Euler-Lagrange systems, that solves the leaderless consensus problem in the Special Euclidean space of dimension three (SE(3)). The controller does not rely on velocity measurements and it is robust to variable time-delays in the communication channels. The solution employs a simple Proportional plus damping scheme that, since velocities are not available, injects damping through a dynamical controller. Using Barbalat's Lemma, it is proved that velocities and pose (position and orientation) errors converge asymptotically to zero. A singularity-free representation, unit-quaternions, is used to describe the robots orientations and the network is modeled as an undirected and connected interconnection graph. Simulation results, with an heterogeneous robot network composed of five agents, are presented to illustrate the performance of the proposed control scheme. 
id857#Patient Health Monitoring Information System#This article presents the developed information system for patient health monitoring during and after rehabilitation. The information system comprises a mobile app and a web app. The mobile app collects the data from a smartwatch and sends them to a relational database. The web app manages the information in the database. 
id858#Technology of Multidimensional Data Formation Using Caching#The purpose of this paper is to develop a technology for Hypercube building from a relational database. We offer a new caching subsystem that parses RDB queries analytically and allows us to retrieve previously loaded rows even if the queries are different. In the end, we provide the experiment results of software that implements the technology of multidimensional data construction using the proposed method of caching. This approach is the evolution of traditional OLAP technology. 
id859#Global partial replicate computation partitioning#Early parallelizing compilers use the owner-computes rule to partition computation. Partial replication is then introduced to reduce near-neighbor communication at the cost of some repeated computation. It is an important optimization that improves the performance and scalability of parallel programs. Former exploration of partial replicate computation partitioning is limited within a single loop nest, and no explicit cost model is used. In this paper, a formal description of more general partial replicate computation partitioning problems is presented, which is called global partial replicate computation partitioning. As redundant message elimination exerts great influence on the effect of such optimizations, a linear cost model is introduced, which considers its effect. A framework is also developed, which employs the integer linear programming method. Experimental results show that the solution is superior to local approaches. Compared with the heuristic method, the new approach can deal with more general cases and is easier to adapt to different data distribution.
id860#Information security in the post quantum era for 5G and beyond networks: Threats to existing cryptography, and post-quantum cryptography#Quantum computing is an emerging field that uses the concepts of quantum mechanics to outperform classical computers. Quantum computing finds plethora of applications in the 5G and Beyond networks. It can process data at an exponential rate, which can address numerous business and scientific challenges. In this paper, we provide a detailed review of the field, starting from the most important applications of quantum computers and then diving into the future of cryptography. The major applications of quantum computing include unstructured search, quantum simulation, and optimization. It can also provide improvements in terms of speed and accuracy for some existing technologies, such as machine learning. These technologies have numerous applications in 5G and beyond networks. However, due to such abilities of quantum computing, it can also pose a serious risk to many existing security systems, especially the asymmetric key cryptography schemes. The risk of quantum computing has also influenced the mobile broadband standards to move from a symmetric key cryptography techniques to PKI-based trust model. We also discuss in detail various alternate cryptosystems based on mathematical problems that are believed to be hard even for a quantum computer to solve. In parallel, we discuss the developments in the field of quantum key distribution that makes use of quantum phenomenon to develop a quantum-resistant crypto systems. Such quantum-resistant systems have a great potential in provisioning secure 5G and beyond networks. 
id861#WeStat: A privacy-preserving mobile data usage statistics system#The preponderance of smart devices, such as smartphones, has boosted the development and use of mobile applications (apps) in the recent years. This prevalence induces a large volume of mobile app usage data. The analysis of such information could lead to a better understanding of users' behaviours in using the apps they have installed, even more if these data can be coupled with a given context (location, time, date, sociological data...). However, mobile and apps usage data are very sensitive, and are today considered as personal. Their collection and use pose serious concerns associated with individuals' privacy. To reconcile harnessing of data and privacy of users, we investigate in this paper the possibility to conduct privacy-preserving mobile data usage statistics that will prevent any inference or re-identification risks. The key idea is for each user to encrypt their (private and sensitive) inputs before sending them to the data processor. The possibility to perform statistics on those data is then possible thanks to the use of functional encryption, a cryptographic building block permitting to perform some allowed operations over encrypted data. In this paper, we first show how it is possible to obtain such individuals' usage of their apps, which step is necessary for our use case, but can at the same time pose some security problems w.r.t. those apps. We then design our new encryption scheme, adding some fault tolerance property to a recent dynamic decentralized function encryption scheme. We finally show how we have implemented all that, and give some benchmarks. 
id862#SEQ2SEQ vs sketch filling structure for natural language to SQL translation#Sequence to sequence models have been widely used in the recent years in the different tasks of Natural Language processing. In particular, the concept has been deeply adopted to treat the problem of translating human language questions to SQL. In this context, many studies suggest the use of sequence to sequence approaches for predicting the target SQL queries using the different available datasets. In this paper, we put the light on another way to resolve natural language processing tasks, especially the Natural Language to SQL one using the method of sketch-based decoding which is based on a sketch with holes that the model incrementally tries to fill. We present the pros and cons of each approach and how a sketch-based model can outperform the already existing solutions in order to predict the wanted SQL queries and to generate to unseen input pairs in different contexts and cross-domain datasets, and finally we discuss the test results of the already proposed models using the exact matching scores and the errors propagation and the time required for the training as metrics. 
id863#Growing simulated robots with environmental feedback: An eco-evo-devo approach#Robots are still missing the ability to adapt to new environments. However, biological systems are able to adapt to new environments with ease; perhaps because they have the ability to react to environmental input during a growth phase with changes not only in behaviour, but also morphology. Yet within the field of robots, environmental based development of morphology is an under researched area. In this paper we use an evolutionary algorithm to evolve neural cellular automata capable of inducing environmental based developmental plasticity in robots. We use the kinetic energy of each cell and its neighbours as an input to our network, the output of which determines the position of new cell growth. We evolve our neural cellular automata first in three individual environments and then also for performance in multiple environments. We show that the networks that use environmental feedback outperform those that do not and that by introducing environmental feedback during development, more adaptive and better performing robots are potentially possible. 
id864#Rapid and accurate timing modeling for SRAM compiler#Static random access memory is usually used in the ASIC design. The performances of memory always play major role in the overall circuit. In this paper, we propose an efficient extrapolation-based timing modeling method for SRAM complier. We build the timing equation of access and cycle time of SRAM. Our proposed method only needs to simulate a small number of memory configurations with relatively small sizes. The obtained results are extrapolated to any other configurations. In order to obtain the SRAM macros, we develop the SRAM compiler. Our proposed method is approximately 5% average error. 
id865#Comparative study between computer vision methods for the estimation and detection of the roadway#Autonomous driving is a field of study that is progressing rapidly to ensure road safety. Recently, researchers have been particularly interested in the detection of the roadway. Several new approaches have been proposed in the last decade. In this paper, we present a comparative study of roadway detection methods using three algorithms. The one is based on HOUGH transform, the second on the RANSAC algorithm and the third on the RADON transform. The objective of this paper is to make a comparison of these method in order to robust estimation methods in computer vision and to apply the method of RADON for the detection of the roadway. 
id866#The verified software repository: A step towards the verifying compiler#The verified software repository is dedicated to a long-term vision of a future in which all computer systems justify the trust that society increasingly places in them. This would be accompanied by a substantial reduction in the current high costs of programming error incurred during the design development testing installation maintenance evolution and retirement of computer software. An important technical contribution to this vision will be a verifying compiler: a tool-set that automatically proves that a program will always meet its specification insofar as this has been formalised without even needing to run it. This has been a challenge for computing research for over 30 years but the current state of the art now gives grounds for hope that it may be implemented in the foreseeable future. Achievement of the overall vision will depend also on continued progress of research into dependability and software evolution as envisaged by the UKCRC Grand Challenge project in dependable systems evolution. The verified software repository is a first step towards the realisation of this long-term vision. It will maintain and develop an evolving collection of state-of-the-art tools together with a representative portfolio of real programs and specifications on which to test evaluate and develop the tools. It will contribute initially to the inter-working of tools and eventually to their integration. It will promote transfer of the relevant technology to industrial tools and into software engineering practice. It will build on the recognised achievements of practical formal development of safety-critical computer applications and contribute to an international initiative in verified software covering theory tools and experimental validation. BCS 
id867#On efficiently storing huge property graphs in relational database management systems#Graph structured data can be found in an increasing amount of use-cases. While there exists a considerable number of solutions to store graphs in NoSQL databases, the combined storage of relationally stored data with huge graph structured data within the same relational database system is not well researched.We present a relational approach for storing and querying huge property graphs by combining NoSQL features, provided by nearly any state-of-the- A rt database system, and an adjacency table approach. Our approach is optimized for read-only queries but also performs well on update queries. Through an empirical evaluation we show that we achieve a 10 times higher throughput than previous works on a graph with up to 650 million edges. This way, we can use all the advantages of full-fledged relational database systems and seamlessly integrate classical relational data with graph-structured data in an efficient way. 
id868#A distortion-free watermarking approach for verifying integrity of relational databases#Due to high availability and easy accessibility of information, it has become quite difficult to assure security of data. Even though watermarking seems to be an effective solution to protect data, it is still challenging to be used with relational databases. Moreover, inserting a watermark in database may lead to distortion. As a result, the contents of database can no longer remain useful. Our proposed distortion-free watermarking approach ensures that integrity of database can be preserved by generating an image watermark from its contents. This image is registered with Certification Authority (CA) before the database is distributed for use. In case, the owner suspects any kind of tampering in the database, an image watermark is generated and compared with the registered image watermark. If both do not match, it can be concluded that the integrity of database has been compromised. Experiments are conducted on Forest Cover Type data set to localize tampering to the finest granularity. Results show that our approach can detect all types of attack with 100% accuracy. 
id869#Utilizing computer vision and artificial intelligence algorithms to predict and design the mechanical compression response of direct ink write 3D printed foam replacement structures#Additive Manufacturing (AM) of porous polymeric materials, such as foams, recently became a topic of intensive research due their unique combination of low density, impressive mechanical properties, and stress dissipation capabilities. Conventional methods for fabricating foams rely on complex and stochastic processes, making it challenging to achieve precise architectural control of structured porosity. In contrast, AM provides access to a wide range of printable materials, where precise spatial control over structured porosity can be modulated during the fabrication process enabling the production of foam replacement structures (FRS). Current approaches for designing FRS are based on intuitive understanding of their properties or an extensive number of finite element method (FEM) simulations. These approaches, however, are computationally expensive and time consuming. Therefore, in this work, we present a novel methodology for determining the mechanical compression response of direct ink write (DIW) 3D printed FRS using a simple cross-sectional image. By obtaining measurement data for a relatively small number of samples, an artificial neural network (ANN) was trained, and a computer vision algorithm was used to make inferences about foam compression characteristics from a single cross-sectional image. Finally, a genetic algorithm (GA) was used to solve the inverse design problem, generating the AM printing parameters that an engineer should use to achieve a desired compression response from a DIW printed FRS. The methods developed herein present an avenue for entirely autonomous design and analysis of additively manufactured structures using artificial intelligence. 
id870#Privacy-preserving image retrieval scheme based on secret sharing in cloud environment [云环境下基于秘密共享的图像安全检索方案]#Aiming at the problem that the encryption domain image retrieval scheme under the traditional cloud environment only considers the use of a single server to provide retrieval services, a secure image retrieval scheme based on secret sharing was proposed, and an image secure retrieval model was constructed using secret sharing technology. In the preprocessing stage, the image owner generated and encrypted multiple secret shares of the image index, and outsourced the encrypted index shares together with the encrypted image to different cloud servers. In the query phase, the user generated and encrypted multiple query trapdoors, and then outsourced them to different cloud servers. The cloud server combined secure multi-party computing technology to achieve secure image retrieval. Experimental results show that this scheme can achieve higher retrieval accuracy and security. 
id871#RDTCheck: A Smartphone App for Monitoring Rapid Diagnostic Test Administration#Rapid diagnostic tests are point-of-care medical tests that are used by clinicians and community healthcare workers to get quicker results at a better cost compared to traditional diagnostic tests. Distributing rapid diagnostic tests to people outside of the healthcare industry would significantly improve access to diagnostic testing; however, there are concerns that novices may administer rapid diagnostic tests incorrectly and thus be left with invalid results. In response to this concern, we propose RDTCheck - a mobile application that guides users through the instructions of Quidel's QuickVue Influenza A+B test and ensures adherence to the procedure using computer vision. RDTCheck provides users with real-time feedback so that they may either correct their mistakes or re-administer their test. In this work, we conducted findings from a pilot study that demonstrates how well RDTCheck is able to detect common mistakes and successes during the various steps of the QuickVue test. For the 7 participants we recruited, RDTCheck had an average success rate of 91.1% at giving the correct feedback during the RDT administration procedure. 
id873#A Cross-Domain Authentication Protocol by Identity-Based Cryptography on Consortium Blockchain [基于身份密码系统和区块链的跨域认证协议]#With the exciting growth of global Internet services and applications in the past decades, tremendous amount of various data and service resources are prevailing on network and attracting users from different administration domains all over the world. The Internet cyberspace is never short of security threats and resource abusers. Reliable and efficient network entity authentications and identification verifications are the corner stones for all types of secure network application environments and usage scenarios. Especially how to verify an entity's identity outside its origin, and how to extend such authentication capability across different administration domains in network without obvious security weak point or performance bottleneck, it is a realistic challenge for traditional cryptography based authentication schemes. Either the encryption key based or the PKI certificate based approaches suffer the threats on credential managements and the inefficiency revocation. Towards the problem of cross-domain authentication when users in heterogeneous network environments access network services from different trust domains, this paper proposes a new design of blockchain certificate to implement cross-domain authentication based on the identity-based cryptosystem and the distributed architecture of blockchain technology. A novel cross-trust-domain authentication scheme based on IBC system is constructed and evaluated. Firstly, to solve the problem of instantaneous entity identity revocation based on the IBC architecture, a security-mediator based identity signature scheme, mIBS, is proposed with optimized identity management scheme. A security mediator serves in a trust domain to approve or decline any authentication attempt. By retaining part of each entity's identity authentication key in the domain, the security mediator can quickly collaborate with other nodes to either verify the entity's identity or fail its request for authentication, i.e. revocation. The proposed mIBS algorithm for IBC-based intro-domain authentication, ensures entity authentication functionality and security, with the computation overhead reduced greatly compared with the ID-BMS scheme. The cross-domain authentication is supported and implemented on a consortium blockchain system. We optimize the PKI certificate structure and design a blockchain certificate to record domain credential on blockchain. Blockchain certificate authorities, just like CAs in X.509, are organized and coordinated together to run the consortium ledger as the domain credential storage, verification and exchange platform. Compared with the centralized CA organization, the distributed ledger on blockchain nodes has better replication of certificate data, higher scalability, cryptography-guaranteed information integrity, and decentralized consensus calculation capability. The proposed mIBS algorithm and the blockchain-based authentication protocol are thoroughly evaluated for security and efficiency. Theoretical analysis and deduction show the new scheme holds the same security strength as the original IBC system, but saves some on the operation execution overhead. The state-of-the-art distributed user authentication schemes in literature are used as benchmarks to evaluate the proposed blockchain-based distribution authentication. The new scheme is robust enough to survive any typical network attacks and interruptions, and with significantly improved computation overhead efficiency when being measured alive on experimental machines. 
id874#A systematic quality assurance framework for the upgrade of radiation oncology information systems#In spite of its importance, no systematic and comprehensive quality assurance (QA) program for radiation oncology information systems (ROIS) to verify clinical and treatment data integrity and mitigate against data errors/corruption and/or data loss risks is available. Based on data organization, format and purpose, data in ROISs falls into five different categories: (1) the ROIS relational database and associated files; (2) the ROIS DICOM data stream; (3) treatment machine beam data and machine configuration data; (4) electronic medical record (EMR) documents; and (5) user-generated clinical and treatment reports from the ROIS. For each data category, this framework proposes a corresponding data QA strategy to very data integrity. This approach verified every bit of data in the ROIS, including billions of data records in the ROIS SQL database, tens of millions of ROIS database-associated files, tens of thousands of DICOM data files for a group of selected patients, almost half a million EMR documents, and tens of thousands of machine configuration files and beam data files. The framework has been validated through intentional modifications with test patient data. Despite the ‘big data’ nature of ROIS, the multiprocess and multithread nature of our QA tools enabled the whole ROIS data QA process to be completed within hours without clinical interruptions. The QA framework suggested in this study proved to be robust, efficient and comprehensive without labor-intensive manual checks and has been implemented for our routine ROIS QA and ROIS upgrades. 
id875#Machine vision based potato species recognition#Potato is one of the tasteful vegetable in the list of our daily delicious food. At present, there are 42000 kind of potatoes available in the world. In Bangladesh, we cultivate 82 species every year. Potatoes are used for other purposes besides eating. So it is produced by thinking of other purposes besides eating. Different varieties of potatoes are used for different purposes. But People usually do not know which variety of potato is suitable for which work. We took this research step to solve this problem. There are currently some conventional common detection methods that are not very convenient. Therefore, we have introduced Machine Vision Recognition (MVR) procedure to discover a suitable technique. As if, through this method people can easily identify the potato species. In this research paper, we want to show how to identify different varieties of potatoes in Bangladesh using machine vision approach. We have been collected total 1200 potato imagers from four fact for our experience. To reach our aim, several machine learning algorithms have been applied to the datasets, like Random Forest Classifier (RF), Linear Discriminant Analysis (LDA), Logistic Regression, Support Vector Machine (SVM), CART, NB, and KNN. Once we have been applied all the algorithms, different results have been shown by each algorithm. Logistic Regression shows the best result, which has an accuracy rate of 98%. In contrast, the lowest rate has been shown by the SVM. The accuracy rate of SVM is 33% which is not only a good fit for future research but also promising. 
id876#A study of fuzzy query systems for relational databases#In many cases information is found to be naturally fuzzy or imprecise, that's why fuzzy query systems have become indispensable to represent and manage this information and especially facilitate interrogation to a non-expert user .In this paper, we present a brief study of fuzzy querying relational databases,we start by the design and operation of a fuzzy system,and we present different types architectures of fuzzy interrogation systems of database . Finally, a comparison of most relevant characteristic in fuzzy query systems of database is included in this work, as well. 
id877#DOEE: Dynamic optimization framework for better energy efficiency#The growing adoption of mobile devices powered by batteries along with the high power costs in datacenters raise the need for energy efficient computing. Dynamic Voltage and Frequency Scaling is often used by the operating system to balance power-performance. However, optimizing for energy-efficiency faces multiple challenges such as when dealing with non-steady state workloads. In this work we develop DOEE - a novel method that optimizes certain processor features for energy efficiency using user-supplied metrics. The optimization is dynamic, taking into account the runtime characteristics of the workload and the platform. The method instruments monitoring code to search for per-program-phase optimal feature-configurations that ultimately improve system energy efficiency. We demonstrate the framework using the LLVM compiler when tuning the Turbo Boost feature on modern Intel Core processors. Our implementation improves energy efficiency by up to 23% on SPEC CPU2006 benchmarks, outperforming the energy-efficient firmware algorithm. This framework paves the way for auto-tuning additional CPU features. 
id878#Automatic pig selection system based on body size using a camera: rotating mechanics for pig selection#Since the delivery of different pig sizes at a same time leads to loss profits, the management of pigs to be grown up uniformly is an important task in pig farms. Therefore, automatic sorting system of pigs by size has been introduced. In our proposed system, camera is used to measure the size of pigs. Camera detects the image of a pig and pig’s size is estimated using computer vision technique. The estimated size is used to select the adequate foods for the pig. The novel mechanism that leads the pig to the adequate direction for the feeding is introduced. The proposed mechanism works robustly in the poor environment of a pig farm. It works satisfactorily without disturbing the feeding cycles of pigs in the farms. The system reduces the labor cost and enables the efficient feeding cycles on considering the size of a pig. The weight is also estimated from the size. Experimental result shows the performance of the weight estimation. 
id880#Rapid and real-time detection of black tea fermentation quality by using an inexpensive data fusion system#Intelligent identification of black tea fermentation quality is becoming a bottleneck to industrial automation. This study presents at-line rapid detection of black tea fermentation quality at industrial scale based on low-cost micro-near-infrared spectroscopy (NIRS) and laboratory-made computer vision system (CVS). High-performance liquid chromatography and a spectrophotometer were used for determining the content of catechins and theaflavins, and the color of tea samples, respectively. Hierarchical cluster analysis combined with sensory evaluation was used to group samples through different fermentation degrees. A principal component analysis–support vector machine (SVM) model was developed to discriminate the black tea fermentation degree using color, spectral, and data fusion information; high accuracy (calibration = 95.89%, prediction = 89.19%) was achieved using mid-level data fusion. In addition, SVM model for theaflavins content prediction was established. The results indicated that the micro-NIRS combined with CVS proved a portable and low-cost tool for evaluating the black tea fermentation quality. 
id881#Learning-based defect recognition for quasi-periodic HRSTEM images#Controlling crystalline material defects is crucial, as they affect properties of the material that may be detrimental or beneficial for the final performance of a device. Defect analysis on the sub-nanometer scale is enabled by high-resolution scanning transmission electron microscopy (HRSTEM), where the identification of defects is currently carried out based on human expertise. However, the process is tedious, highly time consuming and, in some cases, yields ambiguous results. Here we propose a semi-supervised machine learning method that assists in the detection of lattice defects from atomic resolution HRSTEM images. It involves a convolutional neural network that classifies image patches as defective or non-defective, a graph-based heuristic that chooses one non-defective patch as a model, and finally an automatically generated convolutional filter bank, which highlights symmetry breaking such as stacking faults, twin defects and grain boundaries. Additionally, we suggest a variance filter to segment amorphous regions and beam defects. The algorithm is tested on III–V/Si crystalline materials and successfully evaluated against different metrics and a baseline approach, showing promising results even for extremely small training data sets and for noise compromised images. By combining the data-driven classification generality, robustness and speed of deep learning with the effectiveness of image filters in segmenting faulty symmetry arrangements, we provide a valuable open-source tool to the microscopist community that can streamline future HRSTEM analyses of crystalline materials. 
id882#Secure Data Provenance in Internet of Things using Hybrid Attribute based Crypt Technique#In the new era of digital technology, various new applications are developed every day based on Internet of Things where the devices are interconnected between them. When the application based on IoT are used for transferring sensitive and critical data then adequate amount of confidentiality and privacy is needed also dependability is developed based on the assurance and verification of correctness and integrity of the data used. In order to address these necessary requirements of the system, Internet of Things system could be provided with data ownership and mechanisms of data provenance to maintain the data on lineage. However, to make IoT systems dependable and secure, the data provenance has to be protected sufficiently against unauthorized access and tampering of data. In this paper, a framework is proposed to secure the data provenance in Internet of Things systems using blockchain and access control policies. This work is implemented with hybrid attribute based encryption and the results are analyzed based on computational cost and throughput of encryption and decryption and also strength of the key is calculated according to avalanche effect. The experimental results prove that the proposed system is with reduced computational cost, high throughput. Setup time cost and secret key time cost are analyzed based on the access control policy tree depth. Key generation time is analyzed based on number of users. Decryption time cost is analyzed based on the no. of attributes to be decrypted. Ciphertext file size is analyzed based on the length of the plain text. According to the avalanche effect, the calculated strength of the key is good and above 99%. 
id883#A feature binding model in computer vision for object detection#In this paper, the authors propose the “Feature Binding (FB)” strategy in computer vision, a method combined with the biological visual perception theory. Based on feature subspace, the proposed method refers to the biological model and binds features according to certain rules. All features bound in a group are taken as a whole. Besides, all groups with different weight coefficients according to different importance are used to determine the object and its location. The position of the object can be determined based on the calculation according to the corresponding criteria. Feature Binding can significantly enhance the accuracy of object detection and localization. Moreover, the method can accelerate object detection and resist external interference in the unbound feature subspace. Feature Binding has good accuracy not only for the whole object but also for the obscured object. It also has good robustness for different algorithms, which are based on features, including traditional methods and deep learning algorithms. The object positioning system can detect the partially occluded objects more accurately in practice. 
id884#Designing a simple fiducial marker for localization in spatial scenes using neural networks#The paper describes the process of designing a simple fiducial marker. The marker is meant for use in augmented reality applications. Unlike other systems, it does not encode any information, but it can be used for obtaining the position, rotation, relative size, and projective transformation. Also, the system works well with motion blur and is resistant to the marker’s imperfections, which could theoretically be drawn only by hand. Previous systems put constraints on colors that need to be used to form the marker. The proposed system works with any saturated color, leading to better blending with the surrounding environment. The marker’s final shape is a rectangular area of a solid color with three lines of a different color going from the center to three corners of the rectangle. Precise detection can be achieved using neural networks, given that the training set is very varied and well designed. A detailed literature review was performed, and no such system was found. Therefore, the proposed design is novel for localization in the spatial scene. The testing proved that the system works well both indoor and outdoor, and the detections are precise. 
id886#Computer vision for fire detection on uavs—from software to hardware#Fire hazard is a condition that has potentially catastrophic consequences. Artificial intelli-gence, through Computer Vision, in combination with UAVs has assisted dramatically to identify this risk and avoid it in a timely manner. This work is a literature review on UAVs using Computer Vision in order to detect fire. The research was conducted for the last decade in order to record the types of UAVs, the hardware and software used and the proposed datasets. The scientific research was executed through the Scopus database. The research showed that multi-copters were the most common type of vehicle and that the combination of RGB with a thermal camera was part of most applications. In addition, the trend in the use of Convolutional Neural Networks (CNNs) is increas-ing. In the last decade, many applications and a wide variety of hardware and methods have been implemented and studied. Many efforts have been made to effectively avoid the risk of fire. The fact that state-of-the-art methodologies continue to be researched, leads to the conclusion that the need for a more effective solution continues to arouse interest. 
id887#The environment and body-brain complexity#An open question for both natural and artificial evolutionary systems is how, and under what environmental and evolutionary conditions complexity evolves. This study investigates the impact of increasingly complex task environments on the evolution of robot complexity. Specifically, the impact of evolving body-brain couplings on locomotive task performance, where robot evolution was directed by either body-brain exploration (novelty search) or objective-based (fitness function) evolutionary search. Results indicated that novelty search enabled the evolution of increased robot body-brain complexity and efficacy given specific environment conditions. The key contribution is thus the demonstration that body-brain exploration is suitable for evolving robot complexity that enables high fitness robots in specific environments. 
id888#TPFOSSS: A modified TPS technique to improve student's conceptual understanding of compiler construction course#TPFOSSS (Think-Pair Free Open Source Software-Share) is a cooperative learning activity and modified activity of TPS in which free open source software e.g. Parsing Simulator for Compiler Construction or JFLAP simulator for Theory of Computation etc. Can be used in TPS activity. Providing 'think time' in TFOSSPS improves quality of student responses. This activity develops skills of sharing information, improves the interaction, listening, asking questions, summarizing others' ideas etc. In this paper, we present how TPFOSSS works, advantages of TPFOSSS over TPS and experimental results. The experiment carried out is two group experimental studies. 
id889#A data quality approach to conformance checks for business network models#The recent advances in programming languages, compilers and hardware allow software systems to process huge amounts of data. Therefore data is represented in domain specific models, capturing real-world artifacts as structured data for calculation and analysis. To guarantee data quality and reduce uncertainty within the domain information, model and content-based data conformance have to be checked. Domain model changes often require code changes for validations and redeployment to keep the pace. Current programming techniques and tools do not or only partially address this topic. In this paper, we present a novel approach to content-based validation of structured data for arbitrary domains. We define a validation programming model and show how a compiler and run-time system based on Deterministic Finite Automata(DFA) can be generated. We validated this approach by applying it to the domain of Network Mining (NM), which requires conformance checks for discovered raw data, used to compute business networks. The validation programs are illustrated by debugging the run-time system with textual and graphical tools. 
id890#Cairo: A compiler-assisted technique for enabling instruction-level offloading of processing-in-memory#Three-dimensional (3D)-stacking technology and the memory-wall problem have popularized processingin- memory (PIM) concepts again, which offers the benefits of bandwidth and energy savings by offloading computations to functional units inside the memory. Several memory vendors have also started to integrate computation logics into the memory, such as Hybrid Memory Cube (HMC), the latest version of which supports up to 18 in-memory atomic instructions. Although industry prototypes have motivated studies for investigating efficient methods and architectures for PIM, researchers have not proposed a systematic way for identifying the benefits of instruction-level PIM offloading. As a result, compiler support for recognizing offloading candidates and utilizing instruction-level PIM offloading is unavailable. In this article, we analyze the advantages of instruction-level PIM offloading in the context of HMC-atomic instructions for graphcomputing applications and propose CAIRO, a compiler-assisted technique and decision model for enabling instruction-level offloading of PIM without any burden on programmers. To develop CAIRO, we analyzed how instruction offloading enables performance gain in both CPU and GPU workloads. Our studies show that performance gain from bandwidth savings, the ratio of number of cache misses to total cache accesses, and the overhead of host atomic instructions are the key factors in selecting an offloading candidate. Based on our analytical models, we characterize the properties of beneficial and nonbeneficial candidates for offloading. We evaluate CAIRO with 27 multithreaded CPU and 36 GPU benchmarks. In our evaluation, CAIRO not only doubles the speedup for a set of PIM-beneficial workloads by exploiting HMC-atomic instructions but also prevents slowdown caused by incorrect offloading decisions for other workloads. 
id892#Novel coronavirus (COVID-19) diagnosis using computer vision and artificial intelligence techniques: a review#The universal transmission of pandemic COVID-19 (Coronavirus) causes an immediate need to commit in the fight across the whole human population. The emergencies for human health care are limited for this abrupt outbreak and abandoned environment. In this situation, inventive automation like computer vision (machine learning, deep learning, artificial intelligence), medical imaging (computed tomography, X-Ray) has developed an encouraging solution against COVID-19. In recent months, different techniques using image processing are done by various researchers. In this paper, a major review on image acquisition, segmentation, diagnosis, avoidance, and management are presented. An analytical comparison of the various proposed algorithm by researchers for coronavirus has been carried out. Also, challenges and motivation for research in the future to deal with coronavirus are indicated. The clinical impact and use of computer vision and deep learning were discussed and we hope that dermatologists may have better understanding of these areas from the study. 
id893#Automatic construction of formal syntax tree based on regular expressions#The main functionality of a compiler is to translate source code to an executable machine code correctly and efficiently. Compiler construction is an advanced research area due to size and complexity of the code generated from the source program. Design and construction of error-free and verified compiler will remain a challenge of the current century. Verification of a source program does not assure that the generated code is correct because the compiler may lead to an incorrect target program due to bugs and errors in itself. Hence verification of a compiler is more important than verifying the source program. Lexical analyzer is a main part of compiler used for scanning input stream of characters and grouping into tokens. In this paper, formal construction of syntax tree is described directly from the regular expression to verify the lexical analyzer. At first, augmented regular expression is described then an abstract syntax tree is defined based on the regular expression. Finally formal description of some important operators checking null-ability and computing first and last positions of the internal nodes of the tree are formalized. The specification is described using Z notation then validated using Z/Eves toolset. Formal model is analyzed using powerful techniques of reduction and rewriting available in the Z/Eves toolset. 
id894#Robust Geometric Model Fitting Based on Nonnegative Matrix Underapproximation with Pruning Techniques for Multi-Structure Data [基于非负矩阵欠逼近和剪枝技术的多结构几何模型拟合]#Robust geometric model fitting is an important and challenging research problem in computer vision. It has been widely used in many artificial intelligence related applications, such as lane detection, 3D reconstruction, image stitching and motion segmentation, etc. With the rapid development of the artificial intelligence, the data processed by artificial intelligence systems inevitably contain outliers or noise generated by sensors, environment or human factors. The main task of robust geometric model fitting is to estimate the parameters and the number of model instances from multi-structural data contaminated with outliers and noise. However, the performance of current model fitting methods are far from being satisfactory in practical applications in terms of fitting accuracy and computational speed. In this paper, we propose an efficient model fitting method (NPMF) based on nonnegative matrix underapproximation and pruning techniques, to obtain more accurate fitting results from multi-structural data. The proposed NPMF includes a mismatch pruning algorithm, a model hypothesis pruning algorithm and an improved nonnegative matrix underapproximation algorithm. Firstly, a mismatch pruning algorithm is proposed to alleviate the influence of outliers on the data point sampling process by using a mismatch removal technique, thereby reducing the number of insignificant model hypotheses. After retaining significant model hypotheses by using the weighting scores of model hypotheses, a model hypothesis pruning algorithm is introduced to prune insignificant model hypotheses, and a high-quality nonnegative preference matrix is then constructed. Finally, both the spatial constraint and the sparsity constraint are integrated into the optimization problem of nonnegative matrix underapproximation, and the number and parameters of model instances are adaptively estimated by using a structure merging strategy. The comparison experiments on several representative model fitting methods show that the proposed NPMF obtains better fitting performance and robustness on both synthetic data and real images. For fitting accuracy, the proposed NPMF is about 197.2% and 47.7% higher than T-Linkage and RS-NMU, respectively. For fitting speed, the proposed NPMF is about 2.3 times and 1.9 times faster than T-Linkage and RS-NMU, respectively. Furthermore, the proposed NPMF is about 42.5 times faster than the state-of-the-art MCT for 3D planar surface reconstruction. 
id895#Abstraction in data-sparse task transfer#When a robot adapts a learned task for a novel environment, any changes to objects in the novel environment have an unknown effect on its task execution. For example, replacing an object in a pick-and-place task affects where the robot should target its actions, but does not necessarily affect the underlying action model. In contrast, replacing a tool that the robot will use to complete a task will effectively alter its end-effector pose with respect to the robot's base coordinate system, and thus the robot's motion must be replanned accordingly. These examples highlight the relationship among (i) differences between the source and target environments, (ii) the level of abstraction at which a robot's task model should be represented to enable transfer to the target environment, and (iii) the information needed to ground the abstracted task representation in the target environment. In this article, we present a taxonomy of transfer problems based on this relationship. We also describe a knowledge representation called the Tiered Task Abstraction (TTA) and demonstrate its applicability to a variety of transfer problems in the taxonomy. Our experimental results indicate a trade-off between the generality and data requirements of a task representation, and reinforce the need for multiple transfer methods that operate at different levels of abstraction. 
id896#Using Operator Gaze Tracking to Design Wrist Mechanism for Surgical Robots#This article assessed how surgical robot parameters influenced operator viewpoint during a simulated surgical procedure. Surgical robots are useful tools in minimally invasive surgery. However, even with robots, suturing is difficult because the needle is sometimes obscured by tissue or manipulators and is thus not always visible during the procedure. This is especially true in pediatric surgery, where the surgical environment is smaller than in adult surgery. Hence, surgeons must carefully track the instruments and tissues to understand and predict their current and expected situations. In this article, we used gaze-tracking techniques to analyze the location and timing of the gaze of participants while they manipulated a virtual robotic surgical simulation system. To differentiate between the ideal and actual viewpoint trajectories, we conducted experiments with and without obstacles (i.e., simulated tissue and the manipulator arm). In the obstacle condition, we modulated the wrist length of the manipulator to bring it into view. In the no-obstacle condition, the participants mostly watched the suture needle tip. In the with-obstacle condition, the participants spent less time watching the instruments and more time watching the target point. The amount of time spent watching the target point increased as wrist length increased. Given this tradeoff relationship, we examined the proportion of time the participants spent looking at the instruments or target points by wrist length. We calculated the Pareto solutions and clarified the relationship between wrist length and the watching parts. 
id898#A Pairing Free Identity Based Two Party Authenticated Key Agreement Protocol Using Hexadecimal Extended ASCII Elliptic Curve Cryptography#In wireless sensor network (WSN), the secure communication and exchange of confidential information between nodes is a challenging task. To eliminate the security flaws WSN adopts authenticated cryptography mechanisms. The authenticated key agreement protocol guarantees the reality of the users and negotiates the shared session key. In the past years, the two-way authentication scheme has been proved as a better approach for secure and energy efficient communication over Bin and Balls Authentication scheme, Timed Efficient Stream Loss-Tolerant Authentication scheme and so on. However, the recently established identity based protocols are associated with the flaws like inadequate security, high computation cost and latency in communication to a larger extent. The issues related to cost is raised because of the pairing and mapping functionalities in the state-of-the-art techniques. This paper proposes a pairing-free identity based two-party authenticated key agreement protocol based on hexadecimal extended ASCII Elliptic Curve Cryptography. The proposed scheme is developed in an adequate manner with the increased security strength and reduced cost. The security of this protocol is tightened by the extended ASCII code representation of the identity of the user. The performance of this protocol is compared with the Bin and Balls Authentication scheme, Loss-Tolerant Authentication scheme and recently proposed Ramachandran and Shanmugam scheme that show our scheme outperforms these three authentication schemes. 
id899#3D and 4D lithography of untethered microrobots#In the last decades, additive manufacturing (AM), also called three-dimensional (3D) printing, has advanced micro/nano-fabrication technologies, especially in applications like lightweight engineering, optics, energy, and biomedicine. Among these 3D printing technologies, two-photon polymerization (TPP) offers the highest resolution (even at the nanometric scale), reproducibility and the possibility to create monolithically 3D complex structures with a variety of materials (e.g. organic and inorganic, passive and active). Such active materials change their shape upon an applied stimulus or degrade over time at certain conditions making them dynamic and reconfigurable (also called 4D printing). This is particularly interesting in the field of medical microrobotics as complex functions such as gentle interactions with biological samples, adaptability when moving in small capillaries, controlled cargo-release profiles, and protection of the encapsulated cargoes, are required. Here we review the physics, chemistry and engineering principles of TPP, with some innovations that include the use of micromolding and microfluidics, and explain how this fabrication schemes provide the microrobots with additional features and application opportunities. The possibility to create microrobots using smart materials, nano- and biomaterials, for in situ chemical reactions, biofunctionalization, or imaging is also put into perspective. We categorize the microrobots based on their motility mechanisms, function, and architecture, and finally discuss the future directions of this field of research. 
id900#Promoting reproductive isolation through diversity in on-line collective robotics#We present a behavioral diversity selection scheme that favors reproductive isolation to promote the learning of multiple task in online embodied evolutionary robotics (EER). The scheme estimates the behavior of the controllers without the need to access the agent experience, respecting thus the online, distributed properties EER. Reproductive isolation is assessed through coalescence trees and task specialization is tested on a concurrent foraging setting. 
id901#Robust Three-Dimensional Shape Sensing for Flexible Endoscopic Surgery Using Multi-Core FBG Sensors#In this letter, we propose a novel 3D shape sensing algorithm for flexible endoscopic surgery using multi-core fiber Bragg grating (FBG) sensors. Considering the signal noises and environmental perturbations, the direct use of FBG measurements for shape sensing and position estimation is regarded as far from accurate and stable, especially when utilized in the sensing of long and flexible surgical instruments. To solve this problem, a novel and generic model-based filtering technique for the iterative curvature/twist estimation by taking advantage of the configurations of the multi-core FBGs in the optical fiber is introduced to remedy the sensory noises. Besides, we introduce an enhanced moving average approach to smooth the estimated curvatures and twists spatially on the fiber. We extensively validate our algorithm by conducting shape sensing tasks in simulations under varying conditions, and in experiments using a robotic-assisted colonoscope system integrated with a multi-core FBG fiber. The results prove our method substantially outperforms the conventional approach in the aspect of estimation accuracy and robustness, showing the superiority and application feasibility. 
id902#Compiling quantum algorithms for architectures with multi-qubit gates#In recent years, small-scale quantum information processors have been realized in multiple physical architectures. These systems provide a universal set of gates that allow one to implement any given unitary operation. The decomposition of a particular algorithm into a sequence of these available gates is not unique. Thus, the fidelity of the implementation of an algorithm can be increased by choosing an optimized decomposition into available gates. Here, we present a method to find such a decomposition, where a small-scale ion trap quantum information processor is used as an example. We demonstrate a numerical optimization protocol that minimizes the number of required multi-qubit entangling gates by design. Furthermore, we adapt the method for state preparation, and quantum algorithms including in-sequence measurements. 
id903#Recommendations for Evolving Relational Databases#Relational databases play a central role in many information systems. Their schemas contain structural and behavioral entity descriptions. Databases must continuously be adapted to new requirements of a world in constant change while: (1) relational database management systems (RDBMS) do not allow inconsistencies in the schema; (2) stored procedure bodies are not meta-described in RDBMS such as PostgreSQL that consider their bodies as plain text. As a consequence, evaluating the impact of an evolution of the database schema is cumbersome, being essentially manual. We present a semi-automatic approach based on recommendations that can be compiled into a SQL patch fulfilling RDBMS constraints. To support recommendations, we designed a meta-model for relational databases easing computation of change impact. We performed an experiment to validate the approach by reproducing a real evolution on a database. The results of our experiment show that our approach can set the database in the same state as the one produced by the manual evolution in 75% less time. 
id904#Design of an S-ECIES Cryptoprocessor Using Gaussian Normal Bases over GF(2m)#In this article, we present the design of a high-performance simplified elliptic curve integrated encryption scheme (S-ECIES) cryptoprocessor. The cryptoprocessor was designed using a Montgomery ladder scalar multiplier, which was implemented with three finite field multipliers to improve the computational time of the scalar multiplication {kP} , and using random curves and Gaussian normal bases over GF(2163) and GF(2233). Also, considering the National Institute of Standards and Technology (NIST) recommendations, a true random number generator is implemented to generate a secret key k , which is used during the encryption process. The S-ECIES cryptoprocessor was synthesized on field-programmable gate array (FPGA) Stratix IV EP4SGX230KF40C2, simulated in ModelSim, and verified in hardware using the DE4 board and SignalTap tool. According to the synthesis results, the scalar multiplication operation is performed in 5.31 and 8.77~\mu \text{s} for GF(2163) and GF(2233), respectively. Also, the encryption process is performed in 20.70 and 30.90~\mu \text{s} for GF(2163) and GF(2233), respectively, and the decryption process is calculated in 8.10 and 11.9~\mu \text{s} for GF(2163) and GF(2233), respectively. The consumption power for the S-ECIES is 921 and 935 mW for GF(2163) and GF(2233), respectively. 
id905#Haskell before Haskell: An alternative lesson in practical logics of the ENIAC#This article expands on Curry's work on how to implement the problem of inverse interpolation on the ENIAC (1946) and his subsequent work on developing a theory of program composition (1948-1950). It is shown that Curry's hands-on experience with the ENIAC on the one side and his acquaintance with systems of formal logic on the other, were conductive to conceive a compact 'notation for program construction' which in turn would be instrumental to a mechanical synthesis of programs. Since Curry's systematic programming technique pronounces a critique of the Goldstine-von Neumann style of coding, his 'calculus of program composition' not only anticipates automatic programming but also proposes explicit hardware optimizations largely unperceived by computer history until Backus' famous ACM Turing Award lecture (1977). The cohesion of these findings asks for an integrative historiographical approach. An appendix gives, for the first time, a full description of Curry's arithmetic compiler. 
id906#Fast additions on masked integers#Suppose the bits of a computer word are partitioned into d disjoint sets, each of which is used to represent one of a d-tuple of cartesian indices into d-dimensional space. Then, regardless of the partition, simple group operations and comparisons can be implemented for each index on a conventional processor in a sequence of two or three register operations. These indexings allow any blocked algorithm from linear algebra to use some non-standard matrix orderings that increase locality and enhance their performance. The underlying implementations were designed for alternating bit postitions to index Morton-ordered matrices, but they apply, as well, to any bit partitioning. A hybrid ordering of the elements of a matrix becomes possible, therefore, with row-/column-major ordering within cache-sized blocks and Morton ordering of those blocks, themselves. So, one can enjoy the temporal locality of nested blocks, as well as compiler optimizations on row- or column-major ordering in base blocks.
id907#Comparative Performance Analysis of Lightweight Cryptography Algorithms for IoT Sensor Nodes#The Internet of Things (IoT) has become an integral part of future solutions, ranging from industrial to everyday human life applications. Adding a new level of intelligence to objects and automating decisions make this new technology appealing to everyone. However, applications that involve data are more vulnerable to various types of attacks. As a result, researchers are constantly exploring secure connections between IoT edge nodes. On one hand, suitable IoT nodes should be cheap and require low power, which means lower computational performance. On the other hand, a secure connection layer is power hungry and requires powerful hardware resources. Lightweight cryptography (LWC) algorithms are a promising solution to reduce computation complexity while maintaining a desired level of security. In the presented work, we attempt to address the issue of adding security to the IoT network layer by comparing the performance of 32 LWC algorithms with currently well-known algorithms on multiple IoT platforms (Raspberry Pi 3, Raspberry Pi Zero W, and iMX233). These 32 authenticated encryption with associated data algorithms have been selected from the second round of the LWC standardization process conducted by the National Institute of Standards and Technology. Power consumption, random access memory usage, and execution time are measured for these algorithms using the targeted embedded platforms that are used as IoT sensor nodes. The results of this study will assist researchers in choosing a suitable platform and optimal LWC algorithm for IoT applications. 
id908#An algorithmic approach for encryptionusing graph labeling#In the recent computerized world ,cryp tography plays animportant role so as to transfer information safely between at least two substances.Cryp tographyconsists of two parts,one is encryption of the message so that the cypher text will not reveal the original message to anyone.The other part is decryption at the receiver's side which inverts the encryption process and restore the original message.There are numerous encryption calculations exist, we are in the position to find out some new non-usual encryption algorithms to secure the information.The suggested algorithmic approach speaks a new encryption technique to encode and decode the message very securely using graph labeling. 
id909#Review of weed detection methods based on computer vision#Weeds are one of the most important factors affecting agricultural production. The waste and pollution of farmland ecological environment caused by full-coverage chemical herbicide spraying are becoming increasingly evident. With the continuous improvement in the agricultural production level, accurately distinguishing crops from weeds and achieving precise spraying only for weeds are important. However, precise spraying depends on accurately identifying and locating weeds and crops. In recent years, some scholars have used various computer vision methods to achieve this purpose. This review elaborates the two aspects of using traditional image-processing methods and deep learning-based methods to solve weed detection problems. It provides an overview of various methods for weed detection in recent years, analyzes the advantages and disadvantages of existing methods, and introduces several related plant leaves, weed datasets, and weeding machinery. Lastly, the problems and difficulties of the existing weed detection methods are analyzed, and the development trend of future research is prospected. 
id910#Telerobotic Sonography for Remote Diagnostic Imaging: Narrative Review of Current Developments and Clinical Applications#Access to sonographers and sonologists is limited in many communities around the world. Telerobotic sonography (robotic ultrasound) is a new technology to increase access to sonography, providing sonographers and sonologists the ability to manipulate an ultrasound probe from a distant location and remotely perform ultrasound examinations. This narrative review discusses the development of telerobotic ultrasound systems, clinical studies evaluating the feasibility and diagnostic accuracy of telerobotic sonography, and emerging use of telerobotic sonography in clinical settings. Telerobotic sonography provides an opportunity to provide real-time ultrasound examinations to underserviced rural and remote communities to increase equity in the delivery of diagnostic imaging. 
id911#Engaged by a Bartender Robot: Recommendation and Personalisation in Human-Robot Interaction#BRILLO (Bartending Robot for Interactive Long-Lasting Operations) project aims to deploy an autonomous robotic bartender that can naturally interact with customers in a real-world service point. In such a scenario, this work presents a multi-modal personalised recommendation method for increasing users' engagement with a bartending robot. The personalised recommendation is adopted at two levels: one for the recommendation of the service (i.e. drinks), and another for selecting the human-robot interaction (HRI) modalities. The envisioned multi-modal personalised recommendation method enables the BRILLO robot to intelligently adapt its dialogues, pose and gestures, according to the user's moods, attention behaviours, personal traits, and situational context (drink orders, group dynamics, etc.). 
id912#Designing fitness functions for odour source localisation#Locating odour sources is a hard task that has been addressed with a large variety of AI methods to produce search strategies with different levels of efficiency and robustness. However, it is still not clear how to evaluate those strategies. Simply evaluating the robot's ability to reach the goal may produce deceptive fitness values, favouring poor strategies that do not generalise. Conversely, including prior knowledge may bias the learning process. This work studies the impact of evaluation functions with various degrees of prior knowledge, in evolving search strategies. The baseline is set by performing multiple evaluations of each strategy with a function that only evaluates the task efficiency. A function was found that is able to produce strategies with equivalent performance to those of the baseline, whilst performing a single evaluation. 
id913#Efficient mediated semi-quantum key distribution#Mediated semi-quantum key distribution (MSQKD) allows two “classical” participants to establish a secret key with the help of an untrusted quantum third party (TP). In all existing MSQKD protocols, the quantum capability required by TP is high and thus it needs much cost in a real implementation. In this paper, we propose an efficient MSQKD protocol, where TP only needs to prepare and measure qubits in the X basis and two “classical” participants just have the ability to prepare and measure qubits in the Z basis. Compared with other similar MSQKD protocols, the proposed protocol reduces the capability requirements of TP without sacrificing the qubit efficiency. 
id914#EPF-CLPA: An efficient pairing-free certificateless public auditing for cloud-based CPS#Cloud based cyber physical system (CPS) enables individuals to store and share data collected from both cyberspace and the physical world. This leads to the proliferation of massive data at a user's local site. Since local storage systems can't store and maintain huge data, it is a wise and practical way to outsource such huge data to the cloud. Cloud storage provides scalable storage space to manage data economically and flexibly. However, the integrity of outsourced data is a critical challenge because user's lose control of their data once it's transferred to cloud servers. Several auditing schemes have been put forward based on public key infrastructure (PKI) or identity-based cryptography to verify data integrity. However, 'the PKI-based schemes suffer from certificate management problem and identity-based schemes face the key escrow' problem. Therefore, to address these problems, certificateless public auditing schemes have been introduced on the basis of bilinear pairing, which incur high computation overhead, and thus it is not suitable for CPS. To reduce the computation overhead, in this paper, Using elliptic curve cryptography, we propose an efficient pairing-free certificateless public auditing scheme for cloud-based CPS. The proposed scheme is more secure against type I/II/III adversaries and efficient compared to other certificateless based schemes. 
id915#Structure modal identification based on computer vision technology#Mobile phones have the potential to become useful tool in structural modal identification. In this paper, shaking table test videos of a 10-story steel structure captured by mobile phone is processed using computer vision theory and then the modal parameters are identified. A signal processing method based on variational mode decomposition (VMD) is used to improve the accuracy of identification. Using optical flow algorithm, the vibration data is extracted from the video, and then the response of the structure is obtained from the vibration data of selected feature points. Then, the vibration data is processed by VMD and structural modal parameters (mode frequency and mode shapes) are identified using FFD. Finally, the identification results obtained from mobile phone and professional sensors are compared to verify feasibility and accuracy of the proposed modal identification method. Copyright 
id916#A Compact Full Hardware Implementation of PQC Algorithm NTRU#With the emergence and development of quantum computers, the traditional public-key cryptography (PKC) is facing the risk of being cracked. In order to resist quantum attacks and ensure long-term communication security, NIST launched a global collection of Post Quantum Cryptography (PQC) standards in 2016, and it is currently in the third round of selection. There are three Lattice-based PKC algorithms that stand out, and NTRU is one of them. In this article, we proposed the first complete and compact full hardware implementation of NTRU algorithm submitted in the third round. By using one structure to complete the design of the three types of complex polynomial multiplications in the algorithm, we achieved better performance while reducing area costs. 
id917#Quantum rotations: A case study in static and dynamic machine-code generation for quantum computers#Work in quantum computer architecture has focused on communication, layout and fault tolerance, largely driven by Shor's factorization algorithm. For the first time, we study a larger range of benchmarks and find that another critical issue is the generation of code sequences for quantum rotation operations. Specifically, quantum algorithms require arbitrary rotation angles, while quantum technologies and error correction codes provide only for discrete angles and operators. A sequence of quantum machine instructions must be generated to approximate the arbitrary rotation to the required precision. While previous work has focused exclusively on static compilation, we find that some applications require dynamic code generation and explore the advantages and disadvantages of static and dynamic approaches. We find that static code generation can, in some cases, lead to a terabyte of machine code to support required rotations. We also find that some rotation angles are unknown until run time, requiring dynamic code generation. Dynamic code generation, however, exhibits significant trade-offs in terms of time overhead versus code size. Furthermore, dynamic code generation will be performed on classical (non-quantum) computing resources, which may or may not have a clock speed advantage over the target quantum technology. For example, operations on trapped ions run at kilohertz speeds, but superconducting qubits run at gigahertz speeds. We introduce a new method for compiling arbitrary rotations dynamically, designed to minimize compilation time. The new method reduces compilation time by up to five orders of magnitude while increasing code size by one order of magnitude. We explore the design space formed by these trade-offs of dynamic versus static code generation, code quality, and quantum technology. We introduce several techniques to provide smoother trade-offs for dynamic code generation and evaluate the viability of options in the design space. Copyright 2013 ACM.
id918#Design and implementation of hybrid integration of cognitive learning and chaotic countermeasures for side channel attacks#Security in embedded systems is considered to be more important and needs to be a diagnosis for every minute. Also with the advent of the Internet of Things (IoT), security in the embedded system has reached its new peak of dimension. A Mathematically secure algorithm was formulated and runs on the cryptographic chips which are embedded in the systems, but secret keys can be at risk and even information can be retrieved by the prominent side-channel attacks. Fixed encryption keys, non-intelligent detection of side-channel attacks are some of the real-time challenges in an existing system of encryption. Following the limitations of existing systems, this research article focuses on the integration of powerful machine learning algorithms by retrieving the secret key information with countermeasures methodology using the chaotic logistic maps and includes the following contributions: (a) Preparation of Data Sets from the Power consumption traces captured from ARTIX-7 FPGA boards while running the Elliptical Curve Cryptography(ECC) on it (b) Implementation of High Speed and High Accurate Single feed-forward learning machines for the detection and classification of side-channel attacks (c) Design of Chaotic Countermeasures using 3-Dlogistic maps for attacked bits. The test_bed has been developed using the integration of FPGA along with Cortex-A57 architectures for experimentation of the proposed work and various evaluation parameters such as Accuracy, F-calls, Precision rates, sensitivity, and correlation co-efficient, entropy were calculated and analyzed. Moreover, the parameters of the proposed system which has been analyzed prove to outperform the other existing algorithms in terms of performance and detection. 
id920#Multi-level transfer learning for improving the performance of deep neural networks: Theory and practice from the tasks of facial emotion recognition and named entity recognition#Transfer learning has become a promising field in machine learning owing to its wide application prospects. Its effectiveness has spawned various methodologies and practices. Transfer learning refers to improving the performance of target learners in the target domain by transferring the knowledge contained in different yet related source domains. In other words, we can use data from additional domains or tasks to train a model with superior generalization. Using transfer learning, the dependence on considerable target-domain data can be reduced, thereby constructing target learners. Recently, the fields of computer vision (CV) and natural language processing (NLP) have witnessed the emergence of transfer learning, which has significantly improved the most advanced technology on a wide range of CV and NLP tasks. A typical approach of applying transfer learning to deep neural networks is to fine-tune a pretrained model of the source domain with data obtained from the target domain. This paper proposes a novel framework, based on the fine-tuning approach, called multilevel transfer learning (mLTL). Under this framework, we concluded the crucial findings and principles regarding the training sequence of related domain datasets and demonstrated its effectiveness by performing facial emotion and named entity recognition tasks. According to the experimental results, the deep neural network models using mLTL outperformed the original models on the target tasks. 
id921#Synergy between traditional classification and classification based on negative features in deep convolutional neural networks#In recent times, convolutional neural networks became an irreplaceable tool in many different machine learning applications, especially in image classification. On the other hand, new research about robustness and susceptibility of these models to different adversarial attacks has emerged. With the rise in usage and widespread adoption of these models, it is very important to make them suitable for critical applications. In our previous work, we experimented with a new type of learning applicable to all convolutional neural networks: classification based on missing (low-impact) features. In the case of partial inputs/image occlusion, we have shown that our new method creates models that are more robust and perform better when compared to traditional models of the same architecture. In this paper, we explore an interesting characteristic of our newly developed models in that while we see a general increase in validation accuracy, we also lose some important knowledge. We propose one solution to overcome this problem and validate our assumptions against CIFAR-10 image classification dataset. 
id923#Calibration of cross structured light based on linear space rotation [基于直线空间旋转的十字结构光标定]#Aiming at the rapid and high-precision calibration of cross structured light in the measurement of complex surface features, we propose a cursor calibration method for the cross structured light based on linear space rotation. We used the Otsu thresholding algorithm to select the best threshold for extracting the area where the light bar was located, and the Steger algorithm based on the Hessian matrix and the least square method were able to extract and fit the center of the light strip. After the construction of feature point pairs in the image and the target plane, the random sample consensus (RANSAC) algorithm was used to solve the homography matrix, and a straight line was transformed to obtain the linear equation of the light bar in the target plane. The straight line was then converted into the camera coordinate system. From the rotation transformation relationship of the space straight line around any axis, the light strips were rotated around the projection centerline, and the two straight lines before and after the rotation were plane fitted to solve the light plane equation parameters. The results showed that the average error when using this method to measure the distance between the target centers was 0.023 mm and that the root mean square error was 0.026 mm. This method can achieve higher measurement accuracy and can avoid multiple movements of the target plane. 
id924#Unscented Particle Filters with Refinement Steps for UAV Pose Tracking#Particle Filters (PFs) have been successfully employed for monocular 3D model-based tracking of rigid objects. However, these filters depend on the computation of importance weighs that use sub-optimal approximations to the likelihood function. In this paper, we propose to enrich the filter with additional refinement steps to abridge its sub-optimality. We test the proposed approach in two different types of PFs: (i) an Unscented Particle Filter (UPF), and (ii) the recently proposed Unscented Bingham Filter (UBiF). These filters are applied to the outdoor tracking of a fixed-wing Unmanned Aerial Vehicle (UAV) autonomous landing in a Fast Patrol Boat (FPB), tested in a simulated environment with a real sky gradient filled with clouds. The use of the refinement steps significantly improves the overall accuracy of the method. 
id926#Eventanchor: Reducing human interactions in event annotation of racket sports videos#The popularity of racket sports (e.g., tennis and table tennis) leads to high demands for data analysis, such as notational analysis, on player performance. While sports videos ofer many benefts for such analysis, retrieving accurate information from sports videos could be challenging. In this paper, we propose EventAnchor, a data analysis framework to facilitate interactive annotation of racket sports video with the support of computer vision algorithms. Our approach uses machine learning models in computer vision to help users acquire essential events from videos (e.g., serve, the ball bouncing on the court) and ofers users a set of interactive tools for data annotation. An evaluation study on a table tennis annotation system built on this framework shows signifcant improvement of user performances in simple annotation tasks on objects of interest and complex annotation tasks requiring domain knowledge. 
id927#EnORS: An Enhanced Object Relationship Schema#The data model is a primary factor in creating databases. Moreover, it also shows the connection between different data features involved in the information system. A system that has a robust database structure leads to a strong information system. Hence, a poor database structure may resultin a half-baked approach. However, creating a good data model is confusing and complicated. This study focuses on creating a more improved database structure that is simple and less complex. This isdone by integrating the object-relational schema (ORS) proposed by Sinha and the enhanced relational model (ERM) introduced by Villari. Formulating a new structure results in an Enhanced Object Relationship Scheme (EnORS). The improved diagram illustrates not only the entity objects involved in adatabase but also the data objects that are interrelated within the information system. This new approach was implemented in creating an enhanced object-relational schema database structure in the Health Center Information System. It shows a more simplified process in creating an efficient database. However, this study only limits to generate the improved object-relational schema. More extensive research can be conducted to test its reliability as compared with other existing methodologies. 
id928#Computer Vision Tagging the Metropolitan Museum of Art's Collection: A Comparison of Three Systems#Computer vision algorithms are increasingly being applied to museum collections to identify patterns, colors, and subjects by generating tags for each object image. There are multiple off-the-shelf systems that offer an accessible and rapid way to undertake this process. Based on the highlights of the Metropolitan Museum of Art's collection, this article examines the similarities and differences between the tags generated by three well-known computer vision systems (Google Cloud Vision, Amazon Rekognition, and IBM Watson). The results provide insights into the characteristics of these taxonomies in terms of the volume of tags generated for each object, their diversity, typology, and accuracy. In consequence, this article discusses the need for museums to define their own subject tagging strategy and selection criteria of computer vision tools based on their type of collection and tags needed to complement their metadata. 
id929#A High Speed NTT Accelerator for Lattice-Based Cryptography#As the most computing-intensive operation in lattice-based cryptography, polynomial multiplication is the bottleneck for high speed and low-latency implementation of these schemes. In this paper, a vector number theoretic transform (NTT) accelerator is proposed to speed up this process. First, an efficient addressing scheme for the vectorized NTT/INTT is proposed to achieve a conflict free situation and make full use of hardware resources. Then, a flexible architecture is presented with a series of timing and low-power optimizations. Finally, the accelerator is implemented using TSMC 28nm technology. Approximately, 274k equivalent gates and 10kB SRAM is consumed by the proposed accelerator and 47.5ns is only needed to complete a 256-point NTT. Experimental results show that the energy-efficiency and processing speed achieves more than 17.6× and 3× improvements with 50% area saving than the state-of-the-art design. 
id930#Explicit Filterbank Learning for Neural Image Style Transfer and Image Processing#Image style transfer is to re-render the content of one image with the style of another. Most existing methods couple content and style information in their network structures and hyper-parameters, and learn it as a black-box. For better understanding, this paper aims to provide a new explicit decoupled perspective. Specifically, we propose StyleBank, which is composed of multiple convolution filter banks and each filter bank explicitly represents one style. To transfer an image to a specific style, the corresponding filter bank is operated on the intermediate feature produced by a single auto-encoder. The StyleBank and the auto-encoder are jointly learnt in such a way that the auto-encoder does not encode any style information. This explicit representation also enables us to conduct incremental learning to add a new style and fuse styles at not only the image level, but also the region level. Our method is the first style transfer network that links back to traditional texton mapping methods, and provides new understanding on neural style transfer. We further apply this general filterbank learning idea to two different multi-parameter image processing tasks: edge-Aware image smoothing and denoising. Experiments demonstrate that it can achieve comparable results to its single parameter setting counterparts. 
id931#Monitoring the effects of land sizes on private property transformation in an urban regeneration project by regression analysis: Erenler Cedit case study, Kocaeli#The aim of this study is to examine the continuity of private property rights after an urban regeneration project. There may be several social, economic and environmental parameters on the preferences about property rights. The land sizes of property owners are one of the most important economic parameters. Therefore, after the first urban regeneration project in Kocaeli, the relationship between local people's land sizes and their residence preferences is analyzed by using a geographic relational database and various regression models. Consequently, it is understood that there is a high correlation between land sizes and exchange preferences in the case project. 
id932#Ontology Authoring from Relational Database: A Model Based Approach#The ontology authoring is a fundamental task in the Semantic Web. This process enables the domain expert to develop ontologies with the help of dedicated tools. This article presents an approach for building OWL ontologies from relational databases based on Model Driven Engineering (MDE). The proposed approach consists of two phases: (1) Preprocessing phase and (2) Transformation phase. The first one consists of creating an input model from the database which must conform to its meta-model. The second phase takes this model as input and transforms it into an OWL file by executing a set of mapping rules written in Atlas Transformation Language (ATL). The transformation process is done at a higher level of abstraction; it does a matching between the source meta-model elements (Database) and the target meta-model elements (OWL). We have concretized our approach as the DB2OWLOntology tool, and we have evaluated it with a set of databases. The obtained results are encouraging and show the efficiency of the proposed approach. 
id933#Group law on affine conics and applications to cryptography#In this paper, we highlight that the point group structure of elliptic curves, over finite or infinite fields, may be also observed on reducible cubics with an irreducible quadratic component. Starting from this, we introduce in a very general way a group's structure over any kind of conic. In the case of conics over finite fields, we see that the point group is cyclic and lies on the quadratic component. Thanks to this, some applications to cryptography are described, considering convenient parametrizations of the conics. We perform an evaluation of the complexity of the operations involved in the parametric groups and consequently in the cryptographic applications. In the case of the hyperbolas, the Rédei rational functions can be used for performing the operations of encryption and decryption, and the More's algorithm can be exploited for improving the time costs of computation. Finally, we provide also an improvement of the More's algorithm. 
id934#An ontology-based module of the information system ScolioMedIS for 3D digital diagnosis of adolescent scoliosis#Background and objective: Conventional information systems are built on top of a relational database. The main weakness of these systems is impossibility to define stable data schema ahead when the knowledge of the system is evolving and dynamic. The widely accepted alternatives to relational databases are ontologies that can be used for designing information systems. Many research papers describe various methods for improving reliability and precision in generating the type of the Lenke classification based on the image processing techniques or a computer program, but all of them require radiograph images. The main objective of this paper is to demonstrate the development of an ontology-based module of the information system ScolioMedIS for adolescent idiopathic scoliosis (AIS) diagnosis and monitoring, which uses optical 3D methods to determine the Lenke classification of AIS and to avoid harmful effects of traditional radiation diagnosis. Methods: For creating an ontology-based module of the ScolioMedIS we used the following steps: specification, conceptualization, formalization and implementation. In the specification and conceptualization phase we performed data collection and analysis to define domain, concepts and relationships for ontology design. In the formalization and implementation stage we developed the OBR-Scolio ontology and the ontology-based module of the ScolioMedIS. The module employs the Protégé-OWL API, as a collection of Java interfaces for the OBR-Scolio ontology, which enables the creating, deleting, and editing of the basic elements of the OBR-Scolio ontology, as well as the querying of the ontology. Results: The ontology-based module of ScolioMedIS is tested on the datasets of 20 female and 15 male patients with AIS between the ages of 11 and 18, to categorize spinal curvatures and to automatically generate statistical indicators about the frequency of the basic spinal curvatures, degree of progression or regression of deformity and statistical indicators about curvature characteristics according to the Lenke classification system and Lenke scoliosis types. Results are then compared with analysis of the Lenke classification of 315 observed patients, performed using traditional radiation techniques. Conclusions: This part of the system allows continuous monitoring of the progression/regression of spinal curvatures for each registered patient, which may provide a better management of scoliosis (diagnosis and treatment). 
id935#A frequency modulation-based taxel array: A bio-inspired architecture for large-scale artificial skin#This work introduces an array prototype based on a Frequency Modulation (FM) encoding architecture to transfer multiple sensor signals on a single wire. The use case presented adopts Hall-effect sensors as an example to represent a much larger range of sensor types (e.g., proximity and temperature). This work aims to contribute to large area artificial skin systems which are a key element to enhance robotic platforms. Artificial skin will allow robotic platforms to have spatial awareness which will make interaction with objects and users safe. The FM-based architecture has been developed to address limitations in large-scale artificial skin scalability. Scalability issues include power requirements; number of wires needed; as well as frequency, density, and sensitivity bottlenecks. In this work, eight sensor signals are simultaneously acquired, transferred on a single wire and decoded in real-time. The overall taxel array current consumption is 36 mA. The work experimentally validates and demonstrates that different input signals can be effectively transferred using this approach minimizing wiring and power consumption of the taxel array. Four different tests using single as well as multiple stimuli are presented. Observations on performances, noise, and taxel array behaviour are reported. The results show that the taxel array is reliable and effective in detecting the applied stimuli. 
id936#Multi-party quantum summation without a third party based on d-dimensional bell states#This paper presents a novel n-party quantum summation protocol without the help of a third party via using d-dimensional Bell states. The proposed protocol has inserted decoy photons to prevent various types of the outsider attacks. All participants, respectively, perform the shifting operation to encode their private secrets on quantum particle sequences composed of the first particles of all d-dimensional Bell states. Then, the proposed n-party quantum summation protocol can resist individual attack and (n- 2) -party collusion attack. As an example, this paper compares the presented multi-party quantum summation protocol with other schemes in terms of different indicators. 
id937#Marine robots for coastal ocean research in the Western Indian Ocean#Marine robots have the potential to enhance WIO marine research to improve regional adaptation to the challenges presented by climate change by providing enhanced research capacity that bypasses the requirement for expensive infrastructure, such as large research vessels. This paper tests this potential and assesses the readiness of WIO communities to adopt autonomous technologies to meet its marine research priorities. We apply a range of analyses to a marine robots case study undertaken in waters around the island of Pemba, part of the Zanzibar archipelago, in Tanzania in 2019. The campaign formed part of a multinational project focused on increasing WIO capacity to meet food security and ocean sustainability challenges. A community engagement programme with six Tanzanian coastal communities resulted in positive changes in attitudes towards marine robots with reported increases in understanding and acceptance of such technologies. Suspicion of the robots was reduced and a lower risk of removing operational equipment was recorded following the provision of educational material. Cost, risk and benefit analysis shows that marine robots are perceived to provide high level benefits, but come at a high cost that is difficult to achieve using national or regional funding. An assessment of the capacity of WIO marine institutes to adopt such technologies shows that prior to this work, few skills or infrastructure related to marine robots were available to researchers and further confirmed that funding opportunities were perceived to be largely unavailable at institutional, national, regional or international levels. Responses from regional partners following completion of the case study however, revealed an uplift in perceived capacity, particularly related to access to infrastructure and expertise as well as support and opportunities for funding at each level. The presented case study is shown to have been a valuable demonstrator of the benefits of using marine robots to meet WIO coastal ocean research requirements and regional capacity was shown to be substantially increased within the broad range of marine institutes surveyed throughout the case study period. This study demonstrates that taking early steps towards adopting marine autonomous robots has increased WIO regional marine research capacity and increased the confidence and willingness of local researchers to seek alternative solutions to ongoing marine research challenges. Recommendations for future action that will continue to increase the capacity and readiness for regional adoption of marine robots include investment at local, national and regional levels to provide accessible training opportunities and to facilitate regional and international collaborations; investment in a regional hub, or centre of excellence for marine robotic technology; early adoption of newly emerging smaller, cheaper autonomous technologies; investment in local skills and support facilities to aid local buy-in and acceptance while supporting regional capacity. 
id938#Integration of blockchain and remote database access protocol-based database#Many companies are relying on software to manage their businesses. Usually, the software, especially those used by smaller companies, is not secure against unauthorized or unethical data manipulation on the database level. This paper recommends and demonstrates the use of blockchain for securing small enterprises against hacking by alerting the management whenever a change is made to the data without using the authorized channels. This is done through blockchain technology’s inherent hash replication and mining algorithm. The paper shows an application where this idea has successfully been implemented for a desktop application built upon a remote database access (RDA) protocol-based relational database management system (RDBMS) such as remote MySQL and remote Oracle database. 
id939#Design of a 16-Bit Harvard Structure RISC Processor in Cadence 45nm Technology#The architecture of a MIPS (Microprocessor without Interlocked Pipeline Stages) based RISC or Reduced Instruction Set of Computers is a type of microprocessor which was designed by Harvard type data path structure to execute high speed using a small set of Instructions. This project explains the design and implementation of a 4-stage pipelining based low power processor. This feature leads to increase the reliability and speed of the system. The pipelining includes fetch, decode, execute and memory read/write operations. Low power was obtained by using clock gating technique. Clock gating is used to eliminate the unwanted clock usage when the module is not used. The main aim of the project is to design a 4-stage pipelined RISC processor starting from RTL to GDSII (Physical Design). The processor was coded by Verilog HDL language and implemented in Cadence Encounter Compiler tool. Calculated area, power, delay and clock gating using Cadence RTL compiler using slow and fast libraries of 45nm technology. 
id940#PNAS: A privacy preserving framework for neural architecture search services#The success of deep neural networks has contributed to many fields, such as finance, medic and speech recognition. Machine learning models adopted in these fields are always trained with a massive amount of distributed and highly personalized data harvested directly from users. Concerns for data privacy and the demand for better data exploitation have prompted the design of several secure schemes that allow an untrusted server to train ML models for one or multiple parties. However, these existing schemes only focus on network parameter, and hardly extend their optimization range to model architecture scope. Sine the performance of a neural network is closely related to both parameter and its architecture, service providers are difficult to deliver customized and flexible neural networks to each client. To this end, in this paper we propose PNAS, a novel MLaaS framework that enables a server to jointly optimize network parameter and architecture while ensuring the privacy of training sets. A double-encryption scheme is derived to prevent privacy leakage from sample itself, as well as intermediate feature maps during training. Specifically, we adopt functional encryption and feature transformation to secure forward and back propagation. Extensive experiments have demonstrated the superiority of our proposal. 
id941#Formalising Σ -Protocols and Commitment Schemes Using CryptHOL#Machine-checked proofs of security are important to increase the rigour of provable security. In this work we present a formalised theory of two fundamental two party cryptographic primitives: Σ-protocols and Commitment Schemes. Σ-protocols allow a prover to convince a verifier that they possess some knowledge without leaking information about the knowledge. Commitment schemes allow a committer to commit to a message and keep it secret until revealing it at a later time. We use CryptHOL (Lochbihler in Archive of formal proofs, 2017) to formalise both primitives and prove secure multiple examples namely; the Schnorr, Chaum-Pedersen and Okamoto Σ-protocols as well as a construction that allows for compound (AND and OR) Σ-protocols and the Pedersen and Rivest commitment schemes. A highlight of the work is a formalisation of the construction of commitment schemes from Σ-protocols (Damgard in Lecture notes, 2002). We formalise this proof at an abstract level using the modularity available in Isabelle/HOL and CryptHOL. This way, the proofs of the instantiations come for free. 
id942#Scheduling of Operations in Quantum Compiler#When scheduling quantum operations, a shorter overall execution time of the resulting schedule yields a better throughput and higher fidelity output. In this paper, we demonstrate that quantum operation scheduling can be interpreted as a special type of job-shop problem. On this basis, we provide its formulation as Constraint Programming while taking into account commutation between quantum operations. We show that this formulation improves the overall execution time of the resulting schedules in practice through experiments with a real quantum compiler and quantum circuits from two common benchmark sets. 
id943#A reusable code-based SAR ADC design with CDAC compiler and synthesizable analog building blocks#This brief proposes a code-reusable design methodology for synthesizable successive approximation register (SAR) ADCs based on the digital design flow to significantly reduce design effort. The SAR ADCs are composed of a capacitor-DAC (CDAC) macro cell generated by a CDAC compiler and analog functional blocks implemented utilizing digital standard cells. Two prototypes of SAR ADCs (12-bit 100 kS/s and 11-bit 50 MS/s) are fabricated in different CMOS processes (180 nm and 28 nm). The prototype ADCs prove the effectiveness of the proposed design methodology with comparable performances with full-custom designed SAR ADCs. 
id944#A convolutional neural network-based method for workpiece surface defect detection#The surface defects of the workpiece affect the workpiece quality. In order to detect workpiece surface defects more accurately, an automatic detection convolutional neural networks-based method is proposed in this paper. Firstly, a convolution network classification model (SCN) with symmetric modules is proposed, which is used as backbone of our method to extract features. And then, three convolution branches with FPN structure are used to identify the features. Finally, an optimized IOU (XIoU) is designed to define the loss function, which is used for detection model training. In addition to the public datasets NEU-CLS and NEU-DET, a classification dataset and a detection dataset of surface defects on hearth of raw aluminum casting are established to train and evaluate our model. On the basis above, the proposed backbone SCN was compared with Darknet-53 and ResNet-101 to present its superiority in classification performance. The average accuracy of SCN on NEU-CLS and self-made data sets are 99.61% and 95.84% respectively, which is significantly higher than the other two classification models. Then, in order to show the effectiveness and superiority of the proposed automatic detection method, the detection performance of the method is compared with the Faster-RCNN series and the YOLOv3 series. The result shows that our model achieves 79.89% mAP on NEU-DET and 78.44% mAP on self-made detection dataset. Our model can detect at 23f/s when the input image size is 416 × 416 × 3. The detection performance of our model is significantly better than other models. The results show that the proposed method has better performance and can be used for real-time automatic detection of workpiece surface defects. 
id945#State of practice of automation in precast concrete production#A current evaluation of the building industry shows a very low efficiency regarding material use as well as a low level of automation in production and construction. With the industry additionally facing a scarcity of skilled workers in mechanical engineering and construction a development towards automated production processes has been noticeable. With concrete being the main construction material used worldwide this paper will concentrate on the state of the art in automated precast concrete construction, in particular the automated production processes and methods. The high innovation spirit of individual research facilities and companies has resulted in various new applications of robotics and automated systems in the precast industry. A classification of the production processes and production methods according to two level of automation scales will be presented. Furthermore, the environmental, economic and social sustainability dimensions regarding design, production and construction in regard to concrete construction will be addressed. 
id946#Learning directed locomotion in modular robots with evolvable morphologies#The vision behind this paper looks ahead to evolutionary robot systems where morphologies and controllers are evolved together and ‘newborn’ robots undergo a learning process to optimize their inherited brain for the inherited body. The specific problem we address is learning controllers for the task of directed locomotion in evolvable modular robots. To this end, we present a test suite of robots with different shapes and sizes and compare two learning algorithms, Bayesian optimization and HyperNEAT. The experiments in simulation show that both methods obtain good controllers, but Bayesian optimization is more effective and sample efficient. We validate the best learned controllers by constructing three robots from the test suite in the real world and observe their fitness and actual trajectories. The obtained results indicate a reality gap, but overall the trajectories are adequate and follow the target directions successfully. 
id947#Design, fabrication, and hysteresis modeling of soft microtubule artificial muscle (smam) for medical applications#Robotic artificial muscles (RAMs) are promising power sources for medical fields such as surgical robotics. However, existing RAMs are challenged by scalability, material costs and fabrications. The nonlinear hysteresis in fluid-driven RAMs causes oscillations in open-loop systems. To circumvent these limitations, this letter introduces hydraulically soft microtubule artificial muscle (SMAM) that is low-cost and scalable, yet simple to fabricate. The SMAM, which only requires a flexible silicone microtube and a hollow micro-coil, is elongated or contracted under a fluid pressure. The SMAM presents an ideal candidate for flexible robotic systems such as endoscopic surgical robots. Experiments are conducted to characterize the SMAMs. Results show that the hysteresis profiles between the input syringe plunger position and output position are stable regardless of its configuration, as opposed to the highly variable responses for the tendon-sheath mechanisms. A new nonlinear model is developed to characterize the asymmetric hysteresis phenomena of the SMAM. Compared to the Bouc-Wen hysteresis models, the developed model presents a better capture of hysteresis. To demonstrate the muscle capability, a SMAMs-driven pulley and a flexible surgical arm are given. The new SMAM and its asymmetric hysteresis model are expected to provide a path for the development of rapidly efficient and low-cost soft actuators for use in flexible medical devices and surgical robotic systems. 
id948#Automatically indexing millions of databases in Microsoft azure SQL database#An appropriate set of indexes can result in orders of magnitude better query performance. Index management is a challenging task even for expert human administrators. Fully automating this process is of significant value. We describe the challenges, architecture, design choices, implementation, and learnings from building an industrial-strength auto-indexing service for Microsoft Azure SQL Database, a relational database service. Our service has been generally available for more than two years, generating index recommendations for every database in Azure SQL Database, automatically implementing them for a large fraction, and significantly improving performance of hundreds of thousands of databases. We also share our experience from experimentation at scale with production databases which gives us confidence in our index recommendation quality for complex real applications. Copyright 
id949#An Efficient Relational Database Keyword Search Scheme Based on Combined Candidate Network Evaluation#Relational keyword search (R-KWS) systems provide users with a convenient means in relational database queries. There exist two main types of R-KWS: those based on Data Graphs and those based on Schema Graphs. In this paper, we focus on the latter, R-KWS based on Schema Graphs. Most existing methods are typically inefficient due to the large number of repetitive operation caused by overlapping candidate networks and the execution of lots of complex queries. We present the R-KWS approach based on combined candidate network evaluation to improve the query efficiency. The proposed Combined Candidate Network (CCN) can efficiently share the overlapping part between candidate networks, and then avoid the repetitive operation during the evaluation of candidate networks. Meanwhile, CCN possesses another important characteristic that candidate networks within a CCN are still identifiable after candidate networks being compressed into a CCN. We design an algorithm based on this characteristic to evaluate CCN for the generation of final query results. This algorithm is able to eliminate the execution of a large number of complex queries required by most existing approaches, and thus significantly improve the efficiency of keyword search. Experiments on real datasets show that our approach can improve query efficiency without any loss of the quality of query results with respect to existing approaches. 
id950#Parallel Bit Pattern Computing#There are many ways to reduce power consumed in performing a computation. Most focus on making each gate more power efficient. In contrast, the current work focuses on directly reducing the number of gate-level operations needed to produce each word-level result.Compiler optimization of computations at the gate level exposes many redundancies that are not apparent when optimizing word-level operations. In the proposed architecture, all operations on multi-bit data values are performed bit serially. Thus, a k-bit add takes O(k) clock cycles. However, by doing each operation SIMD-parallel on n data, n k-bit operations also complete in O(k) clock cycles using only O(n) gates per clock. Further improvement can be made by using regular expression patterns to represent the n values in each bit position; not only does this compress the data, but it also allows many gate-level operations to be performed directly on the patterns without expanding them to bit vectors. 
id951#Anomaly detection of core failures in die casting X-ray inspection images using a convolutional autoencoder#Core failure inspection is an important issue in die casting. The inspection process is often carried out by manually examining X-ray images. However, human visual inspection suffers from individual biases and eye fatigues. Computer-vision-based automatic inspection, if it can achieve equal to or better than human performance, is favored to assist the inspectors to achieve better quality control. Most existing works are heavily relied on the supervised methods, which require enormous labeling and cannot be deployed quickly and economically. This is particularly difficult for a die casting plant that has many different types of products. Labeling each type of product before applying automated inspection may not be feasible in practice. It is therefore necessary to investigate unsupervised methods for die casting products. In this research, an inspection framework built on top of convolutional autoencoder (CAE) is designed and developed to inspect core failures from real-world die casting X-ray images in an unsupervised manner. Identification of good and scrap product, and localization of the defect are achieved in a single network. The framework is designed to be easily generalized to other image inspection scenarios. The area of interest for inspection is first extracted automatically through the Hough transformation. Then the preprocessed image is inspected by CAE. The noises of the model are removed using edge detection. It achieved an impressive 97.45% classification accuracy on average, and precisely pinpointed the defect regions with a small training set of 30 images. 
id952#Automatic number plate recognition:A detailed survey of relevant algorithms#Technologies and services towards smart-vehicles and Intelligent-Transportation-Systems (ITS), continues to revolutionize many aspects of human life. This paper presents a detailed survey of current techniques and advancements in Automatic-Number-Plate-Recognition (ANPR) systems,with a comprehensive performance comparison of various real-time tested and simulated al-gorithms,including those involving computer vision (CV). ANPR technology has the ability to detect and recognize vehicles by their number-plates using recognition techniques. Even with the best algo-rithms, a successful ANPR system deployment may require additional hardware to maximize its ac-curacy. The number plate condition,non-standardized formats,complex scenes,camera quality,camera mount position,tolerance to distortion,motion-blur,contrast problems,reflections,processing and memory limitations,environmental conditions,indoor/outdoor or day/night shots,software-tools or other hardware-based constraint may undermine its performance. This inconsistency,challenging environments and other complexities make ANPR an interesting field for researchers. The Internet-of-Things is beginning to shape future of many industries and is paving new ways for ITS. ANPR can be well utilized by integrating with RFID-systems,GPS,Android platforms and other similar technologies. Deep-Learning techniques are widely utilized in CV field for better detection rates. This research aims to advance the state-of-knowledge in ITS (ANPR) built on CV algorithms;by citing relevant prior work,analyzing and presenting a survey of extraction,segmentation and recognition techniques whilst providing guidelines on future trends in this area. 
id955#Efficient scaling of a hydrodynamics simulation using compiler-based accelerator technology#In the realm of numerical modeling, there is often a requirement to run simulations with higher spatial and temporal resolutions. Increasing resolution can improve the accuracy of simulations with a corresponding increase in the run time. These run times can become impractical for conventional sequentially coded simulations. Parallelizing simulation code offers great possibilities for improved run-time performance, but it can be very difficult to achieve the necessary speedups when software development and debugging time are considered. This is especially the case when extensive modifications to legacy source code are needed to incorporate parallel processing capabilities. To address this problem, we used a directive-driven compiler to parallelize a hydrodynamics simulation targeting GPU hardware. This compiler-level parallel development environment enabled us to parallelize an existing legacy program with minimal alterations to the original source code. Using compiler directives and testing methodologies, we were able to achieve more than a 15× speedup and reduce the run time of high resolution simulations from months to days while verifying that results were equivalent to the sequential legacy version of the simulation. Copyright (c) 2015 Society for Modelign & Simulation International (SCS).
id956#Research on high precision algorithm of scale-spatial in computer vision#The intra class differences of object categories will bring great challenges to the image matching algorithm in machine vision. In order to guarantee the accuracy and quality of image matching algorithm, it is necessary to accurately define and understand the scene and semantics of the image, so as to find out the typical similar features of the image. On account of this, this paper first analyses the role and traits of scale-spatial theory, then studies the high-precision algorithm of scale-spatial in computer vision, and finally gives the extremum detection of scale-spatial, the determination of extremum position and the generation of feature point descriptor. 
id957#Method for Determining the Parameters of Heating Systems for Hydraulic Presses#A method for determining the parameters of heating systems for hydraulic presses is proposed, including: decomposition of the general problem into optimization tasks for elements of a heating system; mathematical formulations of optimization tasks and determination of information flows between them; the structural determination of automation system for selecting parameter values of hydraulic press heating systems and the functions of its elements; the development of mathematical models describing the functionality of heating systems elements for hydraulic presses along with corresponding information models for storing and processing information about the source data and results of solving optimization tasks; the formation of a relational database of heating system elements for hydraulic presses, whose tables are based on the analysis of information models representing the characteristics of press elements and sets of permissible values of the characteristics to be selected. Elements of the developed automation system for determining the parameters of heating systems were used to perform calculations of heating systems for industrial presses ordered by Tambovpolymermash Plant JSC and ARTI-Zavod JSC. 
id958#Scalable Database Normalization Powered by the Crowd#In the age of information, there is a growing need to process data, both structured and unstructured, with ease and accuracy. Such an intensive task might be achieved through machine learning based applications, but not always. Moreover, the major problems with existing methodologies are scalability and complexity. Crowdsourcing is an alternate solution that deploy mechanisms for breaking the major task at hand into smaller segments (microtasks) requiring quick (often binary) decisions to be exercised by the amateur human participants. The use of such collective and collaborative intelligence can simplify few, if not most, of the difficult database management tasks. In this paper, we address one such problem, precisely determining functional dependencies (FDs) in a database. Our motivation is to pursue scalable normalization of a database through a game-like crowdsourcing approach. The FD realization is an interesting outcome of our research. 
id959#A Relational Database Schema of Construction Costs Tracking from BIM Model to Final Costs#Advanced conceptual cost estimation is based on historical project data. Cost estimators are still experiencing the difficulty of obtaining supportive information because most of the raw data is stored separately and cannot be easily traced. Previously proposed building information modeling (BIM)-based database did not produce cross-project historical data and also required extra effort in the design phase. To address the limitations, this paper considers building an independent relational database (RDB) as a data tracking tool. A preliminary RDB schema for conceptual estimating was developed in this paper. This RDB extracted and fused data from BIM models, bill of quantities (BoQs), and payment records using open tool MySQL. Simulated cost data of VDC Research Center was used as a case study. Three queries were coded and tested as validation. It can be concluded that BIM and cross-project RDB integrated data tracking method is helpful in conceptual cost estimating. It fills the current gap between the project-level BIM implementation and the enterprise-level database evolution by interoperating data of domain-oriented specifications. 
id960#Intelligent Index Tuning Approach for Relational Databases [面向关系数据库的智能索引调优方法]#Indexing is one of the most effective techniques for relational databases to achieve fast queryprocessing. The intelligent index tuning technique can effectively adjust the index of the database instance to obtain efficient query performance. Most of the existing methods utilize the query log to generate candidate indices, and then use the artificially designed models to select indices, thereby the indices are adjusted. However, the candidate indices generated from the query log may not exist in the database instance, so they cannot precisely estimate the effects of such indices on the query processing. This study first designs and implements an intelligent index tuning system for the relational database. Secondly, it proposes a learning-based method to model the effects of indices for query processing, accordingly, the query optimization effect of an index can be accurately estimated when selecting optimized indices. Then, an efficient optimal index selection algorithm is designed to select a set of indices with the maximal utility from candidate indices, which satisfy the space threshold. Finally, experiments are conducted to test the performance of the proposed system in different settings. The experimental results show that the proposed technique can effectively adjust the index and achieve a significant improvement in query performance for a relational database. 
id961#Robust RFID-Based Multi-Object Identification and Tracking with Visual Aids#Obtaining fine-grained spatial information is of practical importance in RFID-based applications. However, high-precision positioning remains a challenging task in commercial-off-The-shelf (COTS) RFID systems. Inspired by progress in the computer vision (CV) field, researchers propose to combine CV with RFID systems and turn the positioning problem into a matching problem. Promising though it seems, current methods fuse CV and RFID through converting traces of tagged objects extracted from videos by CV into phase sequences for matching, which is a dimension-reduced procedure causing loss of spatial resolution. Consequently, they fail in more harsh conditions such as small tag intervals and low reading rates of tags. To address the limitation, we propose TagFocus, a more robust RFID-enabled system for fine-grained multi-object identification and tracking with visual aids. The key observation of TagFocus is that traces generated by different methods shall be compatible if they are acquired from one identical object. Leveraging this observation, an attention-based sequence-To-sequence (seq2seq) model is trained to generate a simulated trace for each candidate tag-object pair. And the trace of the right pair shall best match the observed trace directly extracted by CV. A prototype of TagFocus is implemented and extensively assessed in lab environments. Experimental results show that our system maintains a matching accuracy of over 89% in harsh conditions, outperforming state-of-The-Art schemes by 25%. 
id962#Compile-time type-checking for custom type qualifiers in java#The Checker Framework enables adding custom type qualifiers to the Java language in a backward-compatible way. The Checker Framework allows programmers to write type qualifiers in their programs and to create compiler plug-ins that enforce the semantics of these qualifiers at compile time. Using the plug-ins, programmers can improve the quality of their code without disturbing their development workflow. Our case studies demonstrate that the Checker Framework is scalable, easy to use, and effective in detecting and preventing errors. It has been used to detect real errors in null pointer dereferencing, side effects, equality tests, and initialization, among others.
id963#S-FPN: A shortcut feature pyramid network for sea cucumber detection in underwater images#Convolutional neural network is a prominent innovation in computer vision but is often troubled by problems such as dark light, turbidity, blur and high similarity to the background when applied to underwater object detection. Underwater object detection is one of the basic techniques of underwater grasping automation which plays a very important role in ocean detection and fishery of aquatic products. This paper presented an automatic detection method of underwater sea cucumber based on deep learning, which will provide effective technical support for the automated breeding and harvesting of sea cucumber. The Shortcut Feature Pyramid Network (S-FPN) proposed in this paper improves the existing multi-scale feature fusion strategy through shortcut connection. The ablation experimental results show that the mean average precision (mAP) of S-FPN reaches 91.5% which outperforms the baseline Feature Pyramid Network (88.6%), YOLO v3 (83.7%) and SVM-HOG (61.6%). To resolve the problem of complex environmental background interference of ocean floor, we proposed a Piecewise Focal Loss (PFL) function for balancing the positive and negative samples such that the algorithm can focus on the training difficulty of hard (i.e., positive) samples. And the ablation experimental results show that the mAP of PFL reaches 92.3% which outperforms the baseline Cross Entropy (91.5%) and Focal Loss (91.8%). Also, we chose Exponential Linear Unit as the optimization strategy, and Adaptive Moment Estimation as the activation function by ablation research, finally the mAP reached 94%. 
id964#Client-side Cryptography Based Security for Cloud Computing System#Cryptography indicates to techniques of securing information and communication derived from mathematical perception to convert messages in ways that are tough to interpret. Cryptography is firmly associated with the department of cryptology along with cryptanalysis. It consists of techniques such as blending words with images, microdots, and alternative ways to mask data during storage or else transit. However, in the modern era, cryptography is repeatedly related to cloud computing. But, moving data into a cloud is a huge modification and has real involvement that makes users lapse before one can sign up for the desired service which can cause unwanted instruction on sensitive information and data lost. For the security of cloud data, a symmetric algorithm had been introduced by previous research work which used simple algorithms and had performance issues. In this research paper we have introduced and enforced symmetric key encryption that would encrypt a file locally at the client-side prior to uploading to the cloud and the file would decrypt after downloading on the client-side using key generated during encryption. This algorithm also uses a different algorithm to calculate the key value. As a result, our algorithm offers better security and better performance for large files. This way we can add an extra layer of security which would restrain unwanted attacks on intimated information as well as lack of standardization. 
id965#Radix-2warithmetic for scalar multiplication in elliptic curve cryptography#Elliptic curve scalar multiplication k.P, where k is a nonnegative constant and P is a point on the elliptic curve, requires two distinct operations: addition (ADD) and doubling (DBL). To reduce the number of ADDs without increasing the number of DBLs, a recoding of k with fewer nonzero digits is necessary. Based on Radix- 2w arithmetic, we introduce a principled w -bit windowing method where the properties of speed, memory, and security are described by exact analytic formulas as proof of superiority. Contrary to existing windowing algorithms, to minimize the number of ADDs the window size (w) is guided by an optimum depending on the bit-length (l) of the scalar k. The number of required precomputations is minimal regarding the value of w. The proposed method recodes the binary string k and evaluates the multiplication on-the-fly from right-to-left and left-to-right, likewise. Radix-2w method is very easy to be used and highly reconfigurable, allowing speed-memory and speed-security trade-offs to satisfy different crypto-system constraints. Furthermore, the method shows a high resilience to side-channel attacks based on power, timing, and statistical analysis. All Radix- 2w properties are confronted to standard windowing methods' through an in-depth analysis of the complexities. An overall comparison is made via NIST-recommended GF(2l) finite fields. 
id966#Template matching of a coarse grain reconfigurable architecture datapath using constraint programming#A major hindrance on creating the custom datapath is the config-uration of the reconfigurable structures, something that has been reported on multiple occasions by the respective authors.We were motivated by the lack of tools for the configuration of such units and created a tool that can accept a control data flow graph of an application and map the user-defined operating templates. Our on-line tool that operates on the theory of constraint programming is efficient and can map within seconds a full control data flow graph of an application, to the given operating templates. 
id967#An intuitive surgical handle design for robotic neurosurgery#Purpose: The expanded endoscopic endonasal approach, a representative example of keyhole brain surgery, allows access to the pituitary gland and surrounding areas through the nasal and sphenoid cavities. Manipulating rigid instruments through these constrained spaces makes this approach technically challenging, and thus, a handheld robotic instrument could expand the surgeon’s capabilities. In this study, we present an intuitive handle prototype for such a robotic instrument. Methods: We have designed and fabricated a surgical instrument handle prototype that maps the surgeon’s wrist directly to the robot joints. To alleviate the surgeon’s wrist of any excessive strain and fatigue, the tool is mounted on the surgeon’s forearm, making it parallel with the instrument’s shaft. To evaluate the handle’s performance and limitations, we constructed a surgical task simulator and compared our novel handle with a standard neurosurgical tool, with the tasks being performed by a consultant neurosurgeon. Results: While using the proposed handle, the surgeon’s average success rate was 80 % , compared to 41 % when using a conventional tool. Additionally, the surgeon’s body posture while using the suggested prototype was deemed acceptable by the Rapid Upper Limb Assessment ergonomic survey, while early results indicate the absence of a learning curve. Conclusions: Based on these preliminary results, the proposed handle prototype could offer an improvement over current neurosurgical tools and procedural ergonomics. By redirecting forces applied during the procedure to the forearm of the surgeon, and allowing for intuitive surgeon wrist to robot-joints movement mapping without compromising the robotic end effector’s expanded workspace, we believe that this handle could prove a substantial step toward improved neurosurgical instrumentation. 
id968#An application-driven teaching model for series curriculum#Courses for data structures, compiler theories and databases are the most important fundamental specialty courses of computer science. Because of the highly abstract and strong logical characteristics of these series curricula, it is difficult for students to learn and apply them to actual projects. Based on the design and implementation of the information management system for reservoir resettlement compensation, this paper summarizes the problems encountered in the actual development, and proposes some specific solutions. At the same time, it explores and researches the teaching method of these series curricula. A novel application-driven teaching model for series curriculum is proposed. Either the solution of actual project or their application in teaching is effective and feasible. 
id969#Investigating the importance of shape features, color constancy, color spaces, and similarity measures in open-ended 3D object recognition#Despite the recent success of state-of-the-art 3D object recognition approaches, service robots still frequently fail to recognize many objects in real human-centric environments. For these robots, object recognition is a challenging task due to the high demand for accurate and real-time response under changing and unpredictable environmental conditions. Most of the recent approaches use either the shape information only and ignore the role of color information or vice versa. Furthermore, they mainly utilize the Ln Minkowski family functions to measure the similarity of two object views, while there are various distance measures that are applicable to compare two object views. In this paper, we explore the importance of shape information, color constancy, color spaces, and various similarity measures in open-ended 3D object recognition. Toward this goal, we extensively evaluate the performance of object recognition approaches in three different configurations, including color-only, shape-only, and combinations of color and shape, in both offline and online settings. Experimental results concerning scalability, memory usage, and object recognition performance show that all of the combinations of color and shape yield significant improvements over the shape-only and color-only approaches. The underlying reason is that color information is an important feature to distinguish objects that have very similar geometric properties with different colors and vice versa. Moreover, by combining color and shape information, we demonstrate that the robot can learn new object categories from very few training examples in a real-world setting. 
id970#Placing Conditional Disclosure of Secrets in the Communication Complexity Universe#In the conditional disclosure of secrets (CDS) problem (Gertner et al. in J Comput Syst Sci, 2000) Alice and Bob, who hold n-bit inputs x and y respectively, wish to release a common secret z to Carol, who knows both x and y, if and only if the input (x, y) satisfies some predefined predicate f. Alice and Bob are allowed to send a single message to Carol which may depend on their inputs and some shared randomness, and the goal is to minimize the communication complexity while providing information-theoretic security. Despite the growing interest in this model, very few lower-bounds are known. In this paper, we relate the CDS complexity of a predicate f to its communication complexity under various communication games. For several basic predicates our results yield tight, or almost tight, lower-bounds of Ω (n) or Ω (n1-ϵ) , providing an exponential improvement over previous logarithmic lower-bounds. We also define new communication complexity classes that correspond to different variants of the CDS model and study the relations between them and their complements. Notably, we show that allowing for imperfect correctness can significantly reduce communication—a seemingly new phenomenon in the context of information-theoretic cryptography. Finally, our results show that proving explicit super-logarithmic lower-bounds for imperfect CDS protocols is a necessary step towards proving explicit lower-bounds against the communication complexity class AM cc, or even AM cc∩ co-AM cc—a well known open problem in the theory of communication complexity. Thus imperfect CDS forms a new minimal class which is placed just beyond the boundaries of the “civilized” part of the communication complexity world for which explicit lower-bounds are known. 
id971#Computer vision based food grain classification: A comprehensive survey#This manuscript presents a comprehensive survey on recent computer vision based food grain classification techniques. It includes state-of-the-art approaches intended for different grain varieties. The approaches proposed in the literature are analyzed according to the processing stages considered in the classification pipeline, making it easier to identify common techniques and comparisons. Additionally, the type of images considered by each approach (i.e., images from the: visible, infrared, multispectral, hyperspectral bands) together with the strategy used to generate ground truth data (i.e., real and synthetic images) are reviewed. Finally, conclusions highlighting future needs and challenges are presented. 
id974#A Generic Approach to Schema Evolution in Live Relational Databases#Schema evolution is an important theme for many database users across a broad range of fields. This paper introduces a generic data management layer, GeneRelDB, which allows the schema of a relational database to evolve during run time without the need to rewrite database queries in the application code. It is designed to run as an abstraction layer, handling all communication (queries and data exchange) between the user interface and the database backend. The only restriction to the changes that can be made relate to data type conversion for existing columns in the database. Foreign key constraints are supported and referential integrity is maintained during evolution. 
id975#Agent programming language and its complier implementation based on JavaCC#This paper aims to present an agent-oriented programming language - IAPL based on BDI structure, and implement a compiler according to the lAPL's operational semantics by JavaCC. 
id976#PCT: Point cloud transformer#The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer (PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation, semantic segmentation, and normal estimation tasks. 
id977#Quantum computing, cryptography and compilers#Harnessing the power of quantum mechanical systems will have a profound impact on information processing, in particular on computing and cryptography. I will discuss the current state of the field, including recent work on quantum compilers. 
id979#Six-State Continuous Processing Model for a New Theory of Computing#In contrast to the computation in Von-Neumann Architecture, the human mind executes processing in the brain by improving speed and accuracy over execution cycles. Further, it has been postulated that the memory is a result of continuous processing and is not separated from processing in the human mind. Similar to that mind model, a six-state continuous processing model with the states New, Ready, Running, Blocked, Sleep and Terminate, has been proposed to implement a special compiler which consists of conditionally evolving memory that aids in improving performance in conventional computation by exploiting 24-Casual Relations explained in Buddhist Theory of Mind. The experiments have been conducted to demonstrate how the proposed computing model increases the performance of execution of source codes and compilers. The result shows a clear increase of performance in computation by avoiding overloading the memory, and ensuring the execution of high quality code segments at the right time. 
id980#Server Data Auditing with Encryption Techniques#The management of vast volumes of data with cloud service providers (CSPs) poses data privacy issues. Data confidentiality and privacy could be lost due to cloud administrator, malware, deceptive cloud providers, or other malicious users who may manipulate the data physically transferring the data from one location to another. Hence, corrections to saved data must be reviewed at regular intervals. Cryptography, remote (cloud) data verification, is carried out by third-party auditors (TPAs). Also ideal for public auditing are TPAs, which provide audit services with more efficient analytical and communication capabilities than standard users. Our work focuses on cryptographic algorithms for cloud data auditing and the problems surrounding the validity and privacy of these algorithms. Many approaches to preserving confidentiality and privacy have been suggested in the literature; they are usually categorized according to the different states of the data: static, dynamic, multi-owner, multi-user, etc. 
id981#Fault detection architectures for inverted binary ring-lwe construction benchmarked on fpga#Ring learning with errors (RLWE) is an efficient lattice-based cryptographic scheme that has worst-case reduction to lattice problem, conjectured to be quantum-hard. Ring-BinLWE is an optimized variant of RLWE problem using binary error distribution, resulting in highly-efficient hardware implementation. Efficient and low-complexity architectures in hardware, thwarting natural and malicious faults, are essential for lattice-based post-quantum cryptography (PQC) algorithms. In this brief, we explore efficient fault detection approaches for implementing the Ring-BinLWE problem. This brief, for the first time, investigates fault detection schemes for all three stages of RLWE encryption. Utilizing the stuck-at fault model, we employ recomputing with encoded operands schemes to achieve high error coverage. We simulate and implement our schemes on a field-programmable gate array (FPGA) platform. Our schemes provide low hardware overhead (area overhead of 15.74%, delay overhead of 7.74%, and power consumption overhead of 4.06%), with high error coverage, which can be suitable for resource-constrained as well as high-performance usage models. 
id982#Speed Gain in Elastic Joint Robots: An Energy Conversion-Based Approach#Like humans or animals, robots with compliant joints are capable of performing explosive or cyclic motions by making systematic use of energy storage and release, and it has been shown that they can outperform their rigid counterparts in terms of peak velocity. For rigid joint robots, there exist well-established, computationally inexpensive tools to compute the maximum achievable Cartesian endpoint velocity, which is an important performance and safety characteristic for robot designs. For elastic joint robots, optimal control is usually employed to determine the maximum possible link velocity together with the associated trajectory, which is time consuming and computationally costly for most systems. In this letter, we propose methods to obtain estimates of the maximum achievable Cartesian endpoint velocities of gravity-free elastic joint robots that have computational requirements close to the rigid joint robot case. We formulate an optimal control problem to verify the methods and provide results for a planar 3R robot. Furthermore, we compare the results of our approach with those from real-world throwing experiments which were previously conducted on the elastic DLR David system. Finally, we apply the methods to derive and quantitatively compare the safety properties of DLR David and a hypothetically rigid version of this robot in terms of the Safety Map framework proposed in our previous work. 
id983#Distributed Controllers for Human-Robot Locomotion: A Scalable Approach Based on Decomposition and Hybrid Zero Dynamics#This letter presents a formal foundation, based on decomposition, hybrid zero dynamics (HZD), and a scalable optimization, to develop distributed control algorithms for hybrid models of collaborative human-robot locomotion. The proposed approach considers a centralized controller and then decomposes the dynamics and feedback laws with a parameterization to synthesize local controllers. The Jacobian matrix of the Poincaré map with local controllers is studied and compared to that with centralized ones. An optimization problem is then set up to tune the parameters of the local controllers for asymptotic stability. The proposed approach can significantly reduce the number of controller parameters to be optimized for the synthesis of distributed controllers. The analytical results are numerically evaluated with simulations of a multi-domain hybrid model with 19 degrees of freedom for stable amputee locomotion with a powered knee-ankle prosthetic leg. 
id984#Fuzzy-based multiparty privacy management in social media using modified elliptic curve cryptography#Sharing data via social media may affect the privacy of other user’s in social media. Also, multiparty privacy management is absent in social media, which leads the users incapable of managing to whom the data are shared. Because of the privacy conflicts, it is not easy to combine the privacy preferences of multiple users. For resolving the privacy conflicts in social media, more methods are required. This study promotes a fuzzy-based multiparty privacy management in social media using modified elliptic curve cryptography. The evaluation model used a method based on secure multiparty computing. Next, the fuzzy technique for order of preference by similarity to ideal solution (fuzzy TOPSIS) method is used to rank and select the participants. Finally, data encryption is performed using a modified elliptic curve encryption (MECC). Here, the optimal selection of private key is performed using the cuckoo search optimization algorithm (CSOA). With these presented techniques, the users can manage who the data are shared. In order to overcome privacy conflicts, users may first rank and select the participants based on fuzzy TOPSIS. Also, the privacy of the users is not affected by using the MECC-based data encryption framework. The presented work is implemented on the JAVA platform. The outcomes of the experiment prove that the presented approach outperforms the other existing approaches. 
id985#Modifying queries strategy for graph-based speculative query execution for RDBMS#The paper relates to parallel speculative method that supports query execution in relational database systems. The speculative algorithm is based on a dynamic analysis of input query stream in databases serviced in SQLite. A middleware called the Speculative Layer is introduced, which based on a specific graph representation of query streams chooses the Speculative Queries to be executed. The paper briefly presents the structure of the Speculative Layer and graph modeling method. Then an extended version of speculative algorithm is presented which assumes an increased number of modifying queries in input query stream. Each modifying query present in the analysed query stream endangers already executed Speculative Queries with possibly invalid data and blocks their further use. We propose more sophisticated modifying queries analysis which aims in reducing the number of Speculative Queries which have to be deleted and thus decreases the necessary data manipulations. Experimental results are presented based on the proposed algorithms assessment using a real testbed database serviced in SQLite. 
id986#An efficient privacy preserving on high-order heterogeneous data using fuzzy K-prototype clustering#In this paper, we propose a privacy-preserving high order large amount of heterogeneous data using distributed high order fuzzy k-prototype framework named distributed multiple exponential kernel possibilistic fuzzy clustering (MEKPFCM), incorporates kernel fuzzy c-means and possibilistic fuzzy clustering algorithms. The privacy-preserving high order MEKPFCM, cluster the heterogeneous dataset by representing each heterogeneous data object as a tensor. In this paper, the cloud server directly performs clustering over encrypted datasets, while achieving maximum accuracy. The fully homomorphic encryption algorithm (FHE) is utilized to protect the high order large amount of heterogeneous data. Moreover, we design a secure integration of map-reduce into our proposed work, which makes our proposed work enormously appropriate for cloud computing environment. Detailed security analysis and experimental results show that proposed MEKPFCM method can effectively cluster a large amount of heterogeneous data. The experimentation of the proposed technique was carried out using UCI machinery skin dataset and the performance was compared with the previous techniques using accuracy and encryption time. Furthermore, MEKPFCM can cluster bigdata by using the cloud computing technology without disclosing privacy. 
id987#Visual Question Rewriting for Increasing Response Rate#When a human asks questions online, or when a conversational virtual agent asks a human questions, questions triggering emotions or with details might more likely to get responses or answers. we explore how to automatically rewrite natural language questions to improve the response rate form people. In particular, a new task of Visual Question Rewriting (VQR) task is introduced to explore how visual information can be used to improve the new question(s). A data set containing -4K blandandattractive question-images triples is collected. We developed some baseline sequence to sequence models and more advanced transformer-based models, which take a bland question and a related image as input, and output a rewritten question that's expected to be more attractive. Offline experiments and mechanical Turk based evaluations show that it's possible to rewrite bland questions in a more detailed and attractive way to increase response rate, and images can be helpful. 
id988#Computer vision-based citrus tree detection in a cultivated environment using UAV imagery#Manual inspection has been a common application for counting the trees and plants in orchards in precision agriculture processes. However, it is a time-consuming and, labour-intensive and expensive task. Recent remote sensing tools and methods provide a revolutionizing innovation for monitoring individual trees and crop recognition as an alternative to manual detection useful for long-term agricultural management. Our study adopted a Connected Components Labeling (CCL) algorithm to detect and count the citrus trees based on the high-resolution Unmanned Air Vehicles (UAV) images in two agricultural patches. The workflow consisted of applying morphological image operation algorithms on multi-spectral, 5-banded orthophoto imagery (derived from 1560 scenes) and 3,57 cm spatial resolution. Our approach was able to count 1462 out of 1506 trees resulting in accuracy and precision higher than 95% (average Recall: 0.97, Precision: 0.95) in heterogeneous agricultural patches (multiple trees and tree sizes). According to our understanding, the first time a CCL algorithm has been used with UAV multi-spectral images for detecting citrus trees. It performed significantly for geolocation and counting the trees individually in a heterogenous orchard. We concluded that our methodology provided satisfactory performance to predict the number of trees (in the citrus case study) in dense patches. Therefore it could be promising to replace the conventional tree detection techniques to detect the orchard trees in complex agricultural regions. 
id989#Syntactic Pattern Recognition in Computer Vision#Using techniques derived from the syntactic methods for visual pattern recognition is not new and was much explored in the area called syntactical or structural pattern recognition. Syntactic methods have been useful because they are intuitively simple to understand and have transparent, interpretable, and elegant representations. Their capacity to represent patterns in a semantic, hierarchical, compositional, spatial, and temporal way have made them very popular in the research community. In this article, we try to give an overview of how syntactic methods have been employed for computer vision tasks. We conduct a systematic literature review to survey the most relevant studies that use syntactic methods for pattern recognition tasks in images and videos. Our search returned 597 papers, of which 71 papers were selected for analysis. The results indicated that in most of the studies surveyed, the syntactic methods were used as a high-level structure that makes the hierarchical or semantic relationship among objects or actions to perform the most diverse tasks. 
id990#Safe Tightly-Constrained UAV Swarming in GNSS-denied Environments#A decentralized algorithm for flocking of Unmanned Aerial Vehicles (UAV) in environments with high obstacle density is proposed in this work. The method combines a local planning loop with bio-inspired swarming rules for navigating a compact UAV flock in a real workspace without relying on external infrastructures, such as motion capture system and GNSS. The group stability and coherence are achieved by employing a purposely designed onboard UVDAR system for mutual localization of teammates in local proximity of each UAV. The required robustness and scalability of the multi-UAV system are therefore achieved without any need for communication among the swarm particle. Such minimal sensory and communication requirements have allowed the system to become a backup technique for centralized multi-robot systems in case of communication and GNSS dropout. The proposed approach has been verified in numerous simulations and real experiments inside a forest that represents one of the most challenging environments for deployment of compact groups of aerial vehicles. 
id992#Robotics-based interventions for childrens creativity#Robots have massively been introduced in children's lives, showing promising effects on education and learning. Parallel to this, children's creative levels show a decline related to different factors, including the standardized teaching and learning dynamics present in traditional school systems. This work aims to investigate if the activities with robots already present in schools affect children's creativity levels. To study this, we compared creative levels of children across three study conditions: (1) Experimental condition 1: Children performed STEM activities in school by learning how to program robots; (2) Experimental condition 2: Children performed STEAM activities by learning how to design robots; (3) Control condition: Children engaged in a music class. We applied the Test for Creative Thinking-Drawing Production (TCT-DP), a validated test that measures creative potential, before and after the intervention. Our results showed that the creativity levels of children increased from pre-to post-Testing, revealing the effect of all intervention groups in potentiating creativity. Additionally, results showed that creative levels were significantly higher in the control condition. This result was expected since this condition consisted of an artistic musical intervention where creativity is foreseen to be stimulated. When analyzing the effects of the interventions on the two dimensions of TCT-DP (i.e., adaptiveness and innovativeness), results showed that both the control and the programming condition stimulated innovativeness. This result seems to show that STEM activities can stimulate non-conventional ways of thinking, similarly to creative activities such as a music class. While much has been studied about how STEM activities influence the knowledge of children, little is known if STEM also contributes to the stimulation of their creativity. This study promotes investigation in this topic and shows the potential of using robots to unlock creative potentials in children. 
id993#Quality evaluation of Keemun black tea by fusing data obtained from near-infrared reflectance spectroscopy and computer vision sensors#Keemun black tea is classified into 7 grades according to the difference in its quality. The appearance and flavour are crucial indicators of its quality. This research demonstrates a rapid grading method of jointly using near-infrared reflectance spectroscopy (NIRS) and computer vision systems (CVS) to evaluate the flavour and appearance quality of tea. A Bruker MPA Fourier Transform near-infrared spectrometer was used to record the spectrum of samples. A computer vision system was used to capture the image of tea leaves in an unobstructed manner. 80 tea samples for each grade were analyzed. The performance of four NIRS feature extraction methods (principal component analysis, local linear embedding, isometric feature mapping, and convolutional neural network (CNN)) was compared in this study. Histograms of six geometric features (leaf width, leaf length, leaf area, leaf perimeter, aspect ratio, and rectangularity) of different tea samples were used to describe their appearance. A feature-level fusion strategy was used to combine softmax and artificial neural networks (ANN) to classify NIRS and CVS features. The results indicated that for an individual NIRS signal, CNN achieved the highest classification accuracy with the softmax classification model. The histograms of the combined shape features indicated that when the softmax classification model was used, the classification accuracy was also higher than ANN. The fusion of NIRS and CVS features proved to be the optimal combination; the accuracy of calibration, validation and testing sets increased from 99.29%, 96.67% and 98.57% (when the optimal features from a single-sensor were used) to 100.00%, 99.29% and 100.00% (when features from multiple-sensors were used). This study revealed that the combination of NIRS and CVS features can be a useful strategy for classifying black tea samples of different grades. 
id994#Transforming ML Predictive Pipelines into SQL with MASQ#"Inference of Machine Learning (ML) models, i.e. the process of obtaining predictions from trained models, is often an overlooked problem. Model inference is however one of the main contributors of both technical debt in ML applications and infrastructure complexity. MASQ is a framework able to run inference of ML models directly on DBMSs. MASQ not only averts expensive data movements for those predictive scenarios where data resides on a database, but it also naturally exploits all the ""Enterprise-grade""features such as governance, security and auditability which make DBMSs the cornerstone of many businesses. MASQ compiles trained models and ML pipelines implemented in scikit-learn directly into standard SQL: no UDFs nor vendor-specific syntax are used, and therefore queries can be readily executed on any DBMS. In this demo, we will showcase MASQ's capabilities through a GUI allowing attendees to: (1) train ML pipelines composed of data featurizers and ML models; (2) compile the trained pipelines into SQL, and deploy them on different DBMSs (MySQL and SQLServer in the demo); and (3) compare the related performance under different configurations (e.g., the original pipeline on the ML framework against the SQL implementations). "
id995#Cryptanalysis and improvement of a semi-quantum private comparison protocol based on Bell states#Semi-quantum private comparison (SQPC) allows two participants with limited quantum ability to securely compare the equality of their secrets with the help of a semi-dishonest third party (TP). Recently, Jiang (Quantum Inf Process 19(6): 180, 2020) proposed a SQPC protocol based on Bell states and claimed it is secure. In this paper, we present two types of attack on Jiang’s SQPC protocol. In the first type of attack, an outside eavesdropper can make participants accept a wrong result. In the second type of attack, a malicious participant not only can make the other participant accept an incorrect result, but also can learn the secret of the honest participant. Neither type of attack is possible to be detected. In addition, we propose an improved SQPC protocol that can resist these two types of attack. 
id997#Analyzing Teleoperation Interface Usage of Robots in Therapy for Children with Autism#Therapist-operated robots can play a uniquely impactful role in helping children with autism practice and acquire social skills. While extensive research within Human-Robot Interaction has focused on teleoperation interfaces for robots in general, little work has been done on teleoperation interface design for robots in the context of therapy for children with autism. Moreover, while clinical research has shown the positive impact robots can have on children with autism, much of that research has been performed in a controlled environment, with little understanding of the way these robots are used in practice. We analyze archival data of therapists teleoperating robots as part of their regular therapy sessions, to (1) determine common themes and difficulties in therapists' use of teleoperation interfaces, and (2) provide design recommendations to improve therapists' overall experience. We believe that following these recommendations will help maximize the effectiveness of therapy for children with autism when using Socially Assistive Robotics and the scale at which robots can be deployed in this domain. 
id998#Insect Inspired Self-Righting for Fixed-Wing Drones#Micro Aerial Vehicles (MAVs) are being used in a wide range of applications such as surveillance, reconnaissance, inspection, and search and rescue. However, due to their size and mission profiles, they are prone to tipping over, jeopardizing their operation. Self-righting is an open challenge for fixed-wing drones since existing research focuses on terrestrial and multicopter flying robots with solutions that increase drag and structural weight. Until now, solutions for winged drones remained largely unexplored. Inspired by beetles, we propose a robust and elegant solution where we retrofit a fixed-wing drone with a set of additional wings akin to beetles shell structured wings called elytra. We show that artificial elytra provide additional lift during flight to mitigate their structural weight while also being able to self-right the MAV when it has been flipped over. We performed simulations along with dynamic and aerodynamic experiments to validate our results. 
id999#Motivational engine and long-term memory coupling within a cognitive architecture for lifelong open-ended learning#This paper considers a cognitive architecture that revolves around a network memory based Long-Term Memory and how it can lead to a working lifelong learning system that can deal with open-ended learning. It focuses on the mutual interaction between the Motivational Engine and the Long-Term Memory and, in particular, on autonomously producing high-level utility representations in order to allow for development. Thus, the main point is to study how this architecture allows to start from primitive policies and models operating over continuous and large state/action spaces and progressively move towards higher level structures defined over smaller and discrete state/action spaces. This progression is demonstrated in a series of experiments carried out on a real robotic setup that involves different contexts, both in terms of domains (worlds) and tasks (goals). 
id1000#The contrivance of prism rule-based algorithm using ADLs dataset in context database design#The concept of context-awareness in the database is the contemporary issue which is under discussion and so much of work is in progress, but yet to achieve much progress in context reasoning and behavior adaptability. This research work mainly focuses on context data reasoning using Prism rule-based algorithm in which rules are been generated with the aid of ADLs dataset and context database design with the support of CEAR diagram, Context-aware use case diagram, context dimension tree. Queries are been executed in RDBMS environment by keeping context-awareness scenario in mind and tuple calculus queries are implemented. This research work briefs about query visualization where attribute value block has been introduced here. We have used ADLs dataset of Ordenza et. al [5] which was been captured by different sensors and devices within the living room with two users for 35 days. Our novel contribution is enactment of PRISM rule-based algorithm using ADLs data and instigated attribute value block in the query visualization part. This research work throughout perpetually portrays stratagem of prism rule-based algorithm of context database in ubiquitous computing environment designed for a specific application ADLs. 
id1001#Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications#The generative adversarial network (GAN) framework has emerged as a powerful tool for various image and video synthesis tasks, allowing the synthesis of visual content in an unconditional or input-conditional manner. It has enabled the generation of high-resolution photorealistic images and videos, a task that was challenging or impossible with prior methods. It has also led to the creation of many new applications in content creation. In this article, we provide an overview of GANs with a special focus on algorithms and applications for visual synthesis. We cover several important techniques to stabilize GAN training, which has a reputation for being notoriously difficult. We also discuss its applications to image translation, image processing, video synthesis, and neural rendering. 
id1002#Mathematical model and algorithm design of PDCA cyclic block control based on digital sensor#In this paper we introduce the PDCA theory into the computer communication, realize the computer cyclic block control, and get the overall arrangement network of computer sensor. In the first part we introduce the application of PDCA in computer, the second part we establish of the mathematical model of computer block cyclic control theory. In order to verify the availability of mathematical model, we designed the computer control experiment of digital sensor. After the temperature calibration of DHT11 sensor in temperature check box, the calibration data is stored in the OTP memory in the form of a program. And we use Keilu Vision2 compiler to realize the LED display of detected temperature. It provides the technical reference for the application of computer control technology. 
id1003#Data cryptography in the Internet of Things using the artificial bee colony algorithm in a smart irrigation system#The Internet of Things (IoT) includes various technologies, including sensing devices, Radio-Frequency Identification (RFID), and Microelectromechanical Systems (MEMS). Despite numerous advantages of IoT, security and privacy are important challenges. IoT infrastructures are frequently attacked by different invaders, including white hat hackers whose mission is to test the system's penetrability. Other attacks are orchestrated by adversaries that misuse system vulnerabilities to seize information for personal benefits. Hence, security is a key factor and fundamental requirement of IoT design. Thus, increased cyberattacks call for an appropriate strategic plan to ensure IoT security. Enhancing data security in IoT has proved to be a major concern, and one solution to mitigate this is to apply suitable encryption techniques when storing data in the IoT. An intruder will be able to control IoT devices without physical access if the network is not secure enough. To overcome this challenge, this paper proposes a security design based on Elliptic-Curve Cryptography (ECC), the SHA-256 (Secure Hash Algorithm 256) algorithm, and the Artificial Bee Colony (ABC) algorithm to boost the security of IoT-based smart irrigation systems. The proposed model applies the ABC algorithm to generate the private key for ECC. The results show that the optimal encoding and decoding times were 100 and 150 iterations, respectively. Moreover, compared to 3DES&ECC&SHA-256 and RC4&ECC&SHA-256, the total throughput of the proposed model was about 50.04% and 55.29% higher in encryption and 51.36% and 58.41% higher in decryption. The evaluation indicates a significant improvement (>50%) in the throughput rate. The performance results obtained indicate the efficiency and effectiveness of the proposed scheme in terms of performance and security. 
id1004#An efficient anonymous authentication and confidentiality preservation schemes for secure communications in wireless body area networks#Wireless body area network (WBAN) is utilized in various healthcare applications due to its ability to provide suitable medical services by exchanging the biological data between the patient and doctor through a network of implantable or wearable medical sensors connected in the patients’ body. The collected data are communicated to the medical personals through open wireless channels. Nevertheless, due to the open wireless nature of communication channels, WBAN is susceptible to security attacks by malicious users. For that reason, secure anonymous authentication and confidentiality preservation schemes are essential in WBAN. Authentication and confidentiality play a significant role while transfers, medical images securely across the network. Since medical images contain highly sensitive information, those images should be transferred securely from the patients to the doctor and vice versa. The proposed anonymous authentication technique helps to ensure the legitimacy of the patient and doctors without disclosing their privacy. Even though various cryptographic encryption techniques such as AES and DES are available to provide confidentiality, the key size and the key sharing are the main problems to provide a worthy level of security. Hence, an efficient affine cipher-based encryption technique is proposed in this paper to offer a high level of confidentiality with smaller key size compared to existing encryption techniques. The security strength of the proposed work against various harmful security attacks is proven in security analysis section to ensure that it provides better security. The storage cost, communication cost and computational cost of the proposed scheme are demonstrated in the performance analysis section elaborately. In connection to this, the computational complexity of the proposed scheme is reduced around 29% compared to the existing scheme. 
id1005#Design and optimization of spatial vector data storage model based on HBase [基于HBase的空间矢量数据存储模型设计与优化]#Data storage model is an important part of database model. This paper aims at bottleneck problems in developing relational spatial database and using HBase to manage spatial vector data. Based on the analysis of relational spatial database storage model, the rules fortransforming the relational database storage mode into HBase storage mode are applied to spatial vector data management, a method of transforming spatial vector data relational storage schema into HBase storage schema is proposed, and a HBase storage model is designed for spatial vector data. The model is optimized and improved by using HBase's features such as entity nesting, denormalization and schemalessness.The experimental results show that the query efficiency of the storage model designed in this paper is better than that of the HBase-based spatial vector data storage model in the absence of auxiliary indexes. 
id1006#Integrating computer vision and traffic modeling for near-real-time signal timing optimization of multiple intersections#Adaptive signal timing optimizations can improve the efficiency of road networks and reduce the emissions of pollutants, but most of the current studies still rely on simplified analytical methods to depict complex road transport systems and focus on optimizing traffic signals at an isolated intersection. A framework that integrates computer vision and traffic modeling is proposed to link the real-world transport systems and operable virtual traffic models for the signal timing optimization at multiple intersections. The integrative framework consists of six main steps, including configuring real-time video sources, conducting transfer-learning to develop the vehicle detector, comparing and selecting vehicle trackers, collecting traffic parameters by referring to the CV-TM ontology, establishing and running the traffic model, and operating simulation-based optimizations. The proposed integrative framework is demonstrated through a case study of the signal timing optimization at multi-intersections in a real-world road network. Three critical information items including the traffic volumes, vehicle compositions, and vehicles’ turning ratios are derived from real-time surveillance videos, and the extracted information is then automatically incorporated into TM to optimize the signal timings of interconnected intersections in a near-real-time manner. In comparison with the original signal scheme, the optimized one can reduce 14.2 % of average vehicle delays, 18.9 % of vehicle stops, 9.1 % of average travel time, and 2.3 % of pollutant emissions in this specific case. The results indicate that synchronously optimizing signal timings at multiple intersections increase not only the transportation efficiency but also the environmental friendliness of road transport systems. The proposed CV-TM integration framework is demonstrated to be a promising way for conducting near-real-time signal timing optimizations in intricate traffic scenes instead of at isolated intersections, helping decision-makers to promptly respond to the time-varying traffic conditions during various real-world events, and facilitating the transportation systems and cities to achieve sustainable development goals. 
id1007#From keywords to relational database content: A semantic mapping method#Keyword-based query specification to extract data from structured databases has attracted considerable attention from various researchers, and many interesting proposals may be found in the scientific literature. However, many of these studies focus on finding a set of interconnected tuples containing all or some of the query keywords. The architecture introduced by this paper covers from the selection of databases on the Web to ranked relevant results. The approach also includes important aspects such as the proximity between keywords, query segmentation, and the use of aggregate functions, among others. The empirical evaluation analyzes the relevance of results and proves competitive as regards related studies. 
id1008#HR-SQL: Extending SQL with hypothetical reasoning and improved recursion for current database systems#In this work we present a formalization, backed with a contrasted implementation, of a relational database language (called HR-SQL) that extends SQL in two aspects. On the one hand, including non-linear and mutual recursion. On the other hand, including hypothetical relations and queries. Regarding expressiveness, HR-SQL allows a novel form of hypothetical reasoning, completely integrated with recursive definitions. In addition, aggregate functions are added. Regarding formalization, the extended language is founded on a stratified fixpoint semantics based on logic programming techniques. We include results of the existence of such fixpoints. An algorithm that transforms a database containing hypothetical definitions into an equivalent one without hypothesis and its correctness proof are presented. Regarding the implementation, we introduce here a system that incorporates the aforementioned benefits and enhances former implementations in several areas. The current HR-SQL system is targeted to several state-of-the-art relational database systems, and could be used with any SQL-based system with minor modifications in the implementation. 
id1009#Quantum teleportation for control of dynamical systems and autonomy#The application of quantum teleportation for control of classical dynamic systems and autonomy is proposed in this article. Quantum teleportation is an intrinsically quantum phenomenon, which was first introduced by teleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen channels in 1993. In this article, we consider the possibility of applying this quantum technique to autonomous mobile classical platforms for control and autonomy purposes for the first time in this research. First, a review of how quantum entanglement and quantum cryptography can be integrated into macroscopic mechanical systems for controls and how autonomy applications are presented, as well as how quantum teleportation concepts may be applied to the classical domain. In quantum teleportation, an entangled pair of photons which are correlated in their polarizations are generated and sent to two autonomous platforms, which we call the Alice Robot and the Bob Robot. Alice has been given a quantum system, i.e., a photon, prepared in an unknown state, in addition to receiving an entangled photon. Alice measures the state of her entangled photon and her unknown state jointly and sends the information through a classical channel to Bob. Although Alice's original unknown state is collapsed in the process of measuring the state of the entangled photon (due to the quantum non-cloning phenomenon), Bob can construct an accurate replica of Alice's state by applying a unitary operator. This article and the previous investigations of the applications of hybrid classical-quantum capabilities in control of dynamical systems are aimed to promote the adoption of quantum capabilities and its advantages to the classical domain particularly for autonomy and control of autonomous classical systems. 
id1010#An approximate method for filtering out data dependencies with a sufficiently large distance between memory references#In this paper we present an approximate algorithm for detecting and filtering data dependencies with a sufficiently large distance between memory references. A sequence of the same operations (typically enclosed in a 'for' loop) can be replaced with a single SIMD operation if the distance between memory references is greater than or equal to the number of data processed in the SIMD register. Some loops that could not be vectorized on traditional vector processors, can still be parallelized for short SIMD execution. There are a number of approximate data-dependence tests that have been proposed in the literature but in all of them data dependency will be assumed when actually there is no such a dependence that could restrict parallelization related to the short SIMD execution model. By examining the properties of linear subscript expressions of possibly conflicting data references, our algorithm gives the green light to the parallelization process if some sufficient conditions regarding the dependence distance are met. Our method is based on the Banerjee test and checks the minimum and maximum distances between memory references within the iteration space rather than searching for the existence of an integer solution to the dependence equation. The proposed method extends the accuracy and applicability of the classical Banerjee test. 
id1011#On the effects of pruning on evolved neural controllers for soft robots#Artificial neural networks (ANNs) are commonly used for controlling robotic agents. For robots with many sensors and actuators, ANNs can be very complex, with many neurons and connections. Removal of neurons or connections, i.e., pruning, may be desirable because (a) it reduces the complexity of the ANN, making its operation more energy efficient, and (b) it might improve the generalization ability of the ANN. Whether these goals can actually be achieved in practice is however still not well known. On the other hand, it is widely recognized that pruning in biological neural networks plays a fundamental role in the development of brains and their ability to learn. In this work, we consider the case of Voxel-based Soft Robots, a kind of robots where sensors and actuators are distributed over the body and that can be controlled with ANNs optimized by means of neuroevolution. We experimentally characterize the effect of different forms of pruning on the effectiveness of neuroevolution, also in terms of generalization ability of the evolved ANNs. We find that, with some forms of pruning, a large portion of the connections can be pruned without strongly affecting robot capabilities. We also observe sporadic improvements in generalization ability. 
id1012#A multiple camera position approach for accurate displacement measurement using computer vision#Engineers can today capture high-resolution video recordings of bridge movements during routine visual inspections using modern smartphones and compile a historical archive over time. However, the recordings are likely to be from cameras of different makes, placed at varying positions. Previous studies have not explored whether such recordings can support monitoring of bridge condition. This is the focus of this study. It evaluates the feasibility of an imaging approach for condition assessment that is independent of the camera positions used for individual recordings. The proposed approach relies on the premise that spatial relationships between multiple structural features remain the same even when images of the structure are taken from different angles or camera positions. It employs coordinate transformation techniques, which use the identified features, to compute structural displacements from images. The proposed approach is applied to a laboratory beam, subject to static loading under various damage scenarios and recorded using multiple cameras in a range of positions. Results show that the response computed from the recordings are accurate, with 5% discrepancy in computed displacements relative to the mean. The approach is also demonstrated on a full-scale pedestrian suspension bridge. Vertical bridge movements, induced by forced excitations, are collected with two smartphones and an action camera. Analysis of the images shows that the measurement discrepancy in computed displacements is 6%. 
id1014#Unsupervised azimuth estimation of solar arrays in low-resolution satellite imagery through semantic segmentation and Hough transform#This paper explains the use of a convolutional neural network (CNN) to segment solar panels in a satellite image containing solar arrays, and extract associated metadata from the arrays. A novel unsupervised technique is introduced to estimate the azimuth of each individual solar panel from the predicted mask of the convolutional neural network. This pipeline was developed with the aim of extracting necessary metadata for a solar installation, using only a set of latitude–longitude coordinates. Azimuth prediction results for 669 individual solar installations associated with 387 sites located across the United States are provided. A mean average error and median average error of 21.65 degrees and 1.0 degrees were obtained, respectively, when predicting the azimuth of the solar fleet data set, with about 80% of the results within an error of zero degrees of the ground truth azimuth value and about 85% within an error of 25 degrees. The predicted azimuth was then used to estimate the energy conversion of the solar arrays. Results show a 90.9 and 90.6 R-squared value for estimating alternating current (AC) and direct current (DC) energy, respectively, and a mean absolute percentage error (MAPE) of 1.70% in estimating the alternating current (AC) energy using the fully automated algorithm. 
id1015#PVStereo: Pyramid Voting Module for End-to-End Self-Supervised Stereo Matching#Supervised learning with deep convolutional neural networks (DCNNs) has seen huge adoption in stereo matching. However, the acquisition of large-scale datasets with well-labeled ground truth is cumbersome and labor-intensive, making supervised learning-based approaches often hard to implement in practice. To overcome this drawback, we propose a robust and effective self-supervised stereo matching approach, consisting of a pyramid voting module (PVM) and a novel DCNN architecture, referred to as OptStereo. Specifically, our OptStereo first builds multi-scale cost volumes, and then adopts a recurrent unit to iteratively update disparity estimations at high resolution; while our PVM can generate reliable semi-dense disparity images, which can be employed to supervise OptStereo training. Furthermore, we publish the HKUST-Drive dataset, a large-scale synthetic stereo dataset, collected under different illumination and weather conditions for research purposes. Extensive experimental results demonstrate the effectiveness and efficiency of our self-supervised stereo matching approach on the KITTI Stereo benchmarks and our HKUST-Drive dataset. PVStereo, our best-performing implementation, greatly outperforms all other state-of-the-art self-supervised stereo matching approaches. Our project page is available at sites.google.com/view/pvstereo. 
id1016#Evolutionary algorithms-assisted construction of cryptographic boolean functions#In the last few decades, evolutionary algorithms were successfully applied numerous times for creating Boolean functions with good cryptographic properties. Still, the applicability of such approaches was always limited as the cryptographic community knows how to construct suitable Boolean functions with deterministic algebraic constructions. Thus, evolutionary results so far helped to increase the confidence that evolutionary techniques have a role in cryptography, but at the same time, the results themselves were seldom used. This paper considers a novel problem using evolutionary algorithms to improve Boolean functions obtained through algebraic constructions. To this end, we consider a recent generalization of Hidden Weight Boolean Function construction, and we show that evolutionary algorithms can significantly improve the cryptographic properties of the functions. Our results show that the genetic algorithm performs by far the best of all the considered algorithms and improves the nonlinearity property in all Boolean function sizes. As there are no known algebraic techniques to reach the same goal, we consider this application a step forward in accepting evolutionary algorithms as a powerful tool in the cryptography domain. 
id1017#Memory-equipped quantum architectures: The power of random access#"Resonant cavities can be used to extend conventional superconducting transmon-based quantum architectures by adding a few bits ofquantum memory to each transmon. Such architectures leveragethe long coherence times of cavities creating a ""memory-equipped""quantum architecture (MEQC) extending the amount of quantumstate a machine can manipulate. However, random access to datawill have the greatest effect on improving machine performance.Existing transmon architectures are locally connected and performing gates between distant qubits requires expensive pairwise swapsfor execution. Added swap operations increase the probability oferrors by increasing both operation count and execution time.We develop a complete compilation framework with heuristicsto optimize for the load-store execution model of MEQC. We reduce the gate count and depth of compiled quantum programs byan average 1.62x and 1.70x, respectively compared to traditionaltransmon architectures. Based on small noise simulations, MEQCarchitectures outperform on programs as small as 10 qubits, and ingeneral the probability of no gate errors, dominant in NISQ era, isgreater on MEQC. If idle errors become more significant, MEQCwill have a greater advantage.We conclude with an exploration of different architectural choices,such as transmon-transmon connectivity and cavity size, and explore their effect on the performance of the proposed architecture.While we expect due to small initial physical experiments that wehave O(10) modes per cavity, the particular choice of cavity sizein this 2.5D architecture is an important one. For example, whencoherence times are high and we can withstand greater serialization it becomes more advantageous to favor larger cavity sizes. Inthe early stages of these devices, we expect transmon-transmoninteractions to be potentially more expensive than transmon-cavityinteractions. Our proposed solution can tolerate potentially up to12x worse interconnect error. "
id1018#Adaptive robotic manufacturing using higher order knowledge systems#Despite a well-understood potential to increase productivity of the global construction industry and sustained, international research efforts in recent years, wide-scale adoption of robotic technology currently remains elusive in the industry. As part of a larger industrial research effort to increase the efficiency of automation technologies within construction, this paper proposes a novel multi-layered knowledge encapsulation model to enable low-cost development of highly diverse robotic control applications within a parametric manufacturing paradigm. The effectiveness of proposed theoretical framework has been validated by developing multiple industrial applications and resulted in almost 40% reduction in development time. 
id1019#Interaction trees: Representing recursive and impure programs in Coq#"Interaction trees (ITrees) are a general-purpose data structure for representing the behaviors of recursive programs that interact with their environments. A coinductive variant of ""free monads,"" ITrees are built out of uninterpreted events and their continuations. They support compositional construction of interpreters from event handlers, which give meaning to events by defining their semantics as monadic actions. ITrees are expressive enough to represent impure and potentially nonterminating, mutually recursive computations, while admitting a rich equational theory of equivalence up to weak bisimulation. In contrast to other approaches such as relationally specified operational semantics, ITrees are executable via code extraction, making them suitable for debugging, testing, and implementing software artifacts that are amenable to formal verification. We have implemented ITrees and their associated theory as a Coq library, mechanizing classic domain- and category-theoretic results about program semantics, iteration, monadic structures, and equational reasoning. Although the internals of the library rely heavily on coinductive proofs, the interface hides these details so that clients can use and reason about ITrees without explicit use of Coq's coinduction tactics. To showcase the utility of our theory, we prove the termination-sensitive correctness of a compiler from a simple imperative source language to an assembly-like target whose meanings are given in an ITree-based denotational semantics. Unlike previous results using operational techniques, our bisimulation proof follows straightforwardly by structural induction and elementary rewriting via an equational theory of combinators for control-flow graphs. "
id1020#Effects of wearable ankle robotics for stair and over-ground training on sub-acute stroke: a randomized controlled trial#Background: Wearable ankle robotics could potentially facilitate intensive repetitive task-specific gait training on stair environment for stroke rehabilitation. A lightweight (0.5 kg) and portable exoskeleton ankle robot was designed to facilitate over-ground and stair training either providing active assistance to move paretic ankle augmenting residual motor function (power-assisted ankle robot, PAAR), or passively support dropped foot by lock/release ankle joint for foot clearance in swing phase (swing-controlled ankle robot, SCAR). In this two-center randomized controlled trial, we hypothesized that conventional training integrated with robot-assisted gait training using either PAAR or SCAR in stair environment are more effective to enhance gait recovery and promote independency in early stroke, than conventional training alone. Methods: Sub-acute stroke survivors (within 2 months after stroke onset) received conventional training integrated with 20-session robot-assisted training (at least twice weekly, 30-min per session) on over-ground and stair environments, wearing PAAR (n = 14) or SCAR (n = 16), as compared to control group receiving conventional training only (CT, n = 17). Clinical assessments were performed before and after the 20-session intervention, including functional ambulatory category as primary outcome measure, along with Berg balance scale and timed 10-m walk test. Results: After the 20-session interventions, all three groups showed statistically significant and clinically meaningful within-group functional improvement in all outcome measures (p < 0.005). Between-group comparison showed SCAR had greater improvement in functional ambulatory category (mean difference + 0.6, medium effect size 0.610) with more than 56% independent walkers after training, as compared to only 29% for CT. Analysis of covariance results showed PAAR had greater improvement in walking speed than SCAR (mean difference + 0.15 m/s, large effect size 0.752), which was in line with the higher cadence and speed when wearing the robot during the 20-session robot-assisted training over-ground and on stairs. Conclusions: Robot-assisted stair training would lead to greater functional improvement in gait independency and walking speed than conventional training in usual care. The active powered ankle assistance might facilitate users to walk more and faster with their paretic leg during stair and over-ground walking. Trial registration: ClinicalTrials.gov NCT03184259. Registered on 12 June 2017. 
id1021#Estimation of Driver Vigilance Status Using Real-Time Facial Expression and Deep Learning#Drowsiness is responsible for many fatal accidents on highways. Accuracy and performance are key metrics related to many researched techniques for the detection of drivers' drowsiness. To improve these metrics, a new driver's vigilance detection system based on deep learning is proposed based on facial region diagnosis using the Haar-cascade method and convolutional neural network for drowsiness detection. Evaluation analysis of the proposed system on the University of Texas at Arlington-Real-Life Drowsiness Dataset (UTA-RLDD) dataset with stratified five-fold cross-validation showed a high accuracy of 96.8% at a speed of 8.4 frames per second, which is higher than most algorithms previously reported in the literature. For further investigation, a custom dataset including ten participants in different light conditions was collected. The conducted experiments showed the great potential of the proposed system for practical applications in intelligent transportation systems. 
id1022#Recognition of pedestrian trajectories and attributes with computer vision and deep learning techniques#Analyzing the walking behavior of the public is vital for revealing the need for infrastructure design in a local neighborhood, supporting human-centric urban area development. Traditional walking behavior analysis practices relying on manual on-street surveys to collect pedestrian flow data are labor-intensive and tedious. On the contrary, automated video analytics using surveillance cameras based on computer vision and deep learning techniques appears more effective in generating pedestrian flow statistics. Nevertheless, most existing methods of pedestrian tracking and attribute recognition suffer from several challenging conditions, such as inter-person occlusion and appearance variations, which leads to ambiguous identities and hence inaccurate pedestrian flow statistics. Therefore, this paper proposes a more robust methodology of pedestrian tracking and attribute recognition, facilitating the analysis of pedestrian walking behavior. Specific limitations of a current state-of-the-art method are inferred, based on which several improvement strategies are proposed: 1) incorporating high-level pedestrian attributes to enhance pedestrian tracking, 2) a similarity measure integrating multiple cues for identity matching, and 3) a probation mechanism for more robust identity matching. From our evaluation using two public benchmark datasets, the developed strategies notably enhance the robustness of pedestrian tracking against the challenging conditions mentioned above. Subsequently, the outputs of trajectories and attributes are aggregated into fine-grained pedestrian flow statistics among different pedestrian groups. Overall, our developed framework can support a more comprehensive and reliable decision-making for human-centric planning and design in different urban areas. The framework is also applicable to exploiting pedestrian movement patterns in different scenes for analyses such as urban walkability evaluation. Moreover, the developed mechanisms are generalizable to future researches as a baseline, which provides generic insights of how to fundamentally enhance pedestrian tracking. 
id1023#Modular, higher-order cardinality analysis in theory and practice#Since the mid '80s, compiler writers for functional languages (especially lazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used only once. However it has proved difficult to achieve both power and simplicity in practice. We describe a new, modular analysis for a higher-order language, which is both simple and effective, and present measurements of its use in a full-scale, state of the art optimising compiler. The analysis finds many single-entry thunks and one-shot lambdas and enables a number of program optimisations. 
id1024#Design of Real-time Database in Small Satellite Integration Testing System#Aim at reducing the degree of difficulty in data access and data migration of the real-time database, this paper presents a design method for real-time database in small satellite integration testing system based on SQL Server relational database. This real-time database adopts JSON serialization and WriteToServer method in SqlBulkCopy class to batch import the telemetry data into the database. This method realizes real time telemetry data storage and then sets up the real-time database. 
id1025#Geospatial data conflation: a formal approach based on optimization and relational databases#Geospatial data conflation is aimed at matching counterpart features from two or more data sources in order to combine and better utilize information in the data. Due to the importance of conflation in spatial analysis, different approaches to the conflation problem have been proposed ranging from simple buffer-based methods to probability and optimization based models. In this paper, I propose a formal framework for conflation that integrates two powerful tools of geospatial computation: optimization and relational databases. I discuss the connection between the relational database theory and conflation, and demonstrate how the conflation process can be formulated and carried out in standard relational databases. I also propose a set of new optimization models that can be used inside relational databases to solve the conflation problem. The optimization models are based on the minimum cost circulation problem in operations research (also known as the network flow problem), which generalizes existing optimal conflation models that are primarily based on the assignment problem. Using comparable datasets, computational experiments show that the proposed conflation method is effective and outperforms existing optimal conflation models by a large margin. Given its generality, the new method may be applicable to other data types and conflation problems. 
id1026#An accurate approach for obtaining spatiotemporal information of vehicle loads on bridges based on 3D bounding box reconstruction with computer vision#Vehicle load is an important basis for bridge design, condition assessment, and maintenance and strengthening, and its statistical laws can help us further understand the behavior of bridges under vehicle loads. Therefore, it is important to obtain accurate vehicle weight and vehicle spatiotemporal information that reflects the load time history. As of now, vehicle weight can be obtained using WIM technology, but there are still some issues in the methods of obtaining vehicle information. The coordinate transformation method is simple to operate, but sacrifices accuracy. The existence of a certain distance from the tail of vehicles to bridge decks makes the results based on dual-target detection differ from the actual. Therefore, an accurate approach for obtaining spatiotemporal information of vehicle loads on bridges based on 3D bounding box reconstruction with computer vision is proposed in this paper. To achieve this, a deep convolutional neural network (DCNN) and the You Only Look Once (YOLO) detector are used to detect vehicles and get the 2D bounding box. By establishing the relationship between 2D and 3D bounding box of the vehicle, an algorithm for 3D bounding box reconstruction of vehicles is proposed to get the sizes and position of vehicles. The spatiotemporal information of the vehicle loads is finally obtained by using multiple objects tracking (MOT). To verify the accuracy and reliability of the proposed approach, a bridge vehicle loads identification system (BVLIS) was developed and tested on a cable-stayed bridge in operation. The results show that the approach is accurate and reliable, and can be used to obtain vehicle information and provide vehicle load boundary conditions for bridge finite element modeling. 
id1027#Task-specific training for improving propulsion symmetry and gait speed in people in the chronic phase after stroke: a proof-of-concept study#Background: After stroke, some individuals have latent, propulsive capacity of the paretic leg, that can be elicited during task-specific gait training. The aim of this proof-of-concept study was to investigate the effect of five-week robotic gait training for improving propulsion symmetry by increasing paretic propulsion in chronic stroke survivors. Methods: Twenty-nine individuals with chronic stroke and impaired paretic propulsion (≥ 8% difference in paretic vs. non-paretic propulsive impulse) were enrolled. Participants received ten 60-min sessions of individual robotic gait training targeting paretic propulsion (five weeks, twice a week), complemented with home exercises (15 min/day) focusing on increasing strength and practicing learned strategies in daily life. Propulsion measures, gait kinematics and kinetics, self-selected gait speed, performance of functional gait tasks, and daily-life mobility and physical activity were assessed five weeks (T0) and one week (T1) before the start of intervention, and one week (T2) and five weeks (T3) after the intervention period. Results: Between T0 and T1, no significant differences in outcomes were observed, except for a marginal increase in gait speed (+ 2.9%). Following the intervention, propulsion symmetry (+ 7.9%) and paretic propulsive impulse had significantly improved (+ 8.1%), whereas non-paretic propulsive impulse remained unchanged. Larger gains in propulsion symmetry were associated with more asymmetrical propulsion at T0. In addition, following the intervention significantly greater paretic trailing limb angles (+ 6.6%) and ankle plantarflexion moments (+ 7.1%) were observed. Furthermore, gait speed (+ 7.2%), 6-Minute Walk Test (+ 6.4%), Functional Gait Assessment (+ 6.5%), and daily-life walking intensity (+ 6.9%) had increased following the intervention. At five-week follow-up (T3), gains in all outcomes were retained, and gait speed had further increased (+ 3.6%). Conclusions: The post-intervention gain in paretic propulsion did not only translate into improved propulsion symmetry and gait speed, but also pertained to performance of functional gait tasks and daily-life walking activity levels. These findings suggest that well-selected chronic stroke survivors may benefit from task-specific targeted training to utilize the residual propulsive capacity of the paretic leg. Future research is recommended to establish simple baseline measures for identification of individuals who may benefit from such training and confirm benefits of the used training concepts in a randomized controlled trial. Trial registration: Registry number ClinicalTrials.gov (www.clinicaltrials.gov): NCT04650802, retrospectively registered 3 December 2020. 
id1028#Counteracting dynamical degradation of a class of digital chaotic systems via Unscented Kalman Filter and perturbation#Theoretically, any chaotic system or chaotic map has ideal complex dynamics. However, because of the finite precision of simulation software and digital devices during implementation, chaotic systems often undergo dynamical degradation, which hinders the further application of digital chaotic systems in many fields. Therefore in this paper, the method based on the perturbation and Unscented Kalman Filter (UKF) theory is designed to counteract the dynamical degradation of digital chaotic systems. Specifically, the UKF algorithm is employed to reinstate the original dynamic performance of the chaotic system, and then perturbation feedback technology is used to cause the chaotic system to obtain strong dynamic performance to resist attacks. The experimental and simulation results demonstrate that this method has good effect on improving the dynamic degradation of digital chaotic map. In addition, the corresponding pseudorandom number generator (PRNG) is constructed via this method, and its randomness is evaluated using the National Institute of Standards and Technology (NIST) SP800-22 and TestU01 test suites. By comparing with other schemes, it can be seen that this PRNG has better performance which illustrates the proposed scheme can be applied in the chaos-based cryptography and utilized in other potential applications. 
id1029#A low cost, short range quantum key distribution system#We present a miniaturized quantum key distribution system, designed to augment the more mature quantum key distribution systems currently commercially available. Our device is designed for the consumer market, and so size, weight and power are more important than raw performance. To achieve our form factor, the transmitter is handheld and the receiver is a larger fixed terminal. We envisage users would bring their transmitters to centrally located receivers and exchange keys which they could use at a later point. Transmitting qubits at 80 MHz, the peak key rate is in excess of 20 kbps. The transmitter device fits within an envelope of <150 ml, weighs 65 g and consumes 3.15 W of power. 
id1030#Review of electromagnetic waves-based distance measurement technologies for remote monitoring of civil engineering structures#Measuring distance is essential for health monitoring and condition assessment of civil engineering structures. This paper reviews recent advances in remote sensing technologies for measuring distance based on electromagnetic waves. Specifically, four families of technologies are reviewed, which are the Global Navigation Satellite Systems, microwave radars, laser-based methods, and vision-based methods. The reviewed content covers the measurement principles, signal processing methods, state-of-the-art applications, and key performance metrics. The investigated performance includes the measurement accuracy, sampling frequency, operating distance, robustness to the environment, and compatibility with autonomous platforms. Existing inconsistent viewpoints concerning the performance are discussed. Based on the features of different technologies, a decision tree is presented to facilitate selection of appropriate methods for intended applications. This research is expected to promote development and applications of remote sensing technologies for facilitating condition assessment of engineering structures. 
id1031#Automatic Identification of Idling Reasons in Excavation Operations Based on Excavator-Truck Relationships#Excavators and trucks are important equipment for earthwork operations, which make major contributions to construction productivity. To control the work efficiency and productivity of earthwork equipment, computer vision (CV) methods have been proposed to monitor equipment operations from site surveillance videos. Existing methods can recognize equipment activities to estimate the working and idling times. Idling time is an important factor that influences equipment productivity; however, the causes of equipment idling have not been considered in previous CV methods. Therefore, this research proposes a method to identify the main causes of excavator and truck idling by analyzing their interactive operations. First, the activities of the excavators and trucks are identified using convolutional neural networks. Then, work groups of excavators and trucks are clustered. Finally, the relationships between each excavator and the surrounding trucks are analyzed to identify the potential reason for idling. The proposed method was validated with videos from several construction sites, and the results were promising. 
id1032#Automated generation of positive and negative tests for parsers#In this paper we describe a specification-based approach to automated generation of both positive and negative test sets for parsers. We propose coverage criteria definitions for such test sets and algorithms for generation of the test sets with respect to proposed coverage criteria. We also present practical results of the technique application to testing syntax analyzers of several languages including C and Java. 
id1033#Phase-based displacement measurement on a straight edge using an optimal complex Gabor filter#In recent years, a number of phase-based motion processing techniques have been developed, and these allow for motion signals to be extracted from the local phase. The phase is generated by applying a pair of even- and odd-symmetric filters, and it is more robust to the intensity change. However, the phase can be unstable in certain areas where it is not linearly correlated with motion. This paper proposes an accurate and robust displacement measurement technique using an optimal complex Gabor filter and phase-based optical flow (POF) to measure the vibration response of a straight edge with the generated linear phase. The causes of nonlinear phase are discussed, and the nonlinearity of phase is used as an indicator of the measurement accuracy. Three types of nonlinear phase caused by noise background and inappropriate filter parameters for a straight edge are presented, along with solutions that optimize the parameters of the complex Gabor filter to obtain the phase with low nonlinearity. A numerical experiment is conducted on artificially created patterns to demonstrate the correlation between the proposed phase nonlinearity and the measurement accuracy. The validation experiments are carried out on a 5-story structure using a Laser Doppler Vibrometer (LDV) and a camera to demonstrate the accuracy and robustness of the proposed technique using the phase with low nonlinearity. The results show that the proposed technique can measure the vibration at 0.01-pixel scale. In addition, the robustness of the proposed technique is validated by performing experiments under different illumination conditions. The vibration responses of the 5-story structure with noisy background are measured using both accelerometers and a camera to perform the output-only modal identification. The results confirm that the proposed method can accurately identify modal frequencies, damping ratios, and high-resolution operational deflection shapes (ODS) requiring no pure background. 
id1034#Free-Me: A static analysis for automatic individual object reclamation#Garbage collection has proven benefits, including fewer memory-related errors and reduced programmer effort. Garbage collection, however, trades space for time. It reclaims memory only when it is invoked: invoking it more frequently reclaims memory quickly, but incurs a significant cost; invoking it less frequently fills memory with dead objects. In contrast, explicit memory management provides prompt low cost reclamation, but at the expense of programmer effort. This work comes closer to the best of both worlds by adding novel compiler and runtime support for compiler inserted frees to a garbage-collected system. The compiler's free-me analysis identifies when objects become unreachable and inserts calls to free. It combines a lightweight pointer analysis with liveness information that detects when short-lived objects die. Our approach differs from stack and region allocation in two crucial ways. First, it frees objects incrementally exactly when they become unreachable, instead of based on program scope. Second, our system does not require allocation-site lifetime homogeneity, and thus frees objects on some paths and not on others. It also handles common patterns: it can free objects in loops and objects created by factory methods. We evaluate free() variations for free-list and bump-pointer allocators. Explicit freeing improves performance by promptly reclaiming objects and reducing collection load. Compared to marksweep alone, free-me cuts total time by 22% on average, collector time by 50% to 70%, and allows programs to run in 17% less memory. This combination retains the software engineering benefits of garbage collection while increasing space efficiency and improving performance, and thus is especially appealing for real-time and space constrained systems. Copyright 
id1035#DiagrammER: A Web Application to Support the Teaching-Learning Process of Database Courses Through the Creation of E-R Diagrams#This paper presents a web application to support the teaching-learning process of undergraduate database courses, which allows students to practice their knowledge on data modelling using Entity-Relationship (E-R) diagrams. The web application is oriented to teachers and students: teachers prepare examples and exercises, which can have associated E-R diagrams; on the other hand, students are able to design E-R diagrams, which they can review at any time, they also have the option of viewing and solve some of the exercises designed by the teacher. The development of the web application is explained; a comparison of similar existing E-R diagram systems is presented; and the operation of the web application is shown through the creation of an E-R diagram. The results of an instrument applied to students for the evaluation of the web application are provided. 
id1037#Teaching compiler construction using a domain specific language#Building a compiler for a domain specific language (a language designed for a specific problem domain) can engage students more than traditional compiler course projects. Most students feel that compiler courses are irrelevant because they are not likely to get a job writing compilers[2]. However, the technologies used to construct a compiler are widely applicable [2,5]. Using a domain specific language demonstrates to students the wide applicability of compiler construction techniques. This paper presents the results of using a domain specific language in an upper division compiler course. Copyright 2005 ACM.
id1038#Parametric higher-order abstract syntax for mechanized semantics#We present parametric higher-order abstract syntax (PHOAS), a new approach to formalizing the syntax of programming languages in computer proof assistants based on type theory. Like higher-order abstract syntax (HOAS), PHOAS uses the meta language's binding constructs to represent the object language's binding constructs. Unlike HOAS, PHOAS types are definable in general-purpose type theories that support traditional functional programming, like Coq's Calculus of Inductive Constructions. We walk through how Coq can be used to develop certified, executable program transformations over several statically-typed functional programming languages formalized with PHOAS; that is, each transformation has a machine-checked proof of type preservation and semantic preservation. Our examples include CPS translation and closure conversion for simply-typed lambda calculus, CPS translation for System F, and translation from a language with ML-style pattern matching to a simpler language with no variable-arity binding constructs. By avoiding thesyntactic hassle associated with first-order representation techniques, we achieve a very high degree of proof automation. Copyright 
id1039#An efficient and secure feature location approach in source code using Jacobian matrix-based clustering#In software, a feature is a functionality, which has the chief role in identifying the primary location of the source code (SC), and it is stated by means of requirements as well as accessibility to developers and also users. Prevailing feature location (FL) methods have issues in security effectiveness. This paper proposed an effective and safe FL approach in SC utilizing the Jacobian matrix-based clustering (JMC) to trounce these issues. Originally, the requirement-based approach removes the repeated test cases (TC). Subsequently, the weight-based salp swarms algorithm (Ψ-SSA) selects the significant TC attributes. Followed by which, the associations rule mining (ARM) process is implemented. The ARM encompasses the itemset, support, frequent itemset, closed frequent itemset, confidence, in addition to divergence. Subsequently, the affinity is computed by means of the blend of confidence with divergence. In this affinity computation, the MALO optimizes the weight value. Then, the exponent-based elliptic curves cryptographic encrypts the affinity value. Subsequently, the score value is computed for the encrypted affinity value centered on the entropy computation. Finally, the JMC locates the feature centered on the score value. Experimental assessment exhibits that the proposed system’s performance is better than that of the prevailing research methodologies. 
id1041#Privacy-Preserving Consensus for Multi-Agent Systems via Node Decomposition Strategy#This paper proposes two kinds of algorithms to achieve privacy-preserving consensus of multi-agent systems over undirected graphs via node decomposition mechanism and homomorphic cryptography technique. Based on the number of neighboring nodes (Ni|), every agent is decomposed into N i subagents, which are connected as a chain graph. Note that every subagent connects one and only one non-homologous subagent (generated by different agents). Information interaction between non-homologous subagents is encrypted by a homomorphic cryptography algorithm, and homologous subagents exchange information directly. In this regard, the proposed node decomposition mechanism enhances the privacy of the initial values without increasing the computational complexity of encryption. The first privacy-preserving algorithm can achieve the accurate average consensus, which means that the agreement value of every subagent is consistent with the original average consensus value. The second algorithm studies the privacy-preserving scaled consensus problem without a priori knowledge about the underlying graph. Although the final convergence values of subagents do not keep exactly the same, homologous subagents can compute the original group decision value by resorting to the product of the limit value and agent's degree. Importantly, this algorithm also guarantees the privacy of group decision value of the whole system. Besides, it is proved that the privacy of the initial value can be preserved if the agent has at least one neutral neighbor. 
id1042#Nontarget-Based Measurement of 6-DOF Structural Displacement Using Combined RGB Color and Depth Information#Structural displacement is an important physical quantity that provides essential information regarding the structural conditions and the safety of a structure. To date, a wide variety of displacement measurement methods have been developed. As a noncontact type of measurement, the computer-vision-based approach receives significant attention for its cost effectiveness, measurement convenience, and great performance. However, most existing methods are limited to one- or two-degrees-of-freedom (DOF) displacement measurement and require known dimensions of structural members or predefined target markers attached to the target structure, which limits real-world application. This article proposes a nontarget-based 6-DOF displacement measurement method using the combined RGB and depth information. The proposed method utilizes three-dimensional information of measurement points on the target structure obtained using the RGB-D camera, and suggests a coordinate transform scheme to determine the 6-DOF displacement in the physical domain. The proposed method is validated in two laboratory experiments using a shear building model and a pan-tilt with a measurement plate, in which both translational and rotational displacements are measured. 
id1043#Combining computer vision with semantic reasoning for on-site safety management in construction#Computer vision has been utilized to extract safety-related information from images with the advancement of video monitoring systems and deep learning algorithms. However, construction safety management is a knowledge-intensive task; for instance, safety managers rely on safety regulations and their prior knowledge during a jobsite safety inspection. This paper presents a conceptual framework that combines computer vision and ontology techniques to facilitate the management of safety by semantically reasoning hazards and corresponding mitigations. Specifically, computer vision is used to detect visual information from on-site photos while the safety regulatory knowledge is formally represented by ontology and semantic web rule language (SWRL) rules. Hazards and corresponding mitigations can be inferred by comparing extracted visual information from construction images with pre-defined SWRL rules. Finally, the example of falls from height is selected to validate the theoretical and technical feasibility of the developed conceptual framework. Results show that the proposed framework operates similar to the thinking model of safety managers and can facilitate on-site hazard identification and prevention by semantically reasoning hazards from images and listing corresponding mitigations. 
id1045#CSGAN: Cyclic-Synthesized Generative Adversarial Networks for image-to-image transformation#The primary motivation of image-to-image transformation is to convert an image of one domain to another domain. The Generative Adversarial Network (GAN) is the recent trend for image-to-image transformation. The existing GAN models suffer due to the lack of utilization of proper synthesization objectives. In this paper, we propose a new Cyclic-Synthesized Generative Adversarial Networks (CSGAN) for the development of expert and intelligent systems for image-to-image transformation. The proposed CSGAN uses a new objective function based on the proposed cyclic-synthesized loss between the synthesized image of one domain and cycled image of another domain. The proposed CSGAN enforces the mapping from one domain to another domain more accurately by limiting the scope of redundant transformation with the help of the cyclic-synthesized loss. The performance of the proposed CSGAN is evaluated on four benchmark image-to-image transformation datasets, including CUHK Face dataset, WHU-IIP Thermal-Visible Face Dataset, CMP Facades dataset, and NYU-Depth Dataset. The results are computed using the widely used evaluation metrics such as MSE, SSIM, PSNR, and LPIPS. The experimental results of the proposed CSGAN approach are compared with the latest state-of-the-art approaches, such as GAN, Pix2Pix, DualGAN, CycleGAN, and PS2GAN. The proposed CSGAN technique outperforms all the methods over CUHK dataset, WHU-IIP dataset, NYU-Depth dataset, and exhibits promising and comparable performance over Facades dataset in terms of both qualitative and quantitative measures. The code is available at https://github.com/KishanKancharagunta/CSGAN. 
id1046#Efficient FPGA Cost-Performance Space Exploration using Type-Driven Program Transformations#Many numerical simulation applications from the scientific, financial and machine-learning domains require large amounts of compute capacity. They can often be implemented with a streaming data-flow architecture. Field Programmable Gate Arrays (FPGA) are particularly power-efficient hardware architectures suitable for streaming data-flow applications. Although numerous programming languages and frameworks target FPGAs, expert knowledge is still required to optimise the throughput of such applications for each target FPGA device. The process of selecting which optimising transformations to apply, and where to apply them is dubbed Design Space Exploration (DSE). We contribute an elegant and efficient compiler based DSE strategy for FPGAs by merging information sourced from the compiled application's semantic structure, an accurate cost-performance model and a description of hardware resource limits for particular FPGAs. Our work leverages developments in functional programming and dependent type theory to bring performance portability to the realm of High-Level Synthesis (HLS) tools targeting FPGAs. We showcase our approach by presenting achievable speedups for three example applications. Results indicate considerable improvements in throughput of up to 58× in one example. These results are obtained by traversing a minute fraction of the total Design Space. 
id1048#End-to-end computer vision framework: An open-source platform for research and education#Computer Vision is a cross-research field with the main purpose of understanding the surrounding environment as closely as possible to human perception. The image processing systems is continuously growing and expanding into more complex systems, usually tailored to the certain needs or applications it may serve. To better serve this purpose, research on the architecture and design of such systems is also important. We present the End-to-End Computer Vision Framework, an open-source solution that aims to support researchers and teachers within the image processing vast field. The framework has incorporated Computer Vision features and Machine Learning models that researchers can use. In the continuous need to add new Computer Vision algorithms for a day-to-day research activity, our proposed framework has an advantage given by the configurable and scalar architecture. Even if the main focus of the framework is on the Computer Vision processing pipeline, the framework offers solutions to incorporate even more complex activities, such as training Machine Learning models. EECVF aims to become a useful tool for learning activities in the Computer Vision field, as it allows the learner and the teacher to handle only the topics at hand, and not the interconnection necessary for visual processing flow. 
id1049#Register allocation via graph coloring using an evolutionary algorithm#Register allocation is one of most important compiler optimization techniques. The most famous method for register allocation is graph coloring and the solution presented in this paper (RAGCES) is based on the graph coloring too; for coloring interference graph, evolutionary algorithm is used. In this method graph coloring and selecting variables for spilling is taken place at the same time. This method was tested on several graphs and results were compared with the results of the previous methods. 
id1051#A Secure Image Steganography Using Improved Lsb Technique and Vigenere Cipher Algorithm#Steganography is the practise of concealing information in another type. Many different carrier file formats may be used, but digital images are most popular. This involved with the stronger and weaker image hiding, file compression, file locking, file encryption and file decryption. The applications are having the capability for establishing the characteristics like absolute confidentiality, whereas other applications are in requirement of secrecy on a large message. The aim of this analysis is to include a description which outlines the criteria of a good steganographic algorithm and briefly discusses best techniques which are in use. The knowledge is processed electronically because of development in the area of ICT. Therefore, privacy and security has been a big concern. It may also be used to encrypt messages which are embedded in a digital host before being sent to the network, thus their existence is uncertain in nature. The mechanism of hiding and encrypting sensitive data may be extended to the copyright protection of interactive media, audio, video and images. It can also be utilized to further protect the integrity of digital records. This is the practise of concealed touch. The objective towards aim is to submit the documents and to create numerous uses, then the message to be hidden is encoded with a vegenerous chip algorithm and saved in an image with the LSB steganography and finally, the virgin chip decryption algorithm can be used to obtain a real code. 
id1052#LNGate: Powering IoT with next generation lightning micro-payments using threshold cryptography#Bitcoin has emerged as a revolutionary payment system with its decentralized ledger concept however it has significant problems such as high transaction fees and long confirmation times. Lightning Network (LN), which was introduced much later, solves most of these problems with an innovative concept called off-chain payments. With this advancement, Bitcoin has become an attractive venue to perform micro-payments which can also be adopted in many IoT applications (e.g. toll payments). Nevertheless, it is not feasible to host LN and Bitcoin on IoT devices due to the storage, memory, and processing requirements. Therefore, in this paper, we propose an efficient and secure protocol that enables an IoT device to use LN through an untrusted gateway node. The gateway hosts LN and Bitcoin nodes and can open and close LN channels, send LN payments on behalf of the IoT device. This delegation approach is powered by a (2,2)-threshold scheme that requires the IoT device and the LN gateway to jointly perform all LN operations which in turn secures both parties' funds. Specifically, we propose to thresholdize LN's Bitcoin public and private keys as well as its commitment points. With these and several other protocol level changes, IoT device is protected against revoked state broadcast, collusion, and ransom attacks. We implemented the proposed protocol by changing LN's source code and thoroughly evaluated its performance using a Raspberry Pi. Our evaluation results show that computational and communication delays associated with the protocol are negligible. To the best of our knowledge, this is the first work that implemented threshold cryptography in LN. 
id1053#Adaptively Secure Distributed PRFs from LWE#In distributed pseudorandom functions (DPRFs), a PRF secret key SK is secret shared among N servers so that each server can locally compute a partial evaluation of the PRF on some input X. A combiner that collects t partial evaluations can then reconstruct the evaluation F(SK, X) of the PRF under the initial secret key. So far, all non-interactive constructions in the standard model are based on lattice assumptions. One caveat is that they are only known to be secure in the static corruption setting, where the adversary chooses the servers to corrupt at the very beginning of the game, before any evaluation query. In this work, we construct the first fully non-interactive adaptively secure DPRF in the standard model. Our construction is proved secure under the LWE assumption against adversaries that may adaptively decide which servers they want to corrupt. We also extend our construction in order to achieve robustness against malicious adversaries. 
id1054#Overcurrent-driven LEDs for consistent image colour and brightness in agricultural machine vision applications#Machine vision systems are being utilized extensively in agriculture applications. Daytime imaging in outdoor field conditions presents challenges such as variable lighting and colour inconsistencies due to sunlight. Motion blur can occur due to vehicle movement and vibrations from ground terrain. A camera system with active lighting can be a solution to overcome these challenges. In this study, the usage of over-current driven LEDs to produce a powerful flash was investigated as a viable light source for daytime imaging. The current drawn by an LED was increased by a factor of six times its normal rating resulting in increased illuminance. A circuit was designed for storing and releasing energy to the LEDs for a strobe-like effect and a controller was used for synchronizing the strobe with a camera to acquire images. The system was deployed in an apple orchard on three days in summer of 2020. Images were taken throughout the day in both sunny and cloudy conditions of different canopy structures. There was substantial improvement in image brightness and colour consistency by using the LED flashes. Images captured by the prototype system during an 11-hour period showed an average decrease of 85% in standard deviation for the Hue-Saturation-Value (HSV) channels compared to that of the auto-exposure setting. Additionally, the prototype system was able to fix motion blur in images averaging 7 mm in error for a stereo vision application with the camera moving at 7 km/hr. These results show that the designed LED flash system can reduce the undesirable effects of lighting variability and motion blur in images stemming from outdoor field conditions. 
id1055#A novel one-dimensional chaotic map generator and its application in a new index representation-based image encryption scheme#The fast growth in digital image transmission technologies requires more secure and effective image encryption schemes to provide essential security. In this paper, we present a novel one-dimensional chaotic map amplifier (1-DCMA). The evaluation of the proposed chaotic system shows that the 1-DCMA improve the chaotic behavior, control parameters’ structure, and sensitivity of the 1-D chaotic maps used as input. We further implement a chaotic map generated by the 1-DCMA in a new asymmetric image encryption scheme (Amp-Lg-IE). Using the secret key, the proposed encryption algorithm adds rows and implement a new index representation (IR) concept with shifting sequences to manipulate the pixels’ positions and values synchronously. Finally, we execute bit-level operations to obtain the ciphered image. The simulation and security analysis prove that the Amp-Lg-IE, in a satisfying time, can encrypt a plain image into an unidentified random-like one with high resistance to different types of threats and attacks. 
id1056#Synthesis of Semantic Model of Subject Area at Integration of Relational Databases#The article considers the problem of synthesizing a semantic model of the subject area at integration of heterogeneous information resources. In this case, emphasis is placed on ensuring the universality of the means of description, without regard to artificial limitations on the data typification and categorization. Two types of logical existence rules are introduced: functional and structural ones, which allow analyzing not only explicitly defined, but also logically deducible information objects, that is, determining the boundary of the subject area. Information about all possible information objects of the subject area makes it possible to determine the area of intersection of the integrable data semantics. 
id1057#Face-Fake-Net: The Deep Learning Method for Image Face Anti-Spoofing Detection : 45#Due to the increasingly growing demand for user identification on cell phones, PCs, laptops, and so on, face anti-spoofing has risen to significance and is an active research area in academia and industry. The detection of the real face then recognize it present an important challenge regarding the techniques that can be used to spoof any recognition system like masks, printed photos. This paper we present an anti-spoofing face method to solve the real-world scenario that learns the target domain classifier based on samples used for training in a particular source domain. Specifically, with the conventional regression CNN, the Spatial/Channel-wise Attention Modules were introduced. Two modules, namely the Spatial-wise Attention Module and the Channel-wise Attention Module, were used at spatial and channel levels to improve local features and ignore the irrelevant features. Extensive experiments on current collections with benchmarks datasets verifies that the recommended solution will significantly benefit from the two modules and better generalization capability by providing significantly improved results in anti-spoofing. 
id1058#Comparison of Acceptance and Knowledge Transfer in Patient Information before an MRI Exam Administered by Humanoid Robot Versus a Tablet Computer: A Randomized Controlled Study#Purpose To investigate whether a humanoid robot in a clinical radiological setting is accepted as a source of information in conversations before MRI examinations of patients. In addition, the usability and the information transfer were compared with a tablet. Methods Patients were randomly assigned to a robot or tablet group with their consent prior to MRI. The usability of both devices was compared with the extended System Usability Scale (SUS) and the information transfer with a knowledge query. Reasons for refusal were collected by a non-responder questionnaire. Results At the University Hospital Halle 117 patients were included for participation. There was no statistically significant difference in gender and age. Of 18 non-responders, 4 refused to participate partly because of the robot; for another 3 the reason could not be clarified. The usability according to SUS score was different with statistical significance between the groups in the mean comparison and was one step higher for the tablet on the adjective scale. There was no statistically significant difference in knowledge transfer. On average, 8.41 of 9 questions were answered correctly. Conclusion This study is the first application, in a clinical radiological setting, of a humanoid robot interacting with patients. Tablet and robot are suitable for information transfer in the context of MRI. In comparison to studies in which the willingness to interact with a robot in the health care sector was investigated, the willingness is significantly higher in the present study. This could be explained by the fact that it was a concrete use case that was understandable to the participants and not a hypothetical scenario. Thus, potentially high acceptance for further specific areas of application of robots in radiology can be assumed. The higher level of usability perceived in the tablet group can be explained by the fact that here the interface represents a form of operation that has been established for years in all population groups. More frequent exposure to robots could also improve the response in the future. Key Points: patients accept humanoid robots in clinical radiologic situations at present they can only convey information as well as an inexpensive tablet future systems can relieve the burden on personnel. Citation Format Stoevesandt D, Jahn P, Watzke S et al. Comparison of Acceptance and Knowledge Transfer in Patient Information Before an MRI Exam Administered by Humanoid Robot Versus a Tablet Computer: A Randomized Controlled Study. Fortschr Röntgenstr 2021; 193: 947 - 954. 
id1059#Analysis and Recommendations for MAC and Key Lengths in Delayed Disclosure GNSS Authentication Protocols#Data and signal authentication schemes are being proposed to address Global Navigation Satellite Systems' (GNSS) vulnerability to spoofing. Due to the low power of their signals, the bandwidth available for authentication in GNSS is scarce. Since delayed-disclosure protocols, e.g., TESLA (timed-efficient stream loss-tolerant authentication), are efficient in terms of bandwidth and robust to signal impairments, they have been proposed and implemented by GNSS. The length of message authentication codes (MACs) and cryptographic keys are two crucial aspects of the protocol design as they have an impact on the utilized bandwidth, and therefore on the protocol performance. We analyze both aspects in detail for GNSS-TESLA and present recommendations for efficient yet safe MAC and key lengths. We further complement this analysis by proposing possible authentication success and failure policies and quantify the reduction of the attack surface resulting from employing them. The analysis shows that in some cases it is safe to use MAC and key sizes that are smaller than those proposed in best-practice guidelines. While some of our considerations are general to delayed-disclosure lightweight protocols for data and signal authentication, we particularize them for GNSS-TESLA protocols. 
id1060#An affordable solution for the recognition of abnormality in breast thermogram#Lack of sufficient expertise in the rural regions of the country contributes to a higher mortality rate of breast cancer. Remote breast health monitoring systems, including image acquisition devices and advanced communication technologies, have been laid out of a new lease of life by the conveyance of quality healthcare services in developing parts of the world. Despite the high mortality rate of breast cancer, very limited existing works have been explored in integrating screening techniques with machine learning approaches and real-time communication to remote areas, secondary or tertiary hospitals. This approach is necessary to develop scalable and affordable breast screening technologies for clinical prediction of breast abnormality in the remote regions of the country. In this research work, we propose an affordable and portable infrared imaging solution for remote breast health monitoring. The proposed system integrates an Infrared Image Acquisition Module (IIAM), Screening Module (SM), and Transmission Module (TM). The IIAM includes a thermal camera and associated software to acquire thermal images of the breast. SM is the combination of four submodules such as Pre-processing Module (PM), Automatic Segmentation Module (ASM), Feature Extraction Module (FEM), and Classification Module (CM). The key challenge in implementing SM is that the penetration of thermography based diagnostic approaches are impeded by the frequent misclassifications in the diagnosis of breast cancer. The main reasons for this misclassification is the poor Signal to Noise Ratio (SNR) and inefficient segmentation of breast regions in thermograms. To address these challenges, co-occurrence filter-based edge-preserved technique is adopted to design the PM. Using morphological operations and Distance Regularized Level Set Evolution (DRLSE), ASM delineates the Region of Interest (ROI). FEM extracts both statistical features, and wavelet transform based features from the segmented breast ROI’s. CM depends on the SVM classifier to predict normal and abnormal images in the compiled dataset. The TM accesses and transmits the breast thermograms, predicted results, and patient’s history to the healthcare professionals in the tertiary hospitals for further diagnosis. Detailed in-person screening and experimentation was performed on 71 patients which consisted of 34 healthy and 37 abnormal images. The performance of the proposed solution is evaluated, which demonstrated a classification accuracy of 96.46% competitive compared to state-of-the-art schemes. 
id1061#Securing Relational Database Storage with Attribute Association Aware Shuffling#With the fast-increasing prevalence of mobile devices, mobile cloud computing has emerged as an important computing paradigm. Storing databases in the cloud server has been one significant application of mobile cloud computing. However, there is one severe security problem: the protection of most database system relies on the security schemes of the operating system or the hard drive of the server. If an attacker successfully hacks into a cloud server and accesses the database files, then the attacker is able to recover the database and get information about every attribute of a tuple in the database. Encrypting the database does not fully address the problem as seen in the continued incidents of large-scale database breaches. In this paper, we propose a novel and practical database shuffling approach that breaks the original relations between the value of the attributes while keeping the ostensible integrity. It protects the database storage by storing shuffled relations in data files instead of putting shuffled relations in memory only. For semantically or statistically associated attributes, we shuffle them together as a bundle to ensure that the shuffled database appears deceptively genuine to the attacker. We show that our algorithm is compatible with the current relational database design and can be used to provide an extra layer of protection in addition to encryption. 
id1063#GeoWINE: Geolocation based Wiki, Image, News and Event Retrieval#In the context of social media, geolocation inference on news or events has become a very important task. In this paper, we present the GeoWINE (Geolocation-based Wiki-Image-News-Event retrieval) demonstrator, an effective modular system for multimodal retrieval which expects only a single image as input. The GeoWINE system consists of five modules in order to retrieve related information from various sources. The first module is a state-of-the-art model for geolocation estimation of images. The second module performs a geospatial-based query for entity retrieval using the Wikidata knowledge graph. The third module exploits four different image embedding representations, which are used to retrieve most similar entities compared to the input image. The last two modules perform news and event retrieval from EventRegistry and the Open Event Knowledge Graph (OEKG). GeoWINE provides an intuitive interface for end-users and is insightful for experts for reconfiguration to individual setups. The GeoWINE achieves promising results in entity label prediction for images on Google Landmarks dataset. The demonstrator is publicly available at http://cleopatra.ijs.si/geowine/. 
id1065#Optimal Order Pick-and-Place of Objects in Cluttered Scene by a Mobile Manipulator#In this letter, we present a fast method for autonomously planing manipulation tasks for mobile manipulators. The planner defines an optimal order to perform pick-and-place operations for taking objects from a cluttered scene to specific deposit areas considering both, manipulator and mobile base motion. Our method first examines the grasping feasibility of the objects with an inverse reachability map. Then, it defines all the placing locations for the objects and analyses the corresponding preconditions to reach them. Finally, it defines a sequence for rearranging the objects that minimize the execution time. We take advantage of the environment's underlying combinatorial structure to define the shortest path. In this work, we consider both the monotone case, where each object may be moved at most once, and the non-monotone cases. An experimental evaluation on the Human Support Robot (HSR) shows the effectiveness of our solutions and the scalability of our method as the number of objects increases for both cases. Tests on monotone problem instances with 20 objects show the proposed method can save up to 17% base traveling time when comparing to baseline methods at the cost of less than 9 seconds planning time. In a test on a simple non-monotone instance, the proposed method further reduces the total execution time by 25% by minimizing the total number of actions in a few seconds of planning time. The characteristic speed in re-planning makes our method suitable for online usage. 
id1066#pyPOCQuant — A tool to automatically quantify Point-Of-Care Tests from images#Lateral flow Point-Of-Care Tests (POCTs) are a valuable tool for rapidly detecting pathogens and the associated immune response in humans and animals. In the context of the SARS-CoV-2 pandemic, they offer rapid on-site diagnostics and can relieve centralized laboratory testing sites, thus freeing resources that can be focused on especially vulnerable groups. However, visual interpretation of the POCT test lines is subjective, error prone and only qualitative. Here we present pyPOCQuant, an open-source tool implemented in Python 3 that can robustly and reproducibly analyze POCTs from digital images and return an unbiased and quantitative measurement of the POCT test lines. 
id1067#Viscoelasticity modeling of dielectric elastomers by kelvin voigt-generalized maxwell model#Dielectric elastomers (DEs) are polymer materials consisting of a network of polymer chains connected by covalent cross-links. This type of structural feature allows DEs to generate large displacement outputs owing to the nonlinear electromechanical coupling and time-dependent viscoelastic behavior. The major challenge is to properly actuate the nonlinear soft materials in applications of robotic manipulations. To characterize the complex time-dependent viscoelasticity of the DEs, a nonlinear rheological model is proposed to describe the time-dependent viscoelastic behaviors of DEs by combining the advantages of the Kelvin–Voigt model and the generalized Maxwell model. We adopt a Monte Carlo statistical simulation method as an auxiliary method, to the best knowledge of the author which has never reportedly been used in this field, to improve the quantitative prediction ability of the generalized model. The proposed model can simultane-ously describe the DE deformation processes under step voltage and alternating voltage excitation. Comparisons between the numerical simulation results and experimental data demonstrate the effectiveness of the proposed generalized rheological model with a maximum prediction error of 3.762% and root-mean-square prediction error of 9.03%. The results presented herein can provide theoretical guidance for the design of viscoelastic DE actuators and serve as a basis for manipulation control to suppress the viscoelastic creep and increase the speed response of the dielectric elastomer actuators (DEA). 
id1068#Wearable edge ai applications for ecological environments#Ecological environments research helps to assess the impacts on forests and managing forests. The usage of novel software and hardware technologies enforces the solution of tasks related to this problem. In addition, the lack of connectivity for large data throughput raises the demand for edge-computing-based solutions towards this goal. Therefore, in this work, we evaluate the opportunity of using a Wearable edge AI concept in a forest environment. For this matter, we propose a new approach to the hardware/software co-design process. We also address the possibility of creating wearable edge AI, where the wireless personal and body area networks are platforms for building applications using edge AI. Finally, we evaluate a case study to test the possibility of performing an edge AI task in a wearable-based environment. Thus, in this work, we evaluate the system to achieve the desired task, the hardware resource and performance, and the network latency associated with each part of the process. Through this work, we validated both the design pattern review and case study. In the case study, the developed algorithms could classify diseased leaves with a circa 90% accuracy with the proposed technique in the field. This results can be reviewed in the laboratory with more modern models that reached up to 96% global accuracy. The system could also perform the desired tasks with a quality factor of 0.95, considering the usage of three devices. Finally, it detected a disease epicenter with an offset of circa 0.5 m in a 6 m × 6 m × 12 m space. These results enforce the usage of the proposed methods in the targeted environment and the proposed changes in the co-design pattern. 
id1070#Generating 3D Model for Human Body Shapes from 2D images using Deep Learning#Online shopping started to grow all over the world in the last decade, predominantly in the last year due to COVID19. As a result of the lockdown and because many people did not want to take the risk of visiting stores not to be infected, customers direct their full attention to online shopping. That affected both customers and stores' owners since customers always spend a long time trying to pick the right size and fail most of the time causing high percentage of returns which affects stores' sales. In this paper part of the solution is introduced by collecting 5 anthropometric parameters from the user and the rest is predicted using an imputation strategy. Feature selection techniques are also applied and a 3D model for the human body is presented to the user on a mobile application. 
id1071#Cooperative collapse of helical structure enables the actuation of twisting pneumatic artificial muscle#Pneumatic artificial muscles (PAMs) are the most widely employed flexible actuator in soft robots. The twisting pneumatic artificial muscles (TPAMs) are the PAMs that can generate torsional motion, which greatly enhances the flexibility and mobility of soft robots. However, the challenges remain with developing high-performance TPAMs that possess a bigger torsion angle and lower working pressure. In this study, four high-performance TPAMs are developed in combination with the collapse deformation of flexible material and the helical structure. And experimental results show that the TPAM-4C can provide a considerable torsion angle of up to 2.59 °/mm and an output torque of 18.85 N.mm when the negative pressure of 60 kPa is applied. Additionally, the finite element models (FEMs) of the TPAMs are constructed and validated experimentally. By employing the validated FEMs, the performance of the four TPAMs is studied. The surrogate models are also established to efficiently and quickly predict the performance of the TPAMs. And the effect of structure parameters on torsion performance is studied by means of the developed surrogate models and sensitivity analysis. The results indicate that the torsion angle can be improved by increasing the height and relative rotation angle, and the output torque also can be enhanced with the bigger relative rotation angle. The example of the inverse design of the TPAMs is also conducted based on the surrogate models. This work is significant for extending the researches on PAMs and providing a high-performance flexible actuator for soft robots. 
id1073#Staged translation of graph transformation rules#Graph transformation rules provide an opportunity to specify model transformations in a declarative way at a high level of abstraction. So far, compilers have translated graph transformation rules into conventional programming languages such as Java, C, or C
id1074#More views of a one-sided surface: Mechanical models and stereo vision techniques for Möbius strips#Möbius strips are prototypical examples of ribbon-like structures. Inspecting their shapes and features provides useful insights into the rich mechanics of elastic ribbons. Despite their ubiquity and ease of construction, quantitative experimental measurements of the three-dimensional shapes of Möbius strips are surprisingly non-existent in the literature. We propose two novel stereo vision-based techniques to this end - a marker-based technique that determines a Lagrangian description for the construction of a Möbius strip, and a structured light illumination technique that furnishes an Eulerian description of its shape. Our measurements enable a critical evaluation of the predictive capabilities of mechanical theories proposed to model Möbius strips. We experimentally validate, seemingly for the first time, the developable strip and the Cosserat plate theories for predicting shapes of Möbius strips. Equally significantly, we confirm unambiguous deficiencies in modelling Möbius strips as Kirchhoff rods with slender cross-sections. The experimental techniques proposed and the Cosserat plate model promise to be useful tools for investigating a general class of problems in ribbon mechanics. 
id1075#Object Detection for Autonomous Driving using YOLO algorithm.#In this paper, we have presented the work that we have done pertaining to object detection for the purpose of aiding autonomous vehicles during navigation. The detections are carried out using the state-of-the-art YOLO model which is trained on a custom-made model on a custom-made dataset. The main objective of this paper is to highlight the building of the model on low-cost computing resources. After training, we have implemented the model on video data as well as on web camera. 
id1076#An applicative control-flow graph based on Huet's zipper#We are using ML to build a compiler that does low-level optimization. To support optimizations in classic imperative style, we built a control-flow graph using mutable pointers and other mutable state in the nodes. This decision proved unfortunate: the mutable flow graph was big and complex, and it led to many bugs. We have replaced it by a smaller, simpler, applicative flow graph based on Huet's [Huet, Gérard, 1997. The Zipper. Journal of Functional Programming, 7(5):549-554. Functional Pearl] zipper. The new flow graph is a success; this paper presents its design and shows how it leads to a gratifyingly simple implementation of the dataflow framework developed by [Lerner, Sorin, David Grove, and Craig Chambers. 2002. Composing dataflow analyses and transformations. Conference Record of the 29th Annual ACM Symposium on Principles of Programming Languages, in SIGPLAN Notices, 31(1):270-282]. 
id1077#Cryptographic Framework for Role Control Remedy: A Secure Role Engineering mechanism for Single Authority Organizations#Role Engineering creates a model for enforcing security among organizations and reduces the risk of entities having unauthorized access privileges. In this paper, we realize the role engineering for the organizations where services are associated with the roles. Access to such services depends on the verification and validation of the role and role ownership. In face to face communication, verification and validation of roles and role ownership are possible through physical certificates but in a computer network, it is spacious. In some cases, it relies on the knowledge-based security systems creating a problem of confidentiality. This research proposed an unconventional Cryptographic Role Engineering (CRE) framework to ensure the privacy and role ownership issues of the entities, secure role management for the role assigning organizations and role validation and verification for the service providing organizations. Validation of the framework is performed through the case study/example scenario and feature analysis. Based on validation and achieved features, it is concluded that the proposed framework achieved the feature of a strong connection between easy administration and strong security. As a result, the role assigning organizations can have a secure role management mechanism, the role owner can provide the proof of role ownership independently and service-providing organizations can verify and validate the role without the intervention of role assigning organizations. 
id1078#On the use of feature-maps for improved quality-diversity meta-evolution#Quality-Diversity (QD) algorithms evolve a behaviourally diverse archive of high-performing solutions. In QD meta-evolution, one evolves a population of QD algorithms by modifying algorithmic components (e.g., the behaviour space) to optimise an archive-level objective, the meta-fitness. This paper investigates which feature-map is best for defining the behaviour space for an 8-joint robot arm. Meta-evolution with non-linear feature-maps yields a 15-fold meta-fitness improvement over linear feature-maps. On a damage recovery test, archives evolved with non-linear feature-maps outperform traditional MAP-Elites variants. 
id1080#Assessing Industrial Robot agility through international competitions#Manufacturing and Industrial Robotics have reached a point where to be more useful to small and medium sized manufacturers, the systems must become more agile and must be able to adapt to changes in the environment. This paper describes the process for creating and the lessons learned over multiple years of the Agile Robotics for Industrial Automation Competition (ARIAC) being run by the National Institute of Standards and Technology. 
id1081#Improved dynamic parameter identification method relying on proprioception for manipulators#Focusing on the complex nonlinear control problem of robots, this study concentrates on a dynamic modeling method to demonstrate the influence of excitation trajectory optimization and modeling accuracy of robots. The system has to be properly excited to improve the accuracy of the estimated parameters. An improved optimization method is proposed to deal with a series of influences caused by the under-constraint of a single criterion and over-constraint of multiple criteria. Proprioception perception is used for variable information to avoid the utilization of additional sensors, and parameter identification of the optimized excitation trajectory is performed based on the maximum likelihood estimation method. Simulation and experiment results show that the proposed optimization method can reasonably balance the disadvantages of single-criterion and multi-criterion optimization. A case study is also conducted to compare the proposed approach with other methods. The proposed approach can improve the accuracy of parameter estimation by at least 2.3% and reasonably reduce the drawbacks derived from single-criterion optimization. It can also effectively improve the anti-interference capability of identification parameters against noises, thus maintaining the accuracy of the estimated dynamic model. 
id1082#A chaotic framework and its application in image encryption#A novel image encryption framework is proposed in this article. A new chaotic map and a pseudorandom bit generator are proposed. Apart from this, a novel image encryption system is designed based on the proposed map and the proposed pseudorandom bit generator. These three are the major contributions of this work that makes a complete cryptosystem. The proposed new chaotic map is proposed which will be known as the ‘RCM map’ and its chaotic property is studied based on Devaney’s theory. The proposed pseudorandom bit generator is tested using the NIST test suite. The proposed method is simple to implement and does not involve any highly complex operations. Moreover, the proposed method is completely lossless, and therefore cent percent of data can be recovered from the encrypted image. The decryption process is also simple to implement i.e. just reverse of the encryption procedure. A scrambling algorithm is also proposed to further enhance the security of the overall system. The simulation, detailed analysis, and comparative studies of the proposed overall image encryption framework will help to understand the strengths and weaknesses of it. The experimental results are very promising and show the prospects of chaos theory and its usage in the field of data security. 
id1083#Speakingfaces: A large-scale multimodal dataset of voice commands with visual and thermal video streams#We present SpeakingFaces as a publicly-available large-scale multimodal dataset developed to support machine learning research in contexts that utilize a combination of thermal, visual, and audio data streams; examples include human–computer interaction, biometric authentication, recognition systems, domain transfer, and speech recognition. SpeakingFaces is comprised of aligned high-resolution thermal and visual spectra image streams of fully-framed faces synchronized with audio recordings of each subject speaking approximately 100 imperative phrases. Data were collected from 142 subjects, yielding over 13,000 instances of synchronized data (∼3.8 TB). For technical validation, we demonstrate two baseline examples. The first baseline shows classification by gender, utilizing different combinations of the three data streams in both clean and noisy environments. The second example consists of thermal-to-visual facial image translation, as an instance of domain transfer. 
id1085#Novel Postquantum MQ-Based Signature Scheme for Internet of Things with Parallel Implementation#Internet of Things (IoT) is a paradigm shifting technology that enables many innovative applications in the near future. Proactive measures are required to protect such architecture from cyber attacks. One of the most important security issues in this architecture is the authentication of edge nodes, which can be resolved through the deployment of digital signatures. However, existing standardized digital signatures are vulnerable to attacks from quantum computers, which can be unsafe in the near future. In this article, we propose a new signature scheme based on multivariate polynomials with efficient key and signature sizes, which is resistant to quantum computer attacks. The proposed scheme is also very friendly to parallel implementation, enabling efficient deployment of edge nodes authentication at high throughput. When implemented on a GPU device, the proposed scheme can generate 113 signatures/s and verify 120 signatures/s, which is 12.56× and 10.00× faster than a serial implementation in CPU. 
id1086#Triboelectric bending sensor based smart glove towards intuitive multi-dimensional human-machine interfaces#Recent advances in human-machine interface (HMI) lead to a renewed interest in creating intuitive and immersive interaction. Here, we designed a simple-structured and high-resolution bending angle triboelectric sensor named bending-angle triboelectric nanogenerator (BA-TENG) to construct a glove-based multi-dimensional HMI. With the assistance of a customized print circuit board (PCB), the glove-based HMI exhibits high sensitivity and low crosstalk in real-time multi-channel finger motion sensing. The signal-to-noise ratio (SNR) is improved by 19.36 dB. By systematically extracting and analyzing the multi-dimensional signal features of the BA-TENG, intuitive multi-dimensional HMIs were realized for smart-home, advanced robotic control, and a virtual keyboard with user recognition functionality. The classification accuracy of the virtual keyboard for seven users reached 93.1% by leveraging the advanced machine learning technique. The proposed BA-TENG-based smart glove reveals its potential as a solution for minimalist-design and intuitive multi-dimensional HMI, promising in diversified areas, including the Internet of things (IoT), assistive technology, and intelligent recognition systems. 
id1087#Research of Combining Blockchain in the Course Reform of Cryptography by Experiential Teaching#Cryptography is the core of information security. Cryptography course is the central course in the construction of information security major. However, cryptography has a lot of theoretical knowledge, making students feel abstract and boring, and there is no suitable practical application, which leads to students' unsatisfactory learning effects. As the current advanced technology, blockchain is the perfect combination of cryptography and digital technology, but it is not integrated into the cryptography teaching therefore, in order to strengthen the training of information security talents, improve the effect of cryptography, and enhance students' interest in learning cryptography, this paper proposes a combination of blockchain technology for cryptography reform by experiential teaching, and the knowledge points in cryptography courses are targeted in learning in the blockchain. Taking the hash function in cryptography as an example, the 'mining' of blockchain is experienced, and the important properties of the hash function, such as the antigenic image (unidirectional) and anti-collision, can deeply understood. At last, I hope to provide reference and new ideas for the construction of information security majors and training talent. 
id1088#Versatile Convolutional Networks Applied to Computed Tomography and Magnetic Resonance Image Segmentation#Medical image segmentation has seen positive developments in recent years but remains challenging with many practical obstacles to overcome. The applications of this task are wide-ranging in many fields of medicine, and used in several imaging modalities which usually require tailored solutions. Deep learning models have gained much attention and have been lately recognized as the most successful for automated segmentation. In this work we show the versatility of this technique by means of a single deep learning architecture capable of successfully performing segmentation on two very different types of imaging: computed tomography and magnetic resonance. The developed model is fully convolutional with an encoder-decoder structure and high-resolution pathways which can process whole three-dimensional volumes at once, and learn directly from the data to find which voxels belong to the regions of interest and localize those against the background. The model was applied to two publicly available datasets achieving equivalent results for both imaging modalities, as well as performing segmentation of different organs in different anatomic regions with comparable success. 
id1089#Secure and covert communication using steganography by Wavelet Transform#This paper presents a hybrid image steganography along with cryptography algorithm based on LS (Logical & Statistical) encryption scheme and lift wavelet domain of digital images. Currently, cryptography data can be easily intercepted by an intruder during transmission. So it may be treated as ineffectual in terms of security. In such a scenario, the proposed algorithm provides secured transmission by using cryptography and steganography techniques. In this algorithm a new LS encryption scheme is implemented for converting secret message into ciphertext. It is being done by using several parameters like 'data indicator', 'count indicator', 'gray code' and novel 'data conversion' techniques. The 'gray code' and novel 'data conversion' techniques are used to recover the inherent information at the receiver and to improve the security of the secret data. Moreover, it reduces the ciphertext length compared to the length of original secret data. Hence this scheme improves the payload capacity and helps to improve the quality metrics like Number of Pixels Change in Rate (NPCR), Peak Signal to Noise Ratio (PSNR), and Correlation (CORR) in the stego image. Furthermore, steganography scheme provides an ability to tackle the various attacks like noise, rotational, histogram and visual attacks. It is also shown that how the original message is recovered back from stego image successfully. 
id1090#Incrementalization of vertex-centric programs#As the graphs in our world become ever larger, the need for programmable, easy to use, and highly scalable graph processing has become ever greater. One such popular graph processing model-the vertex-centric computational model-does precisely this by distributing computations across the vertices of the graph being computed over. Due to this distribution of the program to the vertices of the graph, the programmer “thinks like a vertex” when writing their graph computation, with limited to no sense of shared memory and where almost all communication between each on-vertex computation must be sent over the network. Because of this inherent communication overhead in the computational model, reducing the number of messages sent while performing a given computation is a central aspect of any efforts to optimize vertex-centric programs. While previous work has focused on reducing communication overhead by directly changing communication patterns-by altering the way the graph is partitioned and distributed, or by altering the graph topology itself-in this paper we present a different optimization strategy based on a family of complementary compile-time program transformations in order to minimize communication overhead by changing both the messaging and computational structures of programs. Particularly, we present and formalize a method by which a compiler can automatically incrementalize a vertex-centric program through a series of compile-time program transformations-by modifying the on-vertex computation and messaging between vertices so that messages between vertices represent patches to be applied to the other vertex's local state. We empirically evaluate these transformations on a set of common vertex-centric algorithms and graphs and achieve an average reduction of 2.7X in total computational time, and 2.9X in the number of messages sent across all programs in the benchmark suite. Furthermore, since these are compile-time program transformations alone, other prior optimization strategies for vertex-centric programs can work with the resulting vertex-centric program just as they would a non-incrementalized program. 
id1091#Practical consequences of inertia shaping for interaction and tracking in robot control#In trajectory tracking and interaction control of robots, two fundamentally different concepts define the boundaries within which most nonlinear model-based approaches can be located. On the one hand controllers such as the PD+ preserve the natural inertia and avoid feedback of external forces and torques. On the other hand controllers based on feedback linearization, as used in most inverse dynamics approaches, enforce linear closed-loop dynamics by means of external force/torque feedback. Here, these two basic concepts of keeping and shaping of the natural inertia are investigated and compared including aspects such as interaction behavior, tracking performance, tuning parameters, influence of modeling errors, and effective feedback gains. Exemplary case studies on a standard torque-controlled robot are performed. The understanding of these features and differences is of major importance for the proper selection and deployment of interaction and tracking controllers in practice. 
id1092#Soft-Tentacle Gripper for Pipe Crawling to Inspect Industrial Facilities Using UAVs#This paper presents a crawling mechanism using a soft-tentacle gripper integrated into an unmanned aerial vehicle for pipe inspection in industrial environments. The objective was to allow the aerial robot to perch and crawl along the pipe, minimizing the energy consumption, and allowing to perform contact inspection. This paper introduces the design of the soft limbs of the gripper and also the internal mechanism that allows movement along pipes. Several tests have been carried out to ensure the grasping capability on the pipe and the performance and reliability of the developed system. This paper shows the complete development of the system using additive manufacturing techniques and includes the results of experiments performed in realistic environments.
id1094#A review of vision-based on-board obstacle detection and distance estimation in railways#This paper provides a review of the literature on vision-based on-board obstacle detection and distance estimation in railways. Environment perception is crucial for autonomous detection of obstacles in a vehicle’s surroundings. The use of on-board sensors for road vehicles for this purpose is well established, and advances in Artificial Intelligence and sensing technologies have motivated significant research and development in obstacle detection in the automotive field. However, research and development on obstacle detection in railways has been less extensive. To the best of our knowledge, this is the first comprehensive review of on-board obstacle detection methods for railway applications. This paper reviews currently used sensors, with particular focus on vision sensors due to their dominant use in the field. It then discusses and categorizes the methods based on vision sensors into methods based on traditional Computer Vision and methods based on Artificial Intelligence. 
id1095#Automated Quantitative Fractography of Silicate Glasses with Visual Analysis#ASTM C1678 outlines an approach to estimate the fracture strength of glasses and ceramics through the use of empirical relationships relating the strength to characteristic fractographic length-scales, such as the ‘mirror radius’. However, the process of measuring radii is subjective, and the relationship suggested by ASTM standards has been shown to be relatively inaccurate for flexural stress fields. This research introduces and tests a visual analysis algorithm to carry out the fractographic analysis of silicate glasses automatically and objectively. The fracture surfaces of various silicate glasses produced by both tensile and flexural stress fields were considered. First, optical images of the fracture surfaces were gathered and unique; descriptive features such as the shape of the ‘mirror-mist boundary’ were extracted using visual analysis tools. Next, a newly developed algorithm compared the processed images with a database comprised of fracture samples of known strengths, fracture toughness, stress fields, and geometric features. Lastly, dimensional analysis principles coupled with a broad, experimental set of over 2100 fracture surfaces was used to accurately estimate the strengths of the imaged fracture surfaces. 
id1096#A graph-based iterative compiler pass selection and phase ordering approach#Nowadays compilers include tens or hundreds of optimization passes, which makes it difficult to find sequences of optimizations that achieve compiled code more optimized than the one obtained using typical compiler options such as -O2 and -O3. The problem involves both the selection of the compiler passes to use and their ordering in the compilation pipeline. The improvement achieved by the use of custom phase orders for each function can be significant, and thus important to satisfy strict requirements such as the ones present in high-performance embedded computing systems. In this paper we present a new and fast iterative approach to the phase selection and ordering challenges resulting in compiled code with higher performance than the one achieved with the standard optimization levels of the LLVM compiler. The obtained performance improvements are comparable with the ones achieved by other iterative approaches while requiring considerably less time and resources. Our approach is based on sampling over a graph representing transitions between compiler passes. We performed a number of experiments targeting the LEON3 microarchitecture using the Clang/LLVM 3.7 compiler, considering 140 LLVM passes and a set of 42 representative signal and image processing C functions. An exhaustive cross-validation shows our new exploration method is able to achieve a geometric mean performance speedup of 1.28× over the best individually selected -OX flag when considering 100,000 iterations; versus geometric mean speedups from 1.16× to 1.25× obtained with state-of-the-art iterative methods not using the graph. From the set of exploration methods tested, our new method is the only one consistently finding compiler sequences that result in performance improvements when considering 100 or less exploration iterations. Specifically, it achieved geometric mean speedups of 1.08× and 1.16× for 10 and 100 iterations, respectively. 
id1097#FACTORBASE: multi-relational structure learning with SQL all the way#FactorBase is a new SQL-based framework that leverages a relational database management system to support multi-relational model discovery. A multi-relational statistical model provides an integrated analysis of the heterogeneous and interdependent data resources in the database. We adopt the BayesStore design philosophy: Statistical models are stored and managed as first-class citizens inside a database (Wang et al., in: PVLDB, pp 340–351, 2008). Whereas previous systems like BayesStore support multi-relational inference, FactorBase supports multi-relational learning. A case study on six benchmark databases evaluates how our system supports a challenging machine learning application, namely learning a first-order Bayesian network model for an entire database. Model learning in this setting has to examine a large number of potential statistical associations across data tables. Our implementation shows how the SQL constructs in FactorBase facilitate the fast, modular, and reliable development of scalable model learning systems. 
id1098#A privacy-preserving framework for cross-domain recommender systems#User privacy in the recommender systems have received much attention over the years. However, much of this attention has been on privacy protection in single-domain recommender systems and not on cross-domain recommender systems. The privacy-preserving cross-domain recommender systems not only encourages collaboration of data between different domains to solve the problem of data sparsity but also ensures the users’ privacy and secure transfer of auxiliary information between domains. However, existing studies are not suitable for privacy protection in a cross-domain scenario. To this end, we propose a novel privacy-preserving framework for cross-domain recommender systems that provides a generic template for other secure cross-domain recommender systems. Employing a homomorphic encryption scheme, the framework consists of two protocols for users’ privacy in cross-domain recommender systems. We mathematically described every step involved in each protocol, proved that the two protocols are secure against a semi-honest adversary, and compared the complexity of the protocols. 
id1099#ML grid programming with ConCert#Grid computing has become increasingly popular with the growth of the Internet, especially in large-scale scientific computation. Computational Grids are characterized by their scale, their heterogeneity, and their unreliability, making the creation of Grid software quite a challenge. Security concerns make the deployment of Grid infrastructure similarly daunting. We argue that functional programming techniques, both wellknown and new, make an excellent practical foundation for Grid computing. We present a prototype Grid framework called Con-Cert built entirely in Standard ML which allows for the trustless dissemination of Grid programs through the use of certified code. The framework is fault-tolerant and relatively easy to implement, owing to a simplified network abstraction. This network abstraction is tedious to program for directly, so we present a high level ML-like language Grid/ML and a compiler Hemlock for the language. Copyright 
id1100#Performance analysis of machine learning models for object recognition in underwater video images [Sualti video görüntülerinde nesne tanima amaçli yapay öǧrenme modellerinin performans analizi]#In this study, our primary aim is to detect different formations, objects on the images taken from various underwater videos. For this purpose, machine learning models such as SVM, multi-layer perceptron, logistic regression that use attributes, image histogram obtained from images were chosen. In addition, Autoencoder and CNN based deep learning models were used directly over images and their performances were compared. According to the results, it was observed that all models were satisfactory and achieved good classification performances. The highest performance was observed in the Autoencoder based deep learning model, which achieved an accuracy level of %95. In the future, we are planning to continue studies to focus on underwater cable tracking and detecting errors and anomalies in underwater cables. 
id1101#Detecting cooking state of grilled chicken by electronic nose and computer vision techniques#Determination of food doneness remains a challenge for automation in the cooking industry. The complex physicochemical processes that occur during cooking require a combination of several methods for their control. Herein, we utilized an electronic nose and computer vision to check the cooking state of grilled chicken. Thermogravimetry, differential mobility analysis, and mass spectrometry were employed to deepen the fundamental insights towards the grilling process. The results indicated that an electronic nose could distinguish the odor profile of the grilled chicken, whereas computer vision could identify discoloration of the chicken. The integration of these two methods yields greater selectivity towards the qualitative determination of chicken doneness. The odor profile is matched with detected water loss, and the release of aromatic and sulfur-containing compounds during cooking. This work demonstrates the practicability of the developed technique, which we compared with a sensory evaluation, for better deconvolution of food state during cooking. 
id1102#Hybrid workflow process for home based rehabilitation movement capture#Telehealth rehabilitation systems aimed at providing physical and occupational therapy in the home face considerable challenges in terms of clinician and therapist buy-in, system and training costs, and patient and caregiver acceptance. Understanding the optimal workflow process to support practitioners in delivering quality care in partnership with assistive technologies is significant. We describe the iterative co-development of our hybrid physical/digital workflow process for assisting therapists with the setup and calibration of a computer vision based system for remote rehabilitation. Through an interdisciplinary collaboration, we present promising preliminary concepts for streamlining the translation of research outcomes into everyday healthcare experiences. 
id1103#S-Box Construction Method Based on the Combination of Quantum Chaos and PWLCM Chaotic Map#For a security system built on symmetric-key cryptography algorithms, the substitution box (S-box) plays a crucial role to resist cryptanalysis. In this article, we incorporate quantum chaos and PWLCM chaotic map into a new method of S-box design. The secret key is transformed to generate a six tuple system parameter, which is involved in the generation process of chaotic sequences of two chaotic systems. The output of one chaotic system will disturb the parameters of another chaotic system in order to improve the complexity of encryption sequence. S-box is obtained by XOR operation of the output of two chaotic systems. Over the obtained 500 key-dependent S-boxes, we test the S-box cryptographical properties on bijection, nonlinearity, SAC, BIC, differential approximation probability, respectively. Performance comparison of proposed S-box with those chaos-based one in the literature has been made. The results show that the cryptographic characteristics of proposed S-box has met our design objectives and can be applied to data encryption, user authentication and system access control. 
id1104#Soft robotics: the route to true robotic organisms#Soft Robotics has come to the fore in the last decade as a new way of conceptualising, designing and fabricating robots. Soft materials empower robots with locomotion, manipulation, and adaptability capabilities beyond those possible with conventional rigid robots. Soft robots can also be made from biological, biocompatible and biodegradable materials. This offers the tantalising possibility of bridging the gap between robots and organisms. Here, we discuss the properties of soft materials and soft systems that make them so attractive for future robots. In doing so, we consider how future robots can behave like, and have abilities akin to, biological organisms. These include huge numbers, finite lifetime, homeostasis and minimal—and even positive—environmental impact. This paves the way for future robots, not as machines, but as robotic organisms. 
id1106#Detecting apples in the wild: Potential for harvest quantity estimation#Knowing the exact number of fruits and trees helps farmers to make better decisions in their orchard production management. The current practice of crop estimation practice often in-volves manual counting of fruits (before harvesting), which is an extremely time-consuming and costly process. Additionally, this is not practicable for large orchards. Thanks to the changes that have taken place in recent years in the field of image analysis methods and computational perfor-mance, it is possible to create solutions for automatic fruit counting based on registered digital im-ages. The pilot study aims to confirm the state of knowledge in the use of three methods (You Only Look Once—YOLO, Viola–Jones—a method based on the synergy of morphological operations of digital imagesand Hough transformation) of image recognition for apple detecting and counting. The study compared the results of three image analysis methods that can be used for counting apple fruits. They were validated, and their results allowed the recommendation of a method based on the YOLO algorithm for the proposed solution. It was based on the use of mass accessible devices (smartphones equipped with a camera with the required accuracy of image acquisition and accurate Global Navigation Satellite System (GNSS) positioning) for orchard owners to count growing ap-ples. In our pilot study, three methods of counting apples were tested to create an automatic system for estimating apple yields in orchards. The test orchard is located at the University of Warmia and Mazury in Olsztyn. The tests were carried out on four trees located in different parts of the orchard. For the tests used, the dataset contained 1102 apple images and 3800 background images without fruits. 
id1107#Algorithm 871: A C/C++ precompiler for autogeneration of multiprecision programs#In the past decade a number of libraries for multiprecision floating-point arithmetic have been developed. We describe an easy to use, generic C/C++ transcription program or precompiler for the conversion of C or C++ source code into new code that uses a C++ multiprecision library of choice. The precompiler can convert any type in the input source code to another type in the output source code. The input source can be either C or C++ , while the output code generated by the precompiler and using the new types, is C++. The type conversion is based on a simple XML configuration file which is provided by either the developer of the multiprecision library or by the user of the precompiler. The precompiler can also convert to data types with additional features, which are not supported in the types of the source code. Applicability of the precompiler is shown with the successful conversion of large subsets of the GNU Scientific Library and Numerical Recipes. 
id1110#Comparison of three key remote sensing technologies for mobile robot localization in nuclear facilities#Sensor technologies will play a key role in the success of Remote Maintenance (RM) systems for future fusion reactors. In this paper, three key types of sensor technologies of particular interest in the robotics field at the moment are evaluated, namely: Colour-Depth cameras, LIDAR (Light Detection And Ranging), and Millimetre-Wave (mmWave) RADAR. The evaluation of the sensors is performed based on the following criteria: the types of data they provide, the accuracy at different distances, and the potential environmental resistance of the sensor (namely gamma radiation). The authors review the progress in making these three types of sensor capable of operating in Fusion facilities and discuss possible mitigations. Experiments are performed to demonstrate the pros and cons of each type of sensor by collecting data from radar, colour-depth camera and LIDAR, simultaneously. The paper concludes with a performance comparison between sensors, as well as discussing the possibility of combining them, fostering redundancy in case of failure of any individual sensor device. 
id1111#Human Anomalous Activity Detection: Shape and Motion Approach in Crowded Scenes#Detecting anomalous activities in crowded scenes is a very challenging task in computer vision. An enhanced video anomaly detection framework is proposed for frame-wise anomalous activity detection in crowded scenes that is based on both shape and motion based features. The Histogram of Oriented Gradients (HOG) is used to represent the shape based features of the video frames and for representing the motion, Histogram of Oriented Optical Flow (HOOF) is used. These features are modeled using two-class Support Vector Machines (SVM) to detect abnormal events in every frame. The proposed method is modeled with both normal and abnormal behaviors which are learnt from the training data and it is capable of detecting abnormal activities in a live surveillance video. To evaluate the performance of the proposed work, experiments are conducted on the standard benchmark UCSD data set and the results are compared with the HOOF feature bin values. 
id1112#Register allocation for intel processor graphics#Register allocation is a well-studied problem, but surprisingly little work has been published on assigning registers for GPU architectures. In this paper we present the register allocator in the production compiler for Intel HD and Iris Graphics. Intel GPUs feature a large byte-addressable register file organized into banks, an expressive instruction set that supports variable SIMD-sizes and divergent control flow, and high spill overhead due to relatively long memory latencies. These distinctive characteristics impose challenges for register allocation, as input programs may have arbitrarily-sized variables, partial updates, and complex control flow. Not only should the allocator make a program spill-free, but it must also reduce the number of register bank conflicts and anti-dependencies. Since compilation occurs in a JIT environment, the allocator also needs to incur little overhead. To manage compilation overhead, our register allocation framework adopts a hybrid approach that separates the assignment of local and global variables. Several extensions are introduced to the traditional graph-coloring algorithm to support variables with different sizes and to accurately model liveness under divergent branches. Different assignment polices are applied to exploit the trade-offs between minimizing register usage and avoiding bank conflicts and anti-dependencies. Experimental results show our framework produces very few spilling kernels and can improve RA JIT time by up to 4x over pure graph-coloring. Our round-robin and bank-conflict-reduction assignment policies can also achieve up to 20% runtime improvements. 
id1113#A scheduling framework for general-purpose parallel languages#The trend in microprocessor design toward multicore and manycore processors means that future performance gains in software will largely come from harnessing parallelism. To realize such gains, we need languages and implementations that can enable parallelism at many different levels. For example, an application might use both explicit threads to implement course-grain parallelism for independent tasks and implicit threads for fine-grain data-parallel computation over a large array. An important aspect of this requirement is supporting a wide range of different scheduling mechanisms for parallel computation. In this paper, we describe the scheduling framework that we have designed and implemented for Manticore, a strict parallel functional language. We take a micro-kernel, approach in our design: the compiler and runtime support a small, collection of scheduling primitives upon which complex scheduling policies can be implemented. This framework is extremely flexible and can support a wide range of different scheduling policies. It also supports the nesting of schedulers, which is key to both supporting multiple scheduling policies in the same application and to hierarchies of speculative parallel computations. In addition to describing our framework, we also illustrate its expressiveness with several popular scheduling techniques. We present a (mostly) modular approach to extending our schedulers to support cancellation. This mechanism is essential for implementing eager and speculative parallelism. We finally evaluate our framework with a series of benchmarks and an analysis. Copyright 
id1114#Design Guidelines on Deep Learning-based Pedestrian Detection Methods for Supporting Autonomous Vehicles#Intelligent transportation systems (ITS) enable transportation participants to communicate with each other by sending and receiving messages, so that they can be aware of their surroundings and facilitate efficient transportation through better decision making. As an important part of ITS, autonomous vehicles can bring massive benefits by reducing traffic accidents. Correspondingly, much effort has been paid to the task of pedestrian detection, which is a fundamental task for supporting autonomous vehicles. With the progress of computational power in recent years, adopting deep learning-based methods has become a trend for improving the performance of pedestrian detection. In this article, we present design guidelines on deep learning-based pedestrian detection methods for supporting autonomous vehicles. First, we will introduce classic backbone models and frameworks, and we will analyze the inherent attributes of pedestrian detection. Then, we will illustrate and analyze representative pedestrian detectors from occlusion handling, multi-scale feature extraction, multi-perspective data utilization, and hard negatives handling these four aspects. Last, we will discuss the developments and trends in this area, followed by some open challenges. 
id1115#First SN P visual cryptographic circuit with astrocyte control of structural plasticity for security applications#Cryptographic algorithms have been widely used in all secure data transmissions due their capabilities to hide sensible information against eavesdroppers. Nevertheless, technological progress tends to develop new cryptographic methods based on non-classical computing paradigms. In this work, the first circuit of visual cryptography based on Spiking Neural P systems (VCSN P) is presented. The proposed circuit makes use of new biological behaviors in astrocytes, which are involved in processes of plasticity and pruning control in dendrites, that can be the beginning to new cryptographic algorithms based on SN P systems. To validate the SN P circuit, this was implemented on the Cyclone V field programmable gate array (FPGA) development kit. The results demonstrate that this new approach exploits the features of the SN P systems to improve the cryptographic algorithm. 
id1116#Assessing conceptual knowledge in three online engineering courses: Theory of computation and compiler construction, operating systems, and signal and systems#In the current decade understanding conceptual knowledge should be an important area of engineering science. However, it is not as widespread in this field as it is in the areas of education and psychology. Learning conceptual knowledge in engineering science could help instructors to adapt their lectures in order to overcome student misconceptions, to reinforce the learning process, and to check whether students are able to identify key features of a problem. Different methods are provided by authors to assess conceptual knowledge. One is to design and develop a concept inventory with the objective of identifying possible student misconceptions through multiple-choice questions. Another method consists of asking each student to answer a question by submitting a written explanation. This study provides two procedures for the assessment of conceptual knowledge based on the latter method. The first procedure is applied to online computer science students enrolled on an Operating Systems course. The second procedure is applied to online communication students enrolled on a Signals and Systems course. Both procedures are focused not only on assessing but also on searching for causes of potential student misconceptions. These procedures could help other instructors to assess conceptual knowledge on other engineering courses. 
id1118#Applications, databases and open computer vision research from drone videos and images: a survey#Analyzing videos and images captured by unmanned aerial vehicles or aerial drones is an emerging application attracting significant attention from researchers in various areas of computer vision. Currently, the major challenge is the development of autonomous operations to complete missions and replace human operators. In this paper, based on the type of analyzing videos and images captured by drones in computer vision, we have reviewed these applications by categorizing them into three groups. The first group is related to remote sensing with challenges such as camera calibration, image matching, and aerial triangulation. The second group is related to drone-autonomous navigation, in which computer vision methods are designed to explore challenges such as flight control, visual localization and mapping, and target tracking and obstacle detection. The third group is dedicated to using images and videos captured by drones in various applications, such as surveillance, agriculture and forestry, animal detection, disaster detection, and face recognition. Since most of the computer vision methods related to the three categories have been designed for real-world conditions, providing real conditions based on drones is impossible. We aim to explore papers that provide a database for these purposes. In the first two groups, some survey papers presented are current. However, the surveys have not been aimed at exploring any databases. This paper presents a complete review of databases in the first two groups and works that used the databases to apply their methods. Vision-based intelligent applications and their databases are explored in the third group, and we discuss open problems and avenues for future research. 
id1119#Video2Entities: A computer vision-based entity extraction framework for updating the architecture, engineering and construction industry knowledge graphs#Due to the decentralisation and complexity of knowledge in the architecture, engineering and construction (AEC) industry, the research on knowledge graphs (KGs) is still insufficient, and most of the research focuses on text-based KG structuring or updating. Entity extraction, a sub-task of knowledge extraction, is critical in general KG update approaches. While the mainstream approach for this task generally uses textual data, visual data is more readily available, more accurate and has a shorter update cycle than textual data. Therefore, this paper integrates zero-shot learning techniques with general KGs to present a novel framework called “video2entities” that can extract entities from videos to update the AEC KG. The framework combines the perceptual capabilities of computer vision with the cognitive capabilities of KG to improve the accuracy and timeliness of KG updates. Experimental results demonstrate that the framework can extract “new entities” from architectural decoration videos for AEC KG updates. 
id1120#An efficient semi-quantum secret sharing protocol of specific bits#Quantum secret sharing (QSS) allows a trusted party to distribute the secret keys to a group of participates, who can only access the secret cooperatively. The semi-quantum secret sharing (SQSS) takes fewer quantum resources and has higher efficiency than the QSS protocol. However, in the existing SQSS protocols, the shared secrets are generated according to the random operations of Bob and Charlie, which are inefficient and uncertain. An efficient semi-quantum secret sharing protocol based on Bell states was proposed, where Alice can share the specific secrets with Bob and Charlie, by encoding her secrets on the two different Bell states. Then, the security analysis shows that this scheme is secure against intercept–resend attack, entangle–measure attack and Trojan horse attack. Compared with similar studies, the proposed scheme is more flexible and practical, and the qubit efficiency is increased by about 100 %. 
id1121#A symmetric image encryption scheme based on hybrid analog-digital chaotic system and parameter selection mechanism#In recent years, various chaos-based image encryption algorithms have been proposed to meet the growing demand for real-time secure image transmission. However, chaotic system that is the core component of chaos-based cryptosystem usually degrades under finite computing precision, causing many security issues. In this paper, a novel cryptosystem with analog-digital hybrid chaotic model is proposed. Firstly, the analog Chen chaotic system and the digital Logistic map are adopted to depict the capability of the hybrid model, in which analog system is used to perturb digital system. Dynamic analyses demonstrate that the hybrid method has better complexity, larger chaotic parameter range and good ability to counteract dynamical degradation. The chaos-based key streams generated by the perturbed Logistic map are more suitable for image encryption. Secondly, a parameter selection mechanism is introduced to increase security. The state variables of Chen chaotic system and cipher image are involved in parameter selection process to dynamically change the parameter of the perturbed Logistic map. The involvement of cipher image makes the key streams relevant to plain image and can resist known/chosen-plaintext attacks. Performance, security and comparison analyses indicate that this cryptosystem has high security, low time complexity, and ability to resist common attacks. 
id1122#Learning to Map Degrees of Freedom for Assistive User Control: Towards an Adaptive DoF-Mapping Control for Assistive Robots#This paper presents a novel approach to shared control for an assistive robot by adaptively mapping the degrees of freedom (DoFs) for the user to control with a low-dimensional input device. For this, a convolutional neural network interprets camera data of the current situation and outputs a probabilistic description of possible robot motion the user might command. Applying a novel representation of control modes, the network's output is used to generate individual degrees of freedom of robot motion to be controlled by single DoF of the user's input device. These DoFs are not necessarily equal to the cardinal DoFs of the robot but are instead superimpositions of those, thus allowing motions like diagonal directions or orbiting around a point. This enables the user to perform robot motions previously impossible with such a low-dimensional input device. The shared control is implemented for a proof-of-concept 2D simulation and evaluated with an initial user study by comparing it to a standard control approach. The results show a functional control which is both subjectively and objectively significantly faster, but subjectively more complex. 
id1123#Socially Aware Navigation: A Non-linear Multi-objective Optimization Approach#Mobile robots are increasingly populating homes, hospitals, shopping malls, factory floors, and other human environments. Human society has social norms that people mutually accept; obeying these norms is an essential signal that someone is participating socially with respect to the rest of the population. For robots to be socially compatible with humans, it is crucial for robots to obey these social norms. In prior work, we demonstrated a Socially-Aware Navigation (SAN) planner, based on Pareto Concavity Elimination Transformation (PaCcET), in a hallway scenario, optimizing two objectives so the robot does not invade the personal space of people. This article extends our PaCcET-based SAN planner to multiple scenarios with more than two objectives. We modified the Robot Operating System's (ROS) navigation stack to include PaCcET in the local planning task. We show that our approach can accommodate multiple Human-Robot Interaction (HRI) scenarios. Using the proposed approach, we achieved successful HRI in multiple scenarios such as hallway interactions, an art gallery, waiting in a queue, and interacting with a group. We implemented our method on a simulated PR2 robot in a 2D simulator (Stage) and a pioneer-3DX mobile robot in the real-world to validate all the scenarios. A comprehensive set of experiments shows that our approach can handle multiple interaction scenarios on both holonomic and non-holonomic robots; hence, it can be a viable option for a Unified Socially-Aware Navigation (USAN). 
id1124#Integrated encryption in dynamic arithmetic compression#A variant of adaptive arithmetic coding is proposed, adding cryptographic features to this classical compression method. The idea is to perform the updates of the frequency tables for characters of the underlying alphabet selectively, according to some randomly chosen secret key K. We give empirical evidence that with reasonably chosen parameters, the compression performance is not hurt, and discuss also aspects of how to improve the security of the system being used as an encryption method. To keep the paper self-contained, we add a short description of the arithmetic coding algorithm that is necessary to understand the details of the new suggested method. 
id1125#Intelligent perception for cattle monitoring: A review for cattle identification, body condition score evaluation, and weight estimation#There has been an increasing demand for animal protein due to several factors such as global population growth, rising incomes, etc. However, farming productivity is stagnating due to a mix of traditional practice, climate change, socio-economic, and environmental phenomena. Precision livestock farming, with intelligent perception tools at its core, and vast amounts of data being acquired from different sensors or platforms, has the ability to analyse individual animal for improved management, and the potential to dramatically enhance farm productivity. In order to facilitate research and promote the development of related areas, this review summarises and analyses the main existing techniques used in precision cattle farming, focusing on those related to identification, body condition score evaluation, and live weight estimation. More than 100 relevant papers have been discussed in a cohesive manner. From this review and extensive discussions of recent trends, we anticipate that intelligent perception for precision cattle farming will develop through non-contact, high precision, automated technologies, combined with emerging 3D model reconstruction and deep learning technologies. Existing challenges and future research opportunities will also be highlighted and discussed. 
id1126#Multi-bit blinding: A countermeasure for RSA against side channel attacks#Asymmetric algorithms such as RSA are considered secure from an algorithmic point of view, yet their implementations are typically vulnerable as they are used by attackers to comprise the secret key. Many countermeasures have been proposed to thwart these attacks. However, they are typically broken as the key can be easily compromised when attackers succeed figuring out which part of the traces belong to the square and multiply operations. In this paper, a new countermeasure is proposed against side channel attacks, referred to as multi-bit blinding. The proposed method provides a constant execution behavior regardless of the key value without additional cost (i.e., dummy/extra operations). It realizes this by considering multiple bits of the key (i.e., two in this paper) simultaneously and always perform the same operations on them independent of the two-bit value. This makes attacks much harder as the attacker cannot retrieve the key simply by identifying the operations. Instead, the attackers need to guess the correct values of the operations as well. As a case study, the security of an RSA algorithm implementation based on the proposed method is evaluated. Our experimental results show that the new method is secure against profiled and non-profiled side channel attacks with less overhead than currently published countermeasures. 
id1127#HeuristicDB: A hybrid storage database system using a non-volatile memory block device#Hybrid storage systems are widely used in big data fields to balance system performance and cost. However, due to a poor understanding of the characteristics of database block requests, past studies in this area cannot fully utilize the performance gain from emerging storage devices. This study presents a hybrid storage database system, called HeuristicDB, which uses an emerging non-volatile memory (NVM) block device as an extension of the database buffer pool. To consider the unique performance behaviors of NVM block devices and the block-level characteristics of database requests, a set of heuristic rules that associate database (block) requests with the appropriate quality of service for the purpose of caching priority are proposed. Using online analytical processing (OLAP) and online transactional processing (OLTP) benchmarks, both trace-based examination and system implementation on MySQL are carried out to evaluate the effectiveness of the proposed design. The experimental results indicate that HeuristicDB provides up to 75% higher performance and migrates 18X fewer data between storage and the NVM block device than existing systems. 
id1128#Prototype of a Computer Vision-Based CubeSat Detection System for Laser Communications#Up to now, CubeSat nano-satellites have strong limitations in communication data rates (∼ 100 kbps) and bandwidth due to the strictness of CubeSat standard. However, if they could be endowed with optical communications (data rates up to 1 Gbps in optimal state), CubeSat applications would exponentially increase. Nonetheless, laser communications face some important drawbacks as the development of a very strict and accurate tracking mechanism. This work proposes an on-board fine pointing system to locate an optical ground station beacon using an embedded system complying with the restrictive CubeSat standard. Such on-board fine pointing system works based on computer vision. The experimental prototype is implemented in Matlab/Simulink, within a Raspberry Pi 3B. The main outcome is the usage of off-the-shelf components (COTS), obtaining an efficient tracking with low power consumption in very noisy and reflective environments. The developed system proves to be fast, stable and strong. It also satisfies the strict size and power consumption restrictions of CubeSat standard. 
id1129#An innovative technique for image encryption using tri-partite graph and chaotic maps#In this article, the concept of a complete 3-Partite graph is used to substitute the pixels of a color image to achieve encryption. Chen’s chaotic system is deployed to generate vertices of a tri-partite directed graph and then paths are established between these vertices. These paths will serve to perform the substitution of individual pixels. The seeds for the chaotic maps are manipulated by the hash value of the plain image to hinder chosen plaintext attack. The inter and intra correlation of pixels of a three color image in one-dimensional vector is distorted using the non-linear system of equations. The different chaotic maps having divergent features are employed to add up complexity in encryption technique. The extensive experiments are applied on different kinds of images and results are compiled. The results show that the proposed system has a strong robustness against different attacks and prove to be an excellent candidate to encrypt colored digital images. It has high scores for the Number of Pixel Change Rate (NPCR), Unified Average Cipher Intensity (UACI), entropy values, low values for Mean Absolute Error (MAE), Chi-Square and variance of histograms for encrypted images. The palpable lead of proposed system has resilience against transmission impairments such as Gaussian, Salt & Pepper and clipping. 
id1130#SAFECode: Enforcing alias analysis for weakly typed languages#Static analysis of programs in weakly typed languages such as C and C++ is generally not sound because of possible memory errors due to dangling pointer references, uninitialized pointers, and array bounds overflow. We describe a compilation strategy for standard C programs that guarantees that aggressive interprocedural pointer analysis (or less precise ones), a call graph, and type information for a subset of memory, are never invalidated by any possible memory errors. We formalize our approach as a new type system with the necessary run-time checks in operational semantics and prove the correctness of our approach for a subset of C. Our semantics provide the foundation for other sophisticated static analyses to be applied to C programs with a guarantee of soundness. Our work builds on a previously published transformation called Automatic Pool Allocation to ensure that hard-to-detect memory errors (dangling pointer references and certain array bounds errors) cannot invalidate the call graph, points-to information or type information. The key insight behind our approach is that pool allocation can be used to create a run-time partitioning of memory that matches the compile-time memory partitioning in a points-to graph, and efficient checks can be used to isolate the run-time partitions. Furthermore, we show that the sound analysis information enables static checking techniques that eliminate many run-time checks. Our approach requires no source code changes, allows memory to be managed explicitly, and does not use meta-data on pointers or individual tag bits for memory. Using several benchmarks and system codes, we show experimentally that the run-time overheads are low (less than 10% in nearly all cases and 30% in the worst case we have seen). We also show the effectiveness of static analyses in eliminating run-time checks. Copyright 
id1131#Direction Concentration Learning: Enhancing Congruency in Machine Learning#One of the well-known challenges in computer vision tasks is the visual diversity of images, which could result in an agreement or disagreement between the learned knowledge and the visual content exhibited by the current observation. In this work, we first define such an agreement in a concepts learning process as congruency. Formally, given a particular task and sufficiently large dataset, the congruency issue occurs in the learning process whereby the task-specific semantics in the training data are highly varying. We propose a Direction Concentration Learning (DCL) method to improve congruency in the learning process, where enhancing congruency influences the convergence path to be less circuitous. The experimental results show that the proposed DCL method generalizes to state-of-the-art models and optimizers, as well as improves the performances of saliency prediction task, continual learning task, and classification task. Moreover, it helps mitigate the catastrophic forgetting problem in the continual learning task. The code is publicly available at https://github.com/luoyan407/congruency. 
id1132#A Review of Star Schema and Snowflakes Schema#In the new age, digital data is the most important source of acquiring knowledge. For this purpose, collect data from various sources like websites, blogs, webpages, and most important databases. Database and relational databases both provide help to decision making in the future work. Nowadays these approaches become time and resource consuming there for new concept use name data warehouse. Which can analyze many databases at a time on a common plate from with very efficient way. In this paper, we will discuss the database and migration from the database to the data warehouse. Data Warehouse (DW) is the special type of a database that stores a large amount of data. DW schemas organize data in two ways in which star schema and snowflakes schema. Fact and dimension tables organize in them. Distinguished by normalization of tables. Nature of data leads the designer to follow the DW schemas on the base of data, time and resources factor. Both design-modeling techniques compare with the experiment on the same data and results of applying the same query on them. After the performance evaluation, using bitmap indexing to improve the schemas performance. We also present the design modeling techniques with respect to data mining and improve query optimization technique to save time and resource in the analysis of data. 
id1133#On Quasi Cycles in Hypergraph Databases#The notion of hypergraph cyclicity is important in numerous fields of application of hypergraph theory in computer science and relational database theory. The database scheme and query can be represented as a hypergraph. The database scheme (or query) has a cycle if the corresponding hypergraph has a cycle. An Acyclic database has several desired computational properties such as making query optimization easier and can be recognized in linear time. In this paper, we introduce a new type of cyclicity in hypergraphs via the notions of Quasi α -cycle(s) and the set of α -nodes in hypergraphs, which are based on the existence of an α -cycle(s). Then, it is proved that a hypergraph is acyclic if and only if it does not contain any α -nodes. Moreover, a polynomial-time algorithm is proposed to detect the set of α -nodes based on the existence of Quasi α -cycle(s), or otherwise claims the acyclicity of the hypergraph. Finally, a systematic discussion is given to show how to use the detected set of α -nodes to convert the cyclic hypergraph into acyclic one if the conversion is possible. The acyclic database and acyclic query enjoy time and/or space-efficient access paths for answering a query. 
id1134#SAFECode: Enforcing alias analysis for weakly typed languages#Static analysis of programs in weakly typed languages such as C and C++ is generally not sound because of possible memory errors due to dangling pointer references, uninitialized pointers, and array bounds overflow. We describe a compilation strategy for standard C programs that guarantees that aggressive interprocedural pointer analysis (or less precise ones), a call graph, and type information for a subset of memory, are never invalidated by any possible memory errors. We formalize our approach as a new type system with the necessary run-time checks in operational semantics and prove the correctness of our approach for a subset of C. Our semantics provide the foundation for other sophisticated static analyses to be applied to C programs with a guarantee of soundness. Our work builds on a previously published transformation called Automatic Pool Allocation to ensure that hard-to-detect memory errors (dangling pointer references and certain array bounds errors) cannot invalidate the call graph, points-to information or type information. The key insight behind our approach is that pool allocation can be used to create a run-time partitioning of memory that matches the compile-time memory partitioning in a points-to graph, and efficient checks can be used to isolate the run-time partitions. Furthermore, we show that the sound analysis information enables static checking techniques that eliminate many run-time checks. Our approach requires no source code changes, allows memory to be managed explicitly, and does not use meta-data on pointers or individual tag bits for memory. Using several benchmarks and system codes, we show experimentally that the run-time overheads are low (less than 10% in nearly all cases and 30% in the worst case we have seen). We also show the effectiveness of static analyses in eliminating run-time checks. Copyright 
id1135#A Sim-to-Real Pipeline for Deep Reinforcement Learning for Autonomous Robot Navigation in Cluttered Rough Terrain#Robots that autonomously navigate real-world 3D cluttered environments need to safely traverse terrain with abrupt changes in surface normals and elevations. In this letter, we present the development of a novel sim-to-real pipeline for a mobile robot to effectively learn how to navigate real-world 3D rough terrain environments. The pipeline uses a deep reinforcement learning architecture to learn a navigation policy from training data obtained from the simulated environment and a unique combination of strategies to directly address the reality gap for such environments. Experiments in the real-world 3D cluttered environment verified that the robot successfully performed point-to-point navigation from arbitrary start and goal locations while traversing rough terrain. A comparison study between our DRL method, classical, and deep learning-based approaches showed that our method performed better in terms of success rate, and cumulative travel distance and time in a 3D rough terrain environment. 
id1136#Deep learning-based high-throughput phenotyping can drive future discoveries in plant reproductive biology#Key message: Advances in deep learning are providing a powerful set of image analysis tools that are readily accessible for high-throughput phenotyping applications in plant reproductive biology. High-throughput phenotyping systems are becoming critical for answering biological questions on a large scale. These systems have historically relied on traditional computer vision techniques. However, neural networks and specifically deep learning are rapidly becoming more powerful and easier to implement. Here, we examine how deep learning can drive phenotyping systems and be used to answer fundamental questions in reproductive biology. We describe previous applications of deep learning in the plant sciences, provide general recommendations for applying these methods to the study of plant reproduction, and present a case study in maize ear phenotyping. Finally, we highlight several examples where deep learning has enabled research that was previously out of reach and discuss the future outlook of these methods. 
id1139#Compile-time type-checking for custom type qualifiers in Java#We have created a system that enables programmers to add custom type qualifiers to the Java language in a backward-compatible way. The system allows programmers to write type qualifiers in their programs and to create compiler plugins that enforce the semantics of these qualifiers at compile time. The system builds on existing Java tools and APIs, and on JSR 308. As an example, we introduce a plug-in to Sun's Java compiler that uses our system to type-check the NonNull qualifier. Programmers can use the 
id1140#Understanding the role of glaze layer with aligned images from multiple surface characterization techniques#A glaze layer that significantly reduces friction and wear has been found on the surface of many Fe-, Cr-, and Ni-based material systems undergoing fretting/sliding at elevated temperature. In this paper, we propose a novel way to understand the role of glaze layer using computer vision algorithms. Two workflows, one for quantitative glaze layer identification and the other for image alignment, have been developed. For glaze layer identification, we used computer vision concepts that considers the color and reflection of glaze layer under optical microscope (OM). For image alignment, we developed a strategy to conduct pixel-to-pixel alignment of images acquired by multiple techniques (e.g., OM, scanning electron microscopy, 3D optical profilers) with sub-pixel error. As such, the correlation between the height map and locations of the glaze layer within the wear scar can be readily determined. These methods are used to evaluate wear scars generated on 310S stainless steel under like-on-like, cylinder-on-flat fretting conditions from 20 °C to 700 °C. The glaze layer is found to always occupy relatively high locations within wear scar. With temperature rise, the projected coverage of glaze layer follows the same increasing trend with three distinguishable stages, and the threshold temperature of the three stages matched with severe-to-mild wear transition. These results provide evidence that severe-to-mild wear transition resulted from spreading of glaze layer coverage, and glaze layer may reduce friction and wear by reducing real contact area. 
id1141#Type-directed program synthesis and constraint generation for library portability#Fast numerical libraries have been a cornerstone of scientific computing for decades, but this comes at a price. Programs may be tied to vendor specific software ecosystems resulting in polluted, non-portable code. As we enter an era of heterogeneous computing, there is an explosion in the number of accelerator libraries required to harness specialized hardware. We need a system that allows developers to exploit ever-changing accelerator libraries, without over-specializing their code. As we cannot know the behavior of future libraries ahead of time, this paper develops a scheme that assists developers in matching their code to new libraries, without requiring the source code for these libraries. Furthermore, it can recover equivalent code from programs that use existing libraries and automatically port them to new interfaces. It first uses program synthesis to determine the meaning of a library, then maps the synthesized description into generalized constraints which are used to search the program for replacement opportunities to present to the developer. We applied this approach to existing large applications from the scientific computing and deep learning domains. Using our approach, we show speedups ranging from 1.1x to over 10x on end to end performance when using accelerator libraries. 
id1142#Migrating relational databases into XML documents#This paper aims to devise a method for migrating a relational database (RDB) into an XML document. Database migration is concerned with the process of converting schema and data from a source RDB, as a one-time conversion, into a target XML database to be managed and handled in its new environment. The source schema is enriched semantically and translated into a target schema, and the data stored in the source database is converted into a target database based on the new schema. The semantic enrichment process is requisite to produce an enhanced metadata model from the source database and captures essential characteristics of target XML schema, and suitable for converting RDB data into an XML document. Algorithms are developed for producing the target database according to a set of migration rules to translate all constructs of an RDB into an XML Schema, based on which RDB data is then converted. A prototype system has been implemented and empirically evaluated by testing its results, looking at our achievements and reflecting on the results. From the evaluation of results, it is concluded that the proposed solution is efficient and correct. 
id1143#Quantum digital signatures with smaller public keys#We introduce a variant of quantum signatures in which nonbinary symbols are signed instead of bits. The public keys are fingerprinting states, just as in the scheme of Gottesman and Chuang [1], but we allow for multiple ways to reveal the private key partially. The effect of this modification is a reduction of the number of qubits expended per message bit. Asymptotically the expenditure becomes as low as one qubit per message bit. We give a security proof, and we present numerical results that show how the improvement in public key size depends on the message length. 
id1144#A Competent Intelligent Key Cryptography (IKC) Architecture#The human race is becoming too intelligent to create a robotic system to mimic the human actions. On the other hand, the same innovation has led to various negative aspects like theft, cybercrime and plenty of unseen malfunctions. In this digital world, it has become mandatory that everyone in some way or other communicate sensitive information via Internet. Though many security mechanisms are available, there is always a threat in its implementation. The development of fast performing quantum computers even though a boon to robotics and computing systems it has become a nightmare in the field of cryptography. The mathematical toughness in the modern cryptosystems has become very simple to solve by using quantum computers. This made NIST to solicit proposals for post-quantum cryptography, which should include standards to be used as quantum resistant counterparts to the existing standards. In this paper, a competent Intelligent Key Cryptography (IKC) system is proposed, highlighting that, there is no need to exchange the keys either by public or private means among the users but it resides in the transmitting message itself. IKC is a tamil based random key generator, which is a promising challenge to change the current key based crypto system. Furthermore, a performance analysis could be done as a future task. 
id1146#Task-driven and cooperative-working based compiler principle teaching reform#Task-driven teaching method is proposed for the difficulty in the process of teaching and learning of compiler principle course. This method is based on the constructivist teaching theory. Teaching tasks is divided into several tasks which contain several points. Teaching goals are realized by solving the tasks. Cooperative-working based practice teaching method is proposed in order to enhance the teaching effect of this method. Students will cooperate with their classmates to co-accomplish one task. Then they can not only complete the practical task of the point but also have very clear understanding of their relations between all points of the whole task. good results have been achieved in practice. It satisfies the need of teaching reform. 
id1148#Design and application of national reserved cultivated land resources investigation and evaluation database management system#The investigation and evaluation data of reserved cultivated land resources are of great significance to the research and policy-making about cultivated land protection, it is necessary to establish stable and efficient information systems to manage and use the massive data. To this end, this article adopt a hybrid storage strategy, uses relational database, non-relational database and file database to establish the national investigation and evaluation database of reserved cultivated land resources. Automatic and parallel database building technology, spatial big data calculation technology and vector data dynamic rendering technology are researched on, which has solved the technical bottleneck of large-scale spatial data storage, management and visualization. On this basis, developed the database management system, it has been applied in the government administration of cultivated land resources in China and exerts positive results. 
id1149#Circuit compilation methodologies for quantum approximate optimization algorithm#The quantum approximate optimization algorithm (QAOA) is a promising quantum-classical hybrid algorithm to solve hard combinatorial optimization problems. The multi-qubit CPHASE gates used in the quantum circuit for QAOA are commutative i.e., the order of the gates can be altered without changing the output state. This re-ordering leads to the execution of more gates in parallel and a smaller number of additional SWAP gates to compile the QAOA-circuit. Consequently, the circuit-depth and cumulative gate-count become lower which is beneficial for circuit execution time and noise resilience. A less number of gates indicates a lower accumulation of gate-errors, and a reduced circuit-depth means less decoherence time for the qubits. However, finding the best-ordered circuit is a difficult problem and does not scale well with circuit size. This paper presents four generic methodologies to optimize QAOA-circuits by exploiting gate re-ordering. We demonstrate a reduction in gate-count by ≈23.0% and circuit-depth by ≈53.0% on average over a conventional approach without incurring any compilation-time penalty. We also present a variation-aware compilation which enhances the compiled circuit success probability by ≈62.7% for the target hardware over the variation unaware approach. A new metric, Approximation Ratio Gap (ARG), is proposed to validate the quality of the compiled QAOA-circuit instances on actual devices. Hardware implementation of a number of QAOA instances shows ≈25.8% improvement in the proposed metric on average over the conventional approach on ibmq 16 melbourne. 
id1150#Blessing of dimensionality at the edge and geometry of few-shot learning#In this paper we present theory and algorithms enabling classes of Artificial Intelligence (AI) systems to continuously and incrementally improve with a priori quantifiable guarantees – or more specifically remove classification errors – over time. This is distinct from state-of-the-art machine learning, AI, and software approaches. The theory enables building few-shot AI correction algorithms and provides conditions justifying their successful application. Another feature of this approach is that, in the supervised setting, the computational complexity of training is linear in the number of training samples. At the time of classification, the computational complexity is bounded by few inner product calculations. Moreover, the implementation is shown to be very scalable. This makes it viable for deployment in applications where computational power and memory are limited, such as embedded environments. It enables the possibility for fast on-line optimisation using improved training samples. The approach is based on the concentration of measure effects and stochastic separation theorems and is illustrated with an example on the identification faulty processes in Computer Numerical Control (CNC) milling and with a case study on adaptive removal of false positives in an industrial video surveillance and analytics system. 
id1151#A comparative analysis of machine learning models developed from homomorphic encryption based RSA and paillier algorithm#Nowadays, the use of Auto ML for development of machine learning (ML) models is increasing day by day. In which users need to upload their dataset to develop machine learning models. Where security of user's data is a very big problem. Which can be solved using Homomorphic encryption. In this work, first the spiderman correlation of the machine learning models developed using encrypted dataset is compared with each other. In which the algorithms used to encrypt dataset are Homomorphic Encryption (HE) based Rivest Shamir Adleman (RSA) and Paillier algorithm. Where the homomorphic encryption based Rivest Shamir Adleman (RSA) is a multiplicative operation and the HE-based Pailler supports additive operations. These algorithms are used to encrypt data and store it on cloud servers. An in-house key generator is developed to generate keys for HE-based RSA algorithm and used this algorithm to encrypt data. Then a comparison between the time taken by these algorithms to encrypt the data and develop the machine learning model using Azure auto Machine Learning is done where the HE-based RSA algorithm is winner. 
id1152#A secure and efficient Internet of Things cloud encryption scheme with forensics investigation compatibility based on identity-based encryption#Data security is a challenge for end-users of cloud services as the users have no control over their data once it is transmitted to the cloud. A potentially corrupt cloud service provider can obtain the end-users’ data. Conventional PKI-based solutions are insufficient for large-scale cloud systems, considering efficiency, scalability, and security. In large-scale cloud systems, the key management requirements include scalable encryption, authentication, and non-repudiation services, as well as the ability to share files with different users and data recovery when the user keys of encrypted data are not accessible. Further requirements in cloud systems include the ability to provide the means for digital forensic investigations on encrypted data. Once data on the cloud is encrypted with a user's key it becomes impossible to access by forensic investigation teams. In this regard, distributing the trust of key management into multiple authorities is desirable. In the literature, there is no available secure cloud storage system with secure and efficient Type-3 pairings, supporting Encryption-as-a-Service (EaaS) and multiple Public Key Generators (PKGs). This paper proposes an efficient Identity-based cryptography (IBC) architecture for secure cloud storage, named Secure Cloud Storage System (SCSS), which supports distributed key management and encryption mechanisms and support for multiple PKGs. During forensic investigations, the legal authorities will be able to use the multiple PKG mechanism for data access, while an account locking mechanism prevents a single authority to access user data due to trust distribution. We also demonstrate that, the IBC scheme used in SCSS has better performance compared to similar schemes in the literature. For the security levels of 128-bits and above, SCSS has better scalability compared to existing schemes, with respect to encryption and decryption operations. Since the decryption operation is frequently needed for forensic analysis, the improved scalability results in a streamlined forensic investigation process on the encrypted data in the cloud. 
id1153#Card-based Cryptography with Dihedral Symmetry#It is known that secure computation can be done by using a deck of physical cards. This area is called card-based cryptography. Shinagawa et al. (in: Provable security—9th international conference, ProvSec 2015, Kanazawa, Japan, 2015) proposed regular n-sided polygon cards that enable to compute functions over Z/ nZ. In particular, they designed efficient protocols for linear functions (e.g. addition and constant multiplication) over Z/ nZ. Here, efficiency is measured by the number of cards used in the protocol. In this paper, we propose a new type of cards, dihedral cards, as a natural generalization of regular polygon cards. Based on them, we construct efficient protocols for various interesting functions such as carry of addition, equality, and greater-than, whose efficient construction has not been known before. Beside this, we introduce a new protocol framework that captures a wide class of card types including binary cards, regular polygon cards, dihedral cards, and so on. 
id1154#TDRB: An Efficient Tamper-Proof Detection Middleware for Relational Database Based on Blockchain Technology#The relational database has become one of the mainstream tools for data storage and management. However, there are two main types of threats to relational databases: external attacks and internal tampering threats. In this paper, we focus on the internal tampering threats and propose a tamper-proof detection middleware named TDRB to provide efficient tamper-proof detection for relational databases. Within the TDRB middleware framework, raw data is still stored and queried from the relational database, while the hash digest of the critical data in the relational database is synchronously migrated to the blockchain for tamper detection. Based on this method, we leverage blockchain's immutability to detect data tamper and maintain the advanced features of relational databases to better support ease of data persistence, complex queries, and large storage capacity. We also propose a performance improvement mechanism that involves connecting the blockchain and relational database to improve throughput and mitigate performance impact. A series of experiments indicate that the TDRB middleware can accurately detect the tampering information during arbitrary tampering with the relational database and the cache database. Compare with the baseline, with the increase of cache hit rate, the TDRB middleware query speed increased by 93.1%, update speed increased by 16.3%, delete speed increased by 16.1%, and join operation average speed increased by 95.2%. Given its generality, the TDRB middleware can be flexibly and conveniently integrated into third-party platforms. 
id1155#VanHelsing: A Fast Proof Checker for Debuggable Compiler Verification#In this paper we present vanHelsing, a fully automatic proof checker for a subset of first-order problemstailored to a class of problems that arise in compiler verification.vanHelsing accepts input problems formulated in a subset of theTPTP language and is optimized to efficiently solve expressionequivalence problems formulated in first-order logic. Being apractical tool vanHelsing provides also graphical debugging helpwhich makes the visualization of problems and localization offailed proofs much easier. The experimental evaluation showedthat this specialized tool performs up to a factor of 3 better thanstate of the art theorem provers. 
id1156#ATJ-net: Auto-table-join network for automatic learning on relational databases#"A relational database, consisting of multiple tables, provides heterogeneous information across various entities, widely used in real-world services. This paper studies the supervised learning task on multiple tables, aiming to predict one label column with the help of multiple-tabular data. However, classical ML techniques mainly focus on single-tabular data. Multiple-tabular data refers to many-to-many mapping among joinable attributes and n-ary relations, which cannot be utilized directly by classical ML techniques. Besides, current graph techniques, like heterogeneous information network (HIN) and graph neural networks (GNN), are infeasible to be deployed directly and automatically in a multi-table environment, which limits the learning on databases. For automatic learning on relational databases, we propose an auto-table-join network (ATJ-Net). Multiple tables with relationships are considered as a hypergraph, where vertices are joinable attributes and hyperedges are tuples of tables. Then, ATJ-Net builds a graph neural network on the heterogeneous hypergraph, which samples and aggregates the vertices and hyperedges on n-hop sub-graphs as the receptive field. In order to enable ATJ-Net to be automatically deployed to different datasets and avoid the ""no free lunch""dilemma, we use random architecture search to select optimal aggregators and prune redundant paths in the network. For verifying the effectiveness of our methods across various tasks and schema, we conduct extensive experiments on 4 tasks, 8 various schemas, and 19 sub-datasets w.r.t. citing prediction, review classification, recommendation, and task-blind challenge. ATJ-Net achieves the best performance over state-of-the-art approaches on three tasks and is competitive with KddCup Winner solution on task-blind challenge. Â"
id1157#Optimized interval splitting in a linear scan register allocator#We present an optimized implementation of the linear scan register allocation algorithm for Sun Microsystems' Java Hot Spot™ client compiler. Linear scan register allocation is especially suitable for just-in-time compilers because it is faster than the common graph-coloring approach and yields results of nearly the same quality. Our allocator improves the basic linear scan algorithm by adding more advanced optimizations: It makes use of lifetime holes, splits intervals if the register pressure is too high, and models register constraints of the target architecture with fixed intervals. Three additional optimizations move split positions out of loops, remove register-to-register moves and eliminate unnecessary spill stores. Interval splitting is based on use positions, which also capture the kind of use and whether an operand is needed in a register or not. This avoids the reservation of a scratch register. Benchmark results prove the efficiency of the linear scan algorithm: While the compilation speed is equal to the old local register allocator that is part of the Sun JDK 5.0, integer benchmarks execute about 15% faster. Floating-point benchmarks show the high impact of the Intel SSE2 extensions on the speed of numeric Java applications: With the new SSE2 support enabled, SPECjvm98 executes 25% faster compared with the current Sun JDK 5.0. Copyright 2005 ACM.
id1158#Effect of assist-as-needed robotic gait training on the gait pattern post stroke: a randomized controlled trial#Background: Regaining gait capacity is an important rehabilitation goal post stroke. Compared to clinically available robotic gait trainers, robots with an assist-as-needed approach and multiple degrees of freedom (AANmDOF) are expected to support motor learning, and might improve the post-stroke gait pattern. However, their benefits compared to conventional gait training have not yet been shown in a randomized controlled trial (RCT). The aim of this two-center, assessor-blinded, RCT was to compare the effect of AANmDOF robotic to conventional training on the gait pattern and functional gait tasks during post-stroke inpatient rehabilitation. Methods: Thirty-four participants with unilateral, supratentorial stroke were enrolled (&lt; 10 weeks post onset, Functional Ambulation Categories 3–5) and randomly assigned to six weeks of AANmDOF robotic (combination of training in LOPES-II and conventional gait training) or conventional gait training (30 min, 3–5 times a week), focused on pre-defined training goals. Randomization and allocation to training group were carried out by an independent researcher. External mechanical work (WEXT), spatiotemporal gait parameters, gait kinematics related to pre-defined training goals, and functional gait tasks were assessed before training (T0), after training (T1), and at 4-months follow-up (T2). Results: Two participants, one in each group, were excluded from analysis because of discontinued participation after T0, leaving 32 participants (AANmDOF robotic n = 17; conventional n = 15) for intention-to-treat analysis. In both groups, WEXT had decreased at T1 and had become similar to baseline at T2, while gait speed had increased at both assessments. In both groups, most spatiotemporal gait parameters and functional gait tasks had improved at T1 and T2. Except for step width (T0–T1) and paretic step length (T0–T2), there were no significant group differences at T1 or T2 compared to T0. In participants with a pre-defined goal aimed at foot clearance, paretic knee flexion improved more in the AANmDOF robotic group compared to the conventional group (T0–T2). Conclusions: Generally, AANmDOF robotic training was not superior to conventional training for improving gait pattern in subacute stroke survivors. Both groups improved their mechanical gait efficiency. Yet, AANmDOF robotic training might be more effective to improve specific post-stroke gait abnormalities such as reduced knee flexion during swing. Trial registration Registry number Netherlands Trial Register (www.trialregister.nl): NTR5060. Registered 13 February 2015. 
id1159#4D printing: Fundamentals, materials, applications and challenges#4D printing refers to single-material or multi-material printing of a device or object that can be transformed from a 1D strand into pre-programed 3D shape, from a 2D surface into preprogramed 3D shape and is capable of morphing between different dimensions. Such transformations are facilitated by, e.g., heating, light, or swelling in a liquid, electrochemically and by programming different sensitivity to, e.g., swelling into various parts of the designed geometry. These techniques offer adaptability and dynamic response for structures and systems of all sizes, and promises new possibilities for embedding programmability and simple decision making into non-electronic based materials. Potential applications include; robotics-like behavior without the reliance on complex electro-mechanical-chemical devices as well as adaptive products, garments or mechanisms that respond to user-demands and fluctuating environments. In this paper, we have discussed fundamentals and laws governing 4D printing, materials that are employed in 4D printing along with applications such as soft robotics and challenges that need to be overcome for 4D printing to evolve as a mainstream manufacturing technology. 
id1160#On the development of a collaborative robotic system for industrial coating cells#For remaining competitive in the current industrial manufacturing markets, coating companies need to implement flexible production systems for dealing with mass customization and mass production workflows. The introduction of robotic manipulators capable of mimicking with accuracy the motions executed by highly skilled technicians is an important factor in enabling coating companies to cope with high customization. However, there are some limitations associated with the usage of a fully automated system for coating applications, especially when considering customized products of large dimensions and complex geometry. This paper addresses the development of a collaborative coating cell to increase the flexibility and efficiency of coating processes. The robot trajectory is taught with an intuitive programming by demonstration system, in which an icosahedron marker with multicoloured LEDs is attached to the coating tool for tracking its trajectories using a stereoscopic vision system. For avoiding the construction of fixtures and allowing the operator to freely place products within the coating work cell, a modular 3D perception system was developed, relying on principal component analysis for performing the initial point cloud alignment and on the iterative closest point algorithm for 6 DoF pose estimation. Furthermore, to enable safe and intuitive human-robot collaboration, a non-intrusive zone monitoring safety system was employed to track the position of the operator in the cell. 
id1161#Mining naturalistic human behaviors in long-term video and neural recordings#Background: Recent technological advances in brain recording and machine learning algorithms are enabling the study of neural activity underlying spontaneous human behaviors, beyond the confines of cued, repeated trials. However, analyzing such unstructured data lacking a priori experimental design remains a significant challenge, especially when the data is multi-modal and long-term. New method: Here we describe an automated, behavior-first approach for analyzing simultaneously recorded long-term, naturalistic electrocorticography (ECoG) and behavior video data. We identify and characterize spontaneous human upper-limb movements by combining computer vision, discrete latent-variable modeling, and string pattern-matching on the video. Results: Our pipeline discovers and annotates over 40,000 instances of naturalistic arm movements in long term (7–9 day) behavioral videos, across 12 subjects. Analysis of the simultaneously recorded brain data reveals neural signatures of movement that corroborate previous findings. Our pipeline produces large training datasets for brain–computer interfacing applications, and we show decoding results from a movement initiation detection task. Comparison with existing methods: Spontaneous movements capture real-world neural and behavior variability that is missing from traditional cued tasks. Building beyond window-based movement detection metrics, our unsupervised discretization scheme produces a queryable pose representation, allowing localization of movements with finer temporal resolution. Conclusions: Our work addresses the unique analytic challenges of studying naturalistic human behaviors and contributes methods that may generalize to other neural recording modalities beyond ECoG. We publish our curated dataset and believe that it will be a valuable resource for future studies of naturalistic movements. 
id1162#Monocular 3D Detection with Geometric Constraint Embedding and Semi-Supervised Training#In this work, we propose a novel one-stage and keypoint-based framework for monocular 3D object detection using only RGB images, called KM3D-Net. 2D detection only requires a deep neural network to predict 2D properties of objects, as it is a semanticity-aware task. For image-based 3D detection, we argue that the combination of a deep neural network and geometric constraints are needed to synergistically estimate appearance-related and spatial-related information. Here, we design a fully convolutional model to predict object keypoints, dimension, and orientation, and combine these with perspective geometry constraints to compute position attributes. Further, we reformulate the geometric constraints as a differentiable version and embed this in the network to reduce running time while maintaining the consistency of model outputs in an end-to-end fashion. Benefiting from this simple structure, we propose an effective semi-supervised training strategy for settings where labeled training data are scarce. In this strategy, we enforce a consensus prediction of two shared-weights KM3D-Net for the same unlabeled image under different input augmentation conditions and network regularization. In particular, we unify the coordinate-dependent augmentations as the affine transformation for the differential recovering position of objects, and propose a keypoint-dropout module for network regularization. Our model only requires RGB images, without synthetic data, instance segmentation, CAD model, or depth generator. Extensive experiments on the popular KITTI 3D detection dataset indicate that the KM3D-Net surpasses state-of-the-art methods by a large margin in both efficiency and accuracy. And also, to the best of our knowledge, this is the first application of semi-supervised learning in monocular 3D object detection. We surpass most of the previous fully supervised methods with only 13% labeled data on KITTI. 
id1163#Research on the knowledge and ability dual-driven teaching model for the course of compilers principles#"In this paper, the key and difficult problems in the teaching process of the course of Compilers Principles are studied, as well as the theory and practice teaching models are discussed. Then a Knowledge and Ability dual-driven Teaching Model for the course of Compilers Principles is proposed, that is to let students understand and master the compilers theory and technique through the course teaching; in addition, it takes compilation teaching as an approach to reveal and analyze the problem solving rules of ""problem - formalized description - computerization"", and systematically trains and improves students' ability to use language and language processors to solve problems and even to solve complex engineering problems. We have tried and popularized this teaching model in the multiple rounds of teaching practices, and embodied this teaching idea in the study and development of the course standard for the course of Compilers Principles. "
id1164#High throughput novel architectures of TEA family for high speed IoT and RFID applications#The current era of ubiquitous computing has led to the emergence of a sub-domain in cryptography called Lightweight Cryptography, which deals with imparting adequate security to resource constraint devices like IoT devices, RFID tags with a suitable choice of design metrics. The advancement in network connectivity and data handling capabilities shows the tremendous growth of IoT in physical life. The number of connected devices have been increasing throughout IoT applications, leaving severe security concerns behind. There is a need of secure communication which can be fulfilled with lightweight algorithms. The motive of this work is to implement the optimized lightweight ciphers and model its design metrics. Design has to be simulated to implement the cipher in hardware from which different metrics can be measured. TEA, XTEA and XXTEA ciphers have been used to fulfil the objective stated and were modelled, implemented and optimized on specific hardware technology like Field Programmable Gate Array (FPGA) and Application Specific Integrated Circuit (ASIC) platform. Designs have been implemented to examine numerous properties like block sizes, rounds of implementations and key scheduling part. This paper presents four hardware architectures namely TEA (T1), XTEA (T2), XXTEA (T3) and hybrid model (T4). T1, T2 and T3 have been implemented using pipelined method with percentage improvement in frequency is 75.9%, 162% and 89%. Similarly, the enhancement in terms of area has been 85.43%, 57.08% and 90.79%. T4 presents a hybrid model which is also a pipelined architecture that combines TEA family (TEA, XTEA and XXTEA) in a single design. The percentage improvement of gate equivalent (GE) for T2 is 47.50%. The hybrid model (T4) has the same throughput as that of T1, T2 & T3 and has less GE when compared to combined GE of all three. Efficiency improvement of all the novel architectures are more than eighteen times as compared to the existing literature. 
id1165#Bridging the technology gap between industry and semantic web: Generating databases and server code from rdf#Despite great advances in the area of Semantic Web, industry rather seldom adopts Semantic Web technologies and their storage and query approaches. Instead, relational databases (RDB) are often deployed to store business-critical data, which are accessed via REST interfaces. Yet, some enterprises would greatly benefit from Semantic Web related datasets which are usually represented with the Resource Description Framework (RDF). To bridge this technology gap, we propose a fully automatic approach that generates suitable RDB models with REST APIs to access them. In our evaluation, generated databases from different RDF datasets are examined and compared. Our findings show that the databases sufficiently reflect their counterparts while the API is able to reproduce rather simple SPARQL queries. Potentials for improvements are identified, for example, the reduction of data redundancies in generated databases. 
id1166#A verifiable SSA program representation for aggressive compiler optimization#We present a verifiable low-level program representation to embed, propagate, and preserve safety information in high performance compilers for safe languages such as Java and C
id1167#Performance evaluation for Omni XcalableMP compiler on many-core cluster system based on Knights Landing#To reduce the programming cost on a cluster system, Partitioned Global Address Space (PGAS) languages are used. We have designed an XcalableMP (XMP) PGAS language and developed the OmniXMPcompiler (Omni compiler) for XMP. In the present study, we evaluated the performance of the Omni compiler on Oakforest-PACS, which is a cluster system based on Knights Landing, and on a general Linux cluster system. We performed performance tuning for the Omni compiler using a Lattice QCD mini-application and some mathematical functions appearing in that application. As a result, the performance of the Omni compiler after tuning was improved compared to before tuning on both systems. Furthermore, we compared the performance of MPI and OpenMP (MPI+OpenMP), which is an existing programming model, to that of XMP with the tuned Omni compiler. The results showed that the performance of the Lattice QCD mini-application using XMP was achieving more than 94% of the implementation written in MPI + OpenMP. 
id1168#Secure Communication Channel Establishment: TLS 1.3 (over TCP Fast Open) versus QUIC#Secure channel establishment protocols such as Transport Layer Security (TLS) are some of the most important cryptographic protocols, enabling the encryption of Internet traffic. Reducing latency (the number of interactions between parties before encrypted data can be transmitted) in such protocols has become an important design goal to improve user experience. The most important protocols addressing this goal are TLS 1.3, the latest TLS version standardized in 2018 to replace the widely deployed TLS 1.2, and Quick UDP Internet Connections (QUIC), a secure transport protocol from Google that is implemented in the Chrome browser. There have been a number of formal security analyses for TLS 1.3 and QUIC, but their security, when layered with their underlying transport protocols, cannot be easily compared. Our work is the first to thoroughly compare the security and availability properties of these protocols. Toward this goal, we develop novel security models that permit “layered” security analysis. In addition to the standard goals of server authentication and data confidentiality and integrity, we consider the goals of IP spoofing prevention, key exchange packet integrity, secure channel header integrity, and reset authentication, which capture a range of practical threats not usually taken into account by existing security models that focus mainly on the cryptographic cores of the protocols. Equipped with our new models we provide a detailed comparison of three low-latency layered protocols: TLS 1.3 over TCP Fast Open (TFO), QUIC over UDP, and QUIC[TLS] (a new design for QUIC that uses TLS 1.3 key exchange) over UDP. In particular, we show that TFO’s cookie mechanism does provably achieve the security goal of IP spoofing prevention. Additionally, we find several new availability attacks that manipulate the early key exchange packets without being detected by the communicating parties. By including packet-level attacks in our analysis, our results shed light on how the reliability, flow control, and congestion control of the above layered protocols compare, in adversarial settings. We hope that our models will help protocol designers in their future protocol analyses and that our results will help practitioners better understand the advantages and limitations of secure channel establishment protocols. 
id1170#Wirelessly Actuated Thermo- and Magneto-Responsive Soft Bimorph Materials with Programmable Shape-Morphing#Soft materials that respond to wireless external stimuli are referred to as “smart” materials due to their promising potential in real-world actuation and sensing applications in robotics, microfluidics, and bioengineering. Recent years have witnessed a burst of these stimuli-responsive materials and their preliminary applications. However, their further advancement demands more versatility, configurability, and adaptability to deliver their promised benefits. Here, a dual-stimuli-responsive soft bimorph material with three configurations that enable complex programmable 3D shape-morphing is presented. The material consists of liquid crystal elastomers (LCEs) and magnetic-responsive elastomers (MREs) via facile fabrication that orthogonally integrates their respective stimuli-responsiveness without detrimentally altering their properties. The material offers an unprecedented wide design space and abundant degree-of-freedoms (DoFs) due to the LCE's programmable director field, the MRE's programmable magnetization profile, and diverse geometric configurations. It responds to wireless stimuli of the controlled magnetic field and environmental temperature. Its dual-responsiveness allows the independent control of different DoFs for complex shape-morphing behaviors with anisotropic material properties. A diverse set of in situ reconfigurable shape-morphing and an environment-aware untethered miniature 12-legged robot capable of locomotion and self-gripping are demonstrated. Such material can provide solutions for the development of future soft robotic and other functional devices. 
id1173#Mapping Images over Elliptic Curve for Encryption#The emergence of Elliptic curve cryptography as a preferred cryptographic scheme using minimal computational resources is quite discernible. Its extension to the domain of image encryption using various mapping techniques has been analyzed in this paper. The efficiency of an encryption scheme based on ECC shall depend upon the appropriateness of mapping technique used to map pixels onto the Elliptic curve. Parameters like cost efficiency, Completeness, inversibility, and bandwidth used must be taken into consideration before designing any such mapping technique. 
id1174#Efficient Fruit Grading System Using Spectrophotometry and Machine Learning Approaches#Physical Classification of ripe fruits is an expensive affair in the agriculture industry and human error can lead to inaccurate results. This paper introduces the concept of an intelligent AI-based system using spectrophotometry and computer vision for automated fruit segregation based on their grade. When the fruit is fed into the proposed system, the fruit is identified with 95% accuracy, using a cloud-computing platform provided by Microsoft Azure. After that, using spectroscopy and ensemble machine learning approaches, fruit grade is predicted. This ensemble model is trained using 1366 apple readings taken from Unitec's Apple Sorting and Grading Machine from an industrial plant. With the help of H2O's Driverless.AI, the proposed ensemble provides an overall approximate validation accuracy of 82%. The model is also tested on an unseen test dataset containing real-life spectral values and the accuracy of fruit segregation into different classes peaked at 72%. 
id1175#Transforming flow information during code optimization for timing analysis#The steadily growing embedded-systems market comprises many application domains in which real-time constraints must be satisfied. To guarantee that these constraints are met, the analysis of the worst-case execution time (WCET) of software components is mandatory. In general WCET analysis needs additional control-flow information, which may be provided manually by the user or calculated automatically by program analysis. For flexibility and simplicity reasons it is desirable to specify the flow information at the same level at which the program is developed, i.e., at the source level. In contrast, to obtain precise WCET bounds the WCET analysis has to be performed at machine-code level. Mapping and transforming the flow information from the source-level down to the machine code, where flow information is used in the WCET analysis, is challenging, even more so if the compiler generates highly optimized code. In this article we present a method for transforming flow information from source code to machine code. To obtain a mapping that is safe and accurate, flow information is transformed in parallel to code transformations performed by an optimizing compiler. This mapping is not only useful for transforming manual code annotations but also if platform-independent flow information is automatically calculated at the source level. We show that our method can be applied to every type of semantics-preserving code transformation. The precision of this flow-information transformation allows its users to calculate tight WCET bounds. 
id1176#A novel cross cosine map based medical image cryptosystem using dynamic bit-level diffusion#Development in networking technology has made the remote diagnosis and treatment of patients a reality through telemedicine. At the same time, storing and transmitting medical documents over an insecure network has always been a daunting challenge. Literature shows that the existing medical image cryptosystems suffer from serious shortcomings. The present work attempts to address this crucial area and propose a suitable solution. A novel hybrid 2-Dimensional Cross Cosine Map (2D-CCM) with an improved chaotic behaviour cryptosystem is proposed. Chaotic series generated by proposed 2D-CCM is utilized in confusion and diffusion architecture. A new dynamic Bit-Flipping in diffusion approach increases the complexity to achieve good encryption results. Simulation and security level assessment is carried out by executing Statistical and differential attack analysis, key sensitivity and exhaustive attack analysis. Robustness is evidenced by the result of noise and crop attack analysis. In addition, SHA-256 is utilized to feed the seed key value of 2D-CCM to resist plaintext attacks. All the assessment and comparison results illustrate that the cipher possesses enhanced security and efficiency than the existing state of the art. Hence the proposed cipher is absolutely apt for secure medical image communication. 
id1177#What is the state of the art of computer vision-assisted cytology? A Systematic Literature Review#Cytology is a low-cost and non-invasive diagnostic procedure employed to support the diagnosis of a broad range of pathologies. Cells are harvested from tissues by aspiration or scraping, and it is still predominantly performed manually by medical or laboratory professionals extensively trained for this purpose. It is a time-consuming and repetitive process where many diagnostic criteria are subjective and vulnerable to human interpretation. Computer Vision technologies, by automatically generating quantitative and objective descriptions of examinations’ contents, can help minimize the chances of misdiagnoses and shorten the time required for analysis. To identify the state-of-art of computer vision techniques currently applied to cytology, we conducted a Systematic Literature Review, searching for approaches for the segmentation, detection, quantification, and classification of cells and organelles using computer vision on cytology slides. We analyzed papers published in the last 4 years. The initial search was executed in September 2020 and resulted in 431 articles. After applying the inclusion/exclusion criteria, 157 papers remained, which we analyzed to build a picture of the tendencies and problems present in this research area, highlighting the computer vision methods, staining techniques, evaluation metrics, and the availability of the used datasets and computer code. As a result, we identified that the most used methods in the analyzed works are deep learning-based (70 papers), while fewer works employ classic computer vision only (101 papers). The most recurrent metric used for classification and object detection was the accuracy (33 papers and 5 papers), while for segmentation it was the Dice Similarity Coefficient (38 papers). Regarding staining techniques, Papanicolaou was the most employed one (130 papers), followed by H&E (20 papers) and Feulgen (5 papers). Twelve of the datasets used in the papers are publicly available, with the DTU/Herlev dataset being the most used one. We conclude that there still is a lack of high-quality datasets for many types of stains and most of the works are not mature enough to be applied in a daily clinical diagnostic routine. We also identified a growing tendency towards adopting deep learning-based approaches as the methods of choice. 
id1178#Conflict-Free Replicated Relations for Multi-Synchronous Database Management at Edge#In a cloud-edge environment, edge devices may not always be connected to the network. Still, applications may need to access the data on edge devices even when they are not connected. With support for multi-synchronous access, data on an edge device are kept synchronous with the data in the cloud as long as the device is online. When the device is offline, the application can still access the data on the device, asynchronously with concurrent data updates either in the cloud or on other edge devices. Conflict-free Replicated Data Types (CRDTs) emerged as a technology for multi-synchronous data access. CRDTs guarantee that when all sites have applied the same set of updates, the replicated data converge. However, CRDTs have not been successfully applied to relational databases (RDBs) for multi-synchronous access. In this paper, we present Conflict-free Replicated Relations (CRRs) that apply CRDTs to RDBs for support of multi-synchronous data access. With CRR, existing RDB applications, with very little modification, can be enhanced with multi-synchronous access. We also present a prototype implementation of CRR with some preliminary performance results. 
id1180#SQL User Queries Execution Model#Here, a Generalized Net model is developed, modelling the processes of utilization of relational databases, generalizing the execution of the main types of SQL queries. On the basis of the created model and accumulated statistical data from real life processes, evaluations can be made and solutions can be sought of problems arising in servicing the processes. Additional model parameters can be introduced or new tokens’ characteristics can be formulated by taking consideration of various factors that affect the process. 
id1182#Queue Time Estimation in Checkout Counters Using Computer Vision and Deep Neural Network#In this busy world, everyone wants to make decisions that are more efficient and save their time and not cause/ incur them loss. Sometimes they even get skeptical about making the decision. A similar situation arises when we are required to decide which queue, we should join for billing of the items during the weekend rush in a posh supermarket. To crack this problem, this paper recommends using Computer Vision and Deep Learning Algorithms to estimate the time closely, a customer will take to reach the counter if they join a particular queue. The algorithm will estimate the number of items present in the trolley or the basket and the number of such trolleys in the queue. The next thing the program will measure is the employee's efficiency who is present in the checkout/billing counter in terms of scanning the items, accepting the payment, and handing over the bill. This will help the customers to make better decisions and save time; it will also make the supermarket look less crowded and use all the counters/resources effectively. 
id1183#Protection of visual privacy in videos acquired with RGB cameras for active and assisted living applications#Active and assisted living technologies are much needed, but some aspects of them cause user rejection due to concerns on privacy. This is even more concerning to users when visual information is used, processed, and transmitted. To respond to these concerns, and maximise user acceptance, visual privacy protection measures have to be put in place. In the past, human detection and object segmentation in video were constrained by technological limitations, and could only run with specific hardware and sensors. This paper introduces a proposal for an RGB-only based visual privacy preservation filter, which capitalises on ‘deep learning’-based segmentation and pose detectors. A background update scheme is presented, which limits leakage of sensitive information when detection fails. Dilation of the mask can further prevent information leakage, but a trade-off is necessary to correctly update background information. This is achieved via a specific study which is also presented. A comparative study is performed to determine the best configuration for privacy preservation. Results show that union of dilated masks from different deep networks achieves the best overall result. 
id1184#Integral Reinforcement Learning-Based Multi-Robot Minimum Time-Energy Path Planning Subject to Collision Avoidance and Unknown Environmental Disturbances#In this letter, we study the online multi-robot minimum time-energy path planning problem subject to collision avoidance and input constraints in an unknown environment. We develop an online adaptive solution for the problem using integral reinforcement learning (IRL). This is achieved through transforming the finite-horizon minimum time-energy problem with input constraints to an approximate infinite-horizon optimal control problem. To achieve collision avoidance, we incorporate artificial potential fields into the approximate cost function. We develop an IRL-based optimal control strategy and prove its convergence. The theoretical results are verified through simulation studies. 
id1185#Metadata-Based Ontological Framework for Semantic Query in Multilingual Databases#Translation of a natural language query into a semantically sound and semantically meaningful structured query and mapping it onto a target database is a challenge for the interactive application designers. The realization of such a complex translation scenario requires a systematic approach with an end-to-end capability. The proposed deep learning-based ontological framework for semantic query in multilingual databases (DLOMLD) receives a natural language query from the user and maps that onto a semantically meaningful structured query using generative adversarial network and fires the query on an identified database. The DLOMLD possesses salient features such as the language translation capabilities, extraction of metadata from underlying relational databases and utilization of established ontological predicates. The functional merits of the DLOMLD are demonstrated while taking education domain as a case study, in a top-down manner elaborating the negotiations between two consecutive layers. 
id1186#Toward a Graph-Based Dependence Analysis Framework for High Level Design Verification#Recent efforts to deploy FPGA's and application-specific accelerator devices in scalable data center environments has led to a resurgence in research associated with high level synthesis and design verification. The goal of this research has been to accelerate the initial design, verification and deployment process for abstract accelerator platforms. While the research associated with high level synthesis flows has provided significant gains in design acceleration, research in the verification of these designs has largely been based upon augmenting traditional methodologies. This work introduces the CoreGen high level design verification infrastructure. The goal of the CoreGen infrastructure is to provide a rapid, high level design verification infrastructure for complex, heterogeneous hardware architectures. Unlike traditional high-level verification strategies, CoreGen utilizes an intermediate representation (IR) for the target design constructed using a directed acyclic graph (DAG). CoreGen then applies classic compiler dependence analysis techniques using a multitude of graph inference and combinatorial logic solvers. The application of traditional compiler dependence analysis using directed acyclic graphs provides the ability to optimize the performance of the high level verification pipeline regardless of the target design complexity. We highlight this capability by demonstrating the verification performance scaling using a complex, heterogeneous design input. Our results indicate performance competitive with traditional optimizing compilers. 
id1187#How to use indistinguishability obfuscation: Deniable encryption, and more#"We introduce a new technique, that we call punctured programs, to apply indis- tinguishability obfuscation towards cryptographic problems. We use this technique to carry out a systematic study of the applicability of indistinguishability obfuscation to a variety of cryptographic goals. Along the way, we resolve the 16-year-old open question of deniable encryption, posed by Canetti et al. in 1997: In deniable encryption, a sender who is forced to reveal to an adversary both her message and the randomness she used for encrypting it should be able to convincingly provide \fake""randomness that can explain any alternative message that she would like to pretend that she sent. We resolve this question by giving the first construction of deniable encryption that does not require any preplanning by the party that must later issue a denial. In addition, we show the general- ity of our punctured programs technique by also constructing a variety of core cryptographic objects from indistinguishability obfuscation and one-way functions (or close variants). In particular we obtain public-key encryption, short \hash-and-sign""selectively secure signatures, chosen-ciphertext secure public-key encryption, noninteractive zero knowledge arguments and injective trapdoor func- tions. These results suggest the possibility of indistinguishability obfuscation becoming a \central hub""for cryptography. "
id1188#Benchmarking a distributed database design that supports patient cohort identification#In this article we present the implementation and benchmarking of a medical information system on top of a distributed relational database system. We enhanced a distributed database system with the implementation of a clustering (based on similarity of disease terms) that induces a primary horizontal fragmentation of a data table and derived fragmentations of secondary tables. With our clustering-based fragmentation, data locality for similarity-based query answering is ensured so that data do not have to be sent unnecessarily over the network. In our benchmark we show that we achieve a significant efficiency gain when retrieving all relevant related answers. 
id1189#Function Block-Based Multimodal Control for Symbiotic Human-Robot Collaborative Assembly#In human-robot collaborative assembly, robots are often required to dynamically change their preplanned tasks to collaborate with human operators in close proximity. One essential requirement of such an environment is enhanced flexibility and adaptability, as well as reduced effort on the conventional (re)programming of robots, especially for complex assembly tasks. However, the robots used today are controlled by rigid native codes that cannot support efficient human-robot collaboration. To solve such challenges, this article presents a novel function block-enabled multimodal control approach for symbiotic human-robot collaborative assembly. Within the context, event-driven function blocks as reusable functional modules embedded with smart algorithms are used for the encapsulation of assembly feature-based tasks/processes and control commands that are transferred to the controller of robots for execution. Then, multimodal control commands in the form of sensorless haptics, gestures, and voices serve as the inputs of the function blocks to trigger task execution and human-centered robot control within a safe human-robot collaborative environment. Finally, the performed processes of the method are experimentally validated by a case study in an assembly work cell on assisting the operator during the collaborative assembly. This unique combination facilitates programming-free robot control and the implementation of the multimodal symbiotic human-robot collaborative assembly with the enhanced adaptability and flexibility. 
id1190#Quantifying social distancing compliance and the effects of behavioral interventions using computer vision#Social distancing has become a pressing and challenging issue during the Covid-19 pandemic. In a smart cities context, it becomes possible to measure inter-personal distance using networked cameras and computer vision analysis. We deploy a computer vision pipeline based on Retinanet that identifies pedestrians in streaming video frames, then converts their positions to GPS coordinates for distance calculation and further analysis. This processing is applied to nine camera streams at three locations from around Vanderbilt University. We collect 70 hours of baseline distancing data over the course of two weeks, after which time we deploy small behavioral interventions at the three locations aimed at increasing distancing compliance. Another 70 hours of data with the interventions in place will be analyzed against the baseline data to determine if they had an effect on distancing compliance. 
id1192#Area and power efficient post-quantum cryptosystem for IoT resource-constrained devices#Internet of Things (IoT) connects a myriad of small devices over a huge network, encompassing many different and varied applications and environments. As the IoT network continues to grow, providing end-to-end security over IoT is becoming a paramount issue. To mitigate existing and future security risks within IoT, two important factors should be considered. First, some resource-constrained edge devices have an insufficient area to contain the security part. Second, the advent of quantum computers threatens the security of current public-key cryptography algorithms. In response to these challenges, lattice-based cryptography (LBC) has emerged as a promising technique for IoT security in the quantum era. The feasibility of LBC integration onto resource-constrained devices has been demonstrated in previous research. Multiplication is the main operation in Ring-BinLWE, a type of LBC. In this paper, a new multiplication method is proposed, which is called In-place modular Reduction and anti-circular Rotation Column-based Multiplication (In-place Rot-Col-Mul), and new Ring-BinLWE architecture is designed. In-place Rot-Col-Mul performs a column-based multiplication in which one rotation is executed per cycle. The design was implemented on TSMC-65nm technology and FPGA platforms. ASIC implementation results show a respective improvement in power and area over the state-of-the-art design by 48.42% and 57.8%, respectively. 
id1193#Closed-Loop Position Control for Growing Robots Via Online Jacobian Corrections#Growing continuum robots offer improved safety and navigation in confined spaces, due to their inherent compliance and ability to conform to highly curved paths. Navigation of these, and other continuum robots, often involves frequent interactions with the surrounding environment, which are typically unknown and can affect the kinematics of the robot. We propose here an approach for controlling continuum robots based on leveraging real-time position and orientation measurements. This pose information is used to update a local model of the robot's velocity kinematics in an online manner via corrective rotations and magnitude adjustments. We combine the proposed control approach with a method for localizing the tip of a growing robot and evaluate the performance of closed-loop position control on a point-reaching task in unconstrained and constrained environments. The closed-loop system achieves an average total error of 3.22$\pm$1.31 mm in the unconstrained case and 4.56$\pm$1.56 mm in the constrained case, validating our proposed approach. This work also represents the first method for autonomous position control of growing robots that does not require a map of its environment. 
id1195#Comparing lifetime learning methods for morphologically evolving robots#The joint evolution of morphologies and controllers of robots leads to a problem: Even if the parents have well-matching bodies and brains, the stochastic recombination can break this match and cause a body-brain mismatch in their offspring. This can be mitigated by having newborn robots perform a learning process that optimizes their inherited brain quickly after birth. An adequate learning method should work on all possible robot morphologies and be efficient. In this paper we apply Bayesian Optimization and Differential Evolution as learning algorithms and compare them on a test suite of different robot bodies. 
id1196#An uncertainty-focused database approach to extract spatiotemporal trends from qualitative and discontinuous lake-status histories#Changes in lake status are often interpreted as palaeoclimate indicators due to their dependence on precipitation and evaporation. The Global Lake Status Database (GLSDB) has since long provided a standardised synopsis of qualitative lake status over the last 30,000 14C years. Potential sources of uncertainty however are not recorded in the GLSDB. Here we present an updated and improved relational-database framework that incorporates uncertainty in both chronology and the interpretation of palaeoenvironmental data. The database uses peer-reviewed palaeolimnological studies to produce a consensus on qualitative lake-status histories, whose chronologies are revised and standardized through the recalibration of radiocarbon dates and the application of Bayesian age-depth modelling for stratigraphic archives. Quantitative information on absolute water-level elevation is preserved if available from geomorphological sources. We also propose a new probabilistic analytical framework that accounts for these uncertainties to reconstruct synoptic, integrated environmental signals. The process is based on a Monte Carlo algorithm that iteratively samples individual lake-status histories within the limits of their uncertainties to produce many possible scenarios. We then use Recursively-Subtracted Empirical Orthogonal Function analysis to extract dominant patterns of lake-status variability from these scenarios. As a proof of concept, we apply this framework to 67 sites in eastern and southern Africa whose lake-status histories cover part of the late Pleistocene and/or Holocene. We show that, despite the sometimes large temporal and interpretation uncertainties, and the inclusion of highly discontinuous lake-status time series, identifying the major known millennial-scale climatic phases during the last 20,000 years is possible. Our framework was also able to identify an antiphased response between the lake basins in eastern and interior southern Africa to these changes. We propose that our new database and methodology framework serves as a template for efficient lake-status data synthesis, encourages the incorporation of lake-status data in more palaeoclimate syntheses, and expands the possibilities for the use of such data in the evaluation of climate models. 
id1197#WaveSleepNet: An interpretable deep convolutional neural network for the continuous classification of mouse sleep and wake#Background: Recent advancement in deep learning provides a pivotal opportunity to potentially supplement or supplant the limiting step of manual sleep scoring. New method: In this paper, we characterize the WaveSleepNet (WSN), a deep convolutional neural network (CNN) that uses wavelet transformed images of mouse EEG/EMG signals to autoscore sleep and wake. Results: WSN achieves an epoch by epoch mean accuracy of 0.86 and mean F1 score of 0.82 compared to manual scoring by a human expert. In mice experiencing mechanically induced sleep fragmentation, an overall epoch by epoch mean accuracy of 0.80 is achieved by WSN and classification of non-REM (NREM) sleep is not compromised, but the high level of sleep fragmentation results in WSN having greater difficulty differentiating REM from NREM sleep. We also find that WSN achieves similar levels of accuracy on an independent dataset of externally acquired EEG/EMG recordings with an overall epoch by epoch accuracy of 0.91. We also compared conventional summary sleep metrics in mice sleeping ad libitum. WSN systematically biases sleep fragmentation metrics of bout number and bout length leading to an overestimated degree of sleep fragmentation. Comparison with existing methods: In a cross-validation, WSN has a greater macro and stage-specific accuracy compared to a conventional random forest classifier. Examining the WSN, we find that it automatically learns spectral features consistent with manual scoring criteria that are used to define each class. Conclusion: These results suggest to us that WSN is capable of learning visually agreeable features and may be useful as a supplement to human manual scoring. 
id1199#Schema-based JSON data stores in relational databases#JSON is a simple, compact and light weighted data exchange format to communicate between web services and client applications. NoSQL document stores evolve with the popularity of JSON, which can support JSON schema-less storage, reduce cost, and facilitate quick development. However, NoSQL still lacks standard query language and supports eventually consistent BASE transaction model rather than the ACID transaction model. This is very challenging and a burden on the developer. The relational database management systems (RDBMS) support JSON in binary format with SQL functions (also known as SQL/JSON). However, these functions are not standardized yet and vary across vendors along with different limitations and complexities. More importantly, complex searches, partial updates, composite queries, and analyses are cumbersome and time consuming in SQL/JSON compared to standard SQL operations. It is essential to integrate JSON into databases that use standard SQL features, support ACID transactional models, and has the capability of managing and organizing data efficiently. In this article, we empower JSON to use relational databases for analysis and complex queries. The authors reveal that the descriptive nature of the JSON schema can be utilized to create a relational schema for the storage of the JSON document. Then, the powerful SQL features can be used to gain consistency and ACID compatibility for querying JSON instances from the relational schema. This approach will open a gateway to combine the best features of both worlds: the fast development of JSON, consistency of relational model, and efficiency of SQL. Copyright 
id1201#Image inpainting based on deep learning: A review#Image inpainting aims to restore the pixel features of damaged parts in incomplete image and plays a key role in many computer vision tasks. Image inpainting technology based on deep learning is a major current research hotspot. To deeply understand related methods and technologies, this article combs and summarizes the latest research status in this field. Firstly, we summarize inpainting methods of different types of neural network structure based on deep learning, then analyze and study important technical improvement mechanisms. In addition, various algorithms are comprehensively reviewed from the aspects of model network structure and restoration methods. And we select some representative image inpainting methods for comparison and analysis. Finally, the current problems of image inpainting are summarized, and the future development trend and research direction are prospected. 
id1202#A morphology-based radiological image segmentation approach for efficient screening of COVID-19#Computer-aided radiological image interpretation systems can be helpful to reshape the overall workflow of the COVID-19 diagnosis process. This article describes an unsupervised CT scan image segmentation approach. This approach begins by performing a morphological reconstruction operation that is useful to remove the effect of the external disturbances on the infected regions and to locate different regions of interest precisely. The optimal size of the structuring element is selected using the Edge Content-based contrast matrix approach. After performing the opening by using the morphological reconstruction operation, further noise is eliminated using the closing-based morphological reconstruction operation. The original pixel space is restored and the obtained image is divided into some non-overlapping smaller blocks and the mean intensity value for each block is computed that is used as the local threshold value for the binarization purpose. It is preferable to manually determine the range of the infected region. If a region is greater than the upper bound then that region will be considered as an exceptional region and processed separately. Three standard metrics MSE, PSNR, and SSIM are used to quantify the outcomes. Both quantitative and qualitative comparisons prove the efficiency and real-life adaptability of this approach. The proposed approach is evaluated with the help of 400 different images and on average, the proposed approach achieves MSE 307.1888625, PSNR 23.7246505, and SSIM 0.831718459. Moreover, the comparative study shows that the proposed approach outperforms some of the standard methods and obtained results are encouraging to support the battle against the COVID-19. 
id1203#CryptMe: A Cryptographic framework with Steganography for securing data#Data Security has become one of the major concerns in the modern world. As everything is going digital and our data is on the cloud, its security is a major research area. The various suggested method provides better security and more important integrity. However, with more advancement in technology, these proposed methods are getting snooped. This paper proposed an approach of a combination of Steganography, Cryptography, and Transposition cipher technique. In particular, it focuses on the LSB technique in another way i.e., while performing the LSB steganography technique instead of embedding data at LSB of the base image, this method takes MSB of Video Frame and LSB of Stego Image. This combination helps in providing very little distortion and higher quality of the secure image. The value of efficiency of the algorithm would be determined by the PSNR value. 
id1205#AtoCC: Learning environment for teaching theory of automata and formal languages#The learning environment AtoCC is presented to be of use in teaching abstract automata, formal languages, and some of its applications in compiler construction. From a teacher's perspective AtoCC aims to address a broad range of different learning activities forcing the students to actively interact with the subjects being taught.
id1206#Modeling a Symmetrically-Notched Continuum Neurosurgical Robot with Non-Constant Curvature and Superelastic Property#A neurosurgical robot with 3-dimensional (3D) distal manipulation can increase instrument workspace for reaching peripheral regions of intracranial lesions to achieve complete lesion removal and minimal brain manipulation in a keyhole procedure. In this work, we designed a Nitinol-based symmetrically notched continuum robot based on the neurosurgical clinical constraints. A mechanics model is developed for the robot using a second order differential equation that exploits the shearing force formulation based upon the Euler - Bernoulli moment-curvature equation. It affords non-constant curvature analysis, and integrates the piecewise linear material model with three stages that closely approximates the nonlinear superelastic property of Nitinol. A solution was introduced to this complex mechanics model based on the fourth order Runge-Kutta method with variable transformation, and an iterative approach. Comparison with the experimental data using our proposed model shows its high modeling accuracy in terms of the bending angle and tip position for a single symmetric notch bending as well as 2D bending motion of the robot. The significantly improved accuracy over previous models justifies the added complexity of our modeling approach and could better guide the determination of the notch parameters during robot design. 
id1207#Development of a precision 3-row synchronised transplanter#Commercial vegetable crop transplanters currently use several unsynchronised planting units mounted to a common transport frame. The objective of this work was to assess the performance of a new transplanting technology to improve the plant placement accuracy and spatiotemporal planting synchronization across adjacent rows, thus producing a grid-like planting pattern using adjacent vegetable crop transplanters. The feasibility of synchronisation of adjacent transplanting units for vegetable crops was demonstrated using tomato as the target crop. A colour, digital, high-speed computer vision analysis of the motion and dynamics of the plant trajectories of transplanted tomatoes was conducted. The high-speed video analysis led to the design and testing of an improved plant support mechanism to enhance the control and precision of the transplanting of vegetable crops. The absolute deviation values of the final location in the soil were reduced by approximately 25% for both the right planter and left planter compared to those in previous years. These results serve as the fundamental basis for a mechatronic system that can precisely transplant vegetable crops in a grid-like pattern across rows as a critical first step in a systematic approach to fully automated individual plant care. 
id1208#Implementation and Experiment of Join Optimization Algorithm for Inverted Index in an RDBMS#In a relational database management system (RDBMS), when a user searches for a keyword included in a document, a keyword search is attempted using various join methods. When a keyword search is performed using a merge join in an RDBMS that stores a large number of documents, the retrieval time for the query is increased due to unnecessary comparison operations. In this study, we propose a keyword search using a Skip Merge Join that minimizes unnecessary comparison operations when a keyword search is performed using a row direction relational inverse index table. We confirmed the results of improving the keyword search performance by implementing the Skip Merge Join algorithm using the relational database PostgreSQL and verifying it through experiments. 
id1209#Separation of concerns in compiler development using aspect-orientation#A major difficulty in compiler development regards the proper modularization of concerns among the various compiler phases. The traditional object-oriented development paradigm has difficulty in providing an optimal solution towards modularizing the analysis phases of compiler development, because implementation of each phase often crosscuts the class hierarchy defined by language syntax constructs. Object-oriented design patterns, such as the Visitor pattern, also cannot solve the crosscutting problem adequately because an object is not a natural representation of a collection of operations. This paper demonstrates the benefits of applying aspect-oriented programming languages (e.g., AspectJ) and principles to compiler design and implementation. The experience result shows that the various language constructs in AspectJ (e.g., inter-type declaration, pointcut-advice model, static aspect members and aspect inheritance) fit well with the various computation needs of compiler development, which results in a compiler implementation with improved modularity and better separation of concerns. The ideas utilized in this paper can also be generalized to other software systems with a tree-like structure. Copyright 2006 ACM.
id1210#Error term checking: Towards chosen ciphertext security without re-encryption#Chosen ciphertext security for lattice based encryption schemes is generally achieved through a generic transformation such as the Fujisaki-Okamoto transformation. This method requires full re-encryption of the plaintext during decapsulation, which typically dominates the cost of the latter procedure. In this work we show that it is possible to develop alternative transformations specifically designed for lattice based encryption schemes. We propose two novel chosen ciphertext transformations, ETC1 and ETC2, in which re-encryption is replaced by checking the error term of the input ciphertext. We show that our new ciphertext validity check can be securely applied to lattice based encryption schemes under specific conditions. For the NIST post-quantum standardization candidate Threebears we show a speed-up for decapsulation of up to 37.4%. Moreover, as our method only changes the validation check during decapsulation, it is fully backwards compatible with existing implementations of the Fujisaki-Okamoto transformation. 
id1213#Multi-scale fully convolutional neural networks for histopathology image segmentation: From nuclear aberrations to the global tissue architecture#Histopathologic diagnosis relies on simultaneous integration of information from a broad range of scales, ranging from nuclear aberrations (≈O(0.1μm)) through cellular structures (≈O(10μm)) to the global tissue architecture (⪆O(1mm)). To explicitly mimic how human pathologists combine multi-scale information, we introduce a family of multi-encoder fully-convolutional neural networks with deep fusion. We present a simple block for merging model paths with differing spatial scales in a spatial relationship-preserving fashion, which can readily be included in standard encoder-decoder networks. Additionally, a context classification gate block is proposed as an alternative for the incorporation of global context. Our experiments were performed on three publicly available whole-slide images of recent challenges (PAIP 2019: hepatocellular carcinoma segmentation; BACH 2020: breast cancer segmentation; CAMELYON 2016: metastasis detection in lymph nodes). The multi-scale architectures consistently outperformed the baseline single-scale U-Nets by a large margin. They benefit from local as well as global context and particularly a combination of both. If feature maps from different scales are fused, doing so in a manner preserving spatial relationships was found to be beneficial. Deep guidance by a context classification loss appeared to improve model training at low computational costs. All multi-scale models had a reduced GPU memory footprint compared to ensembles of individual U-Nets trained on different image scales. Additional path fusions were shown to be possible at low computational cost, opening up possibilities for further, systematic and task-specific architecture optimisation. The findings demonstrate the potential of the presented family of human-inspired, end-to-end trainable, multi-scale multi-encoder fully-convolutional neural networks to improve deep histopathologic diagnosis by extensive integration of largely different spatial scales. 
id1214#Transactional memory#The advent of multicore processors has renewed interest in the idea of incorporating transactions into the programming model used to write parallel programs. This approach, known as transactional memory, offers an alternative, and hopefully better, way to coordinate concurrent threads. The ACI (atomicity, consistency, isolation) properties of transactions provide a foundation to ensure that concurrent reads and writes of shared data do not produce inconsistent or incorrect results. At a higher level, a computation wrapped in a transaction executes atomically - either it completes successfully and commits its result in its entirety or it aborts. In addition, isolation ensures the transaction produces the same result as if no other transactions were executing concurrently. Although transactions are not a parallel programming panacea, they shift much of the burden of synchronizing and coordinating parallel computations from a programmer to a compiler, runtime system, and hardware. The challenge for the system implementers is to build an efficient transactional memory infrastructure. This book presents an overview of the state of the art in the design and implementation of transactional memory systems, as of early summer 2006.
id1215#SDN Enabled QoE and Security Framework for Multimedia Applications in 5G Networks#The technologies for real-time multimedia transmission and immersive 3D gaming applications are rapidly emerging, posing challenges in terms of performance, security, authentication, data privacy, and encoding. The communication channel for these multimedia applications must be secure and reliable from network attack vectors and data-contents must employ strong encryption to preserve privacy and confidentiality. Towards delivering secure multimedia application environment for 5G networks, we propose an SDN/NFV (Software-Defined-Networking/Network-Function-Virtualization) framework called STREK, which attempts to deliver highly adaptable Quality-of-Experience (QoE), Security, and Authentication functions for multi-domain Cloud to Edge networks. The STREK architecture consists of a holistic SDNFV dataplane, NFV service-chaining and network slicing, a lightweight adaptable hybrid cipher scheme called TREK, and an open RESTful API for applications to deploy custom policies at runtime for multimedia services. For multi-domain/small-cell deployments, the key-generation scheme is dynamic at flow/session-level, and the handover authentication scheme uses a novel method to exchange security credentials with the Access Points (APs) of neighborhood cells. This scheme is designed to improve authentication function during handover with low overhead, delivering the 5G ultra-low latency requirements. We present the experiments with both software and hardware-based implementations and compare our solution with popular lightweight cryptographic solutions, standard open source software, and SDN-based research proposals for 5G multimedia. In the microbenchmarks, STREK achieves smaller hardware, low overhead, low computation, higher attack resistance, and offers better network performance for multimedia streaming applications. In real-time multimedia use-cases, STREK shows greater level of quality distortion for multimedia contents with minimal encryption bitrate overhead to deliver data confidentiality, immunity to common cryptanalysis, and significant resistance to communication channel attacks, in the context of low-latency 5G networks. 
id1216#Certified undecidability of intuitionistic linear logic via binary stack machines and Minsky machines#We formally prove the undecidability of entailment in intuitionistic linear logic in Coq. We reduce the Post correspondence problem (PCP) via binary stack machines and Minsky machines to intuitionistic linear logic. The reductions rely on several technically involved formalisations, amongst them a binary stack machine simulator for PCP, a verified low-level compiler for instruction-based languages and a soundness proof for intuitionistic linear logic with respect to trivial phase semantics. We exploit the computability of all functions definable in constructive type theory and thus do not have to rely on a concrete model of computation, enabling the reduction proofs to focus on correctness properties. 
id1217#Data-Driven Energy Estimation of Individual Instructions in User-Defined Robot Programs for Collaborative Robots#Traditionally, the objective of industrial production focuses on fast and low-cost production, regardless of resources and energy consumption (EC). However, in alignment with the UN Sustainable Development Goals (SDG), governments worldwide have proposed regulations to reduce resources and energy. In their production lines, an increasing number of companies are using collaborative robots (cobots). Cobots are programmed to accomplish their task as fast as possible, ignoring the robot's EC. This letter estimates the cobot EC from individual instructions of user-defined robot programs. Thus, the user has an additional design parameter to create energy-optimal programs. In the literature, current EC estimation models for manipulators are not reliable or have not been assessed to test the model's reliability. Our modeling methodology possesses three steps: motion planning, dynamic model, and EC model. Using cobots of different sizes (UR3e and UR10e) and loading, we collected over 55000 samples per case and trained the model to identify the model's unknown parameters. The model estimated the power consumption of a testing dataset with a maximum RMS error of 6 [W] - 3.85%. In the final experiment, the complete system was tested using a user-defined program composed of six instructions. The results showed an accurate estimation of the power profile with an RMS error of 2.39 [W] and 4.23 [W] for UR3e and UR10e. 
id1218#Can social robots affect children's prosocial behavior? An experimental study on prosocial robot models#The aim of this study was to investigate whether a social robot that models prosocial behavior (in terms of giving away stickers) influences the occurrence of prosocial behavior among children as well as the extent to which children behave prosocially. Additionally, we investigated whether the occurrence and extent of children's prosocial behavior changed when being repeated and whether the behavior modeled by the robot affected children's norms of prosocial behavior. In a one-factorial experiment (weakly prosocial robot vs. strongly prosocial robot), 61 children aged 8 to 10 and a social robot alternately played four rounds of a game against a computer and, after each round, could decide to give away stickers. Children who saw a strongly prosocial robot gave away more stickers than children who saw a weakly prosocial robot. A strongly prosocial robot also increased children's perception of how many other children engage in prosocial behavior (i.e., descriptive norms). The strongly prosocial robot affected the occurrence of prosocial behavior only in the first round, whereas its effect on the extent of children's prosocial behavior was most distinct in the last round. Our study suggests that the principles of social learning also apply to whether children learn prosocial behavior from robots. 
id1219#Construction of stretching‐bending sequential pattern to recognize work cycles for earthmoving excavator from long video sequences#Counting the number of work cycles per unit of time of earthmoving excavators is es-sential in order to calculate their productivity in earthmoving projects. The existing methods based on computer vision (CV) find it difficult to recognize the work cycles of earthmoving excavators effectively in long video sequences. Even the most advanced sequential pattern‐based approach finds recognition difficult because it has to discern many atomic actions with a similar visual ap-pearance. In this paper, we combine atomic actions with a similar visual appearance to build a stretching–bending sequential pattern (SBSP) containing only “Stretching” and “Bending” atomic actions. These two atomic actions are recognized using a deep learning‐based single‐shot detector (SSD). The intersection over union (IOU) is used to associate atomic actions to recognize the work cycle. In addition, we consider the impact of reality factors (such as driver misoperation) on work cycle recognition, which has been neglected in existing studies. We propose to use the time re-quired to transform “Stretching” to “Bending” in the work cycle to filter out abnormal work cycles caused by driver misoperation. A case study is used to evaluate the proposed method. The results show that SBSP can effectively recognize the work cycles of earthmoving excavators in real time in long video sequences and has the ability to calculate the productivity of earthmoving excavators accurately. 
id1220#Cloud Security using Hybrid Cryptography Algorithms#Security in cloud computing is the emerging research issues nowadays. A lot of organization are moving from their traditional data storage to cloud storage, which provides an efficient way to access the data anywhere and anytime. But, the main hitch of organization to use cloud computing is data security. This paper proposed a multilevel cryptography based security system for cloud computing. The model is hybrid approach of symmetric and asymmetric key cryptography algorithms. In this Data Encryption Standard (DES) and RSA are implemented to provide the multilevel of encryption and decryption at both sender and receiver side which increase the security of the cloud storage. This security model gives the transparency to the cloud user as well as cloud service provider in order to reduce the security threats. The proposed model is implemented in java and cloudsim cloud simulator tool. This model increases the data security up to a maximum extant and it takes less time in uploading and downloading the text file as compare to existing system. 
id1222#Fast and Safe Trajectory Planning: Solving the Cobot Performance/Safety Trade-Off in Human-Robot Shared Environments#The rise of collaborative robotics has offered new opportunities for integrating automation into the factories, allowing robots and humans to work side-by-side. However, this close physical coexistence inevitably brings new constraints for ensuring safe human-robot cooperation. The current paramount challenge is integrating human safety constraints without compromising the robotic performance goals, which require minimization of the task execution time alongside ensuring its accomplishment. This letter proposes a novel robot trajectory planning algorithm to produce minimum-time yet safe motion plans along specified paths in shared workspaces with humans. To this end, a safety module was used to evaluate the safety of a time-optimal trajectory iteratively. A safe replanning module was developed to optimally adapt the generated trajectory online whenever the optimal plan violates dynamically provided safety limits. In order to preserve performance, a recovery trajectory planning algorithm was included such that the robot is allowed to restore higher speed motions as soon as the safety concern has been resolved. The proposed solution's effectiveness was evaluated both in simulations and real experiments with two robotic manipulators. 
id1224#Automated mapping from an IFC data model to a relational database model [IFC数据模型至关系型数据库模型的自动映射]#Building information modelling (BIM) is important in the construction industry, while industry foundation classes (IFC), an international data exchange model, are widely used for BIM research and applications. However, the limitations of IFC complicate the use of IFC as the main data format for redevelopment projects. Therefore, an automatic mapping method is developed to map an IFC data model to a relational database model. The IFC data model mapping has mapping rules for entity, type, function, rule, property set, and quantity set for the data analysis, data mapping, and information retrieval. Two tests are conducted to evaluate the validity of this method. The results show that the relational database provides better information integrity and data management than the IFC files with the mapping improving the integrity and efficiency of the BIM data management and also promoting the use of intelligent construction methods as well as the development of informatization in the construction industry. 
id1225#Energy-efficient dynamic homomorphic security scheme for fog computing in IoT networks#Recently, there is an exponential increase in the multimedia and other data over the Internet of Things (IoT). This data is generally send to the cloud for processing and storage. The fog layer in-between readily bridges communication among the IoT devices and the cloud. It delivers services efficiently by computing and analyzing various multimedia information generated by the IoT devices residing on the sensors. However, provision of effective security and energy are critical challenges. The purpose of this work is to enhance the secure transfer of information like multimedia. This scheme uses Message Queue Telemetry Transport (MQTT) protocol over SSL/TLS. Since MQTT is vulnerable to eavesdropping, the Elliptic curve-ElGamal cryptography algorithm is introduced which lends a homomorphic factor thereby mitigating man-in-the-middle attack. The dynamic key change and proportional offloading of data as proposed in the current research work helps to preserve node energy by selectively transferring data to the cloud and the fog according to the data topic. The results depict that the system security and lifetime can be improved in comparison to the existing protocols. 
id1226#Speculative query execution in RDBMS based on analysis of query stream multigraphs#The paper presents an insight into a speculative execution model of queries in RDBMS based on the analysis of the stream of current queries appearing at the database input. A specific multigraph representation of input query stream is created and used to determine the speculative queries for execution. A group of worker threads execute the chosen speculative queries in parallel with the execution of the standard input stream of user queries. The obtained speculative results are then used to support faster query execution. First, the paper briefly reminds the assumed graph modelling and analysis methods. Then, additional rules are presented which enable combining results of multiple speculative queries in execution of a single user input query. The quality of executed and used speculations is then analysed based on the defined quality metrics and structural details of speculative queries. Conclusions from this analysis are used to modify the selection method of target queries for speculative execution. It aims at intensification of the use of multiple speculative query results and further reduction of the user query execution time. Experimental results are presented in a multi-Threaded speculative experimental environment cooperating with a SQLite database. They show that with the improved algorithm we can obtain more varied speculative query results, and thus, more intensive use of multiple speculative query results by the stream of user queries sent to the database. 
id1228#Doe-slam: Dynamic object enhanced visual slam#In this paper, we formulate a novel strategy to adapt monocular-vision-based simultaneous localization and mapping (vSLAM) to dynamic environments. When enough background features can be captured, our system not only tracks the camera trajectory based on static background features but also estimates the foreground object motion from object features. In cases when a moving object obstructs too many background features for successful camera tracking from the background, our system can exploit the features from the object and the prediction of the object motion to estimate the camera pose. We use various synthetic and real-world test scenarios and the well-known TUM sequences to evaluate the capabilities of our system. The experiments show that we achieve higher pose estimation accuracy and robustness over state-of-the-art monocular vSLAM systems. 
id1229#Compiler management of communication and parallelism for quantum computation#Quantum computing (QC) offers huge promise to accelerate a range of computationally intensive benchmarks. Quantum computing is limited, however, by the challenges of decoherence: i.e., a quantum state can only be maintained for short windows of time before it decoheres. While quantum error correction codes can protect against decoherence, fast execution time is the best defense against decoherence, so efficient architectures and effective scheduling algorithms are necessary. This paper proposes the Multi-SIMD QC architecture and then proposes and evaluates effective schedulers to map benchmark descriptions onto Multi-SIMD architectures. The Multi-SIMD model consists of a small number of SIMD regions, each of which may support operations on up to thousands of qubits per cycle. Efficient Multi-SIMD operation requires efficient scheduling. This work develops schedulers to reduce communication requirements of qubits between operating regions, while also improving parallelism. We find that communication to global memory is a dominant cost in QC. We also note that many quantum benchmarks have long serial operation paths (although each operation may be data parallel). To exploit this characteristic, we introduce Longest Path-First Scheduling (LPFS) which pins operations to SIMD regions to keep data in-place and reduce communication to memory. The use of small, local scratchpad memories also further reduces communication. Our results show a 3% to 308% improvement for LPFS over conventional scheduling algorithms, and an additional 3% to 64% improvement using scratchpad memories. Our work is the most comprehensive software-to-quantum toolflow published to date, with efficient and practical scheduling techniques that reduce communication and increase parallelism for full-scale quantum code executing up to a trillion quantum gate operations. Copyright 
id1230#The compiler course in today's curriculum: Three strategies#The broadening of computer science education has called into question the roles of many traditional core courses. In order to remain viable, courses such as compiler construction must provide a coherent view of their subject matter that fits with the rest of the institution's curriculum. Three strategies have evolved for this course. As described in this paper, each strategy provides a model that a professor can use to design an appropriate course for their situation. Copyright 2006 ACM.
id1232#A Performance Evaluation of Correspondence Grouping Methods for 3D Rigid Data Matching#Seeking consistent point-to-point correspondences between 3D rigid data (point clouds, meshes, or depth maps) is a fundamental problem in 3D computer vision. While a number of correspondence selection methods have been proposed in recent years, their advantages and shortcomings remain unclear regarding different applications and perturbations. To fill this gap, this paper gives a comprehensive evaluation of nine state-of-the-art 3D correspondence grouping methods. A good correspondence grouping algorithm is expected to retrieve as many as inliers from initial feature matches, giving a rise in both precision and recall as well as facilitating accurate transformation estimation. Toward this rule, we deploy experiments on three benchmarks with different application contexts, including shape retrieval, 3D object recognition, and point cloud registration. We also investigate various perturbations such as noise, point density variation, clutter, occlusion, partial overlap, different scales of initial correspondences, and different combinations of keypoint detectors and descriptors. The rich variety of application scenarios and nuisances result in different spatial distributions and inlier ratios of initial feature correspondences, thus enabling a thorough evaluation. Based on the outcomes, we give a summary of the traits, merits, and demerits of evaluated approaches and indicate some potential future research directions. 
id1234#Learning Compiler Design: From the Implementation to Theory#In this work, we propose an educational technique that allows to improve the learning of the main theoretical concepts of a Compiler Design course. Instead of starting from the theory, and then explaining to the students how the implementation is obtained in each of the phases of building a compiler, we propose to use as a starting point the implementation obtained by the students through the use of automatic code generation tools. With the help of an Interactive Tutoring System, we guide the learning of the main theoretical concepts from the implementation obtained, deepening and reinforcing their understanding of the theory in relation to the code. In this way, students are able to better relate both parts and apply them together, resulting in a more solid design of language processing tools. As a preliminary evaluation of the described technique, we show the results obtained by the students in the last courses. 
id1235#Equilibrium of redundancy in relational model for optimized data retrieval#Conceptual and relational data models of online transaction processing (OLTP) applications are usually created and maintained following the principle of normalization, which implies avoidance of redundancy. Data retrieval from a disk-based normalized relational database often requires complex and inefficient queries that may cause noticeable performance issues when executed on larger volumes of data. Computer professionals sometimes intentionally trade off the strict normal form to optimize data retrieval queries through error-prone manual tuning and denormalization. We propose a fully automatic optimization approach, based on data redundancy, that relies on a formal cost-benefit model. We prove that finding the optimal level of data redundancy, for given workload statistics, is an NP-Complete optimization problem. A detailed reduction of the problem to binary linear programming is presented in the paper. The proposed optimization approach was evaluated using the TPCE benchmark for OLTP systems. The evaluation has shown that the proposed optimization approach is highly scalable, and that it can be efficiently applied to real-life relational data models. 
id1236#Automatic transformation of owl ontologies to SQL relational databases#Ontology management systems in general do not yet offer a query facility that is as efficient as that of relational database management systems. It will take some time until users will be able to gain query performance comparable to that of relational database management systems. Therefore, relational database management systems are preferable for storing ontologies. Ontologies stored in relational databases can then be queried using SQL. A prerequisite for this storing is transformation of ontologies to relational databases, which is the purpose of this paper. 
id1237#Evaluation of continuous walking speed determination algorithms and embedded sensors for a powered knee & ankle prosthesis#Dynamically altering the parameters for assistance in a lower limb prosthesis is a challenge that depends directly on the ability to estimate gait parameters. Machine learning algorithms present an opportunity to develop methods for continuously determining walking speed in different conditions. Current state-of-the-art solutions involve using wearable sensors such as IMUs to estimate these parameters. These methods require an entire gait cycle to update the walking speed; this leads to delays when responding to changing speeds and ultimately renders these methods ineffective for adaptation into real-time prosthesis control. In this study, we developed subject dependent and independent machine learning models for rapidly determining walking speed and evaluated on data collected from 6 individuals with unilateral transfemoral amputation walking on our robotic knee/ankle prosthesis. We evaluated the performance of these models across a variety of static walking speeds and dynamic speed trials. Our findings suggest that using machine learning models offers excellent accuracy for both subject dependent and subject independent algorithms (DEP RMSE: 0.014 ± 0.001 m/s, IND RMSE: 0.070 ± 0.007 m/s, (p < 0.05), with the advantage of real-time continuous determination at 50 Hz, which allows for good performance when rapidly changing walking speed. We also determine the most effective sensors to use for improving model performance. Our study provides valuable information for determining walking speed more reliably across different users and is robust to dynamic changes experienced in gait. 
id1238#Countermeasure to protect sensitive information stealthily in remote interchanges using steganography in educational environments#Nowadays, students of electronics, electrical and computer science engineering are very much curious to know about the various security issues in wireless communication. Therefore, these students require a platform (i.e. by using Matlab software) that can fetch knowledge about the security issues in wireless communication. The Matlab software will give a perfect understanding about the mathematical concepts through a graphical representation. Therefore, students can understand the typical mathematical equation very easily. The main objective of our paper is to create a countermeasure environment that would protect the sensitive information passing through the multi-input and multi-output (MIMO)-OFDM channel. A novel remote steganography method is proposed, and the essential execution of the system was assessed. In this research work, a remote steganography utilizing the MIMO-OFDM structure, the sensitive information processing and also an Eigenbeam-space Division Multiplexing conveyance are proposed. Based on the Matlab simulation results, our proposed technique aims to reduce the bit error rate. In addition, SNR values for different modulation techniques successfully retrieved from the original information are analyzed. And finally, we make two project groups: one is related to the cryptography method and the second group is related to combining the cryptography and steganography methods, and we differentiate which group information is more secure in the wireless communication platform. 
id1239#Human Action Recognition Methods Based on CNNs for RGB Video Input#Human Action recognition is a complex problem that attracts more and more researchers from the scientific community due to its applicability in domains such as security and behavior analysis. At its core, this problem entails classifying an action into a finite set of classes. Neural network based approaches, and especially convolutional neural networks, are a good starting point for solving the problem of human action recognition. Due to their nature, they can recognize spatio-temporal features very well, making them ideal for working with sequences of RGB images. In this paper are proposed three types of convolutional neural network architectures that contribute to solving the problem of Human Action Recognition. The first one is based on 2D kernels, the second one on 3D kernels, and the third one on TCN (Temporal Convolutional Network) units. Each one is presented with its structure, advantages and disadvantages, along with metrics that measure their performance. The one based on 2D convolutions is the fastest, but it also has the lowest performances. The second one is a good middle ground, useful in certain situations which require a fast classifier operating on different action classes. Finally, the one based on TCNs performs close to some of the best existent models. It represents a viable solution to the proposed problem. It can classify many actions, using only RGB images of fairly low resolution, in real time. The three models have been tested on the RGB part of the NTU RGB+D dataset. The 2D convolution-based model obtained an accuracy of 7.43% on the Cross-Subject split and 10.28% on the Cross-View split. The 3D convolution-based model obtained 58.77% on Cross Subject and 56.11% on Cross-View. Finally, the TCN-based model obtained an accuracy of 80.45% on Cross-Subject and an accuracy of 82.57% on Cross-View. 
id1240#A Set of Tools to Teach Language Processors Construction#Writing a language processor (or a compiler) is a common assignment in Computer Science. It requires knowledge in a lot of areas, such as finite automata, backtracking, rule-based systems, hi-level programming languages structure, assembly languages, microprocessors, etc. Compiler construction is a widely used software engineering exercise, and in our opinion it is a very important phase in computer specialists' education. In this paper, we introduce LP-Tools: a set of applications specially designed for compiler construction educative project development. This set of tools not only facilitates teaching language processors but it also serves as an aid for students to gradually design their own hi-level programming languages. In addition, it provides a soft landing from theory to practice. The final project, every student or a small team has to develop, is an IDE consisting of a source code editor, a compiler, and an intermediate code interpreter. The interface and the main features of LP-Tools are described in the paper. 
id1241#A New Mutual Authentication and Key Agreement Protocol for Mobile Client - Server Environment#Mobile devices are becoming an essential part of many users' lives. Users exchange sometimes very sensitive data with remote servers. This raises a security problem in terms of the confidentiality and integrity of these data, and users' privacy. Mutual authentication protocols allow a user and a server to confirm each other's legitimacy and share a session key to encrypt subsequent communications. Several protocols have been proposed to achieve this goal. However, these have certain weaknesses, such as impersonation, lack of anonymity, the use of additional hardware, and the synchronization problem associated with the use of timestamps. In this paper, we propose a mutual authentication protocol based on elliptic curve cryptography for mobile client - server environments, which addresses the above problems. This protocol is intended to be lightweight as it is designed for resource constrained mobile devices. Moreover, we present a formal and informal analysis of the security of the proposed protocol. This latter has security attributes, such as session key security, perfect forward secrecy, user anonymity, resistance to impersonation, replay and insider attacks. Performance evaluation shows that we outperform similar protocols. Therefore, the proposed protocol is secure, efficient and suitable for mobile environments. 
id1243#Dcfnet++: More advanced correlation filters network for real-time object tracking#Visual object tracking has been widely addressed in Siamese networks, where accurate and fast object tracking can be achieved. However, it is challenging to discriminate foregrounds from semantic backgrounds, because the semantic backgrounds are always considered as distractors, which would hinder the robustness of Siamese trackers. In this paper, we address the visual object tracking problem in complex scenarios, including occlusions, out-of-view, deformation, background cluttering, and other variations. We introduce multiple combinations in the training set to improve the discriminative ability of the learned features. As a result, the robustness of the model can be improved. We also propose an advanced method to fuse multi-layer features, so that the feature representation can be enhanced. Finally, we develop a flow-based tracking method, which makes the tracker very robust to occlusion scenarios. Extensive evaluations on OTB-2015, VOT2018, UAV123, and LaSOT benchmarks demonstrate that the proposed DCFNet++ has strong robustness when facing challenging scenarios. Without bells and whistles, our proposed tracker can run at more than 56 FPS during test time. 
id1244#Computer Vision-based Web Scraping for Internet Forums#With the amount of data available on websites the need to transform this data from a human-understandable format, the visual representation, to a computer-understandable format, e.g. as entries in a database, rises. The approaches to solving web scraping that were published in the last two decades have the drawback that they all to a certain degree rely on the structure and existence of the underlying Hypertext Markup Language (HTML) or Cascading Style Sheets (CSS). To reduce this dependency and move to understanding websites more human-like, this short paper presents a scientific project that proposes a web scraping approach based solely on the visual representation of a given website. For this purpose existing approaches from the domain of Document Layout Analysis and Optical Character Recognition (OCR) are taken into concern. This short paper provides relevant background knowledge to the involved fields of science and proposes a methodology along which the suggested approach can be implemented and tested in further work. 
id1245#Lightweight Security in IoT: A Survey#As the world is moving towards the enhanced connectivity in our day-to-day activities with the handheld devices, PDAs and many other resource constrained devices. The scope for the intrusion into those devices is also increasing rapidly. From days when computing something is not just completed in one machine but on a shared platform with the use of Internet, regardless of time and place. In order to perform such tasks, first the networks of connected devices are formed. As the strength of the network grows in terms of the number of connected devices, it becomes more susceptible to the outside threats. Inadvertently it paves way to access/modify the information stored in the computer. This challenge can be overcome with the use of cryptographic algorithms to transmit/receive or access the data stored in the system. Usually, the normal cryptographic algorithms require lots of computation/resources in the end devices. The problem of unauthorized data access/modification is becoming more worrisome when the end device is not equipped enough to tackle the security intrusion as in the case with the resource constrained devices. Therefore, this paper discusses about the different lightweight algorithms that remain suitable to be used for such resource constrained devices. 
id1246#Algorithmic gaze classification for mobile eye-Tracking#Mobile eye tracking traditionally requires gaze to be coded manually. We introduce an open-source Python package (GazeClassify) that algorithmically annotates mobile eye tracking data for the study of human interactions. Instead of manually identifying objects and identifying if gaze is directed towards an area of interest, computer vision algorithms are used for the identification and segmentation of human bodies. To validate the algorithm, mobile eye tracking data from short combat sport sequences were analyzed. The performance of the algorithm was compared against three manual raters. The algorithm performed with substantial reliability in comparison to the manual raters when it came to annotating which area of interest gaze was closest to. However, the algorithm was more conservative than the manual raters for classifying if gaze was directed towards an object of interest. The algorithmic approach represents a viable and promising means for automating gaze classification for mobile eye tracking. 
id1247#A high-frequency low-cost technique for measuring small-scale water level fluctuations using computer vision#Measuring and monitoring the water depth/level is a key issue when studying free-surface flows. These tasks are usually expensive in terms of time and money; however, even that expenditure is sometimes not enough to assure reliable and/or accurate results. Free-surface flows are complicated to deal with, as the instability caused by e.g., turbulence, wind, or air-entrainment, can cause important spatial and temporal fluctuations at the surface level. This work presents a non-intrusive, computer vision-based image treatment and segmentation technique that assures the detection and measurement of the free-surface water fluctuations along space and time. The laboratory physically based tests under steady flow conditions, and different channel bed roughness and slopes, showed a very good fit with manually direct measurements carried out with a point-gauge micrometre. Under unsteady flow conditions this technique also showed to successfully deal with applications requiring high spatiotemporal resolution of water depth/level measurements. 
id1249#Minimizing the traversing steps in the code generated by OCL 2.0 compilers#Models and model-based software development is one of the most focused research fields. Flexible, highly customizable models are required. Metamodeling is a proven solution for this problem, but the information represented by metamodels has a tendency to be incomplete, informal, imprecise, and sometimes even inconsistent. Object Constraint Language (OCL) is a wide-spread formalism to express model constraints, and it is also useful in graph transformation-based model transformation. There exist several interpreters and compilers that handle OCL constraints in modeling, but these tools do not support constraint optimization, therefore, the model validation is not always efficient. This paper presents efficient algorithms to optimize OCL compilers with respect to the traversing steps of the generated code, and accelerate the validation process by normalizing the OCL constraints. Proofs are also provided to shows that the optimized and the unoptimized code are functionally equivalent.
id1250#A Correlation-Preserving Fingerprinting Technique for Categorical Data in Relational Databases#Fingerprinting is a method of embedding a traceable mark into digital data, to verify the owner and identify the recipient a certain copy of a data set has been released to. This is crucial when releasing data to third parties, especially if it involves a fee, or if the data is of sensitive nature, due to which further sharing and leaks should be discouraged and deterred from. Fingerprinting and watermarking are well explored in the domain of multimedia content, such as images, video, or audio. The domain of relational databases is explored specifically for numerical data types, for which most state-of-art techniques are designed. However, many datasets also, or even exclusively, contain categorical data. We, therefore, propose a novel approach for fingerprinting categorical type of data, focusing on preserving the semantic relations between attributes, and thus limiting the perceptibility of marks, and the effects of the fingerprinting on the data quality and utility. We evaluate the utility, especially for machine learning tasks, as well as the robustness of the fingerprinting scheme, by experiments on benchmark data sets. 
id1251#SmartSORT: an MLP-based method for tracking multiple objects in real-time#With the recent advances in the object detection research field, tracking-by-detection has become the leading paradigm adopted by multi-object tracking algorithms. By extracting different features from detected objects, those algorithms can estimate the similarities and association patterns of objects along with successive frames. However, since similarity functions applied by tracking algorithms are handcrafted, it is difficult to use them in new contexts. In this study, it is investigated the use of artificial neural networks to learning a similarity function that can be used among detections. During training, multilayer perceptron (MLP) neural networks were introduced to correct and incorrect association patterns, sampled from a pedestrian tracking data set. For such, different motion and appearance feature combinations have been explored. Finally, a trained MLP has been inserted into a multiple-object tracking framework, which has been assessed on the MOT Challenge benchmark. Throughout the experiments, the proposed tracker matched the results obtained by state-of-the-art methods by scoring a tracking accuracy of 60.4%, while running 58% faster than DeepSORT, a recent and similar method used as a baseline. After all, this work demonstrates its method can be automatically trained for different tracking contexts and it has highly competitive cost-effectiveness for online real-time tracking applications. 
id1252#COVID-19 Database Management: A Non-relational Approach (NoSQL and XML)#The advancing COVID-19 pandemic caused by the novel coronavirus has taken the world by a storm due to its unprecedented nature. In order to increase the understanding of the disease and create countermeasures for the same, collecting and storing data in a proper and efficient format is of utmost importance. However, this tremendous amount of data is obtained from various heterogeneous sources and is usually dynamic in nature. Traditional RDBMS might not be the most efficient choice for the sporadic and ever-changing clinical data associated with COVID patients due to its highly rigid nature. This paper utilized a primary dataset acquired from COVID-19 patients as a premise to portray the inefficiencies of RDBMS and further proposes two new schemaless, unstructured databases, NoSQL and XML databases, as an offset to this drawback. The intention is to propose the two most efficient technologies and delineate the findings through a sample implementation. 
id1253#Spacecraft relative navigation with an omnidirectional vision sensor#With the onset of autonomous spacecraft formation flying missions, the ability of satellites to autonomously navigate relatively to other space objects has become essential. To implement spacecraft relative navigation, relative measurements should be taken, and processed using relative state estimation. An efficient way to generate such information is by using vision-based measurements. Cameras are passive, low-energy, and information-rich sensors that do not actively interact with other space objects. However, pointing cameras with a conventional field-of-view to other space objects requires much a-priori initialization data; in particular, dedicated attitude maneuvers are needed, which may interfere with the satellite's main mission. One way to overcome these difficulties is to use an omnidirectional vision sensor, which has a 360-degree horizontal field of view. In this work, we present the development of an omnidirectional vision sensor for satellites, which can be used for spacecraft relative navigation, formation flying, and space situational awareness. The study includes the development of the measurement equations, dynamical models, and state estimation algorithms, as well as a numerical study, an experimental investigation, and a space scalability analysis. 
id1254#Synthesizing database programs for schema refactoring#Many programs that interact with a database need to undergo schema refactoring several times during their life cycle. Since this process typically requires making significant changes to the program's implementation, schema refactoring is often non-trivial and error-prone. Motivated by this problem, we propose a new technique for automatically synthesizing a new version of a database program given its original version and the source and target schemas. Our method does not require manual user guidance and ensures that the synthesized program is equivalent to the original one. Furthermore, our method is quite efficient and can synthesize new versions of database programs (containing up to 263 functions) that are extracted from real-world web applications with an average synthesis time of 69.4 seconds. 
id1256#Research on the maritime communication cryptographic chip's compiler optimization#In the process of ocean development, the technology for maritime communication system is a hot research field, of which information security is vital for the normal operation of the whole system, and that is also one of the difficulties in the research of maritime communication system. In this paper, a kind of maritime communication cryptographic SOC(system on chip) is introduced, and its compiler framework is put forward through analysis of working mode and problems faced by compiler front end. Then, a loop unrolling factor calculating algorithm based on queue theory, named UFBOQ (unrolling factor based on queue), is proposed to make parallel optimization in the compiler frontend with consideration of the instruction memory capacity limit. Finally, the scalar replacement method is used to optimize unrolled code to solve the memory access latency on the parallel computing efficiency, for continuous data storage characteristics of cryptographic algorithm. The UFBOQ algorithm and scalar replacement prove effective and appropriate, of which the effect achieves the linear speedup.
id1257#Monitoring and mapping vineyard water status using non-invasive technologies by a ground robot#There is a growing need to provide support and applicable tools to farmers and the agroindustry in order to move from their traditional water status monitoring and high-water-demand cropping and irrigation practices to modern, more precise, reduced-demand systems and technologies. In precision viticulture, very few approaches with ground robots have served as moving platforms for carrying non-invasive sensors to deliver field maps that help growers in decision making. The goal of this work is to demonstrate the capability of the VineScout (developed in the context of a H2020 EU project), a ground robot designed to assess and map vineyard water status using thermal infrared radiometry in commercial vineyards. The trials were carried out in Douro Superior (Portugal) under different irrigation treatments during seasons 2019 and 2020. Grapevines of Vitis vinifera L. Touriga Nacional were monitored at different timings of the day using leaf water potential (Ψl ) as reference indicators of plant water status. Grapevines’ canopy temperature (Tc ) values, recorded with an infrared radiometer, as well as data acquired with an environmental sensor (Tair, RH, and AP) and NDVI measurements collected with a multispectral sensor were automatically saved in the computer of the autonomous robot to assess and map the spatial variability of a commercial vineyard water status. Calibration and prediction models were performed using Partial Least Squares (PLS) regression. The best prediction models for grapevine water status yielded a determination coefficient of cross-validation (r2 cv) of 0.57 in the morning time and a r2 cv of 0.42 in the midday. The root mean square error of cross-validation (RMSEcv) was 0.191 MPa and 0.139 MPa at morning and midday, respectively. Spatial–temporal variation maps were developed at two different times of the day to illustrate the capability to monitor the grapevine water status in order to reduce the consumption of water, implementing appropriate irrigation strategies and increase the efficiency in the real time vineyard management. The promising outcomes gathered with the VineScout using different sensors based on thermography, multispectral imaging and environmental data disclose the need for further studies considering new variables related with the plant water status, and more grapevine cultivars, seasons and locations to improve the accuracy, robustness and reliability of the predictive models, in the context of precision and sustainable viticulture. 
id1258#Design of Digital Clock Manager Using Tunable BFD-True Random Number Generator#By demanding authentication exercises for example communications, electronic cash frameworks, plate encryption, the cryptographic frameworks have become a core aspect in our daily lives. Random numbers are an significant factor in strengthening and anchoring the protection of e-mail messages used in multiple encryption applications, including key ages, encryption, convention coverage, web wagering. Random flight numbers for basic mystery keys are basic to the protection of criphotographical calculations. In a variety of cryptographic systems, actual random number generators (TRNGs), The part has been incorporated, including PIN / secret word age, validation agreements, key age, random cushions and age of nunce. The circuit uses an undetermined random mechanism as a fundamental source, largely as electric commotion. Programmable field door arrays (FPGAs) are the optimal stage for the success of equipment that offers substantial safety conditions. The TRNG suggested is subject to the Xilinx-FPGA Bit Recurrence Recognition Guideline. 
id1259#A Survey of Multiple Pedestrian Tracking Based on Tracking-by-Detection Framework#Multiple pedestrian tracking (MPT) has gained significant attention due to its huge potential in a commercial application. It aims to predict multiple pedestrian trajectories and maintain their identities, given a video sequence. In the past decade, due to the advancement in pedestrian detection algorithms, Tracking-by-Detection (TBD) based algorithms have achieved tremendous successes. TBD has become the most popular MPT framework, and it has been actively studied in the past decade. In this paper, we give a comprehensive survey of recent advances in TBD-based MPT algorithms. We systematically analyze the existing TBD-based algorithms and organize the survey into four major parts. At first, this survey draws a timeline to introduce the milestones of TBD-based works which briefly reviews the development of the existing TBD-based methods. Second, the main procedures of the TBD framework are summarized, and each stage in the procedure is described in detail. Afterward, this survey analyzes the performance of existing TBD-based algorithms on MOT challenge datasets and discusses the factors that affect tracking performance. Finally, open issues and future directions in the TBD framework are discussed. 
id1261#Characterization and analysis of HMMER and SVM-RFE parallel bioinformatics applications#Bioinformatics applications constitute an emerging data-intensive, high-performance computing (HPC) domain. While there is much research on algorithmic improvements, [2], the actual performance of an application also depends on how well the program maps to the target hardware. This paper presents a performance study of two parallel bioinformatics applications HMMER (sequence alignment) and SVM-RFE(gene expression analysis), on Intel x86 based hyperthread-capable [11] shared-memory multiprocessor systems. The performance characteristics varied according to the application and target hardware characteristics. For instance, HMMER is compute intensive and showed better scalability on a 3.0 Ghz system versus a 2.2 Ghz system. However, SVM-RFE is memory intensive and showed better absolute performance on the 2.2 Ghz machine which has better memory bandwidth. The performance is also impacted by processor features, e.g. hyperthreading(HT) [11] and prefetching. With HMMER we could obtain ∼75% of the performance with HT enabled with respect to doubling the number of CPUs. While load balancing optimizations can provide speedup of ∼30% for HMMER on a hyperthreading-enabled system, the load balancing has to adapt to the target number of processors and threads. SVM-RFE benefits differently from the same load-balancing and thread scheduling tuning. We conclude that compiler and runtime optimizations play an important role to achieve the best performance for a given bioinformatics algorithm. 
id1262#Loop Quasi-Invariant Chunk Detection#Several techniques for analysis and transformations are used in compilers. Among them, the peeling of loops for hoisting quasi-invariants can be used to optimize generated code, or simply ease developers’ lives. In this paper, we introduce a new concept of dependency analysis borrowed from the field of Implicit Computational Complexity (ICC), allowing to work with composed statements called “Chunks” to detect more quasi-invariants. Based on an optimization idea given on a WHILE language, we provide a transformation method - reusing ICC concepts and techniques [8, 10] - to compilers. This new analysis computes an invariance degree for each statement or chunks of statements by building a new kind of dependency graph, finds the “maximum” or “worst” dependency graph for loops, and recognizes if an entire block is Quasi-Invariant or not. This block could be an inner loop, and in that case the computational complexity of the overall program can be decreased. In this paper, we introduce the theory around this concept and present a prototype analysis pass implemented on LLVM. We already implemented a proof of concept on a toy C parser (https://github.com/ThomasRuby/LQICM_On_C_Toy_Parser) analysing and transforming the AST representation. In a very near future, we will implement the corresponding transformation within our prototype LLVM pass and provide benchmarks comparisons. 
id1263#Human Joint Torque Modelling with MMG and EMG during Lower Limb Human-Exoskeleton Interaction#Human-robot cooperation is vital for optimising powered assist of lower limb exoskeletons (LLEs). Robotic capacity to intelligently adapt to human force, however, demands a fusion of data from exoskeleton and user state for smooth human-robot synergy. Muscle activity, mapped through electromyography (EMG) or mechanomyography (MMG) is widely acknowledged as usable sensor input that precedes the onset of human joint torque. However, competing and complementary information between such physiological feedback is yet to be exploited, or even assessed, for predictive LLE control. We investigate complementary and competing benefits of EMG and MMG sensing modalities as a means of calculating human torque input for assist-as-needed (AAN) LLE control. Three biomechanically agnostic machine learning approaches, linear regression, polynomial regression, and neural networks, are implemented for joint torque prediction during human-exoskeleton interaction experiments. Results demonstrate MMG predicts human joint torque with slightly lower accuracy than EMG for isometric human-exoskeleton interaction. Performance is comparable for dynamic exercise. Neural network models achieve the best performance for both MMG and EMG (94.8$\pm$0.7% with MMG and 97.6$\pm$0.8% with EMG (Mean $\pm$ SD)) at the expense of training time and implementation complexity. This investigation represents the first MMG human joint torque models for LLEs and their first comparison with EMG. We provide our implementations for future investigations (https://github.com/cic12/ieee_appx). 
id1264#Towards Collaborative Robotics in Top View Surveillance: A Framework for Multiple Object Tracking by Detection Using Deep Learning#Collaborative Robotics is one of the high-interest research topics in the area of academia and industry. It has been progressively utilized in numerous applications, particularly in intelligent surveillance systems. It allows the deployment of smart cameras or optical sensors with computer vision techniques, which may serve in several object detection and tracking tasks. These tasks have been considered challenging and high-level perceptual problems, frequently dominated by relative information about the environment, where main concerns such as occlusion, illumination, background, object deformation, and object class variations are commonplace. In order to show the importance of top view surveillance, a collaborative robotics framework has been presented. It can assist in the detection and tracking of multiple objects in top view surveillance. The framework consists of a smart robotic camera embedded with the visual processing unit. The existing pre-trained deep learning models named SSD and YOLO has been adopted for object detection and localization. The detection models are further combined with different tracking algorithms, including GOTURN, MEDIANFLOW, TLD, KCF, MIL, and BOOSTING. These algorithms, along with detection models, help to track and predict the trajectories of detected objects. The pre-trained models are employed; therefore, the generalization performance is also investigated through testing the models on various sequences of top view data set. The detection models achieved maximum True Detection Rate 93% to 90% with a maximum 0.6% False Detection Rate. The tracking results of different algorithms are nearly identical, with tracking accuracy ranging from 90% to 94%. Furthermore, a discussion has been carried out on output results along with future guidelines. 
id1265#Towards commissioning, resilience and added value of Augmented Reality in robotics: Overcoming technical obstacles to industrial applicability#Augmented Reality (AR) has the potential for facilitating the interaction with robots by enhancing the operator's spatial understanding as well as providing further cognitive support, e.g. in order to make manual programming processes more efficient and provide on-site simulation. However, we consider major issues that prevent a widespread use of AR towards robotics in industries. Initially, the commissioning of AR devices for robotic applications demands spatial referencing of robots and AR devices. Fiducial markers are a popular artificial aid, but hard to implement in industrial use cases because of additional efforts and a lack of robustness. A further bottleneck for developing AR applications in robotics is the restricted technical and ergonomic maturity of AR devices, e.g. limited local computation and short product life-cycles. Furthermore, benefit through AR in comparison to classic online and offline programming techniques is still unclear for most applicators. In this paper, we present approaches towards a distributed, hardware-agnostic microservice architecture with standard interfaces for an interoperable usage of AR in robotics. Furthermore, we present marker-less pose estimation methods for articulated robot arms as well as mobile robots based on RGB and depth images that serve automatic referencing. Finally, we reveal further application potential of AR in robotics in terms of combining advantages from online and offline programming. 
id1267#“It's a chance to make mistakes”: Processes and outcomes of coding in 2nd grade classrooms#Several gaps exist in the literature on coding. First, little exploration has focused on early elementary school students. In addition, close description of the overall context of coding tasks at this level is rare. Further, there is a need for both teacher and student voices around coding experiences to be heard. Moreover, a task engagement framework has not been used to evaluate the process or outcomes of early elementary coding tasks. Therefore, an exploratory holistic case study design was used to investigate student and teacher processes and outcomes of coding lessons in order to fill gaps in the literature. In this study, forty-six 2nd grade students, two teachers, and four researchers completed two one-week units on basic coding. Multiple descriptive and numeric data sources were employed to describe the process and outcomes of learning coding. Conclusions include: (1) teachers should start learning about coding first with short awareness sessions and then move to their own classrooms with knowledge brokers and other forms of assistance; (2) a focus on content and process, including problem-solving, is effective for coding with young children; (3) there can be a high level of engagement for teachers and students with the use of robots and welldesigned, age-appropriate coding tasks, and; (4) multiple data sources and the inclusion of both teacher and student data are essential in exploring coding in classrooms. 
id1268#A Hybrid System for Querying Flight Data#Time Series databases have been used since their introduction in early year 2000 for storing different types of time-correlated data, for instance, metrics acquired from weather stations and information generated in trading operations. Lately, they have been evaluated as an alternative for efficiently storing and retrieving flight test data. Time Series databases are designed to return values stored for given time intervals and this is the usual response required by flight test data consumers. However, another type of demand has become very frequent inside flight test management, whereby the response to a query is not a set of time-correlated values, but value-constrained time intervals. This paper proposes the creation of a hybrid system, consisting of a relational database and time-stamped data files, so that it is possible to search for value-constrained time intervals in the relational database and, from the returned intervals, to search and retrieve all metrics contained in the time-stamped data files. 
id1271#Multimodal deep neural networks for attribute prediction and applications to e-commerce catalogs enhancement#Compiling and managing huge e-commerce catalogs is a hard and time-consuming task for a retailer. In particular, deriving standardized and structured descriptions from unstructured data modalities, such as texts and images, is crucial to the performance of search engines and the general organization of virtual store databases. In this paper, we propose methodologies and strategies based on Deep Learning classifiers to structure, update, and inspect large e-commerce catalogs. To this purpose, we exploit multimodal representations combining data from images and unstructured textual descriptions to identify relevant labels for e-commerce applications. Such modalities of data are employed to train deep neural network architectures, which are then able to automatically recognize attributes. Three classes of architecture were investigated: variations of the VGG architecture for recognition from images; architectures combining embedding, convolutional and recurrent layers for text recognition; and hybrid architectures that combine elements from each of the previous architectures. We also propose tools that allow the detection of insufficiently descriptive visual and textual data, which can be later manually improved; and automatic enhancement of attribute annotations through neural network predictions. Using a database that we collected through a Web Crawler from a large e-commerce site, we show in our experiments that hybrid architectures achieve a better result in the classification task by combining both types of data. Finally, we show results of a case study performed to demonstrate the potential of our strategy for insufficiently descriptive data detection. We conclude that the proposed tools are effective to rectify, enhance, and efficiently update e-commerce catalogs. 
id1274#Application of deep convolutional neural networks for the detection of anthracnose in olives using VIS/NIR hyperspectral images#Anthracnose is one of the primary diseases that affect olive production before and after harvest, causing severe damage and economic losses. The objective of this work is to detect this disease in the early stages, using hyperspectral images and advanced modelling techniques of Deep Learning (DL) and convolutional neural networks (CNN). The olives were artificially inoculated with the fungus. Hyperspectral images (450–1050 nm) of each olive were acquired until visual symptoms of the disease were observed, in some cases up to 9 days. The olives were classified into two classes: control, inoculated with water, and fungi composed of olives inoculated with the fungus. The ResNet101 architecture was chosen and adapted to process 61-band hyperspectral images with only two classes. The result showed that the applied model is very effective in detecting infected olives since the sensitivity of the method was very high from the beginning (85% on day 3 and 100% onwards). From a commercial point of view, these results align with the need to detect the maximum number of infected fruits. 
id1275#TalkSQL: A Tool for the Synthesis of SQL Queries from Verbal Specifications#Recent advances in the field of Natural Language Processing (NLP) have led to many robust user interfaces (UIs) designed as intelligent tutoring systems (ITS) that help students learn, query and access data in relational databases. Such tools are generally referred to as Natural Language Interfaces to Databases (NLIDBs). Many of these UIs rely on voice or typewritten for further processing. Research has shown that typewritten remains the preferred input method used by database UIs designers for querying relational databases due to its flexibility. Still, there is a dearth of tools that require voice-based inputs for querying relational databases. Despite the scarcity of these tools, many of them fail to provide a comprehensive feedback to a user. In this paper, we introduce a voice-based query system named TalkSQL that takes voice inputs from a user, converts these words into SQL queries and returns a feedback to the user. Automatic feedback generation is of immense importance. To achieve this, we have used regular expressions, a representation of regular languages for the recognition of the Create, Read, Update, Delete (CRUD) operations in SQL and automatically generate a feedback using pre-defined templates. A survey on 53 participants showed that 91.2% agreed that they were able to understand the CRUD command using TalkSQL. The expected contributions are in two-fold: this work may assist a special (e.g. visually impaired) learner to understand SQL queries, and show that a voice-based interface can assist users in understanding SQL queries. 
id1276#Linking administrative data sets of inpatient infectious diseases diagnoses in far North Queensland: a cohort profile#PURPOSE: To design a linked hospital database using administrative and clinical information to describe associations that predict infectious diseases outcomes, including long-term mortality. PARTICIPANTS: A retrospective cohort of Townsville Hospital inpatients discharged with an International Classification of Diseases and Related Health Problems 10th Revision Australian Modification code for an infectious disease between 1 January 2006 and 31 December 2016 was assembled. This used linked anonymised data from: hospital administrative sources, diagnostic pathology, pharmacy dispensing, public health and the National Death Registry. A Created Study ID was used as the central identifier to provide associations between the cohort patients and the subsets of granular data which were processed into a relational database. A web-based interface was constructed to allow data extraction and evaluation to be performed using editable Structured Query Language. FINDINGS TO DATE: The database has linked information on 41 367 patients with 378 487 admissions and 1 869 239 diagnostic/procedure codes. Scripts used to create the database contents generated over 24 000 000 database rows from the supplied data. Nearly 15% of the cohort was identified as Aboriginal or Torres Strait Islanders. Invasive staphylococcal, pneumococcal and Group A streptococcal infections and influenza were common in this cohort. The most common comorbidities were smoking (43.95%), diabetes (24.73%), chronic renal disease (17.93%), cancer (16.45%) and chronic pulmonary disease (12.42%). Mortality over the 11-year period was 20%. FUTURE PLANS: This complex relational database reutilising hospital information describes a cohort from a single tropical Australian hospital of inpatients with infectious diseases. In future analyses, we plan to explore analyses of risks, clinical outcomes, healthcare costs and antimicrobial side effects in site and organism specific infections. 
id1277#Deep-learning based monitoring of FOG layer dynamics in wastewater pumping stations#Accumulation of fat, oil and grease (FOG) in the sumps of wastewater pumping stations is a common failure cause for these facilities. Floating solids are often not transported by the pump suction inlets and the individual solids can accumulate to stiff and thick FOG layers. The lack of data about the dynamics in FOG layer formation still hampers the design of effective measures towards its mitigation. In this article, we present a low-cost camera-based automated system for the observation of FOG layer dynamics in wastewater pumping stations at high-frequency (minutes) over extended time windows (months). Optical imagery is processed through a deep-learning computer vision routine that allows describing FOG layer dynamics (e.g. accumulation rate and changes in shape) and various hydraulic processes in the pump sump (e.g. the water level, surface flow velocity fields, vorticity, or circulation). Furthermore, the system can perform in-camera image processing, thus allowing the transfer of compressed-processed datasets when deployed in remote locations (Edge AI computing), which could be of great utility for the hydro-ecological monitoring community. In this study, the technology applied is illustrated with a dataset (six months, two-minute frequency) collected at a wastewater pumping station at the municipality of Rotterdam, The Netherlands. This monitoring system represents a source of information for the management of (waste)water pumping stations (e.g. detection of free-surface vortices and scheduling of sump cleaning operations) and facilitates the collection of standardized high-frequency FOG layer dynamics data for a detailed description of FOG build-up and transport processes. 
id1278#ProcessAR: An augmented reality-based tool to create in-situ procedural 2D/3D AR Instructions#Augmented reality (AR) is an efficient form of delivering spatial information and has great potential for training workers. However, AR is still not widely used for such scenarios due to the technical skills and expertise required to create interactive AR content. We developed ProcessAR, an AR-based system to develop 2D/3D content that captures subject matter expert's (SMEs) environment-object interactions in situ. The design space for ProcessAR was identified from formative interviews with AR programming experts and SMEs, alongside a comparative design study with SMEs and novice users. To enable smooth workflows, ProcessAR locates and identifies different tools/objects through computer vision within the workspace when the author looks at them. We explored additional features such as embedding 2D videos with detected objects and user-adaptive triggers. A final user evaluation comparing ProcessAR and a baseline AR authoring environment showed that, according to our qualitative questionnaire, users preferred ProcessAR. 
id1279#Towards the Detection of Promising Processes by Analysing the Relational Data#Business process discovery provides mechanisms to extract the general process behaviour from event observations. However, not always the logs are available and must be extracted from repositories, such as relational databases. Derived from the references that exist between the relational tables, several are the possible combinations of traces of events that can be extracted from a relational database. Different traces can be extracted depending on which attribute represents the case-id, what are the attributes that represent the execution of an activity, or how to obtain the timestamp to define the order of the events. This paper proposes a method to analyse a wide range of possible traces that could be extracted from a relational database, based on measuring the level of interest of extracting a trace log, later used for a discovery process. The analysis is done by means of a set of proposed metrics before the traces are generated and the process is discovered. This analysis helps to reduce the computational cost of process discovery. For a possible case-id every possible traces are analysed and measured. To validate our proposal, we have used a real relational database, where the detection of processes (most and least promising) are compared to rely on our proposal. 
id1281#Reducing NoC energy consumption through compiler-directed channel voltage scaling#While scalable NoC (Network-on-Chip) based communication architectures have clear advantages over long point-to-point communication channels, their power consumption can be very high. In contrast to most of the existing hardware-based efforts on NoC power optimization, this paper proposes a compiler-directed approach where the compiler decides the appropriate voltage/frequency levels to be used for each communication channel in the NoC. Our approach builds and operates on a novel graph based representation of a parallel program and has been implemented within an optimizing compiler and tested using 12 embedded benchmarks. Our experiments indicate that the proposed approach behaves better -from both performance and power perspectives - than a hardware-based scheme and the energy savings it achieves are very close to the savings that could be obtained from an optimal, but hypothetical voltage/frequency scaling scheme. Copyright 
id1282#Multi-camera vessel-speed enforcement by enhancing detection and re-identification techniques#This paper presents a camera-based vessel-speed enforcement system based on two cameras. The proposed system detects and tracks vessels per camera view and employs a re-identification (re-ID) function for linking vessels between the two cameras based on multiple bounding-box images per vessel. Newly detected vessels in one camera (query) are compared to the gallery set of all vessels detected by the other camera. To train and evaluate the proposed detection and re-ID system, a new Vessel-reID dataset is introduced. This extensive dataset has captured a total of 2474 different vessels covered in multiple images, resulting in a total of 136,888 vessel bounding-box images. Multiple CNN detector architectures are evaluated in-depth. The SSD512 detector performs best with respect to its speed (85.0% Recall@95Precision at 20.1 frames per second). For the re-ID of vessels, a large portion of the total trajectory can be covered by the successful detections of the SSD model. The re-ID experiments start with a baseline single-image evaluation obtaining a score of 55.9% Rank-1 (49.7% mAP) for the existing TriNet network, while the available MGN model obtains 68.9% Rank-1 (62.6% mAP). The performance significantly increases with 5.6% Rank-1 (5.7% mAP) for MGN by applying matching with multiple images from a single vessel. When emphasizing more fine details by selecting only the largest bounding-box images, another 2.0% Rank-1 (1.4% mAP) is added. Application-specific optimizations such as travel-time selection and applying a cross-camera matching constraint further enhance the results, leading to a final 88.9% Rank-1 and 83.5% mAP performance. 
id1284#Active learning in robotics: A review of control principles#Active learning is a decision-making process. In both abstract and physical settings, active learning demands both analysis and action. This is a review of active learning in robotics, focusing on methods amenable to the demands of embodied learning systems. Robots must be able to learn efficiently and flexibly through continuous online deployment. This poses a distinct set of control-oriented challenges—one must choose suitable measures as objectives, synthesize real-time control, and produce analyses that guarantee performance and safety with limited knowledge of the environment or robot itself. In this work, we survey the fundamental components of robotic active learning systems. We discuss classes of learning tasks that robots typically encounter, measures with which they gauge the information content of observations, and algorithms for generating action plans. Moreover, we provide a variety of examples – from environmental mapping to nonparametric shape estimation – that highlight the qualitative differences between learning tasks, information measures, and control techniques. We conclude with a discussion of control-oriented open challenges, including safety-constrained learning and distributed learning. 
id1285#Federated Learning in the Sky: Aerial-Ground Air Quality Sensing Framework with UAV Swarms#Due to air quality significantly affects human health, it is becoming increasingly important to accurately and timely predict the air quality index (AQI). To this end, this article proposes a new federated learning (FL)-based aerial-ground air quality sensing framework for fine-grained 3-D air quality monitoring and forecasting. Specifically, in the air, this framework leverages a lightweight Dense-MobileNet model to achieve energy-efficient end-to-end learning from haze features of haze images taken by unmanned aerial vehicles (UAVs) for predicting AQI scale distribution. Furthermore, the FL framework not only allows various organizations or institutions to collaboratively learn a well-trained global model to monitor AQI without compromising privacy but also expands the scope of UAV swarms monitoring. For ground sensing systems, we propose a graph convolutional neural network-based long short-term memory (GC-LSTM) model to achieve accurate, real time, and future AQI inference. The GC-LSTM model utilizes the topological structure of the ground monitoring station to capture the spatiotemporal correlation of historical observation data, which helps the aerial-ground sensing system to achieve accurate AQI inference. Through extensive case studies on a real-world data set, numerical results show that the proposed framework can achieve accurate and energy-efficient AQI sensing without compromising the privacy of raw data. 
id1286#A review of multimodal image matching: Methods and applications#Multimodal image matching, which refers to identifying and then corresponding the same or similar structure/content from two or more images that are of significant modalities or nonlinear appearance difference, is a fundamental and critical problem in a wide range of applications, including medical, remote sensing and computer vision. An increasing number and diversity of methods have been proposed over the past decades, particularly in this deep learning era, due to the challenges in eliminating modality variance and geometrical deformation that intrinsically exist in multimodal image matching. However, a comprehensive review and analysis of traditional and recent trainable methods and their applications in different research fields are lacking. To this end and in this survey, we first introduce two general frameworks, saying area- and feature-based, in terms of their core components, taxonomy, and procedure details. Second, we provide a comprehensive review of multimodal image matching methods from handcrafted to deep methods for each research field according to their imaging nature, including medical, remote sensing and computer vision. Extensive experimental comparisons of interest point detection, description and matching, and image registration are performed on various datasets containing common types of multimodal image pairs that we collected and annotated. Finally, we briefly introduce and analyze several typical applications to reveal the significance of multimodal image matching and provide insightful discussions and conclusions to these multimodal image matching approaches, and simultaneously deliver their future trends for researchers and engineers in related research areas to achieve further breakthroughs. 
id1288#Improved cube-attack-like cryptanalysis of reduced-round Ketje-Jr and Keccak-MAC#At EUROCRYPT 2015, Dinur et al. proposed cube-attack-like cryptanalysis on reduced-round Keccak. The process of recovering the key is divided into the preprocessing and the online phase. The preprocessing phase is setting a look-up table by computing the cube sum of involved key bits. The online phase is computing the cube sum of auxiliary variables and recording the matching values in the table as candidates. Auxiliary variables help balance the complexity of the two phases by reducing the number of involved key bits. Following this idea, a series of works has been presented, mainly focusing on a better selection of cube variables, auxiliary variables and involved key bits. We provide new methods to select auxiliary variables and involved key bits. The first step is to get a precise algebraic expression of each bit after one round permutation. Then, combined with the corresponding constraints on these variables, we can construct a Mixed-integer Linear Programming (MILP) model. Secondly, unlike the previous idea that auxiliary variables are chosen to satisfy the CP-kernel property just for the consideration of controlling diffusion, we cancel this restriction and adopt a more skilled selection of auxiliary variables. Based on these two steps, we improve the cube-attack-like cryptanalysis in terms of the complexity. 
id1289#Automatic Attendance System Based on Face Recognition Using HOG Features and Cosine Distance#Advancements in machine learning have been applying deeply and widely in numerous fields. Especially, computer vision tasks for object detection in recent years have achieved great performances which are even better than human recognition ability. This work leverages machine learning methods and information systems to present a framework for student attendance combining machine learning-based face recognition algorithm and relational databases to store, recognize, and record student attendance. The proposed method is tested on various scenarios and is expected to apply in practical cases. We investigate the Histogram of Oriented Gradients (HOG) for face detection and to use cosine distance to recognize faces. The purpose of this study is face recognition in real-time i.e. using a webcam, camera of the mobile device, and from a photograph or from a set of faces tracked in a video. We measured the distance between the landmarks and compared the test image with different known encoded image landmarks in the recognition stage. Face Recognition includes extracting features and then recognizing it, in any case, such as brightness, transformations as translation, rotation, and scale image. We recognized that using the HOG algorithm to detect faces improves more and more efficient model and avoids time-consuming. 
id1290#An Interactive Tutoring System for Learning Language Processing and Compiler Design#This poster presents an Interactive Tutoring System (ITS) that allows teachers to tutor and evaluate interactively the learning process that the students of a Compiler Design course must experience from each theoretical concept to obtain the code of its corresponding implementation (scanners, parsers, translators, interpreters, compilers), regardless of the specific tools chosen for the automatic code generation and the programming language. Through the use of the ITS, each teacher will be able to select those tools that he/she considers more adequate for the development of the course, and integrate them modularly into a common educational environment, so that if later on she/he decides to change these tools or the programming language, the ITS will continue to be valid, and the tutoring and evaluation process carried out by the ITS will remain the same to guide the students from theory to implementation. 
id1291#"Contribution to the knowledge of cultural heritage via a Heritage Information System (HIS). The case of ""La Cultura del Agua"" in Valverde de Burguillos, Badajoz (Spain)"#"Modern science is going through a period of important reflection on the role of different agents and multiple disciplines in the management and safeguarding of architectural heritage. This new focus generates a greater amount and diversity of information, so the implementation of a unifying tool in the framework of digital information models would mean a better knowledge of cultural heritage as well as aiding its safeguarding and protection. In addition, it must be taken into account that, for the correct management of information in its broadest dimension, this tool must make it possible to relate alphanumeric data about an item of heritage to its spatial location. In this sense, this article proposes a Heritage Information System (HIS)-understood as a digital knowledge tool-that consists of a relational database and a map manager with Geographic Information System (GIS) technology (a geodatabase). The methodology suggested here sets out the steps that make up the HIS, so that the system can be applied to other geographical elements or realities. For this reason, a study was made of ""La Cultura del Agua"" in Valverde de Burguillos (Spain), a heritage ensemble that consists of rural architecture and dispersed preindustrial elements, which are currently at risk. The HIS seeks to develop a more complete identification of these elements (individually and as a system) and a justified argument for their being given value and great visibility. This new approach encourages sustainable development in terms of efficiency and effectiveness for the analysis, diagnosis, and reactivation of cultural heritage, always placing importance on the balance of social participation with the territory in which the system is applied, and with global society. "
id1293#A Narrative Review of Storing and Querying XML Documents Using Relational Database#Extensible Markup Language (XML) has become a common language for data interchange and data representation in the Web. The evolution of the big data environment and the large volume of data which is being represented by XML on the Web increase the challenges in effectively managing such data in terms of storing and querying. Numerous solutions have been introduced to store and query XML data, including the file systems, Object-Oriented Database (OODB), Native XML Database (NXD), and Relational Database (RDB). Previous research attempts indicate that RDB is the most powerful technology for managing XML data to date. Because of the structure variations of XML and RDB, the need to map XML data to an RDB scheme is increased. This growth has prompted numerous researchers and database vendors to propose different approaches to map XML documents to an RDB, translating different types of XPath queries to SQL queries and returning the results to an XML format. This paper aims to comprehensively review most cited and latest mapping approaches and database vendors that use RDB solution to store and query XML documents, in a narrative manner. The advantages and the drawbacks of each approach is discussed, particularly in terms of storing and querying. The paper also provides some insight into managing XML documents using RDB solution in terms of storing and querying and contributes to the XML community. 
id1294#Efficient local locking for massively multithreaded in-memory hash-based operators#The join and group-by aggregation are two memory intensive operators that are affecting the performance of relational databases. Hashing is a common approach used to implement both operators. Recent paradigm shifts in multi-core processor architectures have reinvigorated research into how the join and group-by aggregation operators can leverage these advances. However, the poor spatial locality of the hashing approach has hindered performance on multi-core processor architectures which rely on using large cache hierarchies for latency mitigation. Multithreaded architectures can better cope with poor spatial locality by masking memory latency with many outstanding requests. Nevertheless, the number of parallel threads, even in the most advanced multithreaded processors, such as UltraSPARC, is not enough to fully cover the main memory access latency. In this paper, we explore the hardware re-configurability of FPGAs to enable deeper execution pipelines that maintain hundreds (instead of tens) of outstanding memory requests across four FPGAs-drastically increasing concurrency and throughput. We present two end-to-end in-memory accelerators for the join and group-by aggregation operators using FPGAs. Both accelerators use massive multithreading to mask long memory delays of traversing linked-list data structures, while concurrently managing hundreds of thread states across four FPGAs locally. We explore how content addressable memories can be intermixed within our multithreaded designs to act as a synchronizing cache, which enforces locks and merges jobs together before they are written to memory. Throughput results for our hash-join operator accelerator show a speedup between 2× and 3.4× over the best multi-core approaches with comparable memory bandwidths on uniform and skewed datasets. The accelerator for the hash-based group-by aggregation operator demonstrates that leveraging CAMs achieves average speedup of 3.3× with a best case of 9.4× in terms of throughput over CPU implementations across five types of data distributions. 
id1295#Winning ARIAC 2020 by KISSing The BEAR: Keeping things simple in Best Effort Agile Robotics#In this paper we discuss our evolved understanding on the meaning of agile robotics. The lessons learned are concluded from the four years of our participation in ARIAC, the Agile Robotics for Industrial Automation Competition, that we managed to win in 2020. After elaborating on ARIAC tasks, challenges and scoring and their consequences on algorithm design, error handling and software development processes and methodologies, we introduce the concept of Best Effort Agile Robotics (BEAR). The conclusions are mapped to agility standardization directions. The concept of Best Effort Robotics (BER) is also proposed and discussed, highlighting the similarities with best effort networking, a proven concept in Internet technology. 
id1296#Compiler testing via a theory of sound optimisations in the C11/C++11 memory model#Compilers sometimes generate correct sequential code but break the concurrency memory model of the programming language: these subtle compiler bugs are observable only when the miscompiled functions interact with concurrent contexts, making them particularly hard to detect. In this work we design a strategy to reduce the hard problem of hunting concurrency compiler bugs to differential testing of sequential code and build a tool that puts this strategy to work. Our first contribution is a theory of sound optimisations in the C11/C++11 memory model, covering most of the optimisations we have observed in real compilers and validating the claim that common compiler optimisations are sound in the C11/C++11 memory model. Our second contribution is to show how, building on this theory, concurrency compiler bugs can be identified by comparing the memory trace of compiled code against a reference memory trace for the source code. Our tool identified several mistaken write introductions and other unexpected behaviours in the latest release of the gcc compiler.
id1297#Endo-Depth-and-Motion: Reconstruction and Tracking in Endoscopic Videos Using Depth Networks and Photometric Constraints#Estimating a scene reconstruction and the camera motion from in-body videos is challenging due to several factors, e.g. the deformation of in-body cavities or the lack of texture. In this paper we present Endo-Depth-and-Motion, a pipeline that estimates the 6-degrees-of-freedom camera pose and dense 3D scene models from monocular endoscopic sequences. Our approach leverages recent advances in self-supervised depth networks to generate pseudo-RGBD frames, then tracks the camera pose using photometric residuals and fuses the registered depth maps in a volumetric representation. We present an extensive experimental evaluation in the public dataset Hamlyn, showing high-quality results and comparisons against relevant baselines. We also release all models and code1 for future comparisons. 
id1298#Lifting preferences to the semantic web: PreferenceSPARQL#PreferenceSQL is an SQL extension for standard relational databases supporting soft constraints and is used to find relevant data intuitively. Meanwhile, the Semantic Web has interoperability advantages and helps to retrieve information with machine-readable data. We use the benefits of both technologies by combining preferences from SQL with SPARQL, the query language of the Semantic Web. This work provides implementation details in Apache Jena for the new composite called 'PreferenceSPARQL'. Furthermore, we contribute comprehensive benchmarks that show which preference algorithm is best suited for our approach. 
id1299#Development and application of computer vision-based acupuncture manipulation classification system#"目的：为了提升针刺手法建模与传承的准确性，面向针刺实践中的手法视频，本文探讨利用计算机视觉技术对中医针灸学中“捻转”和“提插”这两类基本针刺手法进行分类的可行性。方法：构建一种计算机视觉下的基于三维卷积神经网络和长短时记忆网络的混合深度学习网络模型，提取针刺手法视频帧序列的时空特征，将其输入分类器中实现分类。结果：针对200组录制的医师针刺手法视频，应用所提混合网络模型对“捻转”和“提插”两类手法进行分类，训练准确率达到95.4%，验证准确率达到95.3%。结论：本系统可为针刺手法的数据提取与传承提供一条有效途径。.OBJECTIVE: To improve the accuracy of acupuncture manipulation modeling and inheritance, this article explores the feasibility of automatically classifying ""twirling"" and ""lifting and thrusting"", two basic acupuncture manipulations in science of acupuncture and moxibustion, with the computer vision technology. METHODS: A hybrid deep learning network model was designed based on 3D convolutional neural network and long-short term memory neural network to extract the spatial-temporal features of video frame sequences, which were then input into the classifier for classification. RESULTS: The model discriminated between ""twirling"" and ""lifting and thrusting"" manipulations in 200 videos, with the training and verification accuracy reaching up to 95.4% and 95.3%, respectively. CONCLUSION: This computer vision-based acupuncture manipulation classification system provides an effective way for the data extraction and inheritance of acupuncture manipulations."
id1301#Recent granular computing frameworks for mining relational data#A lot of data currently being collected is stored in databases with a relational structure. The process of knowledge discovery from such data is a more challenging task compared with single table data. Granular computing, which has successfully been applied to mining data storable in single tables, is a promising direction for discovering knowledge from relational data. This paper summarizes some recent developments in the area of application of granular computing to mining relational data. Four granular computing frameworks for processing relational data are introduced and compared. The paper shows how each of the frameworks represents relational data, constructs information granules and build patterns based on the granules. A generic system that can employ any of the frameworks to discover knowledge from relational data is also outlined. 
id1303#Concurrent, parallel garbage collection in linear time#"This paper presents a new concurrent garbage collection algorithm based on two types of reference, strong and weak, to link the graph of objects. Strong references connect the roots to all the nodes in the graph but do not contain cycles.Weak references may, however, contain cycles. Advantages of this system include: (1) reduced processing, nontrivial garbage collection work is only required when the last strong reference is lost; (2) fewer memory traces to delete objects, a garbage cycle only needs to be traversed twice to be deleted; (3) fewer memory traces to retain objects, since the collector can often prove objects are reachable without fully tracing support cycles to which the objects belong; (4) concurrency, it can run in parallel with a live system without ""stopping the world;"" (5) parallel, because collection operations in different parts of the memory can proceed at the same time. Previous variants of this technique required exponential cleanup time [27, 31], but our algorithm is linear in total time, i.e. any changes in the graph take only O(N) time steps, where N is the number of edges in the affected subgraph (e.g. the subgraph whose strong support is affected by the operations). Copyright "
id1304#Reconciliation based key exchange schemes using lattices: a review#Lattice-based cryptography is one of the emerging fields of cryptography in the post-quantum world. It is resistive to quantum attacks and has performance competitive to that of prevalent cryptosystem such as Rivest–Shamir–Adleman (RSA), Diffie Hellman etc. Till now, various basic cryptographic primitives like encryption and decryption, digital signature, hash-based functions, and key exchange are proposed in lattice-based cryptography. The key exchange primitive is one of the basic cryptographic primitives of the Public Key Infrastructure (PKI). Lattices are preferably used to design provably secure reconciliation based key exchange protocols against quantum attacks. However, the literature pertaining to the study of reconciliation based key exchange protocols is limited and often the schemes are studied independently. Therefore, in this work, we have reviewed the reconciliation based key exchange schemes and classify these schemes under two different categories depending on the reconciliation mechanism used by the scheme. We also point out the basic key exchange schemes upon which all other key exchange schemes are based. We conduct a complete review, security analysis, implementation and comparison of these basic key exchange schemes. 
id1305#Graph Database and Relational Database Performance Comparison on a Transportation Network#Facing the problem of structuring irregular data in the big data era, graph databases are a powerful solution to handle link relationship without costly operations and enjoy great flexibility as data model changes. Though it’s well-known that graph databases have superior performance in a certain area than relational databases, little effort has been put into investigating the detail of these advantages. In this paper, we report a systematic performance study of graph databases and relational databases on a transportation network. We design a database benchmark considering traversal and searching performance to evaluate system performance in different data organizations, initial states, and running modes. Our results show that graph databases outperform relational databases system in three main graph algorithms testing. Furthermore, we discuss the reasonable practice in applications based on graph databases from our experiment results. 
id1306#Unsupervised skill transfer learning for autonomous robots using distributed Growing Self Organizing Maps#A persistent challenge in the cognitive development of autonomous robotics is the unsupervised and unstructured nature of skill transfer learning where the Self Organizing Map (SOM) has been used as the enabling technology. The Growing Self-Organizing Map (GSOM) algorithm is an unsupervised, structure-adapting machine learning algorithm conventionally used for data exploration, clustering, visualization, outlier detection and dimensionality reduction. In this paper, we present the design and development of a new distributed algorithm based on the GSOM for unsupervised skill transfer learning in autonomous robotics settings which overcomes the key limitations of the SOM in real-life scenarios. We posit this new algorithm will be directly applicable to skill transfer learning scenarios that require unsupervised, incremental and on-going self-learning of multi-tasks and knowledge transfer. The distributed and scalable properties of the proposed algorithm handle large volumes of data required for unsupervised skill transfer learning, based on data parallelization. It generates multiple maps representing diverse skill knowledge, which are then projected together to a single embedding. The new algorithm is positioned within an autonomous developmental robotics framework for knowledge acquisition and skill transfer learning. This framework was further adapted to three contemporary distributed computing platforms, Hadoop, Spark and Hama. Empirical evaluation of these three adaptations using several benchmark and real-life datasets demonstrates its practical value and computational efficiency for unsupervised skill transfer learning in autonomous robots. 
id1307#Automated operative phase identification in peroral endoscopic myotomy#Background: Artificial intelligence (AI) and computer vision (CV) have revolutionized image analysis. In surgery, CV applications have focused on surgical phase identification in laparoscopic videos. We proposed to apply CV techniques to identify phases in an endoscopic procedure, peroral endoscopic myotomy (POEM). Methods: POEM videos were collected from Massachusetts General and Showa University Koto Toyosu Hospitals. Videos were labeled by surgeons with the following ground truth phases: (1) Submucosal injection, (2) Mucosotomy, (3) Submucosal tunnel, (4) Myotomy, and (5) Mucosotomy closure. The deep-learning CV model—Convolutional Neural Network (CNN) plus Long Short-Term Memory (LSTM)—was trained on 30 videos to create POEMNet. We then used POEMNet to identify operative phases in the remaining 20 videos. The model’s performance was compared to surgeon annotated ground truth. Results: POEMNet’s overall phase identification accuracy was 87.6% (95% CI 87.4–87.9%). When evaluated on a per-phase basis, the model performed well, with mean unweighted and prevalence-weighted F1 scores of 0.766 and 0.875, respectively. The model performed best with longer phases, with 70.6% accuracy for phases that had a duration under 5 min and 88.3% accuracy for longer phases. Discussion: A deep-learning-based approach to CV, previously successful in laparoscopic video phase identification, translates well to endoscopic procedures. With continued refinements, AI could contribute to intra-operative decision-support systems and post-operative risk prediction. 
id1308#Fast parallel equivalence relations in a datalog compiler#Modern parallelizing Datalog compilers are employed in industrial applications such as networking and static program analysis. These applications regularly reason about equivalences, e.g., computing bitcoin user groups, fast points-To analyses, and optimal network routes. State-of-The-Art Datalog engines represent equivalence relations verbatim by enumerating all possible pairs in an equivalence class. This approach inhibits scalability for large datasets. In this paper, we introduce EQREL, a specialized parallel union-find data structure for scalable equivalence relations, and its integration into a Datalog compiler. Our data structure provides a quadratic worst-case speed-up and space improvement. We demonstrate the efficacy of our data structure in SOUFFLÉ, which is a Datalog compiler that synthesizes parallel C ++ code. We use real-world benchmarks and show that the new data structure scales on shared-memory multi-core architectures storing up to a half-billion pairs for a static program analysis scenario. 
id1310#A Novel Approach of Symmetric Key Cryptography#Cryptography plays an important role in secured data communication over the network using unknown, untrusted mediums. In the fast digital transformation, traffic on network increasing rapidly where users are always connected, being online, anytime anywhere. As part of data, extortions like altering, spoofing and snooping of are quite common over the network by unauthorized access. Cryptography is well known mechanism where asymmetric or symmetric algorithms using public or private keys provide secure data communication. There are many more cryptography algorithms existing but with emerging technologies and diverse application domains seeking consistent enhancement and better performance with resource restriction. In this paper, we present a novel symmetric cryptography technique based on Caesar cipher symmetric cryptography technique to transform the original text/message into secret text/message. In this technique, the sender transmits hash code instead of symmetric key and Hash code provides the symmetric key to the receiver to decrypt the message of a sender. Proposed method works for all 256 characters having ASCII value from 0 to 255. 
id1311#A self-learning teacher-student framework for gastrointestinal image classification#We present a semi-supervised teacher-student framework to improve classification performance on gastrointestinal image data. As labeled data is scarce in medical settings, this framework is built specifically to take advantage of vast amounts of unlabeled data. It consists of three main steps: (1) train a teacher model with labeled data, (2) use the teacher model to infer pseudo labels with unlabeled data, and (3) train a new and larger student model with a combination of labeled images and inferred pseudo labels. These three steps are repeated several times by treating the student as a teacher to relabel the unlabeled data and consequently train a new student. We demonstrate that our framework can classify both video capsule endoscopy (VCE) and standard endoscopy images. Our results indicate that our teacher-student framework can significantly increase the performance compared to traditional supervised-learning-based models, i.e., an overall increase in the F1-score of 4.7% for the Kvasir-Capsule VCE dataset and 3.2% for the HyperKvasir colonoscopy dataset. We believe that our framework can use more of the data collected at hospitals without the need for expert labels, contributing to overall better models for medical multimedia systems for automatic disease detection. 
id1312#Quantifying Parkinson's disease motor severity under uncertainty using MDS-UPDRS videos#Parkinson's disease (PD) is a brain disorder that primarily affects motor function, leading to slow movement, tremor, and stiffness, as well as postural instability and difficulty with walking/balance. The severity of PD motor impairments is clinically assessed by part III of the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS), a universally-accepted rating scale. However, experts often disagree on the exact scoring of individuals. In the presence of label noise, training a machine learning model using only scores from a single rater may introduce bias, while training models with multiple noisy ratings is a challenging task due to the inter-rater variabilities. In this paper, we introduce an ordinal focal neural network to estimate the MDS-UPDRS scores from input videos, to leverage the ordinal nature of MDS-UPDRS scores and combat class imbalance. To handle multiple noisy labels per exam, the training of the network is regularized via rater confusion estimation (RCE), which encodes the rating habits and skills of raters via a confusion matrix. We apply our pipeline to estimate MDS-UPDRS test scores from their video recordings including gait (with multiple Raters, R=3) and finger tapping scores (single rater). On a sizable clinical dataset for the gait test (N=55), we obtained a classification accuracy of 72% with majority vote as ground-truth, and an accuracy of ∼84% of our model predicting at least one of the raters’ scores. Our work demonstrates how computer-assisted technologies can be used to track patients and their motor impairments, even when there is uncertainty in the clinical ratings. The latest version of the code will be available at https://github.com/mlu355/PD-Motor-Severity-Estimation. 
id1313#Identification of wheat leaf diseases and their severity based on elliptical-maximum margin criterion metric learning#Rapid and accurate identification of wheat leaf diseases and their severity is benefit for the precise prevention and control of wheat leaf diseases. Taking powdery mildew and stripe rust as research objects, this study proposes an algorithm for identification of wheat leaf diseases and their severity based on Elliptical-Maximum Margin Criterion (E-MMC) metric learning. Compared with other metrics, elliptic metric combined with MMC can find the non-linear transformation that reflects the spatial structure or semantic information of the wheat leaf disease image, which can enlarge the distance between different classes and better complete the identification task. In the proposed algorithm, Otsu method is used to segment the disease spots according to the characteristics of disease distribution in wheat leaf images. Moreover, the best combination of color and texture features in the wheat disease spot image is determined to construct training set. By using the maximum margin criterion and gradient rise method, the optimal elliptic metric matrix is obtained, thereby transforming the sample feature space and reducing the correlation between features. Then, the wheat powdery mildew, stripe rust, and their severity are identified. The experimental results show that the proposed algorithm is superior to the traditional support vector machines and other algorithms. The highest identification accuracy obtained by the proposed algorithm is 94.16 %. These findings can provide valuable help for the intelligent identification and classification of wheat leaf diseases. 
id1314#Fast liveness checking for SSA-form programs#Liveness analysis is an important analysis in optimizing compilers. Liveness information is used in several optimizations and is mandatory during the code-generation phase. Two drawbacks of conventional liveness analyses are that their computations are fairly expensive and their results are easily invalidated by program transformations. We present a method to check liveness of variables that overcomes both obstacles. The major advantage of the proposed method is that the analysis result survives all program transformations except for changes in the control-flow graph. For common program sizes our technique is faster and consumes less memory than conventional data-flow approaches. Thereby, we heavily make use of SSA-form properties, which allow us to completely circumvent data-flow equation solving. We evaluate the competitiveness of our approach in an industrial strength compiler. Our measurements use the integer part of the SPEC2000 benchmarks and investigate the liveness analysis used by the SSA destruction pass. We compare the net time spent in liveness computations of our implementation against the one provided by that compiler. The results show that in the vast majority of cases our algorithm, while providing the same quality of information, needs less time: an average speed-up of 16%. Copyright 2008 ACM.
id1315#Generalizations of the theory and deployment of triangular inequality for compiler-based strength reduction#Triangular Inequality (TI) has been used in many manual algorithm designs to achieve good efficiency in solving some distance calculation-based problems. This paper presents our generalization of the idea into a compiler optimization technique, named TI-based strength reduction. The generalization consists of three parts. The first is the establishment of the theoretic foundation of this new optimization via the development of a new form of TI named Angular Triangular Inequality, along with several fundamental theorems. The second is the revealing of the properties of the new forms of TI and the proposal of guided TI adaptation, a systematic method to address the difficulties in effective deployments of TI optimizations. The third is an integration of the new optimization technique in an open-source compiler. Experiments on a set of data mining and machine learning algorithms show that the new technique can speed up the standard implementations by as much as 134X and 46X on average for distance-related problems, outperforming previous TI-based optimizations by 2.35X on average. It also extends the applicability of TI-based optimizations to vector related problems, producing tens of times of speedup. 
id1316#On the Analysis and Design of Visual Cryptography with Error Correcting Capability#A ( k , n ) visual cryptographic scheme (VCS) shares a secret image into n shadow images that are distributed over n involved participants. When k participants stack their shadow images, the secret is revealed. The secret image of VCS is a visual secret. Even though black/white dots in shadows suffer from interference by noise, the color may still retain the corresponding darkness with high probability. Therefore, VCS has noise immunity for secret recovery. Hence, it seems that there is no need to design a VCS that is robust to noise interference when transmitting or storing the files of shadow images. However, some VCSs use the permutations of subpixels in shadow images as information to realize multiple decoding options. For such schemes, we absolutely should ensure the correctness of the shadows. In this article, we investigate a VCS with t -error correcting capability (VCS- t EC). To the best of our knowledge, VCS- t EC is introduced for the first time. Three ( k , n )-VCS- t EC schemes are proposed: the separated scheme, the integrated scheme, and the nonsystematic scheme. 
id1317#4D printing soft robots guided by machine learning and finite element models#This paper presents a method for four-dimensional (4D) printing of soft pneumatic actuator robot (SPA)s, using nonlinear machine learning (ML) and finite element model (FEM). A FEM is developed to accurately simulate experimental actuation to obtain training data for the ML modeling. More than a thousand data training samples from the hyperelastic material FEM model generated to use as training data for the ML model, which was developed to predict the geometrical requirements of the 4D-printed SPA to realize the bending required for specific tasks. The ML model accurately predicted FEM and experimental data and proved to be a viable solution for 4D printing of soft robots and dynamic structures. This work helps to understand how to develop geometrical soft robots’ designs for nonlinear 4D printing problems using ML and FEM. 
id1318#Fast Automatic Visibility Optimization for Thermal Synthetic Aperture Visualization#In this letter, we describe and validate the first fully automatic parameter optimization for thermal synthetic aperture visualization. It replaces previous manual exploration of the parameter space, which is time-consuming and error-prone. We prove that the visibility of targets in thermal integral images is proportional to the variance of the targets' image. Since this is invariant to occlusion, it represents a suitable objective function for optimization. Our findings have the potential to enable fully autonomous search and recuse operations with camera drones. 
id1319#Dynamic interactive self organizing aggregation method in swarm robots#The present study aims to propose a dynamic interactive self-organizing aggregation (DISA) method for swarm robots. The proposed method was determined by way of the movement of swarm robots, obstacle, and robot sensors. The controller used in the DISA method helps the state selector decide through the utilization of these sensors. Systematic simulations were conducted a different number of robots {10, 25, 50}, different detection radii {3, 4} and different arena sizes {40 × 40, 50 × 50, 60 × 60}. The performance of aggregation behavior was compared with other aggregation methods recommended in literature using Total Distance (TD) between robots, Cluster Metrics (CM), Expected Cluster Size (ECS) metric and aggregation completion time. Moreover, noise at different intensities was applied to sensor inputs of the robots. The robustness of the effect of increasing noise on aggregation behavior was examined comparatively. Consequently, the simulation results based on the other compared methods indicated that the utilization of the proposed DISA method led to a higher performance by 88% in the ECS and CM metric as well as in all TD metric measurements and aggregation completion time results. 
id1321#Selective Stiffening in Soft Actuators by Triggered Phase Transition of Hydrogel-Filled Elastomers#Nature has inspired a new generation of robots that not only imitate the behavior of natural systems but also share their adaptability to the environment and level of compliance due to the materials used to manufacture them, which are typically made of soft matter. In order to be adaptable and compliant, these robots need to be able to locally change the mechanical properties of their soft material-based bodies according to external feedback. In this work, a soft actuator that embodies a highly controllable thermo-responsive hydrogel and changes its stiffness on direct stimulation is proposed. At a critical temperature, this stimulation triggers the reversible transition of the hydrogel, which locally stiffens the elastomeric containment at the targeted location. By dividing the actuator into multiple sections, it is possible to control its macroscopic behavior as a function of the stiffened sections. These properties are evaluated by arranging three actuators into a gripper configuration used to grasp objects. The results clearly show that the approach can be used to develop soft actuators that can modify their mechanical properties on-demand in order to conform to objects or to exert the required force. 
id1322#A novel DNA-inspired encryption strategy for concealing cloud storage#Over the last few years, the need of a cloud environment with the ability to detect illegal behaviours along with a secured data storage capability has increased largely. This study presents such a secured cloud storage framework comprising of a deoxyribonucleic acid (DNA) based encryption key which has been generated to make the framework unbreakable, thus ensuring a better and secured distributed cloud storage environment. Furthermore, this work proposes a novel DNA-based encryption technique inspired by the biological characteristics of DNA and the protein synthesis mechanism. The introduced DNA based model also has an additional advantage of being able to decide on selecting suitable storage servers from an existing pool of storage servers on which the data must be stored. A fuzzy-based technique for order of preference by similarity to ideal solution (TOPSIS) multi-criteria decisionmaking (MCDM) model has been employed to achieve the above-mentioned goal. This can decide the set of suitable storage servers and also results in a reduction in execution time by keeping up the level of security to an improved grade. This study also investigates and analyzes the strength of the proposed S-Box and encryption technique against some standard criteria and benchmarks, such as avalanche effect, correlation coefficient, information entropy, linear probability, and differential probability etc. After the avalanche effect analysis, the average change in cipher-text has been found to be 51.85%. Moreover, thorough security, sensitivity and functionality analysis show that the proposed scheme guarantees high security with robustness. 
id1323#Leveraging Compiler Intermediate Representation for Multi- and Cross-Language Verification#Developers nowadays regularly use numerous programming languages with different characteristics and trade-offs. Unfortunately, implementing a software verifier for a new language from scratch is a large and tedious undertaking, requiring expert knowledge in multiple domains, such as compilers, verification, and constraint solving. Hence, only a tiny fraction of the used languages has readily available software verifiers to aid in the development of correct programs. In the past decade, there has been a trend of leveraging popular compiler intermediate representations (IRs), such as LLVM IR, when implementing software verifiers. Processing IR promises out-of-the-box multi- and cross-language verification since, at least in theory, a verifier ought to be able to handle a program in any programming language (and their combination) that can be compiled into the IR. In practice though, to the best of our knowledge, nobody has explored the feasibility and ease of such integration of new languages. In this paper, we provide a procedure for adding support for a new language into an IR-based verification toolflow. Using our procedure, we extend the SMACK verifier with prototypical support for 6 additional languages. We assess the quality of our extensions through several case studies, and we describe our experience in detail to guide future efforts in this area. 
id1324#Julia: Fast and secure key agreement for IoT devices#Even the most resource-constrained IoT devices need to communicate securely. In order to establish a secure channel, key agreement between the communicating parties is used. Today's key agreement protocols require at least three scalar multiplications in the handshake to achieve mutual authentication, forward and backward secrecy, and protection against key-compromise impersonation. As this is a computationally heavy operation, resource-constrained devices benefit from a lower number of scalar multiplications. In this paper we present Julia Key Agreement (JKA), a protocol that satisfies the aforementioned security properties using two scalar multiplications, and thus saves both time and energy. In addition, we define an optimized JKA that only requires a single scalar multiplication for a particular use case. 
id1325#On the Energy Costs of Post-Quantum KEMs in TLS-based Low-Power Secure IoT#Recent achievements in designing quantum computers place a serious threat on the security of state-of-the-art public key cryptography and on all communication that relies on it. Meanwhile, security is seen as one of the most critical issues of low-power IoT devices even with pre-quantum public key cryptography since IoT devices have strict energy constraints and limited computational power. Thus, state-of-the-art dedicated hardware accelerators have been deployed to facilitate secure and confidential communication with well established protocols on such devices. It is common belief that the complexity of the cryptographic computations are also the bottleneck of the new, quantum-resistant algorithms and that hardware accelerators are necessary to use them efficiently on energy constrained embedded devices. In this paper, we carried out an in-depth investigation of the application of potential Post-Quantum Cryptography algorithms, which were proposed in the associated US NIST process, to a representative TLS-based low-power IoT infrastructure. First, we show that the main contributor to the TLS handshake latency are the higher bandwidth requirements of post-quantum Key-Encapsulation Mechanisms rather than the cryptographic computations itself. Second, from the perspective of crypto-agility we show that edge devices with code-based, isogeny-based as well as lattice-based algorithms have low energy consumption, which enables long battery run times in typical IoT scenarios without dedicated hardware accelerators. Third, we increase the level of security further by combining pre-quantum and post-quantum algorithms to a hybrid key exchange, and quantify the overhead in energy consumption and latency of it. 
id1326#The pastwatch: On the usability of provenance data in relational databases#Provenance information can be large and overwhelming to users. We present a set of criteria that any provenance exploration tool must have and introduce Pastwatch, a provenance exploration system that adheres to those criteria. We also address the issues associated with provenance of aggregation queries, including the creation of a summarization method that makes provenance of aggregation queries manageable for users. Finally, we conduct a quantitative user study to show statistically significant results that Pastwatchmakes provenance information more efficient and easier to use than standard approaches. 
id1327#Development of a high-throughput plant disease symptom severity assessment tool using machine learning image analysis and integrated geolocation#Tomato spotted wilt virus (TSWV) has the potential to cause severe yield losses in peanut (Arachis hypogaea L.), an important annual legume grown around the world. The most effective approach to manage the disease caused by TSWV is to grow disease resistant peanut varieties. One of the key challenges to breeding for disease resistance is to develop an accurate, reproducible and efficient disease assessment method. Accurate field-based assessment of disease incidence and severity is technically challenging and time-consuming. To address this challenge, a field-based, high-throughput assessment tool was developed to quantify the spatial distribution of disease symptoms over experimental peanut plots using a Real Time Kinematic Global Positioning System (RTK-GPS), consumer-grade cameras, a microcontroller, and an open-source machine learning software. A field experiment was designed to establish a range of disease incidence and severity scenarios. This field experiment was imaged for two seasons to develop and validate the tool. Using transfer learning, an existing Convolutional Neural Network (CNN) was trained from supervised training imagery to classify and quantify areas within the plot-level imagery as, symptomatic, asymptomatic, or ground. Multiple images were assessed by the machine learning model and georeferenced to individual experimental plots using RTK-GPS data. The CNN model trained to detect the symptom, “stunting and mottling”, was evaluated using Receiver Operating Characteristic (ROC) curve analysis and yielded an Area Under the Curve (AUC) of 0.97, sensitivity of 0.77, and specificity of 0.98 on the test set. Results from the disease assessment tool were compared with results from visual disease assessments, conducted by a trained plant pathologist. Field plot level means from CNN-based assessment of stunting and mottling correlated with plot level means from visual assessment of severity (r = 0.78; P < 0.0001). To further validate the CNN-based method, the TSWV field experiment was analyzed using linear mixed models with both visual severity and CNN-based severity assessments used as responses. Both models (visual or CNN-based assessment) identified the same main effects as being significant and post hoc analysis resulted in the same separation of varieties for their severity of TSWV symptoms. The results of this study demonstrate the successful application of this tool for high-throughput disease severity assessment in peanut under field conditions. 
id1328#Colorimetric point-of-care paper-based sensors for urinary creatinine with smartphone readout#Creatinine is a clinically significant analyte used to diagnose kidney condition. However, the literature still lacks in creatinine sensors fulfilling point-of-care testing requirements. In this paper, we have developed colorimetric paper-based creatinine sensors adhering to point-of-care testing principles. The signal readout is accomplished with a smartphone modified with 3D-printed elements and processed with a self-written application compromising computer vision algorithm for automatic detection of the colored zone. Two colorimetric methods – routinely used Jaffé method and an alternative one with 3,5-dinitrobenzoic acid were both tested and compared. Hue channel intensity from HSV color space and green channel from RGB color space was used as the analytical signal in Jaffé and 3,5-dinitrobenzoate method, respectively. For both kinds of sensors the linear range of the response covered the range significant for urinary analysis, with precision, expressed as RSD, below 5%. Limit of quantification for Jaffé method was 1.05 mmol·L−1 whereas it was 0.82 mmol·L−1 for 3,5-dinitrobenzoate method. The utility of the developed sensors to selectively quantify creatinine in undiluted urine was proved using artificial urine samples and the obtained recoveries were in the range from 70 to 129 %. 
id1329#Calcium identification and scoring based on echocardiography. An exploratory study on aortic valve stenosis#Currently, an echocardiography expert is needed to identify calcium in the aortic valve, and a cardiac CT-Scan image is needed for calcium quantification. When performing a CT-scan, the patient is subject to radiation, and therefore the number of CT-scans that can be performed should be limited, restricting the patient’s monitoring. Computer Vision (CV) has opened new opportunities for improved efficiency when extracting knowledge from an image. Applying CV techniques on echocardiography imaging may reduce the medical workload for identifying the calcium and quantifying it, helping doctors to maintain a better tracking of their patients. In our approach, a simple technique to identify and extract the calcium pixel count from echocardiography imaging, was developed by using CV. Based on anonymized real patient echocardiographic images, this approach enables semi-automatic calcium identification. As the brightness of echocardiography images (with the highest intensity corresponding to calcium) vary depending on the acquisition settings, echocardiographic adaptive image binarization has been performed. Given that blood maintains the same intensity on echocardiographic images-being always the darker region-blood areas in the image were used to create an adaptive threshold for binarization. After binarization, the region of interest (ROI) with calcium, was interactively selected by an echocardiography expert and extracted, allowing us to compute a calcium pixel count, corresponding to the spatial amount of calcium. The results obtained from these experiments are encouraging. With this technique, from echocardiographic images collected for the same patient with different acquisition settings and different brightness, obtaining a calcium pixel count, where pixel values show an absolute pixel value margin of error of 3 (on a scale from 0 to 255), achieving a Pearson Correlation of 0.92 indicating a strong correlation with the human expert assessment of calcium area for the same images. 
id1330#Considering filter importance and irreplaceability in filter pruning#Deep convolutional neural network (CNNs) have gained a great success in computer vision tasks. However, the computation and parameter storage costs of CNNs are very large, thus a large number of studies have tried to reduce the computation and parameters of CNNs. Quantization and pruning are the usual strategies for model compression. Previous filter pruning works usually prune filters with small norm because the common viewpoint is that the smaller norm of filters, the less contribution of filters. However, above strategy ignores that features extracted by those filters with large norm may be redundancy and the features extracted by small norm filters are irreplaceable. In this paper, in light of above findings, we propose a novel filter pruning method called Importance Diversity Filter Pruning (IDFP), which considers both filter contribution and filter irreplaceability. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets. The results illustrate the effectiveness of our method. 
id1331#Low-complexity bit-serial sequential polynomial basis finite field GF(2m) Montgomery multipliers#GF(2m) multiplication is a complex and performance-critical operation in Elliptic curve cryptography algorithms. Many techniques have been proposed in the literature for efficient implementation of GF(2m) multipliers. Montgomery multiplication is a technique used for fast GF(2m) multiplications, which is more efficient when there is a need for computation of many consecutive multiplications. In this paper, we present two modified bit-serial algorithms namely most significant bit (MSB) first algorithm and least significant bit (LSB) first algorithm for Montgomery multiplication where the modification involves employing more efficient logical relations in the formulation of the algorithms. Furthermore, the hardware structures developed for the proposed modified algorithms using bit-serial sequential architectures are also presented in this paper. Comparison of the analytical as well as implementation results of the proposed multipliers with the existing multipliers shows that the proposed multipliers require low area and time complexities. The proposed MSB-first multiplier and the proposed LSB-first multiplier achieve around 17% and 13% reduction in area-delay-product (ADP) complexities for m=409, respectively, when compared with the respective best multipliers available in the literature. The proposed bit-serial sequential multipliers can be used in low-hardware and low-cost applications such as IoT edge devices. 
id1332#Preventing reverse engineering threat in java using byte code obfuscation techniques#Java programs are compiled into a platform independent byte code format. Much of the information contained in the source code is retained in the byte code. Consequently reverse engineering becomes much easier. Several software protection techniques have been developed, of which, code obfuscation seems to be a promising one. In this paper, two new byte code obfuscation techniques have been evolved. These techniques involve applying obfuscating transformations to the Java byte code. These techniques prevent automatic software analysis tools, De-compilers, from producing correct source code by introducing syntax and semantic errors in the generated source code. The proposed techniques are applied on sample Java class files to examine the effectiveness of the techniques in impeding reverse engineering. The results reveal the erroneous codes generated by the tested de-compilers. 
