{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First documents:\n",
      "Formulation of Low-Order Dominant Poles for Y-Matrix of Interconnects: This paper presents an efficient approach to compute the dominant poles for the reduced-order admittance (Y parameter) matrix of lossy interconnects.\n",
      "\n",
      "Our algorithm succeeds with high probability against an adaptive adversary, which can take over processors at any time during the protocol, up to the point of taking over arbitrarily close to a 1/3 fraction.\n",
      "\n",
      "We present an all-pairs shortest path algorithm whose running time on a complete directed graph on n vertices whose edge weights are chosen independently and uniformly at random from [0,1] is O(n2), in expectation and with high probability.\n",
      "\n",
      "We consider the problem of re-ranking the top-k documents returned by a retrieval system given some search query.\n",
      "\n",
      "In this paper, we combine the learning-to-rank paradigm with the recent developments on axioms for information retrieval.\n",
      "\n",
      "We outline important details on cross-validation techniques that can enhance the performance.\n",
      "\n",
      "“Next-fit” allocation differs from first-fit in that a first-fit allocator commences its search for free space at a fixed end of memory, whereas a next-fit allocator commences its search wherever it previously stopped searching.\n",
      "\n",
      "It is important to choose an appropriate network structure because simple networks are likely to under-fit while complex networks are less plastic and more computationally expensive to train.\n",
      "\n",
      "Both of the LL-based algorithms in these papers attempt to minimize the reparsing on the original parse tree and the parse table.\n",
      "\n",
      "In this paper, L2-norm Deep Belief Network (L2DBN) is proposed, which uses L2-norm regularization to optimize the network parameters of DBN.\n",
      "\n",
      "\n",
      "After tokenization and lowercasing:\n",
      "['formulation', 'of', 'low-order', 'dominant', 'poles', 'for', 'y-matrix', 'of', 'interconnects', ':', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "['we', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'o', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "['we', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "['in', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "['we', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "['“', 'next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "['both', 'of', 'the', 'll-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "['in', 'this', 'paper', ',', 'l2-norm', 'deep', 'belief', 'network', '(', 'l2dbn', ')', 'is', 'proposed', ',', 'which', 'uses', 'l2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '.']\n",
      "\n",
      "Original number of tokens: 612\n",
      "\n",
      "NLTK stopwords:\n",
      "{'over', 'mustn', 'do', 'off', 'by', 'll', 'while', 'aren', 'those', \"hadn't\", 'into', 'there', 'on', 'where', \"you've\", 'his', 'm', 'were', 'don', 'between', 'ourselves', 'didn', \"shouldn't\", 'yourselves', 're', \"didn't\", 'should', 'other', 'am', 'which', 'through', 'then', 'just', \"haven't\", \"mustn't\", \"you'll\", 'hers', 'it', 'at', 'its', 'i', 'o', 'be', \"don't\", 'against', 'needn', 'doesn', 'about', 'each', 'herself', 'me', 'further', 'of', 'them', 'once', 'these', 'myself', 'hadn', 'yourself', 've', 'this', 'the', \"hasn't\", 'having', 'what', 'all', 'himself', 'her', 'been', 'does', 'both', 't', 'and', 'until', 'nor', 'did', 'him', 'no', 'during', 'after', 'too', 'for', 'here', 'hasn', \"you're\", 'have', 'wasn', 'was', 'he', 'who', \"doesn't\", 'you', 'y', \"weren't\", 'they', 'couldn', 'yours', 'below', \"wasn't\", 'mightn', 'under', 'wouldn', 'again', 'our', 'so', 'doing', 'most', 'now', 'ours', 'only', 'above', 'had', 'your', 'from', 'own', 'ma', 'to', 'more', \"should've\", 'my', \"aren't\", 'whom', 'but', 'theirs', 'has', \"isn't\", 'weren', 'she', 'themselves', 's', \"mightn't\", 'won', 'are', 'an', 'same', 'when', 'we', 'few', 'd', 'haven', 'out', \"wouldn't\", \"that'll\", 'any', 'down', 'ain', \"needn't\", 'or', 'not', 'such', 'with', \"shan't\", 'shan', 'how', 'as', 'up', \"you'd\", 'that', 'why', \"won't\", 'some', 'can', 'very', 'in', 'because', 'a', \"it's\", 'if', 'their', 'is', 'than', \"couldn't\", 'itself', 'being', 'isn', 'shouldn', 'will', 'before', \"she's\"}\n",
      "\n",
      "Number of tokens after stopword and punctuation removal: 530\n",
      "\n",
      "After removing stop words, punctuation and numbers:\n",
      "formulation low-order dominant poles y-matrix interconnects paper presents efficient approach compute dominant poles reduced-order admittance parameter matrix lossy interconnects\n",
      "algorithm succeeds high probability adaptive adversary take processors time protocol point taking arbitrarily close / fraction\n",
      "present all-pairs shortest path algorithm whose running time complete directed graph n vertices whose edge weights chosen independently uniformly random , n expectation high probability\n",
      "consider problem re-ranking top-k documents returned retrieval system given search query\n",
      "paper combine learning-to-rank paradigm recent developments axioms information retrieval\n",
      "outline important details cross-validation techniques enhance performance\n",
      "“ next-fit ” allocation differs first-fit first-fit allocator commences search free space fixed end memory whereas next-fit allocator commences search wherever previously stopped searching\n",
      "important choose appropriate network structure simple networks likely under-fit complex networks less plastic computationally expensive train\n",
      "ll-based algorithms papers attempt minimize reparsing original parse tree parse table\n",
      "paper l-norm deep belief network ldbn proposed uses l-norm regularization optimize network parameters dbn\n",
      "\n",
      "Number of tokens after stemming: 474\n",
      "\n",
      "After stemming:\n",
      "formul low-ord domin pole y-matrix interconnect paper present effici approach comput domin pole reduced-ord admitt paramet matrix lossi interconnect  \n",
      "algorithm succe high probabl adapt adversari take processor time protocol point take arbitrarili close / fraction  \n",
      "present all-pair shortest path algorithm whose run time complet direct graph n vertic whose edg weight chosen independ uniformli random , n expect high probabl  \n",
      "consid problem re-rank top-k document return retriev system given search queri  \n",
      "paper combin learning-to-rank paradigm recent develop axiom inform retriev  \n",
      "outlin import detail cross-valid techniqu enhanc perform  \n",
      "“ next-fit ” alloc differ first-fit first-fit alloc commenc search free space fix end memori wherea next-fit alloc commenc search wherev previous stop search  \n",
      "import choos appropri network structur simpl network like under-fit complex network less plastic comput expens train  \n",
      "ll-base algorithm paper attempt minim repars origin pars tree pars tabl  \n",
      "paper l-norm deep belief network ldbn propos use l-norm regular optim network paramet dbn  \n",
      "\n",
      "Most common words (total 474)\n",
      "[('paper', 15), ('algorithm', 13), ('present', 10), ('n', 6), ('system', 5), ('network', 5), ('propos', 5), ('data', 5), ('method', 5), ('interconnect', 4), ('comput', 4), ('problem', 4), ('search', 4), ('perform', 4), ('use', 4), ('optim', 4), ('iot', 4), ('task', 4), ('effici', 3), ('high', 3), ('processor', 3), ('independ', 3), ('queri', 3), ('develop', 3), ('inform', 3), ('“', 3), ('”', 3), ('alloc', 3), ('differ', 3), ('train', 3), ('pars', 3), ('achiev', 3), ('two', 3), ('internet', 3), ('thing', 3), ('tool', 3), ('target', 3), ('describ', 3), ('gener', 3), ('°', 3), ('number', 3), ('model', 3), ('result', 3), ('lexic', 3), ('grammar', 3), ('domin', 2), ('pole', 2), ('approach', 2), ('paramet', 2), ('probabl', 2), ('take', 2), ('time', 2), ('point', 2), ('whose', 2), ('run', 2), ('direct', 2), ('chosen', 2), ('expect', 2), ('retriev', 2), ('paradigm', 2), ('import', 2), ('cross-valid', 2), ('next-fit', 2), ('first-fit', 2), ('commenc', 2), ('simpl', 2), ('attempt', 2), ('minim', 2), ('tree', 2), ('l-norm', 2), ('increas', 2), ('absolut', 2), ('amount', 2), ('process', 2), ('also', 2), ('gather', 2), ('b', 2), ('matric', 2), ('r', 2), ('real', 2), ('softwar', 2), ('devic', 2), ('new', 2), ('effort', 2), ('tradit', 2), ('variabl', 2), ('fitt', 2), ('shown', 2), ('difficulti', 2), ('underrepres', 2), ('student', 2), ('design', 2), ('numer', 2), ('opportun', 2), ('singl', 2), ('applic', 2), ('servic', 2), ('research', 2), ('emerg', 2), ('on-chip', 2)]\n",
      "\n",
      "The tf-idf values of the first document\n",
      "\n",
      "reduced 0.19235191180010291\n",
      "present 0.10218404323955707\n",
      "pole 0.38470382360020583\n",
      "paramet 0.16520867871829137\n",
      "paper 0.08630626973836336\n",
      "ord 0.38470382360020583\n",
      "matrix 0.38470382360020583\n",
      "low 0.19235191180010291\n",
      "lossi 0.19235191180010291\n",
      "interconnect 0.33041735743658274\n",
      "formul 0.19235191180010291\n",
      "effici 0.14933090521709766\n",
      "domin 0.38470382360020583\n",
      "comput 0.13806544563647982\n",
      "approach 0.16520867871829137\n",
      "admitt 0.19235191180010291\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# author Zhang Jun 2021 (with some later modfications)\n",
    "# Example how to preprocess raw text data. This is not complete/optimized,\n",
    "# but you can you use the code as a skeleton for your own program.\n",
    "\n",
    "# To be able to run this code, make sure you have the packages NLTK and scikit-learn installed.\n",
    "# If not, you can install them with pip via command \"pip3 install scikit-learn nltk\"\n",
    "# Load the required packages that are used in this example.\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load data \n",
    "data_path = 'acmdocuments.txt'\n",
    "\n",
    "reader = open(data_path, 'r', encoding='utf-8')\n",
    "lines = reader.readlines()\n",
    "\n",
    "# Extract the text from each line\n",
    "text = [i.split('\\t')[0] for i in lines ] \n",
    "\n",
    "# some examples\n",
    "print('First documents:')\n",
    "for i in text[:10]:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "# Step 1: tokenization and lowercasing\n",
    "tokens_list = [word_tokenize(i) for i in text]\n",
    "\n",
    "lc_tokens_list = []    \n",
    "for i in tokens_list: \n",
    "    lc_tokens_list.append([token.lower() for token in i]) \n",
    "\n",
    "print('After tokenization and lowercasing:')\n",
    "for i in lc_tokens_list[:10]:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "# original number of tokens\n",
    "uniques = np.unique([tok for doc in lc_tokens_list for tok in doc])\n",
    "print(\"Original number of tokens: {}\\n\".format(len(uniques)))\n",
    "\n",
    "\n",
    "\n",
    "# Steps 2 and 3: remove stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('NLTK stopwords:')\n",
    "print(stop_words)\n",
    "print()\n",
    "\n",
    "# Here we include the punctuation in the stop words set. There are alternative\n",
    "#ways to remove punctuation.\n",
    "stop_words.update(punctuation)\n",
    "stop_words.add(\"...\")\n",
    "\n",
    "#you can check updated stopwords\n",
    "#print(stop_words)\n",
    "\n",
    "filtered_sentence = []    \n",
    "for i in lc_tokens_list: \n",
    "    filtered_sentence.append([token for token in i if token not in stop_words]) \n",
    "    \n",
    "# Numbers are also removed\n",
    "filtered_sentence = [ ' '.join(i) for i in filtered_sentence ]\n",
    "filtered_sentence = [ re.sub(r'\\d+', '', sentence) for sentence in filtered_sentence ]\n",
    "\n",
    "# number of tokens\n",
    "uniques = np.unique([tok for doc in filtered_sentence for tok in doc.split()])\n",
    "print(\"Number of tokens after stopword and punctuation removal: {}\\n\".format(len(uniques)))\n",
    "\n",
    "\n",
    "print('After removing stop words, punctuation and numbers:')\n",
    "for i in filtered_sentence[:10]:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "\n",
    "# Step 4: stemming\n",
    "porter = PorterStemmer()\n",
    "#or snowball stemmer\n",
    "#stemmer = SnowballStemmer(\"english\",ignore_stopwords=True)\n",
    "stemmed_tokens_list = []\n",
    "\n",
    "for i in filtered_sentence:\n",
    "\tstemmed_tokens_list.append([porter.stem(j) for j in i.split()])\n",
    "\n",
    "# number of tokens\n",
    "uniques = np.unique([tok for doc in stemmed_tokens_list for tok in doc])\n",
    "print(\"Number of tokens after stemming: {}\\n\".format(len(uniques)))\n",
    "\n",
    "print('After stemming:')\n",
    "for i in stemmed_tokens_list[:10]:\n",
    "\tfor j in i:\n",
    "\t\tprint(j,end=\" \")\n",
    "\tprint(\" \")\n",
    "\n",
    "\n",
    "\n",
    "#5. Check most frequent words - candidates to add to the stopword list\n",
    "listofall = [ item for elem in stemmed_tokens_list for item in elem]\n",
    "\n",
    "freq = FreqDist(listofall);\n",
    "wnum=freq.B();\n",
    "print(\"\\nMost common words (total %d)\"%wnum)\n",
    "print(freq.most_common(100))\n",
    "\n",
    "\n",
    "#6. Present as tf-idf\n",
    "cleaned_documents = [ ' '.join(i) for i in stemmed_tokens_list]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf=False)\n",
    "#only tf part:\n",
    "#tfidf_vectorizer = TfidfVectorizer(use_idf=False)\n",
    "\n",
    "tfidf_vectorizer.fit(cleaned_documents)\n",
    "tf_idf_vectors = tfidf_vectorizer.transform(cleaned_documents)\n",
    "\n",
    "print(\"\\nThe tf-idf values of the first document\\n\");\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_index = tf_idf_vectors[0,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [tf_idf_vectors[0, x] for x in feature_index])\n",
    "for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "    print(w, s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
