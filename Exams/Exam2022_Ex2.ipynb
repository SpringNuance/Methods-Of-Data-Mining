{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data items $t_1$,...,$t_8$ have the following distance matrix\n",
    "\n",
    "|   | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    |\n",
    "|---|------|------|------|------|------|------|------|------|\n",
    "| 1 | 0    | 0.33 | 0.89 | 0.75 | 0.75 | 0.57 | 0.89 | 0.89 |\n",
    "| 2 | 0.33 | 0    | 1.00 | 0.57 | 0.57 | 0.57 | 0.75 | 0.75 |\n",
    "| 3 | 0.89 | 1.00 | 0    | 0.89 | 1.00 | 0.89 | 0.75 | 0.75 |\n",
    "| 4 | 0.75 | 0.57 | 0.89 | 0    | 0.33 | 0.75 | 0.75 | 0.57 |\n",
    "| 5 | 0.75 | 0.57 | 1.00 | 0.33 | 0    | 0.75 | 0.75 | 0.57 |\n",
    "| 6 | 0.57 | 0.57 | 0.89 | 0.75 | 0.75 | 0    | 0.89 | 0.89 |\n",
    "| 7 | 0.89 | 0.75 | 0.75 | 0.75 | 0.75 | 0.89 | 0    | 0.33 |\n",
    "| 8 | 0.89 | 0.75 | 0.75 | 0.57 | 0.57 | 0.89 | 0.33 | 0    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Study clustering tendency based on the distance matrix. What threshold value would you choose for discriminating between intra- and inter-cluster distances?\n",
    "\n",
    "b) Apply agglomerative hierarchical clustering algorithm with single linkage metric on the data items. Draw an easily comprehensible dendrogram as the final result by re-arranging the data items appropriately.\n",
    "\n",
    "c) How do the shapes of the clusters generally differ between clustering results with complete versus single linkage metric?\n",
    "\n",
    "d) How could you perform K-means clustering based on the same distance matrix? If not, explain why.\n",
    "\n",
    "e) Name and describe two clustering validation indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Study clustering tendency based on the distance matrix. What threshold value would you choose for discriminating between intra- and inter-cluster distances?\n",
    "\n",
    "To discriminate between intra-cluster (within-cluster) and inter-cluster (between-cluster) distances, you need to choose a threshold value that separates high similarity (low distance) from low similarity (high distance). Here's a general approach:\n",
    "\n",
    "Intra-Cluster Distances: These should be smaller than the threshold, as items within the same cluster are expected to be similar to each other.\n",
    "\n",
    "Inter-Cluster Distances: These should be larger than the threshold, as items from different clusters are expected to be less similar.\n",
    "\n",
    "In other word, we must choose threshold $\\epsilon$ in such a way that the graph is divided into two connected separate clusters, where connection in each cluster is similarity\n",
    "\n",
    "First we have 5 clusters, where inter distance is 0 to 0.33\n",
    "\n",
    "(1), (2), (3), (4), (5), (6), (7), (8)\n",
    "\n",
    "(3), (6), (1 2), (4, 5), (7, 8)\n",
    "We can see that \n",
    "(1st cluster): Nodes 1,2,6 with maximum intercluster distance at 0.57\n",
    "\n",
    "(2nd cluster): Nodes 3,4,5,7,8 with maximum intercluster distance at 0.75\n",
    "\n",
    "Every other distance between these two clusters have a distance at 0.89\n",
    "\n",
    "a threshold value in the range of 0.75 to 0.89 to distinguish between intra-cluster and inter-cluster distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Apply agglomerative hierarchical clustering algorithm with single linkage metric on the data items. Draw an easily comprehensible dendrogram as the final result by re-arranging the data items appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) How do the shapes of the clusters generally differ between clustering results with complete versus single linkage metric?\n",
    "\n",
    "Complete Linkage:\n",
    "\n",
    "In complete linkage clustering, clusters are formed based on the maximum pairwise distance between any two points from different clusters. In other words, clusters are joined when the maximum distance between any pair of points (one from each cluster) is minimized.\n",
    "This tends to create compact, spherical, and well-separated clusters.\n",
    "Complete linkage is sensitive to outliers and can be influenced by noise in the data, as it's trying to minimize the maximum distance.\n",
    "Single Linkage:\n",
    "\n",
    "In single linkage clustering, clusters are formed based on the minimum pairwise distance between any two points from different clusters. Clusters are joined when the minimum distance between any pair of points (one from each cluster) is minimized.\n",
    "This linkage method tends to create elongated and chain-like clusters.\n",
    "Single linkage is more robust to outliers and noise because it focuses on the closest points.\n",
    "To illustrate the difference further:\n",
    "\n",
    "In complete linkage, you're looking for the tightest clusters where the farthest points in different clusters are still relatively close to each other. This can lead to more compact, well-defined, and equally sized clusters.\n",
    "\n",
    "In single linkage, you're looking for the closest points between clusters, which can result in chains or elongated clusters. It tends to be more flexible and less sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) How could you perform K-means clustering based on the same distance matrix? If not, explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is typically performed using the original data points, not a distance matrix. \n",
    "\n",
    " The algorithm works by iteratively assigning data points to clusters based on their proximity to the cluster centroids and updating the centroids accordingly. This process requires direct access to the data points' numerical values.\n",
    "\n",
    "However, if you already have a precomputed distance matrix and want to perform clustering based on it, you can use a variation of K-means called \"K-medoids\" \n",
    "\n",
    "Here are the steps to perform K-medoids clustering based on a distance matrix:\n",
    "\n",
    "Initialize K medoids: Select K data points as initial medoids. You can choose them randomly or use some other method.\n",
    "\n",
    "Assign each data point to the nearest medoid based on the distance matrix.\n",
    "\n",
    "Recalculate the medoids for each cluster by considering all data points within the cluster and selecting the data point that minimizes the sum of distances to other points in the cluster.\n",
    "\n",
    "Repeat steps 2 and 3 until convergence. Convergence occurs when there is no change in the medoids or when the change is below a certain threshold.\n",
    "\n",
    "The final clusters are determined by the data points assigned to each medoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Name and describe two clustering validation indices.\n",
    "\n",
    "1. Silhouette Coefficient\n",
    "The Silhouette Coefficient is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The Silhouette Coefficient for a single data point is calculated as follows:\n",
    "\n",
    "$$s = \\frac{b - a}{max(a, b)}$$\n",
    "\n",
    "where $a$ is the average distance between a sample and all other points in the same class, and $b$ is the average distance between a sample and all other points in the next nearest cluster. The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample.\n",
    "\n",
    "2. Calinski-Harabasz Index\n",
    "The Calinski-Harabasz Index is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.\n",
    "\n",
    "$$s(k) = \\frac{tr(B_k)}{tr(W_k)} \\times \\frac{N - k}{k - 1}$$\n",
    "\n",
    "where $N$ is the number of points, $k$ is the number of clusters, $B_k$ is the between-cluster dispersion matrix, and $W_k$ is the within-cluster dispersion matrix.\n",
    "\n",
    "3. Davies-Bouldin Index\n",
    "The Davies-Bouldin Index is the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of inter-cluster distances to intra-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.\n",
    "\n",
    "$$s(k) = \\frac{1}{k} \\sum_{i=1}^{k} max_{i \\neq j} \\left( \\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)} \\right)$$\n",
    "\n",
    "where $k$ is the number of clusters, $c_i$ is the centroid of cluster $i$, $d(c_i, c_j)$ is the distance between centroids $c_i$ and $c_j$, and $\\sigma_i$ is the average distance of all points in cluster $i$ to centroid $c_i$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
