{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Spectral clustering of the cow data\n",
    "\n",
    "Learning goal: Idea of spectral embedding (and clustering)\n",
    "\n",
    "In this task, you should perform 1-dimensional spectral embedding for the cow data, based on Goodall similarity. Since the file cowdist.csv gives the Goodall distance $d_{GO}$, you need to convert it to Goodall similarity by $sim = 1 − d_{GO}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Similarity/Weight Matrix (W)**:\n",
    "Suppose we have a set of data points, and we compute the pairwise similarities between them to get the similarity matrix. The entry $W_{ij}$ represents the similarity between data points i and j.\n",
    "For simplicity, let's consider a dataset with 3 points and the following similarity matrix:\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "1 & 0.8 & 0.3 \\\\\n",
    "0.8 & 1 & 0.5 \\\\\n",
    "0.3 & 0.5 & 1 \\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "2. **Diagonal Degree Matrix ($\\Lambda$)**:\n",
    "The degree of a node in the similarity graph is the sum of the weights (similarities) of its edges. The degree matrix is a diagonal matrix where the diagonal entry $ \\Lambda_{ii} $ is the sum of the $ i^{th} $ row of the similarity matrix.\n",
    "\n",
    "For our example:\n",
    "$ \\Lambda = \\begin{bmatrix}\n",
    "2.1 & 0 & 0 \\\\\n",
    "0 & 2.3 & 0 \\\\\n",
    "0 & 0 & 1.8 \\\\\n",
    "\\end{bmatrix} $, where $ \\Lambda_{11} = 2.1 = 1 + 0.8 + 0.3 $, $ \\Lambda_{22} = 2.3 = 0.8 + 1 + 0.5 $ and so on\n",
    "\n",
    "3. **Laplacian Matrix (L)**:\n",
    "The Laplacian matrix is defined as:\n",
    "$ L = \\Lambda - W $\n",
    "\n",
    "For our example:\n",
    "$ L = \\begin{bmatrix}\n",
    "2.1 & 0 & 0 \\\\\n",
    "0 & 2.3 & 0 \\\\\n",
    "0 & 0 & 1.8 \\\\\n",
    "\\end{bmatrix} - \\begin{bmatrix}\n",
    "1 & 0.8 & 0.3 \\\\\n",
    "0.8 & 1 & 0.5 \\\\\n",
    "0.3 & 0.5 & 1 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1.1 & -0.8 & -0.3 \\\\\n",
    "-0.8 & 1.3 & -0.5 \\\\\n",
    "-0.3 & -0.5 & 0.8 \\\\\n",
    "\\end{bmatrix} $\n",
    "\n",
    "This Laplacian matrix $L$ captures the structure of the similarity graph. The eigenvalues and eigenvectors of $L$ can be used in spectral clustering to identify clusters in the data.\n",
    "\n",
    "Note: There are also other variants of the Laplacian matrix, such as the normalized Laplacian, which can be used in spectral clustering. The choice of which Laplacian to use depends on the specific application and dataset.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Create a 2-nearest neighbour similarity graph, where the edge weight is the Goodall similarity. Present the corresponding weight matrix $\\textbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity matrix W is: \n",
      "[[1.     0.     0.5463 0.2963 0.25   0.    ]\n",
      " [0.     1.     0.     0.25   0.     0.5463]\n",
      " [0.5463 0.     1.     0.     0.5463 0.    ]\n",
      " [0.2963 0.25   0.     1.     0.     0.5463]\n",
      " [0.25   0.     0.5463 0.     1.     0.    ]\n",
      " [0.     0.5463 0.     0.5463 0.     1.    ]]\n",
      "\n",
      "The 2-nearest neighbor similarity graph is: \n",
      "[[0.     0.     0.5463 0.2963 0.     0.    ]\n",
      " [0.     0.     0.     0.25   0.     0.5463]\n",
      " [0.5463 0.     0.     0.     0.5463 0.    ]\n",
      " [0.2963 0.     0.     0.     0.     0.5463]\n",
      " [0.25   0.     0.5463 0.     0.     0.    ]\n",
      " [0.     0.5463 0.     0.5463 0.     0.    ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the cow data\n",
    "df = pd.read_csv('cowdist.csv')\n",
    "\n",
    "# Convert Goodall distance to Goodall similarity\n",
    "df['sim'] = 1 - df['dGO']\n",
    "\n",
    "# Create a similarity matrix\n",
    "\n",
    "sim_matrix = np.ones((6, 6))\n",
    "\n",
    "index_name = {\n",
    "    \"Clover\": 0,\n",
    "    \"Sunny\": 1,\n",
    "    \"Rose\": 2,\n",
    "    \"Daisy\": 3,\n",
    "    \"Strawberry\": 4,\n",
    "    \"Molly\": 5,\n",
    "}\n",
    "for index, row in df.iterrows():\n",
    "    cow1, cow2 = row['pair'][5:-1].split('-')\n",
    "    i, j = index_name[cow1], index_name[cow2]\n",
    "    sim_matrix[i][j] = row['sim']\n",
    "    sim_matrix[j][i] = row['sim']  # since it's symmetric\n",
    "\n",
    "print(\"The similarity matrix W is: \")\n",
    "for i in range(6):\n",
    "    sim_matrix[i][i] = 0\n",
    "print(sim_matrix)\n",
    "\n",
    "# Create a 2-nearest neighbor similarity graph\n",
    "W = np.zeros((6, 6))\n",
    "\n",
    "for i in range(6):\n",
    "    # Get the indices of the two largest similarities for each row\n",
    "    neighbors = sim_matrix[i].argsort()[-3:-1]  # -3:-1 because the largest similarity is with itself\n",
    "    for j in neighbors:\n",
    "        W[i][j] = sim_matrix[i][j]\n",
    "\n",
    "print()\n",
    "print(\"The 2-nearest neighbor similarity graph is: \")\n",
    "print(W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Calculate the corresponding (unnormalized) Laplacian matrix $\\mathbf{L = \\Lambda − W}$, where $\\mathbf{\\Lambda}$ is the degree matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The degree matrix Λ is: \n",
      "[[0.8426 0.     0.     0.     0.     0.    ]\n",
      " [0.     0.7963 0.     0.     0.     0.    ]\n",
      " [0.     0.     1.0926 0.     0.     0.    ]\n",
      " [0.     0.     0.     0.8426 0.     0.    ]\n",
      " [0.     0.     0.     0.     0.7963 0.    ]\n",
      " [0.     0.     0.     0.     0.     1.0926]]\n",
      "\n",
      "The unnormalized Laplacian matrix L = Λ - W is: \n",
      "[[ 0.8426  0.     -0.5463 -0.2963  0.      0.    ]\n",
      " [ 0.      0.7963  0.     -0.25    0.     -0.5463]\n",
      " [-0.5463  0.      1.0926  0.     -0.5463  0.    ]\n",
      " [-0.2963  0.      0.      0.8426  0.     -0.5463]\n",
      " [-0.25    0.     -0.5463  0.      0.7963  0.    ]\n",
      " [ 0.     -0.5463  0.     -0.5463  0.      1.0926]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the degree matrix Lambda\n",
    "Lambda = np.diag(np.sum(W, axis=1))\n",
    "\n",
    "print(\"The degree matrix Λ is: \")\n",
    "print(Lambda)\n",
    "\n",
    "# Calculate the Laplacian matrix\n",
    "L = Lambda - W\n",
    "\n",
    "print(\"\\nThe unnormalized Laplacian matrix L = Λ - W is: \")\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Calculate eigenvalues and eigenvectors of $\\textbf{L}$ and present the data in one dimension. Remember to skip the smallest eigenvalue $\\lambda \\approx 0$. What is the corresponding clustering of cows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The Fiedler vector provides a way to embed the data points (in this case, cows) into a one-dimensional space. The idea is that this one-dimensional representation captures the inherent structure of the data in a way that makes clustering more evident.\n",
    "\n",
    "Here's how you can use the Fiedler vector for clustering:\n",
    "\n",
    "One-dimensional Embedding: Each component of the Fiedler vector corresponds to a data point (cow). The value of each component provides a one-dimensional representation of that data point.\n",
    "\n",
    "Thresholding: To cluster the data into two groups, you can use the sign of the components of the Fiedler vector. Data points with positive values in the Fiedler vector can be assigned to one cluster, while those with negative values can be assigned to another cluster. This is a simple binary clustering approach.\n",
    "\n",
    "More than Two Clusters: If you want to cluster the data into more than two clusters, you can use additional eigenvectors (beyond the Fiedler vector) and perform k-means clustering on the multi-dimensional representation provided by these eigenvectors.\n",
    "\n",
    "In essence, the Fiedler vector provides a way to represent the data in a lower-dimensional space (in this case, one-dimensional) where the clustering structure is more evident. By simply looking at the sign of the components of the Fiedler vector, you can achieve a binary clustering of the data.\n",
    "\n",
    "In this exercise, the Fiedler vector is the eigenvector corresponding to the second smallest eigenvalue of the Laplacian matrix. The smallest eigenvalue is 0, and the corresponding eigenvector is a vector of ones. This eigenvector does not provide any useful information for clustering, and can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues:\n",
      "[-1.11022302e-16  1.76252371e-01  7.96300000e-01  1.09260000e+00\n",
      "  1.63890000e+00  1.75894763e+00]\n",
      "\n",
      "The eigenvectors:\n",
      "[[ 0.40824829 -0.25110149 -0.42505624 -0.49910455 -0.28867513 -0.45305199]\n",
      " [ 0.40824829  0.49108747  0.53055701 -0.49910455 -0.28867513  0.17415595]\n",
      " [ 0.40824829 -0.44247163  0.19451594 -0.04230009  0.57735027  0.51421163]\n",
      " [ 0.40824829  0.25110149 -0.42505624  0.49910455 -0.28867513  0.45305199]\n",
      " [ 0.40824829 -0.49108747  0.53055701  0.49910455 -0.28867513 -0.17415595]\n",
      " [ 0.40824829  0.44247163  0.19451594  0.04230009  0.57735027 -0.51421163]]\n",
      "\n",
      "The second smallest eigenvalue is: 0.17625237095193105\n",
      "The second smallest eigenvector is:\n",
      "[-0.25110149  0.49108747 -0.44247163  0.25110149 -0.49108747  0.44247163]\n",
      "\n",
      "Clustering (based on sign of the second smallest eigenvalue components): [-1.  1. -1.  1. -1.  1.]\n",
      "The clustering depends only on categorical features, so numerical features are ignored\n",
      "\n",
      "The cow data looks like this\n",
      "\n",
      "Clover,Holstein,2,10,calm,rock\n",
      "Sunny,Ayrshire,2,15,lively,country\n",
      "Rose,Holstein,5,20,calm,classical\n",
      "Daisy,Ayrshire,4,25,kind,rock\n",
      "Strawberry,Finncattle,7,35,calm,classical\n",
      "Molly,Ayrshire,8,45,kind,country\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(L)\n",
    "\n",
    "# Sort eigenvalues in ascending order and get the indices\n",
    "sorted_indices = np.argsort(eigenvalues)\n",
    "\n",
    "# Reorder eigenvalues and eigenvectors\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Fiedler vector (corresponding to the second smallest eigenvalue)\n",
    "second_smallest_eigenvector = eigenvectors[:, 1]\n",
    "\n",
    "# Cluster data based on the sign of the Fiedler vector components\n",
    "clusters = np.sign(second_smallest_eigenvector)\n",
    "\n",
    "print(\"The eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nThe eigenvectors:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "print(\"\\nThe second smallest eigenvalue is:\", eigenvalues[1])\n",
    "print(\"The second smallest eigenvector is:\")\n",
    "print(second_smallest_eigenvector)\n",
    "\n",
    "print(\"\\nClustering (based on sign of the second smallest eigenvalue components):\", clusters)\n",
    "print(\"The clustering depends only on categorical features, so numerical features are ignored\")\n",
    "\n",
    "print(\"\\nThe cow data looks like this\")\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "Clover,Holstein,2,10,calm,rock\n",
    "Sunny,Ayrshire,2,15,lively,country\n",
    "Rose,Holstein,5,20,calm,classical\n",
    "Daisy,Ayrshire,4,25,kind,rock\n",
    "Strawberry,Finncattle,7,35,calm,classical\n",
    "Molly,Ayrshire,8,45,kind,country\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) (Optional): Calculate the random-walk Laplacian $\\mathbf{L}_{rw} = \\mathbf{\\Lambda}^{−1} \\mathbf{L}$ and repeat step (c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Walk Laplacian is a variation of the graph Laplacian that is normalized differently. \n",
    "\n",
    "Here's the intuition behind the Random Walk Laplacian:\n",
    "\n",
    "Random Walk on a Graph: Imagine a random walker moving around the graph. At each node, the walker randomly chooses an edge to traverse to the next node. The probability of choosing a particular edge is proportional to the weight of that edge relative to the total weight of all edges connected to the current node.\n",
    "\n",
    "Transition Probability Matrix: The matrix $\\mathbf{L}_{rw} = \\mathbf{\\Lambda}^{−1} \\mathbf{L}$ can be thought of as a transition probability matrix for this random walk. The entry $\\mathbf{L}rw_{ij}$ represents the probability of transitioning from node i to node j in one step.\n",
    "\n",
    "Clustering Intuition: In the context of spectral clustering, the Random Walk Laplacian provides a way to cluster nodes that are more likely to be visited together during a random walk. If two nodes are in the same cluster, a random walker should frequently transition between them. Conversely, if two nodes are in different clusters, transitions between them should be infrequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues:\n",
      "[-8.32667268e-17  1.89640609e-01  9.72126374e-01  1.32976665e+00\n",
      "  1.67622397e+00  1.83224240e+00]\n",
      "\n",
      "The eigenvectors:\n",
      "[[-0.40824829  0.25253466  0.46566175 -0.49432852 -0.32199758  0.55829789]\n",
      " [-0.40824829 -0.48101314 -0.47862545 -0.5053325  -0.36840617 -0.13053406]\n",
      " [-0.40824829  0.45260646 -0.2325442  -0.01668449  0.51048453 -0.41384094]\n",
      " [-0.40824829 -0.25253466  0.46566175  0.49432852 -0.32199758 -0.55829789]\n",
      " [-0.40824829  0.48101314 -0.47862545  0.5053325  -0.36840617  0.13053406]\n",
      " [-0.40824829 -0.45260646 -0.2325442   0.01668449  0.51048453  0.41384094]]\n",
      "\n",
      "The second smallest eigenvalue is: 0.18964060913962968\n",
      "The second smallest eigenvector is:\n",
      "[ 0.25253466 -0.48101314  0.45260646 -0.25253466  0.48101314 -0.45260646]\n",
      "\n",
      "Clustering (based on sign of the second smallest eigenvalue components): [ 1. -1.  1. -1.  1. -1.]\n",
      "The clustering depends only on categorical features, so numerical features are ignored\n",
      "\n",
      "The cow data looks like this\n",
      "\n",
      "Clover,Holstein,2,10,calm,rock\n",
      "Sunny,Ayrshire,2,15,lively,country\n",
      "Rose,Holstein,5,20,calm,classical\n",
      "Daisy,Ayrshire,4,25,kind,rock\n",
      "Strawberry,Finncattle,7,35,calm,classical\n",
      "Molly,Ayrshire,8,45,kind,country\n",
      "    \n",
      "The clustering result is the same as the one obtained with the unnormalized Laplacian matrix\n"
     ]
    }
   ],
   "source": [
    "# Calculate random-walk Laplacian matrix\n",
    "Lrw = np.linalg.inv(Lambda) @ L\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(Lrw)\n",
    "\n",
    "# Sort eigenvalues in ascending order and get the indices\n",
    "sorted_indices = np.argsort(eigenvalues)\n",
    "\n",
    "# Reorder eigenvalues and eigenvectors\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Fiedler vector (corresponding to the second smallest eigenvalue)\n",
    "second_smallest_eigenvector = eigenvectors[:, 1]\n",
    "\n",
    "# Cluster data based on the sign of the Fiedler vector components\n",
    "clusters = np.sign(second_smallest_eigenvector)\n",
    "\n",
    "print(\"The eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nThe eigenvectors:\")\n",
    "print(eigenvectors)\n",
    "\n",
    "print(\"\\nThe second smallest eigenvalue is:\", eigenvalues[1])\n",
    "print(\"The second smallest eigenvector is:\")\n",
    "print(second_smallest_eigenvector)\n",
    "\n",
    "print(\"\\nClustering (based on sign of the second smallest eigenvalue components):\", clusters)\n",
    "print(\"The clustering depends only on categorical features, so numerical features are ignored\")\n",
    "\n",
    "print(\"\\nThe cow data looks like this\")\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "Clover,Holstein,2,10,calm,rock\n",
    "Sunny,Ayrshire,2,15,lively,country\n",
    "Rose,Holstein,5,20,calm,classical\n",
    "Daisy,Ayrshire,4,25,kind,rock\n",
    "Strawberry,Finncattle,7,35,calm,classical\n",
    "Molly,Ayrshire,8,45,kind,country\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"The clustering result is the same as the one obtained with the unnormalized Laplacian matrix\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
