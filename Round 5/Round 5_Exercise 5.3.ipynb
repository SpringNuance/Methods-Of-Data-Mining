{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Preprocessing NLP data\n",
    "\n",
    "Learning goal: preprocessing text data, detecting errors and special cases; how to increase frequency of important terms and collocations despite different spelling and grammatic forms\n",
    "\n",
    "In this task, we will practise preprocessing of text data with Python Natural Language Toolkit (nltk). The idea is that you can utilize the results in the homework task, even if you were using another programming language\n",
    "for the actual processing.\n",
    "\n",
    "Install packages nltk, scikit-learn and numpy with command pip3 install\n",
    "scikit-learn nltk numpy. There is also a book “Natural Language Processing with Python” https://www.nltk.org/book/ with examples.\n",
    "\n",
    "Load example data acmdocuments.txt from MyCourses. Each line is\n",
    "considered as one document. They are sentences from scientific abstracts in the ACM digital library https://dl.acm.org/. In MyCourses, you can find a code skeleton preprocSbS.py that you can use as a starting point, unless\n",
    "you already know how to do all preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\springnuance\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\springnuance\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\springnuance\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\springnuance\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) The main tasks of preprocessing text data are tokenization, lowercasing, removing punctuation, stemming (or lemmatization), and stop-word removal. However, order of these steps depends on the library and your implementation. The stopword list may contain only stopwords in their normal (unreduced) form, in a stemmed form, or with punctuation characters like (apostrophes). Lemmatization tools often require full sentences so that they can utilize part-of-speech analysis. The first task is always to determine the right order to do the preprocessing steps. Make a fast sanity check to the example code: is it performing the steps as desired? What would happen if you skipped lowercasing or performed stemming before stopword removal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order in which we apply the five preprocessing stages for text data can greatly impact the outcome of the text analysis. The order of application is\n",
    "\n",
    "1. Tokenization: This should be the first step. Tokenization involves splitting the text into individual words (tokens), which provides a basis for all subsequent preprocessing steps.\n",
    "\n",
    "2. Lowercasing: After tokenization, converting all tokens to lowercase is recommended. This standardizes the tokens, so that words like \"Computer\" and \"computer\" are treated as the same word. We should do this before removing stop words or stemming.\n",
    "\n",
    "3. Removing Punctuation: Punctuation can be attached to the end of words and might affect later stages of stemming or stop-words removal. For instance, \"end.\" and \"end\" would be treated differently if punctuation is not removed.\n",
    "\n",
    "4. Stemming/Lemmatization: This step reduces words to their base or root form. Stemming is a more crude method that chops off word endings based on common patterns, while lemmatization involves looking up words in a dictionary to find their base form. \n",
    "\n",
    "5. Removing Stop Words: Stop words are common words that usually do not contribute to the deeper meaning of the text (like \"the\", \"is\", \"at\", etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast sanity check to the example code: is it performing the steps as desired?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Formulation of Low-Order Dominant Poles for Y-Matrix of Interconnects: This paper presents an efficient approach to compute the dominant poles for the reduced-order admittance (Y parameter) matrix of lossy interconnects.\n",
      "\n",
      "1. Tokenization: ['Formulation', 'of', 'Low-Order', 'Dominant', 'Poles', 'for', 'Y-Matrix', 'of', 'Interconnects', ':', 'This', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'Y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "2. Lowercasing: ['formulation', 'of', 'low-order', 'dominant', 'poles', 'for', 'y-matrix', 'of', 'interconnects', ':', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "3. Punctuation: ['formulation', 'of', 'loworder', 'dominant', 'poles', 'for', 'ymatrix', 'of', 'interconnects', '', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reducedorder', 'admittance', '', 'y', 'parameter', '', 'matrix', 'of', 'lossy', 'interconnects', '']\n",
      "4. Stemming: ['formul', 'of', 'loword', 'domin', 'pole', 'for', 'ymatrix', 'of', 'interconnect', '', 'thi', 'paper', 'present', 'an', 'effici', 'approach', 'to', 'comput', 'the', 'domin', 'pole', 'for', 'the', 'reducedord', 'admitt', '', 'y', 'paramet', '', 'matrix', 'of', 'lossi', 'interconnect', '']\n",
      "5. Stopwords: ['formul', 'loword', 'domin', 'pole', 'ymatrix', 'interconnect', 'thi', 'paper', 'present', 'effici', 'approach', 'comput', 'domin', 'pole', 'reducedord', 'admitt', 'paramet', 'matrix', 'lossi', 'interconnect']\n",
      "\n",
      "\n",
      "Our algorithm succeeds with high probability against an adaptive adversary, which can take over processors at any time during the protocol, up to the point of taking over arbitrarily close to a 1/3 fraction.\n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', '', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "4. Stemming: ['our', 'algorithm', 'succe', 'with', 'high', 'probabl', 'against', 'an', 'adapt', 'adversari', '', 'which', 'can', 'take', 'over', 'processor', 'at', 'ani', 'time', 'dure', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'take', 'over', 'arbitrarili', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "5. Stopwords: ['algorithm', 'succe', 'high', 'probabl', 'adapt', 'adversari', 'take', 'processor', 'ani', 'time', 'dure', 'protocol', 'point', 'take', 'arbitrarili', 'close', '13', 'fraction']\n",
      "\n",
      "\n",
      "We present an all-pairs shortest path algorithm whose running time on a complete directed graph on n vertices whose edge weights are chosen independently and uniformly at random from [0,1] is O(n2), in expectation and with high probability.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'O', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'o', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'allpairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expectation', 'and', 'with', 'high', 'probability', '']\n",
      "4. Stemming: ['we', 'present', 'an', 'allpair', 'shortest', 'path', 'algorithm', 'whose', 'run', 'time', 'on', 'a', 'complet', 'direct', 'graph', 'on', 'n', 'vertic', 'whose', 'edg', 'weight', 'are', 'chosen', 'independ', 'and', 'uniformli', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expect', 'and', 'with', 'high', 'probabl', '']\n",
      "5. Stopwords: ['present', 'allpair', 'shortest', 'path', 'algorithm', 'whose', 'run', 'time', 'complet', 'direct', 'graph', 'n', 'vertic', 'whose', 'edg', 'weight', 'chosen', 'independ', 'uniformli', 'random', '01', 'n2', 'expect', 'high', 'probabl']\n",
      "\n",
      "\n",
      "We consider the problem of re-ranking the top-k documents returned by a retrieval system given some search query.\n",
      "\n",
      "1. Tokenization: ['We', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "2. Lowercasing: ['we', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "3. Punctuation: ['we', 'consider', 'the', 'problem', 'of', 'reranking', 'the', 'topk', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '']\n",
      "4. Stemming: ['we', 'consid', 'the', 'problem', 'of', 'rerank', 'the', 'topk', 'document', 'return', 'by', 'a', 'retriev', 'system', 'given', 'some', 'search', 'queri', '']\n",
      "5. Stopwords: ['consid', 'problem', 'rerank', 'topk', 'document', 'return', 'retriev', 'system', 'given', 'search', 'queri']\n",
      "\n",
      "\n",
      "In this paper, we combine the learning-to-rank paradigm with the recent developments on axioms for information retrieval.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'combine', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'combin', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'develop', 'on', 'axiom', 'for', 'inform', 'retriev', '']\n",
      "5. Stopwords: ['thi', 'paper', 'combin', 'learningtorank', 'paradigm', 'recent', 'develop', 'axiom', 'inform', 'retriev']\n",
      "\n",
      "\n",
      "We outline important details on cross-validation techniques that can enhance the performance.\n",
      "\n",
      "1. Tokenization: ['We', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "2. Lowercasing: ['we', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "3. Punctuation: ['we', 'outline', 'important', 'details', 'on', 'crossvalidation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '']\n",
      "4. Stemming: ['we', 'outlin', 'import', 'detail', 'on', 'crossvalid', 'techniqu', 'that', 'can', 'enhanc', 'the', 'perform', '']\n",
      "5. Stopwords: ['outlin', 'import', 'detail', 'crossvalid', 'techniqu', 'enhanc', 'perform']\n",
      "\n",
      "\n",
      "“Next-fit” allocation differs from first-fit in that a first-fit allocator commences its search for free space at a fixed end of memory, whereas a next-fit allocator commences its search wherever it previously stopped searching.\n",
      "\n",
      "1. Tokenization: ['“', 'Next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "2. Lowercasing: ['“', 'next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "3. Punctuation: ['“', 'nextfit', '”', 'allocation', 'differs', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', '', 'whereas', 'a', 'nextfit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '']\n",
      "4. Stemming: ['“', 'nextfit', '”', 'alloc', 'differ', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'alloc', 'commenc', 'it', 'search', 'for', 'free', 'space', 'at', 'a', 'fix', 'end', 'of', 'memori', '', 'wherea', 'a', 'nextfit', 'alloc', 'commenc', 'it', 'search', 'wherev', 'it', 'previous', 'stop', 'search', '']\n",
      "5. Stopwords: ['“', 'nextfit', '”', 'alloc', 'differ', 'firstfit', 'firstfit', 'alloc', 'commenc', 'search', 'free', 'space', 'fix', 'end', 'memori', 'wherea', 'nextfit', 'alloc', 'commenc', 'search', 'wherev', 'previous', 'stop', 'search']\n",
      "\n",
      "\n",
      "It is important to choose an appropriate network structure because simple networks are likely to under-fit while complex networks are less plastic and more computationally expensive to train.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "2. Lowercasing: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "3. Punctuation: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'underfit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '']\n",
      "4. Stemming: ['it', 'is', 'import', 'to', 'choos', 'an', 'appropri', 'network', 'structur', 'becaus', 'simpl', 'network', 'are', 'like', 'to', 'underfit', 'while', 'complex', 'network', 'are', 'less', 'plastic', 'and', 'more', 'comput', 'expens', 'to', 'train', '']\n",
      "5. Stopwords: ['import', 'choos', 'appropri', 'network', 'structur', 'becaus', 'simpl', 'network', 'like', 'underfit', 'complex', 'network', 'less', 'plastic', 'comput', 'expens', 'train']\n",
      "\n",
      "\n",
      "Both of the LL-based algorithms in these papers attempt to minimize the reparsing on the original parse tree and the parse table.\n",
      "\n",
      "1. Tokenization: ['Both', 'of', 'the', 'LL-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "2. Lowercasing: ['both', 'of', 'the', 'll-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "3. Punctuation: ['both', 'of', 'the', 'llbased', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '']\n",
      "4. Stemming: ['both', 'of', 'the', 'llbase', 'algorithm', 'in', 'these', 'paper', 'attempt', 'to', 'minim', 'the', 'repars', 'on', 'the', 'origin', 'pars', 'tree', 'and', 'the', 'pars', 'tabl', '']\n",
      "5. Stopwords: ['llbase', 'algorithm', 'paper', 'attempt', 'minim', 'repars', 'origin', 'pars', 'tree', 'pars', 'tabl']\n",
      "\n",
      "\n",
      "In this paper, L2-norm Deep Belief Network (L2DBN) is proposed, which uses L2-norm regularization to optimize the network parameters of DBN.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'L2-norm', 'Deep', 'Belief', 'Network', '(', 'L2DBN', ')', 'is', 'proposed', ',', 'which', 'uses', 'L2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'DBN', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'l2-norm', 'deep', 'belief', 'network', '(', 'l2dbn', ')', 'is', 'proposed', ',', 'which', 'uses', 'l2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'proposed', '', 'which', 'uses', 'l2norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'propos', '', 'which', 'use', 'l2norm', 'regular', 'to', 'optim', 'the', 'network', 'paramet', 'of', 'dbn', '']\n",
      "5. Stopwords: ['thi', 'paper', 'l2norm', 'deep', 'belief', 'network', 'l2dbn', 'propos', 'use', 'l2norm', 'regular', 'optim', 'network', 'paramet', 'dbn']\n",
      "\n",
      "\n",
      "The dramatic development of IT technology has increased absolute amount of data to store, analyze, and process for computers and it has also rapidly increased the amount of realtime processing for data stream\n",
      "\n",
      "1. Tokenization: ['The', 'dramatic', 'development', 'of', 'IT', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "2. Lowercasing: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "3. Punctuation: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', '', 'analyze', '', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "4. Stemming: ['the', 'dramat', 'develop', 'of', 'it', 'technolog', 'ha', 'increas', 'absolut', 'amount', 'of', 'data', 'to', 'store', '', 'analyz', '', 'and', 'process', 'for', 'comput', 'and', 'it', 'ha', 'also', 'rapidli', 'increas', 'the', 'amount', 'of', 'realtim', 'process', 'for', 'data', 'stream']\n",
      "5. Stopwords: ['dramat', 'develop', 'technolog', 'ha', 'increas', 'absolut', 'amount', 'data', 'store', 'analyz', 'process', 'comput', 'ha', 'also', 'rapidli', 'increas', 'amount', 'realtim', 'process', 'data', 'stream']\n",
      "\n",
      "\n",
      "We present an algorithm achieving gathering in O(n2) rounds in expectation.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'O', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '', 'n2', '', 'rounds', 'in', 'expectation', '']\n",
      "4. Stemming: ['we', 'present', 'an', 'algorithm', 'achiev', 'gather', 'in', 'o', '', 'n2', '', 'round', 'in', 'expect', '']\n",
      "5. Stopwords: ['present', 'algorithm', 'achiev', 'gather', 'n2', 'round', 'expect']\n",
      "\n",
      "\n",
      "Let A and B two n×n matrices over a ring R (e.g., the reals or the integers) each containing at most m nonzero elements.\n",
      "\n",
      "1. Tokenization: ['Let', 'A', 'and', 'B', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'R', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "2. Lowercasing: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "3. Punctuation: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'reals', 'or', 'the', 'integers', '', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '']\n",
      "4. Stemming: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matric', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'real', 'or', 'the', 'integ', '', 'each', 'contain', 'at', 'most', 'm', 'nonzero', 'element', '']\n",
      "5. Stopwords: ['let', 'b', 'two', 'n×n', 'matric', 'ring', 'r', 'eg', 'real', 'integ', 'contain', 'nonzero', 'element']\n",
      "\n",
      "\n",
      "Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited.\n",
      "\n",
      "1. Tokenization: ['Firmware', 'is', 'the', 'enable', 'software', 'of', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'IoT', 'devices', 'being', 'exploited', '.']\n",
      "2. Lowercasing: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '(', 'iot', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '.']\n",
      "3. Punctuation: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '', 'iot', '', 'devices', '', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '']\n",
      "4. Stemming: ['firmwar', 'is', 'the', 'enabl', 'softwar', 'of', 'internet', 'of', 'thing', '', 'iot', '', 'devic', '', 'and', 'it', 'softwar', 'vulner', 'are', 'one', 'of', 'the', 'primari', 'reason', 'of', 'iot', 'devic', 'be', 'exploit', '']\n",
      "5. Stopwords: ['firmwar', 'enabl', 'softwar', 'internet', 'thing', 'iot', 'devic', 'softwar', 'vulner', 'one', 'primari', 'reason', 'iot', 'devic', 'exploit']\n",
      "\n",
      "\n",
      "We present a new algorithm that multiplies A and B using O(m0.7n1.2+n2+o(1)) algebraic operations (i.e., multiplications, additions and subtractions) over R.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'A', 'and', 'B', 'using', 'O', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'R', '.']\n",
      "2. Lowercasing: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'r', '.']\n",
      "3. Punctuation: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebraic', 'operations', '', 'ie', '', 'multiplications', '', 'additions', 'and', 'subtractions', '', 'over', 'r', '']\n",
      "4. Stemming: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multipli', 'a', 'and', 'b', 'use', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebra', 'oper', '', 'ie', '', 'multipl', '', 'addit', 'and', 'subtract', '', 'over', 'r', '']\n",
      "5. Stopwords: ['present', 'new', 'algorithm', 'multipli', 'b', 'use', 'm07n12n2o', '1', 'algebra', 'oper', 'ie', 'multipl', 'addit', 'subtract', 'r']\n",
      "\n",
      "\n",
      "Efforts in “explainable AI” are under way, hopefully eliminating the “black-box” concept in future clinical decision tools.\n",
      "\n",
      "1. Tokenization: ['Efforts', 'in', '“', 'explainable', 'AI', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "2. Lowercasing: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "3. Punctuation: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', '', 'hopefully', 'eliminating', 'the', '“', 'blackbox', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '']\n",
      "4. Stemming: ['effort', 'in', '“', 'explain', 'ai', '”', 'are', 'under', 'way', '', 'hope', 'elimin', 'the', '“', 'blackbox', '”', 'concept', 'in', 'futur', 'clinic', 'decis', 'tool', '']\n",
      "5. Stopwords: ['effort', '“', 'explain', 'ai', '”', 'way', 'hope', 'elimin', '“', 'blackbox', '”', 'concept', 'futur', 'clinic', 'decis', 'tool']\n",
      "\n",
      "\n",
      "Target distance (D) and target width (W), traditionally treated as independent variables in Fitts' target acquisition paradigm, are shown to suffer inextricable confounds with task difficulty.\n",
      "\n",
      "1. Tokenization: ['Target', 'distance', '(', 'D', ')', 'and', 'target', 'width', '(', 'W', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'Fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "2. Lowercasing: ['target', 'distance', '(', 'd', ')', 'and', 'target', 'width', '(', 'w', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "3. Punctuation: ['target', 'distance', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', '', 'target', 'acquisition', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '']\n",
      "4. Stemming: ['target', 'distanc', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'tradit', 'treat', 'as', 'independ', 'variabl', 'in', 'fitt', '', 'target', 'acquisit', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextric', 'confound', 'with', 'task', 'difficulti', '']\n",
      "5. Stopwords: ['target', 'distanc', 'target', 'width', 'w', 'tradit', 'treat', 'independ', 'variabl', 'fitt', 'target', 'acquisit', 'paradigm', 'shown', 'suffer', 'inextric', 'confound', 'task', 'difficulti']\n",
      "\n",
      "\n",
      "This paper describes an NSF-funded initiative involving 600 underrepresented high school students and 60 teachers designed to introduce underrepresented students to the numerous and varied career opportunities in the computing sciences.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'an', 'NSF-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'an', 'nsf-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'an', 'nsffunded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '']\n",
      "4. Stemming: ['thi', 'paper', 'describ', 'an', 'nsffund', 'initi', 'involv', '600', 'underrepres', 'high', 'school', 'student', 'and', '60', 'teacher', 'design', 'to', 'introduc', 'underrepres', 'student', 'to', 'the', 'numer', 'and', 'vari', 'career', 'opportun', 'in', 'the', 'comput', 'scienc', '']\n",
      "5. Stopwords: ['thi', 'paper', 'describ', 'nsffund', 'initi', 'involv', '600', 'underrepres', 'high', 'school', 'student', '60', 'teacher', 'design', 'introduc', 'underrepres', 'student', 'numer', 'vari', 'career', 'opportun', 'comput', 'scienc']\n",
      "\n",
      "\n",
      "The gathering problem, where n autonomous robots with restricted capabilities are required to meet in a single point of the plane, is widely studied.\n",
      "\n",
      "1. Tokenization: ['The', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "2. Lowercasing: ['the', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "3. Punctuation: ['the', 'gathering', 'problem', '', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', '', 'is', 'widely', 'studied', '']\n",
      "4. Stemming: ['the', 'gather', 'problem', '', 'where', 'n', 'autonom', 'robot', 'with', 'restrict', 'capabl', 'are', 'requir', 'to', 'meet', 'in', 'a', 'singl', 'point', 'of', 'the', 'plane', '', 'is', 'wide', 'studi', '']\n",
      "5. Stopwords: ['gather', 'problem', 'n', 'autonom', 'robot', 'restrict', 'capabl', 'requir', 'meet', 'singl', 'point', 'plane', 'wide', 'studi']\n",
      "\n",
      "\n",
      "The information captured by IoT present an unprecedented opportunity to solve large-scale problems in those application domains to deliver services\n",
      "\n",
      "1. Tokenization: ['The', 'information', 'captured', 'by', 'IoT', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "2. Lowercasing: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "3. Punctuation: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'largescale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "4. Stemming: ['the', 'inform', 'captur', 'by', 'iot', 'present', 'an', 'unpreced', 'opportun', 'to', 'solv', 'largescal', 'problem', 'in', 'those', 'applic', 'domain', 'to', 'deliv', 'servic']\n",
      "5. Stopwords: ['inform', 'captur', 'iot', 'present', 'unpreced', 'opportun', 'solv', 'largescal', 'problem', 'applic', 'domain', 'deliv', 'servic']\n",
      "\n",
      "\n",
      "Compliance with the information system (IS) security policy is an established theme in IS research for protecting the IS from user actions.\n",
      "\n",
      "1. Tokenization: ['Compliance', 'with', 'the', 'information', 'system', '(', 'IS', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'IS', 'research', 'for', 'protecting', 'the', 'IS', 'from', 'user', 'actions', '.']\n",
      "2. Lowercasing: ['compliance', 'with', 'the', 'information', 'system', '(', 'is', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '.']\n",
      "3. Punctuation: ['compliance', 'with', 'the', 'information', 'system', '', 'is', '', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '']\n",
      "4. Stemming: ['complianc', 'with', 'the', 'inform', 'system', '', 'is', '', 'secur', 'polici', 'is', 'an', 'establish', 'theme', 'in', 'is', 'research', 'for', 'protect', 'the', 'is', 'from', 'user', 'action', '']\n",
      "5. Stopwords: ['complianc', 'inform', 'system', 'secur', 'polici', 'establish', 'theme', 'research', 'protect', 'user', 'action']\n",
      "\n",
      "\n",
      "In this paper, we develop Re-Vibe, the first system that re-identifies people through footstep-induced floor vibrations.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'develop', 'Re-Vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'develop', 're-vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'develop', 'revibe', '', 'the', 'first', 'system', 'that', 'reidentifies', 'people', 'through', 'footstepinduced', 'floor', 'vibrations', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'develop', 'revib', '', 'the', 'first', 'system', 'that', 'reidentifi', 'peopl', 'through', 'footstepinduc', 'floor', 'vibrat', '']\n",
      "5. Stopwords: ['thi', 'paper', 'develop', 'revib', 'first', 'system', 'reidentifi', 'peopl', 'footstepinduc', 'floor', 'vibrat']\n",
      "\n",
      "\n",
      "I/O is emerging as a major bottleneck for machine learning training, especially in distributed environments.\n",
      "\n",
      "1. Tokenization: ['I/O', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "2. Lowercasing: ['i/o', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "3. Punctuation: ['io', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', '', 'especially', 'in', 'distributed', 'environments', '']\n",
      "4. Stemming: ['io', 'is', 'emerg', 'as', 'a', 'major', 'bottleneck', 'for', 'machin', 'learn', 'train', '', 'especi', 'in', 'distribut', 'environ', '']\n",
      "5. Stopwords: ['io', 'emerg', 'major', 'bottleneck', 'machin', 'learn', 'train', 'especi', 'distribut', 'environ']\n",
      "\n",
      "\n",
      "In this paper, we attempt to improve the query likelihood function by bringing back the negative query generation.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'attempt', 'to', 'improv', 'the', 'queri', 'likelihood', 'function', 'by', 'bring', 'back', 'the', 'neg', 'queri', 'gener', '']\n",
      "5. Stopwords: ['thi', 'paper', 'attempt', 'improv', 'queri', 'likelihood', 'function', 'bring', 'back', 'neg', 'queri', 'gener']\n",
      "\n",
      "\n",
      "In this paper, we propose a new on-chip interconnect scheme called Y-architecture, which can utilize the on-chip routing resources more efficiently than traditional Manhattan interconnect architecture by allowing wires routed in three directions (0°, 60°, and 120°).\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'Y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'Manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'propose', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'called', 'yarchitecture', '', 'which', 'can', 'utilize', 'the', 'onchip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'propos', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'call', 'yarchitectur', '', 'which', 'can', 'util', 'the', 'onchip', 'rout', 'resourc', 'more', 'effici', 'than', 'tradit', 'manhattan', 'interconnect', 'architectur', 'by', 'allow', 'wire', 'rout', 'in', 'three', 'direct', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "5. Stopwords: ['thi', 'paper', 'propos', 'new', 'onchip', 'interconnect', 'scheme', 'call', 'yarchitectur', 'util', 'onchip', 'rout', 'resourc', 'effici', 'tradit', 'manhattan', 'interconnect', 'architectur', 'allow', 'wire', 'rout', 'three', 'direct', '0°', '60°', '120°']\n",
      "\n",
      "\n",
      "We describe an algorithm for Byzantine agreement that is scalable in the sense that each processor sends only O(√n) bits, where n is the total number of processors.\n",
      "\n",
      "1. Tokenization: ['We', 'describe', 'an', 'algorithm', 'for', 'Byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'O', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "2. Lowercasing: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "3. Punctuation: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '', '√n', '', 'bits', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '']\n",
      "4. Stemming: ['we', 'describ', 'an', 'algorithm', 'for', 'byzantin', 'agreement', 'that', 'is', 'scalabl', 'in', 'the', 'sens', 'that', 'each', 'processor', 'send', 'onli', 'o', '', '√n', '', 'bit', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processor', '']\n",
      "5. Stopwords: ['describ', 'algorithm', 'byzantin', 'agreement', 'scalabl', 'sens', 'processor', 'send', 'onli', '√n', 'bit', 'n', 'total', 'number', 'processor']\n",
      "\n",
      "\n",
      "This paper presents an image-based rendering (IBR) system based on RGB-D images.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'IBR', ')', 'system', 'based', 'on', 'RGB-D', 'images', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'ibr', ')', 'system', 'based', 'on', 'rgb-d', 'images', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'imagebased', 'rendering', '', 'ibr', '', 'system', 'based', 'on', 'rgbd', 'images', '']\n",
      "4. Stemming: ['thi', 'paper', 'present', 'an', 'imagebas', 'render', '', 'ibr', '', 'system', 'base', 'on', 'rgbd', 'imag', '']\n",
      "5. Stopwords: ['thi', 'paper', 'present', 'imagebas', 'render', 'ibr', 'system', 'base', 'rgbd', 'imag']\n",
      "\n",
      "\n",
      "In this paper we present a framework and methodology for aligning the business strategy and IT/IS for an organization offering an e-service in a multi-organizational setting.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'IT/IS', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'it/is', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'itis', 'for', 'an', 'organization', 'offering', 'an', 'eservice', 'in', 'a', 'multiorganizational', 'setting', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodolog', 'for', 'align', 'the', 'busi', 'strategi', 'and', 'iti', 'for', 'an', 'organ', 'offer', 'an', 'eservic', 'in', 'a', 'multiorganiz', 'set', '']\n",
      "5. Stopwords: ['thi', 'paper', 'present', 'framework', 'methodolog', 'align', 'busi', 'strategi', 'iti', 'organ', 'offer', 'eservic', 'multiorganiz', 'set']\n",
      "\n",
      "\n",
      "The D Programming Language is a hybrid of C++ and modern scripting languages: it compiles statically to native code, but is also garbage collected.\n",
      "\n",
      "1. Tokenization: ['The', 'D', 'Programming', 'Language', 'is', 'a', 'hybrid', 'of', 'C++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "2. Lowercasing: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "3. Punctuation: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'scripting', 'languages', '', 'it', 'compiles', 'statically', 'to', 'native', 'code', '', 'but', 'is', 'also', 'garbage', 'collected', '']\n",
      "4. Stemming: ['the', 'd', 'program', 'languag', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'script', 'languag', '', 'it', 'compil', 'static', 'to', 'nativ', 'code', '', 'but', 'is', 'also', 'garbag', 'collect', '']\n",
      "5. Stopwords: ['program', 'languag', 'hybrid', 'c', 'modern', 'script', 'languag', 'compil', 'static', 'nativ', 'code', 'also', 'garbag', 'collect']\n",
      "\n",
      "\n",
      "However, the main aim is precisely to present an algorithm which gives the so-called minimal solutions: Boolean matrices M satisfying the equation with the least possible number of unity entries.\n",
      "\n",
      "1. Tokenization: ['However', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'Boolean', 'matrices', 'M', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "2. Lowercasing: ['however', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "3. Punctuation: ['however', '', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'socalled', 'minimal', 'solutions', '', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '']\n",
      "4. Stemming: ['howev', '', 'the', 'main', 'aim', 'is', 'precis', 'to', 'present', 'an', 'algorithm', 'which', 'give', 'the', 'socal', 'minim', 'solut', '', 'boolean', 'matric', 'm', 'satisfi', 'the', 'equat', 'with', 'the', 'least', 'possibl', 'number', 'of', 'uniti', 'entri', '']\n",
      "5. Stopwords: ['howev', 'main', 'aim', 'precis', 'present', 'algorithm', 'give', 'socal', 'minim', 'solut', 'boolean', 'matric', 'satisfi', 'equat', 'least', 'possibl', 'number', 'uniti', 'entri']\n",
      "\n",
      "\n",
      "A number of algorithms have been proposed for LR incremental parsers, but few have been proposed for LL incremental parsers [1, 2].\n",
      "\n",
      "1. Tokenization: ['A', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'LR', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'LL', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "2. Lowercasing: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "3. Punctuation: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', '', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '', '1', '', '2', '', '']\n",
      "4. Stemming: ['a', 'number', 'of', 'algorithm', 'have', 'been', 'propos', 'for', 'lr', 'increment', 'parser', '', 'but', 'few', 'have', 'been', 'propos', 'for', 'll', 'increment', 'parser', '', '1', '', '2', '', '']\n",
      "5. Stopwords: ['number', 'algorithm', 'propos', 'lr', 'increment', 'parser', 'propos', 'increment', 'parser', '1', '2']\n",
      "\n",
      "\n",
      "I will discuss the use of graphical models for data mining.\n",
      "\n",
      "1. Tokenization: ['I', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "2. Lowercasing: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "3. Punctuation: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '']\n",
      "4. Stemming: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphic', 'model', 'for', 'data', 'mine', '']\n",
      "5. Stopwords: ['discuss', 'use', 'graphic', 'model', 'data', 'mine']\n",
      "\n",
      "\n",
      "Over the past decade, a pair of synchronization instructions known as LL/SC has emerged as the most suitable set of instructions to be used in the design of lock-free algorithms.\n",
      "\n",
      "1. Tokenization: ['Over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'LL/SC', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "2. Lowercasing: ['over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'll/sc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "3. Punctuation: ['over', 'the', 'past', 'decade', '', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'llsc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lockfree', 'algorithms', '']\n",
      "4. Stemming: ['over', 'the', 'past', 'decad', '', 'a', 'pair', 'of', 'synchron', 'instruct', 'known', 'as', 'llsc', 'ha', 'emerg', 'as', 'the', 'most', 'suitabl', 'set', 'of', 'instruct', 'to', 'be', 'use', 'in', 'the', 'design', 'of', 'lockfre', 'algorithm', '']\n",
      "5. Stopwords: ['past', 'decad', 'pair', 'synchron', 'instruct', 'known', 'llsc', 'ha', 'emerg', 'suitabl', 'set', 'instruct', 'use', 'design', 'lockfre', 'algorithm']\n",
      "\n",
      "\n",
      "Our algorithm has latency that is polylogarithmic in n. \n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '']\n",
      "4. Stemming: ['our', 'algorithm', 'ha', 'latenc', 'that', 'is', 'polylogarithm', 'in', 'n', '']\n",
      "5. Stopwords: ['algorithm', 'ha', 'latenc', 'polylogarithm', 'n']\n",
      "\n",
      "\n",
      "Typical person re-identification (re-ID) systems rely on cameras to match the same person across different locations.\n",
      "\n",
      "1. Tokenization: ['Typical', 'person', 're-identification', '(', 're-ID', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "2. Lowercasing: ['typical', 'person', 're-identification', '(', 're-id', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "3. Punctuation: ['typical', 'person', 'reidentification', '', 'reid', '', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '']\n",
      "4. Stemming: ['typic', 'person', 'reidentif', '', 'reid', '', 'system', 'reli', 'on', 'camera', 'to', 'match', 'the', 'same', 'person', 'across', 'differ', 'locat', '']\n",
      "5. Stopwords: ['typic', 'person', 'reidentif', 'reid', 'system', 'reli', 'camera', 'match', 'person', 'across', 'differ', 'locat']\n",
      "\n",
      "\n",
      "The Internet of Things (IoT) is the latest Internet evolution that incorporates a diverse range of things such as sensors, actuators, and services deployed by different organizations and individuals to support a variety of applications.\n",
      "\n",
      "1. Tokenization: ['The', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'is', 'the', 'latest', 'Internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "2. Lowercasing: ['the', 'internet', 'of', 'things', '(', 'iot', ')', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "3. Punctuation: ['the', 'internet', 'of', 'things', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', '', 'actuators', '', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '']\n",
      "4. Stemming: ['the', 'internet', 'of', 'thing', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolut', 'that', 'incorpor', 'a', 'divers', 'rang', 'of', 'thing', 'such', 'as', 'sensor', '', 'actuat', '', 'and', 'servic', 'deploy', 'by', 'differ', 'organ', 'and', 'individu', 'to', 'support', 'a', 'varieti', 'of', 'applic', '']\n",
      "5. Stopwords: ['internet', 'thing', 'iot', 'latest', 'internet', 'evolut', 'incorpor', 'divers', 'rang', 'thing', 'sensor', 'actuat', 'servic', 'deploy', 'differ', 'organ', 'individu', 'support', 'varieti', 'applic']\n",
      "\n",
      "\n",
      "This subsection compares the state of the art methods and the proposed WRA-Net for the BoniRob dataset through the same method.\n",
      "\n",
      "1. Tokenization: ['This', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'WRA-Net', 'for', 'the', 'BoniRob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "2. Lowercasing: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wra-net', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "3. Punctuation: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "4. Stemming: ['thi', 'subsect', 'compar', 'the', 'state', 'of', 'the', 'art', 'method', 'and', 'the', 'propos', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "5. Stopwords: ['thi', 'subsect', 'compar', 'state', 'art', 'method', 'propos', 'wranet', 'bonirob', 'dataset', 'method']\n",
      "\n",
      "\n",
      "Hence, WRA-Net achieved higher values for PSNRand SSIM compared to the state-of-the-art methods.\n",
      "\n",
      "1. Tokenization: ['Hence', ',', 'WRA-Net', 'achieved', 'higher', 'values', 'for', 'PSNRand', 'SSIM', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "2. Lowercasing: ['hence', ',', 'wra-net', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "3. Punctuation: ['hence', '', 'wranet', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'stateoftheart', 'methods', '']\n",
      "4. Stemming: ['henc', '', 'wranet', 'achiev', 'higher', 'valu', 'for', 'psnrand', 'ssim', 'compar', 'to', 'the', 'stateoftheart', 'method', '']\n",
      "5. Stopwords: ['henc', 'wranet', 'achiev', 'higher', 'valu', 'psnrand', 'ssim', 'compar', 'stateoftheart', 'method']\n",
      "\n",
      "\n",
      "It is shown that relative movement amplitude D/W(which determines difficulty) and absolute movement amplitude D (or scale) are the only two variables that can be manipulated independently in a Fitts' task experiment.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'D/W', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'D', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'Fitts', \"'\", 'task', 'experiment', '.']\n",
      "2. Lowercasing: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'd/w', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'd', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', \"'\", 'task', 'experiment', '.']\n",
      "3. Punctuation: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'dw', '', 'which', 'determines', 'difficulty', '', 'and', 'absolute', 'movement', 'amplitude', 'd', '', 'or', 'scale', '', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', '', 'task', 'experiment', '']\n",
      "4. Stemming: ['it', 'is', 'shown', 'that', 'rel', 'movement', 'amplitud', 'dw', '', 'which', 'determin', 'difficulti', '', 'and', 'absolut', 'movement', 'amplitud', 'd', '', 'or', 'scale', '', 'are', 'the', 'onli', 'two', 'variabl', 'that', 'can', 'be', 'manipul', 'independ', 'in', 'a', 'fitt', '', 'task', 'experi', '']\n",
      "5. Stopwords: ['shown', 'rel', 'movement', 'amplitud', 'dw', 'determin', 'difficulti', 'absolut', 'movement', 'amplitud', 'scale', 'onli', 'two', 'variabl', 'manipul', 'independ', 'fitt', 'task', 'experi']\n",
      "\n",
      "\n",
      "The area-under-the-curve (AUC) was the chosen performance metric for comparison and cross-validation was performed.\n",
      "\n",
      "1. Tokenization: ['The', 'area-under-the-curve', '(', 'AUC', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "2. Lowercasing: ['the', 'area-under-the-curve', '(', 'auc', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "3. Punctuation: ['the', 'areaunderthecurve', '', 'auc', '', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'crossvalidation', 'was', 'performed', '']\n",
      "4. Stemming: ['the', 'areaunderthecurv', '', 'auc', '', 'wa', 'the', 'chosen', 'perform', 'metric', 'for', 'comparison', 'and', 'crossvalid', 'wa', 'perform', '']\n",
      "5. Stopwords: ['areaunderthecurv', 'auc', 'wa', 'chosen', 'perform', 'metric', 'comparison', 'crossvalid', 'wa', 'perform']\n",
      "\n",
      "\n",
      "In this paper we present a tool to assist in teaching top-down and bottom-up analysis algorithms. The tool provides simulation for the following analysis algorithms: LL, SLR, LALR and LR.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'The', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'LL', ',', 'SLR', ',', 'LALR', 'and', 'LR', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'll', ',', 'slr', ',', 'lalr', 'and', 'lr', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'topdown', 'and', 'bottomup', 'analysis', 'algorithms', '', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teach', 'topdown', 'and', 'bottomup', 'analysi', 'algorithm', '', 'the', 'tool', 'provid', 'simul', 'for', 'the', 'follow', 'analysi', 'algorithm', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "5. Stopwords: ['thi', 'paper', 'present', 'tool', 'assist', 'teach', 'topdown', 'bottomup', 'analysi', 'algorithm', 'tool', 'provid', 'simul', 'follow', 'analysi', 'algorithm', 'slr', 'lalr', 'lr']\n",
      "\n",
      "\n",
      "In this paper, we formally show that standard gradient methods never overfit on separable data: If we run these methods for T iterations on a dataset of size m, both the empirical risk and the generalization error decrease at an essentially optimal rate of Õ(1/γ2T) up till T ∼ m.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'If', 'we', 'run', 'these', 'methods', 'for', 'T', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'Õ', '(', '1/γ2T', ')', 'up', 'till', 'T', '∼', 'm', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '(', '1/γ2t', ')', 'up', 'till', 't', '∼', 'm', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', '', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'formal', 'show', 'that', 'standard', 'gradient', 'method', 'never', 'overfit', 'on', 'separ', 'data', '', 'if', 'we', 'run', 'these', 'method', 'for', 't', 'iter', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empir', 'risk', 'and', 'the', 'gener', 'error', 'decreas', 'at', 'an', 'essenti', 'optim', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "5. Stopwords: ['thi', 'paper', 'formal', 'show', 'standard', 'gradient', 'method', 'never', 'overfit', 'separ', 'data', 'run', 'method', 'iter', 'dataset', 'size', 'empir', 'risk', 'gener', 'error', 'decreas', 'essenti', 'optim', 'rate', 'õ', '1γ2t', 'till', '∼']\n",
      "\n",
      "\n",
      "This paper is concerned with goodness of fit evaluation for virtual commissioning modelling purposes.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "3. Punctuation: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '']\n",
      "4. Stemming: ['thi', 'paper', 'is', 'concern', 'with', 'good', 'of', 'fit', 'evalu', 'for', 'virtual', 'commiss', 'model', 'purpos', '']\n",
      "5. Stopwords: ['thi', 'paper', 'concern', 'good', 'fit', 'evalu', 'virtual', 'commiss', 'model', 'purpos']\n",
      "\n",
      "\n",
      "The results showed that the optimal performance was achieved under natural complexification of the EANN and that back-propagation tended to over fit the data.\n",
      "\n",
      "1. Tokenization: ['The', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'EANN', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "2. Lowercasing: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "3. Punctuation: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'backpropagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "4. Stemming: ['the', 'result', 'show', 'that', 'the', 'optim', 'perform', 'wa', 'achiev', 'under', 'natur', 'complexif', 'of', 'the', 'eann', 'and', 'that', 'backpropag', 'tend', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "5. Stopwords: ['result', 'show', 'optim', 'perform', 'wa', 'achiev', 'natur', 'complexif', 'eann', 'backpropag', 'tend', 'fit', 'data']\n",
      "\n",
      "\n",
      "The second approach does not train models that generalize across tasks, but rather over-fit a single instance of a problem\n",
      "\n",
      "1. Tokenization: ['The', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "2. Lowercasing: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "3. Punctuation: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', '', 'but', 'rather', 'overfit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "4. Stemming: ['the', 'second', 'approach', 'doe', 'not', 'train', 'model', 'that', 'gener', 'across', 'task', '', 'but', 'rather', 'overfit', 'a', 'singl', 'instanc', 'of', 'a', 'problem']\n",
      "5. Stopwords: ['second', 'approach', 'doe', 'train', 'model', 'gener', 'across', 'task', 'rather', 'overfit', 'singl', 'instanc', 'problem']\n",
      "\n",
      "\n",
      "This paper describes two significant contributions to the NILM community in an effort towards reproducible state-of-the-art research.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'NILM', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'stateoftheart', 'research', '']\n",
      "4. Stemming: ['thi', 'paper', 'describ', 'two', 'signific', 'contribut', 'to', 'the', 'nilm', 'commun', 'in', 'an', 'effort', 'toward', 'reproduc', 'stateoftheart', 'research', '']\n",
      "5. Stopwords: ['thi', 'paper', 'describ', 'two', 'signific', 'contribut', 'nilm', 'commun', 'effort', 'toward', 'reproduc', 'stateoftheart', 'research']\n",
      "\n",
      "\n",
      "Accurate numerical results for a definite integral are easily obtained by simple substitutions of upper and lower bounds of integral into obtained approximate symbolic results\n",
      "\n",
      "1. Tokenization: ['Accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "2. Lowercasing: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "3. Punctuation: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "4. Stemming: ['accur', 'numer', 'result', 'for', 'a', 'definit', 'integr', 'are', 'easili', 'obtain', 'by', 'simpl', 'substitut', 'of', 'upper', 'and', 'lower', 'bound', 'of', 'integr', 'into', 'obtain', 'approxim', 'symbol', 'result']\n",
      "5. Stopwords: ['accur', 'numer', 'result', 'definit', 'integr', 'easili', 'obtain', 'simpl', 'substitut', 'upper', 'lower', 'bound', 'integr', 'obtain', 'approxim', 'symbol', 'result']\n",
      "\n",
      "\n",
      "This paper presents an optimal algorithm for jumper insertion under the ratio upper-bound.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "4. Stemming: ['thi', 'paper', 'present', 'an', 'optim', 'algorithm', 'for', 'jumper', 'insert', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "5. Stopwords: ['thi', 'paper', 'present', 'optim', 'algorithm', 'jumper', 'insert', 'ratio', 'upperbound']\n",
      "\n",
      "\n",
      "In particular, we argue that inter-tagger agreement is not a real upperbound for the Basque WSD task.\n",
      "\n",
      "1. Tokenization: ['In', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'Basque', 'WSD', 'task', '.']\n",
      "2. Lowercasing: ['in', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '.']\n",
      "3. Punctuation: ['in', 'particular', '', 'we', 'argue', 'that', 'intertagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '']\n",
      "4. Stemming: ['in', 'particular', '', 'we', 'argu', 'that', 'intertagg', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basqu', 'wsd', 'task', '']\n",
      "5. Stopwords: ['particular', 'argu', 'intertagg', 'agreement', 'real', 'upperbound', 'basqu', 'wsd', 'task']\n",
      "\n",
      "\n",
      "Lexicalized context-free grammar(LCFG) is an attractive compromise between the parsing efficiency of context-free grammar (CFG) and the elegance and lexical sensitivity of lexicalized tree adjoining grammar (LTAG).\n",
      "\n",
      "1. Tokenization: ['Lexicalized', 'context-free', 'grammar', '(', 'LCFG', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'CFG', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'LTAG', ')', '.']\n",
      "2. Lowercasing: ['lexicalized', 'context-free', 'grammar', '(', 'lcfg', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'cfg', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'ltag', ')', '.']\n",
      "3. Punctuation: ['lexicalized', 'contextfree', 'grammar', '', 'lcfg', '', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'contextfree', 'grammar', '', 'cfg', '', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '', 'ltag', '', '']\n",
      "4. Stemming: ['lexic', 'contextfre', 'grammar', '', 'lcfg', '', 'is', 'an', 'attract', 'compromis', 'between', 'the', 'pars', 'effici', 'of', 'contextfre', 'grammar', '', 'cfg', '', 'and', 'the', 'eleg', 'and', 'lexic', 'sensit', 'of', 'lexic', 'tree', 'adjoin', 'grammar', '', 'ltag', '', '']\n",
      "5. Stopwords: ['lexic', 'contextfre', 'grammar', 'lcfg', 'attract', 'compromis', 'pars', 'effici', 'contextfre', 'grammar', 'cfg', 'eleg', 'lexic', 'sensit', 'lexic', 'tree', 'adjoin', 'grammar', 'ltag']\n",
      "\n",
      "\n",
      "Formulation of Low-Order Dominant Poles for Y-Matrix of Interconnects: This paper presents an efficient approach to compute the dominant poles for the reduced-order admittance (Y parameter) matrix of lossy interconnects.\n",
      "\n",
      "1. Tokenization: ['Formulation', 'of', 'Low-Order', 'Dominant', 'Poles', 'for', 'Y-Matrix', 'of', 'Interconnects', ':', 'This', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'Y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "2. Lowercasing: ['formulation', 'of', 'low-order', 'dominant', 'poles', 'for', 'y-matrix', 'of', 'interconnects', ':', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "3. Punctuation: ['formulation', 'of', 'loworder', 'dominant', 'poles', 'for', 'ymatrix', 'of', 'interconnects', '', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reducedorder', 'admittance', '', 'y', 'parameter', '', 'matrix', 'of', 'lossy', 'interconnects', '']\n",
      "4. Stemming: ['formul', 'of', 'loword', 'domin', 'pole', 'for', 'ymatrix', 'of', 'interconnect', '', 'this', 'paper', 'present', 'an', 'effici', 'approach', 'to', 'comput', 'the', 'domin', 'pole', 'for', 'the', 'reducedord', 'admitt', '', 'y', 'paramet', '', 'matrix', 'of', 'lossi', 'interconnect', '']\n",
      "5. Stopwords: ['formul', 'loword', 'domin', 'pole', 'ymatrix', 'interconnect', 'paper', 'present', 'effici', 'approach', 'comput', 'domin', 'pole', 'reducedord', 'admitt', 'paramet', 'matrix', 'lossi', 'interconnect']\n",
      "\n",
      "\n",
      "Our algorithm succeeds with high probability against an adaptive adversary, which can take over processors at any time during the protocol, up to the point of taking over arbitrarily close to a 1/3 fraction.\n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', '', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "4. Stemming: ['our', 'algorithm', 'succeed', 'with', 'high', 'probabl', 'against', 'an', 'adapt', 'adversari', '', 'which', 'can', 'take', 'over', 'processor', 'at', 'ani', 'time', 'dure', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'take', 'over', 'arbitrarili', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "5. Stopwords: ['algorithm', 'succeed', 'high', 'probabl', 'adapt', 'adversari', 'take', 'processor', 'ani', 'time', 'dure', 'protocol', 'point', 'take', 'arbitrarili', 'close', '13', 'fraction']\n",
      "\n",
      "\n",
      "We present an all-pairs shortest path algorithm whose running time on a complete directed graph on n vertices whose edge weights are chosen independently and uniformly at random from [0,1] is O(n2), in expectation and with high probability.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'O', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'o', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'allpairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expectation', 'and', 'with', 'high', 'probability', '']\n",
      "4. Stemming: ['we', 'present', 'an', 'allpair', 'shortest', 'path', 'algorithm', 'whose', 'run', 'time', 'on', 'a', 'complet', 'direct', 'graph', 'on', 'n', 'vertic', 'whose', 'edg', 'weight', 'are', 'chosen', 'independ', 'and', 'uniform', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expect', 'and', 'with', 'high', 'probabl', '']\n",
      "5. Stopwords: ['present', 'allpair', 'shortest', 'path', 'algorithm', 'whose', 'run', 'time', 'complet', 'direct', 'graph', 'n', 'vertic', 'whose', 'edg', 'weight', 'chosen', 'independ', 'uniform', 'random', '01', 'n2', 'expect', 'high', 'probabl']\n",
      "\n",
      "\n",
      "We consider the problem of re-ranking the top-k documents returned by a retrieval system given some search query.\n",
      "\n",
      "1. Tokenization: ['We', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "2. Lowercasing: ['we', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "3. Punctuation: ['we', 'consider', 'the', 'problem', 'of', 'reranking', 'the', 'topk', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '']\n",
      "4. Stemming: ['we', 'consid', 'the', 'problem', 'of', 'rerank', 'the', 'topk', 'document', 'return', 'by', 'a', 'retriev', 'system', 'given', 'some', 'search', 'queri', '']\n",
      "5. Stopwords: ['consid', 'problem', 'rerank', 'topk', 'document', 'return', 'retriev', 'system', 'given', 'search', 'queri']\n",
      "\n",
      "\n",
      "In this paper, we combine the learning-to-rank paradigm with the recent developments on axioms for information retrieval.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'combine', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'combin', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'develop', 'on', 'axiom', 'for', 'inform', 'retriev', '']\n",
      "5. Stopwords: ['paper', 'combin', 'learningtorank', 'paradigm', 'recent', 'develop', 'axiom', 'inform', 'retriev']\n",
      "\n",
      "\n",
      "We outline important details on cross-validation techniques that can enhance the performance.\n",
      "\n",
      "1. Tokenization: ['We', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "2. Lowercasing: ['we', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "3. Punctuation: ['we', 'outline', 'important', 'details', 'on', 'crossvalidation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '']\n",
      "4. Stemming: ['we', 'outlin', 'import', 'detail', 'on', 'crossvalid', 'techniqu', 'that', 'can', 'enhanc', 'the', 'perform', '']\n",
      "5. Stopwords: ['outlin', 'import', 'detail', 'crossvalid', 'techniqu', 'enhanc', 'perform']\n",
      "\n",
      "\n",
      "“Next-fit” allocation differs from first-fit in that a first-fit allocator commences its search for free space at a fixed end of memory, whereas a next-fit allocator commences its search wherever it previously stopped searching.\n",
      "\n",
      "1. Tokenization: ['“', 'Next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "2. Lowercasing: ['“', 'next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "3. Punctuation: ['“', 'nextfit', '”', 'allocation', 'differs', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', '', 'whereas', 'a', 'nextfit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '']\n",
      "4. Stemming: ['“', 'nextfit', '”', 'alloc', 'differ', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'alloc', 'commenc', 'it', 'search', 'for', 'free', 'space', 'at', 'a', 'fix', 'end', 'of', 'memori', '', 'wherea', 'a', 'nextfit', 'alloc', 'commenc', 'it', 'search', 'wherev', 'it', 'previous', 'stop', 'search', '']\n",
      "5. Stopwords: ['“', 'nextfit', '”', 'alloc', 'differ', 'firstfit', 'firstfit', 'alloc', 'commenc', 'search', 'free', 'space', 'fix', 'end', 'memori', 'wherea', 'nextfit', 'alloc', 'commenc', 'search', 'wherev', 'previous', 'stop', 'search']\n",
      "\n",
      "\n",
      "It is important to choose an appropriate network structure because simple networks are likely to under-fit while complex networks are less plastic and more computationally expensive to train.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "2. Lowercasing: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "3. Punctuation: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'underfit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '']\n",
      "4. Stemming: ['it', 'is', 'import', 'to', 'choos', 'an', 'appropri', 'network', 'structur', 'becaus', 'simpl', 'network', 'are', 'like', 'to', 'underfit', 'while', 'complex', 'network', 'are', 'less', 'plastic', 'and', 'more', 'comput', 'expens', 'to', 'train', '']\n",
      "5. Stopwords: ['import', 'choos', 'appropri', 'network', 'structur', 'becaus', 'simpl', 'network', 'like', 'underfit', 'complex', 'network', 'less', 'plastic', 'comput', 'expens', 'train']\n",
      "\n",
      "\n",
      "Both of the LL-based algorithms in these papers attempt to minimize the reparsing on the original parse tree and the parse table.\n",
      "\n",
      "1. Tokenization: ['Both', 'of', 'the', 'LL-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "2. Lowercasing: ['both', 'of', 'the', 'll-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "3. Punctuation: ['both', 'of', 'the', 'llbased', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '']\n",
      "4. Stemming: ['both', 'of', 'the', 'llbase', 'algorithm', 'in', 'these', 'paper', 'attempt', 'to', 'minim', 'the', 'repars', 'on', 'the', 'origin', 'pars', 'tree', 'and', 'the', 'pars', 'tabl', '']\n",
      "5. Stopwords: ['llbase', 'algorithm', 'paper', 'attempt', 'minim', 'repars', 'origin', 'pars', 'tree', 'pars', 'tabl']\n",
      "\n",
      "\n",
      "In this paper, L2-norm Deep Belief Network (L2DBN) is proposed, which uses L2-norm regularization to optimize the network parameters of DBN.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'L2-norm', 'Deep', 'Belief', 'Network', '(', 'L2DBN', ')', 'is', 'proposed', ',', 'which', 'uses', 'L2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'DBN', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'l2-norm', 'deep', 'belief', 'network', '(', 'l2dbn', ')', 'is', 'proposed', ',', 'which', 'uses', 'l2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'proposed', '', 'which', 'uses', 'l2norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'propos', '', 'which', 'use', 'l2norm', 'regular', 'to', 'optim', 'the', 'network', 'paramet', 'of', 'dbn', '']\n",
      "5. Stopwords: ['paper', 'l2norm', 'deep', 'belief', 'network', 'l2dbn', 'propos', 'use', 'l2norm', 'regular', 'optim', 'network', 'paramet', 'dbn']\n",
      "\n",
      "\n",
      "The dramatic development of IT technology has increased absolute amount of data to store, analyze, and process for computers and it has also rapidly increased the amount of realtime processing for data stream\n",
      "\n",
      "1. Tokenization: ['The', 'dramatic', 'development', 'of', 'IT', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "2. Lowercasing: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "3. Punctuation: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', '', 'analyze', '', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "4. Stemming: ['the', 'dramat', 'develop', 'of', 'it', 'technolog', 'has', 'increas', 'absolut', 'amount', 'of', 'data', 'to', 'store', '', 'analyz', '', 'and', 'process', 'for', 'comput', 'and', 'it', 'has', 'also', 'rapid', 'increas', 'the', 'amount', 'of', 'realtim', 'process', 'for', 'data', 'stream']\n",
      "5. Stopwords: ['dramat', 'develop', 'technolog', 'increas', 'absolut', 'amount', 'data', 'store', 'analyz', 'process', 'comput', 'also', 'rapid', 'increas', 'amount', 'realtim', 'process', 'data', 'stream']\n",
      "\n",
      "\n",
      "We present an algorithm achieving gathering in O(n2) rounds in expectation.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'O', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '', 'n2', '', 'rounds', 'in', 'expectation', '']\n",
      "4. Stemming: ['we', 'present', 'an', 'algorithm', 'achiev', 'gather', 'in', 'o', '', 'n2', '', 'round', 'in', 'expect', '']\n",
      "5. Stopwords: ['present', 'algorithm', 'achiev', 'gather', 'n2', 'round', 'expect']\n",
      "\n",
      "\n",
      "Let A and B two n×n matrices over a ring R (e.g., the reals or the integers) each containing at most m nonzero elements.\n",
      "\n",
      "1. Tokenization: ['Let', 'A', 'and', 'B', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'R', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "2. Lowercasing: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "3. Punctuation: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'reals', 'or', 'the', 'integers', '', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '']\n",
      "4. Stemming: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matric', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'real', 'or', 'the', 'integ', '', 'each', 'contain', 'at', 'most', 'm', 'nonzero', 'element', '']\n",
      "5. Stopwords: ['let', 'b', 'two', 'n×n', 'matric', 'ring', 'r', 'eg', 'real', 'integ', 'contain', 'nonzero', 'element']\n",
      "\n",
      "\n",
      "Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited.\n",
      "\n",
      "1. Tokenization: ['Firmware', 'is', 'the', 'enable', 'software', 'of', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'IoT', 'devices', 'being', 'exploited', '.']\n",
      "2. Lowercasing: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '(', 'iot', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '.']\n",
      "3. Punctuation: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '', 'iot', '', 'devices', '', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '']\n",
      "4. Stemming: ['firmwar', 'is', 'the', 'enabl', 'softwar', 'of', 'internet', 'of', 'thing', '', 'iot', '', 'devic', '', 'and', 'it', 'softwar', 'vulner', 'are', 'one', 'of', 'the', 'primari', 'reason', 'of', 'iot', 'devic', 'be', 'exploit', '']\n",
      "5. Stopwords: ['firmwar', 'enabl', 'softwar', 'internet', 'thing', 'iot', 'devic', 'softwar', 'vulner', 'one', 'primari', 'reason', 'iot', 'devic', 'exploit']\n",
      "\n",
      "\n",
      "We present a new algorithm that multiplies A and B using O(m0.7n1.2+n2+o(1)) algebraic operations (i.e., multiplications, additions and subtractions) over R.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'A', 'and', 'B', 'using', 'O', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'R', '.']\n",
      "2. Lowercasing: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'r', '.']\n",
      "3. Punctuation: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebraic', 'operations', '', 'ie', '', 'multiplications', '', 'additions', 'and', 'subtractions', '', 'over', 'r', '']\n",
      "4. Stemming: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multipli', 'a', 'and', 'b', 'use', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebra', 'oper', '', 'ie', '', 'multipl', '', 'addit', 'and', 'subtract', '', 'over', 'r', '']\n",
      "5. Stopwords: ['present', 'new', 'algorithm', 'multipli', 'b', 'use', 'm07n12n2o', '1', 'algebra', 'oper', 'ie', 'multipl', 'addit', 'subtract', 'r']\n",
      "\n",
      "\n",
      "Efforts in “explainable AI” are under way, hopefully eliminating the “black-box” concept in future clinical decision tools.\n",
      "\n",
      "1. Tokenization: ['Efforts', 'in', '“', 'explainable', 'AI', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "2. Lowercasing: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "3. Punctuation: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', '', 'hopefully', 'eliminating', 'the', '“', 'blackbox', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '']\n",
      "4. Stemming: ['effort', 'in', '“', 'explain', 'ai', '”', 'are', 'under', 'way', '', 'hope', 'elimin', 'the', '“', 'blackbox', '”', 'concept', 'in', 'futur', 'clinic', 'decis', 'tool', '']\n",
      "5. Stopwords: ['effort', '“', 'explain', 'ai', '”', 'way', 'hope', 'elimin', '“', 'blackbox', '”', 'concept', 'futur', 'clinic', 'decis', 'tool']\n",
      "\n",
      "\n",
      "Target distance (D) and target width (W), traditionally treated as independent variables in Fitts' target acquisition paradigm, are shown to suffer inextricable confounds with task difficulty.\n",
      "\n",
      "1. Tokenization: ['Target', 'distance', '(', 'D', ')', 'and', 'target', 'width', '(', 'W', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'Fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "2. Lowercasing: ['target', 'distance', '(', 'd', ')', 'and', 'target', 'width', '(', 'w', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "3. Punctuation: ['target', 'distance', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', '', 'target', 'acquisition', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '']\n",
      "4. Stemming: ['target', 'distanc', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'tradit', 'treat', 'as', 'independ', 'variabl', 'in', 'fitt', '', 'target', 'acquisit', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextric', 'confound', 'with', 'task', 'difficulti', '']\n",
      "5. Stopwords: ['target', 'distanc', 'target', 'width', 'w', 'tradit', 'treat', 'independ', 'variabl', 'fitt', 'target', 'acquisit', 'paradigm', 'shown', 'suffer', 'inextric', 'confound', 'task', 'difficulti']\n",
      "\n",
      "\n",
      "This paper describes an NSF-funded initiative involving 600 underrepresented high school students and 60 teachers designed to introduce underrepresented students to the numerous and varied career opportunities in the computing sciences.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'an', 'NSF-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'an', 'nsf-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'an', 'nsffunded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '']\n",
      "4. Stemming: ['this', 'paper', 'describ', 'an', 'nsffund', 'initi', 'involv', '600', 'underrepres', 'high', 'school', 'student', 'and', '60', 'teacher', 'design', 'to', 'introduc', 'underrepres', 'student', 'to', 'the', 'numer', 'and', 'vari', 'career', 'opportun', 'in', 'the', 'comput', 'scienc', '']\n",
      "5. Stopwords: ['paper', 'describ', 'nsffund', 'initi', 'involv', '600', 'underrepres', 'high', 'school', 'student', '60', 'teacher', 'design', 'introduc', 'underrepres', 'student', 'numer', 'vari', 'career', 'opportun', 'comput', 'scienc']\n",
      "\n",
      "\n",
      "The gathering problem, where n autonomous robots with restricted capabilities are required to meet in a single point of the plane, is widely studied.\n",
      "\n",
      "1. Tokenization: ['The', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "2. Lowercasing: ['the', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "3. Punctuation: ['the', 'gathering', 'problem', '', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', '', 'is', 'widely', 'studied', '']\n",
      "4. Stemming: ['the', 'gather', 'problem', '', 'where', 'n', 'autonom', 'robot', 'with', 'restrict', 'capabl', 'are', 'requir', 'to', 'meet', 'in', 'a', 'singl', 'point', 'of', 'the', 'plane', '', 'is', 'wide', 'studi', '']\n",
      "5. Stopwords: ['gather', 'problem', 'n', 'autonom', 'robot', 'restrict', 'capabl', 'requir', 'meet', 'singl', 'point', 'plane', 'wide', 'studi']\n",
      "\n",
      "\n",
      "The information captured by IoT present an unprecedented opportunity to solve large-scale problems in those application domains to deliver services\n",
      "\n",
      "1. Tokenization: ['The', 'information', 'captured', 'by', 'IoT', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "2. Lowercasing: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "3. Punctuation: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'largescale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "4. Stemming: ['the', 'inform', 'captur', 'by', 'iot', 'present', 'an', 'unpreced', 'opportun', 'to', 'solv', 'largescal', 'problem', 'in', 'those', 'applic', 'domain', 'to', 'deliv', 'servic']\n",
      "5. Stopwords: ['inform', 'captur', 'iot', 'present', 'unpreced', 'opportun', 'solv', 'largescal', 'problem', 'applic', 'domain', 'deliv', 'servic']\n",
      "\n",
      "\n",
      "Compliance with the information system (IS) security policy is an established theme in IS research for protecting the IS from user actions.\n",
      "\n",
      "1. Tokenization: ['Compliance', 'with', 'the', 'information', 'system', '(', 'IS', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'IS', 'research', 'for', 'protecting', 'the', 'IS', 'from', 'user', 'actions', '.']\n",
      "2. Lowercasing: ['compliance', 'with', 'the', 'information', 'system', '(', 'is', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '.']\n",
      "3. Punctuation: ['compliance', 'with', 'the', 'information', 'system', '', 'is', '', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '']\n",
      "4. Stemming: ['complianc', 'with', 'the', 'inform', 'system', '', 'is', '', 'secur', 'polici', 'is', 'an', 'establish', 'theme', 'in', 'is', 'research', 'for', 'protect', 'the', 'is', 'from', 'user', 'action', '']\n",
      "5. Stopwords: ['complianc', 'inform', 'system', 'secur', 'polici', 'establish', 'theme', 'research', 'protect', 'user', 'action']\n",
      "\n",
      "\n",
      "In this paper, we develop Re-Vibe, the first system that re-identifies people through footstep-induced floor vibrations.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'develop', 'Re-Vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'develop', 're-vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'develop', 'revibe', '', 'the', 'first', 'system', 'that', 'reidentifies', 'people', 'through', 'footstepinduced', 'floor', 'vibrations', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'develop', 'revib', '', 'the', 'first', 'system', 'that', 'reidentifi', 'peopl', 'through', 'footstepinduc', 'floor', 'vibrat', '']\n",
      "5. Stopwords: ['paper', 'develop', 'revib', 'first', 'system', 'reidentifi', 'peopl', 'footstepinduc', 'floor', 'vibrat']\n",
      "\n",
      "\n",
      "I/O is emerging as a major bottleneck for machine learning training, especially in distributed environments.\n",
      "\n",
      "1. Tokenization: ['I/O', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "2. Lowercasing: ['i/o', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "3. Punctuation: ['io', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', '', 'especially', 'in', 'distributed', 'environments', '']\n",
      "4. Stemming: ['io', 'is', 'emerg', 'as', 'a', 'major', 'bottleneck', 'for', 'machin', 'learn', 'train', '', 'especi', 'in', 'distribut', 'environ', '']\n",
      "5. Stopwords: ['io', 'emerg', 'major', 'bottleneck', 'machin', 'learn', 'train', 'especi', 'distribut', 'environ']\n",
      "\n",
      "\n",
      "In this paper, we attempt to improve the query likelihood function by bringing back the negative query generation.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improv', 'the', 'queri', 'likelihood', 'function', 'by', 'bring', 'back', 'the', 'negat', 'queri', 'generat', '']\n",
      "5. Stopwords: ['paper', 'attempt', 'improv', 'queri', 'likelihood', 'function', 'bring', 'back', 'negat', 'queri', 'generat']\n",
      "\n",
      "\n",
      "In this paper, we propose a new on-chip interconnect scheme called Y-architecture, which can utilize the on-chip routing resources more efficiently than traditional Manhattan interconnect architecture by allowing wires routed in three directions (0°, 60°, and 120°).\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'Y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'Manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'propose', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'called', 'yarchitecture', '', 'which', 'can', 'utilize', 'the', 'onchip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'propos', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'call', 'yarchitectur', '', 'which', 'can', 'util', 'the', 'onchip', 'rout', 'resourc', 'more', 'effici', 'than', 'tradit', 'manhattan', 'interconnect', 'architectur', 'by', 'allow', 'wire', 'rout', 'in', 'three', 'direct', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "5. Stopwords: ['paper', 'propos', 'new', 'onchip', 'interconnect', 'scheme', 'call', 'yarchitectur', 'util', 'onchip', 'rout', 'resourc', 'effici', 'tradit', 'manhattan', 'interconnect', 'architectur', 'allow', 'wire', 'rout', 'three', 'direct', '0°', '60°', '120°']\n",
      "\n",
      "\n",
      "We describe an algorithm for Byzantine agreement that is scalable in the sense that each processor sends only O(√n) bits, where n is the total number of processors.\n",
      "\n",
      "1. Tokenization: ['We', 'describe', 'an', 'algorithm', 'for', 'Byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'O', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "2. Lowercasing: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "3. Punctuation: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '', '√n', '', 'bits', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '']\n",
      "4. Stemming: ['we', 'describ', 'an', 'algorithm', 'for', 'byzantin', 'agreement', 'that', 'is', 'scalabl', 'in', 'the', 'sens', 'that', 'each', 'processor', 'send', 'onli', 'o', '', '√n', '', 'bit', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processor', '']\n",
      "5. Stopwords: ['describ', 'algorithm', 'byzantin', 'agreement', 'scalabl', 'sens', 'processor', 'send', 'onli', '√n', 'bit', 'n', 'total', 'number', 'processor']\n",
      "\n",
      "\n",
      "This paper presents an image-based rendering (IBR) system based on RGB-D images.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'IBR', ')', 'system', 'based', 'on', 'RGB-D', 'images', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'ibr', ')', 'system', 'based', 'on', 'rgb-d', 'images', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'imagebased', 'rendering', '', 'ibr', '', 'system', 'based', 'on', 'rgbd', 'images', '']\n",
      "4. Stemming: ['this', 'paper', 'present', 'an', 'imagebas', 'render', '', 'ibr', '', 'system', 'base', 'on', 'rgbd', 'imag', '']\n",
      "5. Stopwords: ['paper', 'present', 'imagebas', 'render', 'ibr', 'system', 'base', 'rgbd', 'imag']\n",
      "\n",
      "\n",
      "In this paper we present a framework and methodology for aligning the business strategy and IT/IS for an organization offering an e-service in a multi-organizational setting.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'IT/IS', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'it/is', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'itis', 'for', 'an', 'organization', 'offering', 'an', 'eservice', 'in', 'a', 'multiorganizational', 'setting', '']\n",
      "4. Stemming: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodolog', 'for', 'align', 'the', 'busi', 'strategi', 'and', 'iti', 'for', 'an', 'organ', 'offer', 'an', 'eservic', 'in', 'a', 'multiorganiz', 'set', '']\n",
      "5. Stopwords: ['paper', 'present', 'framework', 'methodolog', 'align', 'busi', 'strategi', 'iti', 'organ', 'offer', 'eservic', 'multiorganiz', 'set']\n",
      "\n",
      "\n",
      "The D Programming Language is a hybrid of C++ and modern scripting languages: it compiles statically to native code, but is also garbage collected.\n",
      "\n",
      "1. Tokenization: ['The', 'D', 'Programming', 'Language', 'is', 'a', 'hybrid', 'of', 'C++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "2. Lowercasing: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "3. Punctuation: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'scripting', 'languages', '', 'it', 'compiles', 'statically', 'to', 'native', 'code', '', 'but', 'is', 'also', 'garbage', 'collected', '']\n",
      "4. Stemming: ['the', 'd', 'program', 'languag', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'script', 'languag', '', 'it', 'compil', 'static', 'to', 'nativ', 'code', '', 'but', 'is', 'also', 'garbag', 'collect', '']\n",
      "5. Stopwords: ['program', 'languag', 'hybrid', 'c', 'modern', 'script', 'languag', 'compil', 'static', 'nativ', 'code', 'also', 'garbag', 'collect']\n",
      "\n",
      "\n",
      "However, the main aim is precisely to present an algorithm which gives the so-called minimal solutions: Boolean matrices M satisfying the equation with the least possible number of unity entries.\n",
      "\n",
      "1. Tokenization: ['However', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'Boolean', 'matrices', 'M', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "2. Lowercasing: ['however', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "3. Punctuation: ['however', '', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'socalled', 'minimal', 'solutions', '', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '']\n",
      "4. Stemming: ['howev', '', 'the', 'main', 'aim', 'is', 'precis', 'to', 'present', 'an', 'algorithm', 'which', 'give', 'the', 'socal', 'minim', 'solut', '', 'boolean', 'matric', 'm', 'satisfi', 'the', 'equat', 'with', 'the', 'least', 'possibl', 'number', 'of', 'uniti', 'entri', '']\n",
      "5. Stopwords: ['howev', 'main', 'aim', 'precis', 'present', 'algorithm', 'give', 'socal', 'minim', 'solut', 'boolean', 'matric', 'satisfi', 'equat', 'least', 'possibl', 'number', 'uniti', 'entri']\n",
      "\n",
      "\n",
      "A number of algorithms have been proposed for LR incremental parsers, but few have been proposed for LL incremental parsers [1, 2].\n",
      "\n",
      "1. Tokenization: ['A', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'LR', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'LL', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "2. Lowercasing: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "3. Punctuation: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', '', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '', '1', '', '2', '', '']\n",
      "4. Stemming: ['a', 'number', 'of', 'algorithm', 'have', 'been', 'propos', 'for', 'lr', 'increment', 'parser', '', 'but', 'few', 'have', 'been', 'propos', 'for', 'll', 'increment', 'parser', '', '1', '', '2', '', '']\n",
      "5. Stopwords: ['number', 'algorithm', 'propos', 'lr', 'increment', 'parser', 'propos', 'increment', 'parser', '1', '2']\n",
      "\n",
      "\n",
      "I will discuss the use of graphical models for data mining.\n",
      "\n",
      "1. Tokenization: ['I', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "2. Lowercasing: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "3. Punctuation: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '']\n",
      "4. Stemming: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphic', 'model', 'for', 'data', 'mine', '']\n",
      "5. Stopwords: ['discuss', 'use', 'graphic', 'model', 'data', 'mine']\n",
      "\n",
      "\n",
      "Over the past decade, a pair of synchronization instructions known as LL/SC has emerged as the most suitable set of instructions to be used in the design of lock-free algorithms.\n",
      "\n",
      "1. Tokenization: ['Over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'LL/SC', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "2. Lowercasing: ['over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'll/sc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "3. Punctuation: ['over', 'the', 'past', 'decade', '', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'llsc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lockfree', 'algorithms', '']\n",
      "4. Stemming: ['over', 'the', 'past', 'decad', '', 'a', 'pair', 'of', 'synchron', 'instruct', 'known', 'as', 'llsc', 'has', 'emerg', 'as', 'the', 'most', 'suitabl', 'set', 'of', 'instruct', 'to', 'be', 'use', 'in', 'the', 'design', 'of', 'lockfre', 'algorithm', '']\n",
      "5. Stopwords: ['past', 'decad', 'pair', 'synchron', 'instruct', 'known', 'llsc', 'emerg', 'suitabl', 'set', 'instruct', 'use', 'design', 'lockfre', 'algorithm']\n",
      "\n",
      "\n",
      "Our algorithm has latency that is polylogarithmic in n. \n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '']\n",
      "4. Stemming: ['our', 'algorithm', 'has', 'latenc', 'that', 'is', 'polylogarithm', 'in', 'n', '']\n",
      "5. Stopwords: ['algorithm', 'latenc', 'polylogarithm', 'n']\n",
      "\n",
      "\n",
      "Typical person re-identification (re-ID) systems rely on cameras to match the same person across different locations.\n",
      "\n",
      "1. Tokenization: ['Typical', 'person', 're-identification', '(', 're-ID', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "2. Lowercasing: ['typical', 'person', 're-identification', '(', 're-id', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "3. Punctuation: ['typical', 'person', 'reidentification', '', 'reid', '', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '']\n",
      "4. Stemming: ['typic', 'person', 'reidentif', '', 'reid', '', 'system', 'reli', 'on', 'camera', 'to', 'match', 'the', 'same', 'person', 'across', 'differ', 'locat', '']\n",
      "5. Stopwords: ['typic', 'person', 'reidentif', 'reid', 'system', 'reli', 'camera', 'match', 'person', 'across', 'differ', 'locat']\n",
      "\n",
      "\n",
      "The Internet of Things (IoT) is the latest Internet evolution that incorporates a diverse range of things such as sensors, actuators, and services deployed by different organizations and individuals to support a variety of applications.\n",
      "\n",
      "1. Tokenization: ['The', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'is', 'the', 'latest', 'Internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "2. Lowercasing: ['the', 'internet', 'of', 'things', '(', 'iot', ')', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "3. Punctuation: ['the', 'internet', 'of', 'things', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', '', 'actuators', '', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '']\n",
      "4. Stemming: ['the', 'internet', 'of', 'thing', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolut', 'that', 'incorpor', 'a', 'divers', 'rang', 'of', 'thing', 'such', 'as', 'sensor', '', 'actuat', '', 'and', 'servic', 'deploy', 'by', 'differ', 'organ', 'and', 'individu', 'to', 'support', 'a', 'varieti', 'of', 'applic', '']\n",
      "5. Stopwords: ['internet', 'thing', 'iot', 'latest', 'internet', 'evolut', 'incorpor', 'divers', 'rang', 'thing', 'sensor', 'actuat', 'servic', 'deploy', 'differ', 'organ', 'individu', 'support', 'varieti', 'applic']\n",
      "\n",
      "\n",
      "This subsection compares the state of the art methods and the proposed WRA-Net for the BoniRob dataset through the same method.\n",
      "\n",
      "1. Tokenization: ['This', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'WRA-Net', 'for', 'the', 'BoniRob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "2. Lowercasing: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wra-net', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "3. Punctuation: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "4. Stemming: ['this', 'subsect', 'compar', 'the', 'state', 'of', 'the', 'art', 'method', 'and', 'the', 'propos', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "5. Stopwords: ['subsect', 'compar', 'state', 'art', 'method', 'propos', 'wranet', 'bonirob', 'dataset', 'method']\n",
      "\n",
      "\n",
      "Hence, WRA-Net achieved higher values for PSNRand SSIM compared to the state-of-the-art methods.\n",
      "\n",
      "1. Tokenization: ['Hence', ',', 'WRA-Net', 'achieved', 'higher', 'values', 'for', 'PSNRand', 'SSIM', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "2. Lowercasing: ['hence', ',', 'wra-net', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "3. Punctuation: ['hence', '', 'wranet', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'stateoftheart', 'methods', '']\n",
      "4. Stemming: ['henc', '', 'wranet', 'achiev', 'higher', 'valu', 'for', 'psnrand', 'ssim', 'compar', 'to', 'the', 'stateoftheart', 'method', '']\n",
      "5. Stopwords: ['henc', 'wranet', 'achiev', 'higher', 'valu', 'psnrand', 'ssim', 'compar', 'stateoftheart', 'method']\n",
      "\n",
      "\n",
      "It is shown that relative movement amplitude D/W(which determines difficulty) and absolute movement amplitude D (or scale) are the only two variables that can be manipulated independently in a Fitts' task experiment.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'D/W', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'D', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'Fitts', \"'\", 'task', 'experiment', '.']\n",
      "2. Lowercasing: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'd/w', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'd', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', \"'\", 'task', 'experiment', '.']\n",
      "3. Punctuation: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'dw', '', 'which', 'determines', 'difficulty', '', 'and', 'absolute', 'movement', 'amplitude', 'd', '', 'or', 'scale', '', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', '', 'task', 'experiment', '']\n",
      "4. Stemming: ['it', 'is', 'shown', 'that', 'relat', 'movement', 'amplitud', 'dw', '', 'which', 'determin', 'difficulti', '', 'and', 'absolut', 'movement', 'amplitud', 'd', '', 'or', 'scale', '', 'are', 'the', 'onli', 'two', 'variabl', 'that', 'can', 'be', 'manipul', 'independ', 'in', 'a', 'fitt', '', 'task', 'experi', '']\n",
      "5. Stopwords: ['shown', 'relat', 'movement', 'amplitud', 'dw', 'determin', 'difficulti', 'absolut', 'movement', 'amplitud', 'scale', 'onli', 'two', 'variabl', 'manipul', 'independ', 'fitt', 'task', 'experi']\n",
      "\n",
      "\n",
      "The area-under-the-curve (AUC) was the chosen performance metric for comparison and cross-validation was performed.\n",
      "\n",
      "1. Tokenization: ['The', 'area-under-the-curve', '(', 'AUC', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "2. Lowercasing: ['the', 'area-under-the-curve', '(', 'auc', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "3. Punctuation: ['the', 'areaunderthecurve', '', 'auc', '', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'crossvalidation', 'was', 'performed', '']\n",
      "4. Stemming: ['the', 'areaunderthecurv', '', 'auc', '', 'was', 'the', 'chosen', 'perform', 'metric', 'for', 'comparison', 'and', 'crossvalid', 'was', 'perform', '']\n",
      "5. Stopwords: ['areaunderthecurv', 'auc', 'chosen', 'perform', 'metric', 'comparison', 'crossvalid', 'perform']\n",
      "\n",
      "\n",
      "In this paper we present a tool to assist in teaching top-down and bottom-up analysis algorithms. The tool provides simulation for the following analysis algorithms: LL, SLR, LALR and LR.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'The', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'LL', ',', 'SLR', ',', 'LALR', 'and', 'LR', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'll', ',', 'slr', ',', 'lalr', 'and', 'lr', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'topdown', 'and', 'bottomup', 'analysis', 'algorithms', '', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "4. Stemming: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teach', 'topdown', 'and', 'bottomup', 'analysi', 'algorithm', '', 'the', 'tool', 'provid', 'simul', 'for', 'the', 'follow', 'analysi', 'algorithm', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "5. Stopwords: ['paper', 'present', 'tool', 'assist', 'teach', 'topdown', 'bottomup', 'analysi', 'algorithm', 'tool', 'provid', 'simul', 'follow', 'analysi', 'algorithm', 'slr', 'lalr', 'lr']\n",
      "\n",
      "\n",
      "In this paper, we formally show that standard gradient methods never overfit on separable data: If we run these methods for T iterations on a dataset of size m, both the empirical risk and the generalization error decrease at an essentially optimal rate of Õ(1/γ2T) up till T ∼ m.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'If', 'we', 'run', 'these', 'methods', 'for', 'T', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'Õ', '(', '1/γ2T', ')', 'up', 'till', 'T', '∼', 'm', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '(', '1/γ2t', ')', 'up', 'till', 't', '∼', 'm', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', '', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'formal', 'show', 'that', 'standard', 'gradient', 'method', 'never', 'overfit', 'on', 'separ', 'data', '', 'if', 'we', 'run', 'these', 'method', 'for', 't', 'iter', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empir', 'risk', 'and', 'the', 'general', 'error', 'decreas', 'at', 'an', 'essenti', 'optim', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "5. Stopwords: ['paper', 'formal', 'show', 'standard', 'gradient', 'method', 'never', 'overfit', 'separ', 'data', 'run', 'method', 'iter', 'dataset', 'size', 'empir', 'risk', 'general', 'error', 'decreas', 'essenti', 'optim', 'rate', 'õ', '1γ2t', 'till', '∼']\n",
      "\n",
      "\n",
      "This paper is concerned with goodness of fit evaluation for virtual commissioning modelling purposes.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "3. Punctuation: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '']\n",
      "4. Stemming: ['this', 'paper', 'is', 'concern', 'with', 'good', 'of', 'fit', 'evalu', 'for', 'virtual', 'commiss', 'model', 'purpos', '']\n",
      "5. Stopwords: ['paper', 'concern', 'good', 'fit', 'evalu', 'virtual', 'commiss', 'model', 'purpos']\n",
      "\n",
      "\n",
      "The results showed that the optimal performance was achieved under natural complexification of the EANN and that back-propagation tended to over fit the data.\n",
      "\n",
      "1. Tokenization: ['The', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'EANN', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "2. Lowercasing: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "3. Punctuation: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'backpropagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "4. Stemming: ['the', 'result', 'show', 'that', 'the', 'optim', 'perform', 'was', 'achiev', 'under', 'natur', 'complexif', 'of', 'the', 'eann', 'and', 'that', 'backpropag', 'tend', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "5. Stopwords: ['result', 'show', 'optim', 'perform', 'achiev', 'natur', 'complexif', 'eann', 'backpropag', 'tend', 'fit', 'data']\n",
      "\n",
      "\n",
      "The second approach does not train models that generalize across tasks, but rather over-fit a single instance of a problem\n",
      "\n",
      "1. Tokenization: ['The', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "2. Lowercasing: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "3. Punctuation: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', '', 'but', 'rather', 'overfit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "4. Stemming: ['the', 'second', 'approach', 'doe', 'not', 'train', 'model', 'that', 'general', 'across', 'task', '', 'but', 'rather', 'overfit', 'a', 'singl', 'instanc', 'of', 'a', 'problem']\n",
      "5. Stopwords: ['second', 'approach', 'doe', 'train', 'model', 'general', 'across', 'task', 'rather', 'overfit', 'singl', 'instanc', 'problem']\n",
      "\n",
      "\n",
      "This paper describes two significant contributions to the NILM community in an effort towards reproducible state-of-the-art research.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'NILM', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'stateoftheart', 'research', '']\n",
      "4. Stemming: ['this', 'paper', 'describ', 'two', 'signific', 'contribut', 'to', 'the', 'nilm', 'communiti', 'in', 'an', 'effort', 'toward', 'reproduc', 'stateoftheart', 'research', '']\n",
      "5. Stopwords: ['paper', 'describ', 'two', 'signific', 'contribut', 'nilm', 'communiti', 'effort', 'toward', 'reproduc', 'stateoftheart', 'research']\n",
      "\n",
      "\n",
      "Accurate numerical results for a definite integral are easily obtained by simple substitutions of upper and lower bounds of integral into obtained approximate symbolic results\n",
      "\n",
      "1. Tokenization: ['Accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "2. Lowercasing: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "3. Punctuation: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "4. Stemming: ['accur', 'numer', 'result', 'for', 'a', 'definit', 'integr', 'are', 'easili', 'obtain', 'by', 'simpl', 'substitut', 'of', 'upper', 'and', 'lower', 'bound', 'of', 'integr', 'into', 'obtain', 'approxim', 'symbol', 'result']\n",
      "5. Stopwords: ['accur', 'numer', 'result', 'definit', 'integr', 'easili', 'obtain', 'simpl', 'substitut', 'upper', 'lower', 'bound', 'integr', 'obtain', 'approxim', 'symbol', 'result']\n",
      "\n",
      "\n",
      "This paper presents an optimal algorithm for jumper insertion under the ratio upper-bound.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "4. Stemming: ['this', 'paper', 'present', 'an', 'optim', 'algorithm', 'for', 'jumper', 'insert', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "5. Stopwords: ['paper', 'present', 'optim', 'algorithm', 'jumper', 'insert', 'ratio', 'upperbound']\n",
      "\n",
      "\n",
      "In particular, we argue that inter-tagger agreement is not a real upperbound for the Basque WSD task.\n",
      "\n",
      "1. Tokenization: ['In', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'Basque', 'WSD', 'task', '.']\n",
      "2. Lowercasing: ['in', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '.']\n",
      "3. Punctuation: ['in', 'particular', '', 'we', 'argue', 'that', 'intertagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '']\n",
      "4. Stemming: ['in', 'particular', '', 'we', 'argu', 'that', 'intertagg', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basqu', 'wsd', 'task', '']\n",
      "5. Stopwords: ['particular', 'argu', 'intertagg', 'agreement', 'real', 'upperbound', 'basqu', 'wsd', 'task']\n",
      "\n",
      "\n",
      "Lexicalized context-free grammar(LCFG) is an attractive compromise between the parsing efficiency of context-free grammar (CFG) and the elegance and lexical sensitivity of lexicalized tree adjoining grammar (LTAG).\n",
      "\n",
      "1. Tokenization: ['Lexicalized', 'context-free', 'grammar', '(', 'LCFG', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'CFG', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'LTAG', ')', '.']\n",
      "2. Lowercasing: ['lexicalized', 'context-free', 'grammar', '(', 'lcfg', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'cfg', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'ltag', ')', '.']\n",
      "3. Punctuation: ['lexicalized', 'contextfree', 'grammar', '', 'lcfg', '', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'contextfree', 'grammar', '', 'cfg', '', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '', 'ltag', '', '']\n",
      "4. Stemming: ['lexic', 'contextfre', 'grammar', '', 'lcfg', '', 'is', 'an', 'attract', 'compromis', 'between', 'the', 'pars', 'effici', 'of', 'contextfre', 'grammar', '', 'cfg', '', 'and', 'the', 'eleg', 'and', 'lexic', 'sensit', 'of', 'lexic', 'tree', 'adjoin', 'grammar', '', 'ltag', '', '']\n",
      "5. Stopwords: ['lexic', 'contextfre', 'grammar', 'lcfg', 'attract', 'compromis', 'pars', 'effici', 'contextfre', 'grammar', 'cfg', 'eleg', 'lexic', 'sensit', 'lexic', 'tree', 'adjoin', 'grammar', 'ltag']\n",
      "\n",
      "\n",
      "Formulation of Low-Order Dominant Poles for Y-Matrix of Interconnects: This paper presents an efficient approach to compute the dominant poles for the reduced-order admittance (Y parameter) matrix of lossy interconnects.\n",
      "\n",
      "1. Tokenization: ['Formulation', 'of', 'Low-Order', 'Dominant', 'Poles', 'for', 'Y-Matrix', 'of', 'Interconnects', ':', 'This', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'Y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "2. Lowercasing: ['formulation', 'of', 'low-order', 'dominant', 'poles', 'for', 'y-matrix', 'of', 'interconnects', ':', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "3. Punctuation: ['formulation', 'of', 'loworder', 'dominant', 'poles', 'for', 'ymatrix', 'of', 'interconnects', '', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reducedorder', 'admittance', '', 'y', 'parameter', '', 'matrix', 'of', 'lossy', 'interconnects', '']\n",
      "4. Lemmatization: ['formulation', 'of', 'loworder', 'dominant', 'pole', 'for', 'ymatrix', 'of', 'interconnects', '', 'this', 'paper', 'present', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'pole', 'for', 'the', 'reducedorder', 'admittance', '', 'y', 'parameter', '', 'matrix', 'of', 'lossy', 'interconnects', '']\n",
      "5. Stopwords: ['formulation', 'loworder', 'dominant', 'pole', 'ymatrix', 'interconnects', 'paper', 'present', 'efficient', 'approach', 'compute', 'dominant', 'pole', 'reducedorder', 'admittance', 'parameter', 'matrix', 'lossy', 'interconnects']\n",
      "\n",
      "\n",
      "Our algorithm succeeds with high probability against an adaptive adversary, which can take over processors at any time during the protocol, up to the point of taking over arbitrarily close to a 1/3 fraction.\n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', '', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "4. Lemmatization: ['our', 'algorithm', 'succeed', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', '', 'which', 'can', 'take', 'over', 'processor', 'at', 'any', 'time', 'during', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'take', 'over', 'arbitrarily', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "5. Stopwords: ['algorithm', 'succeed', 'high', 'probability', 'adaptive', 'adversary', 'take', 'processor', 'time', 'protocol', 'point', 'take', 'arbitrarily', 'close', '13', 'fraction']\n",
      "\n",
      "\n",
      "We present an all-pairs shortest path algorithm whose running time on a complete directed graph on n vertices whose edge weights are chosen independently and uniformly at random from [0,1] is O(n2), in expectation and with high probability.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'O', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'o', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'allpairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expectation', 'and', 'with', 'high', 'probability', '']\n",
      "4. Lemmatization: ['we', 'present', 'an', 'allpairs', 'shortest', 'path', 'algorithm', 'whose', 'run', 'time', 'on', 'a', 'complete', 'direct', 'graph', 'on', 'n', 'vertex', 'whose', 'edge', 'weight', 'be', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '', '01', '', 'be', 'o', '', 'n2', '', '', 'in', 'expectation', 'and', 'with', 'high', 'probability', '']\n",
      "5. Stopwords: ['present', 'allpairs', 'shortest', 'path', 'algorithm', 'whose', 'run', 'time', 'complete', 'direct', 'graph', 'n', 'vertex', 'whose', 'edge', 'weight', 'chosen', 'independently', 'uniformly', 'random', '01', 'n2', 'expectation', 'high', 'probability']\n",
      "\n",
      "\n",
      "We consider the problem of re-ranking the top-k documents returned by a retrieval system given some search query.\n",
      "\n",
      "1. Tokenization: ['We', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "2. Lowercasing: ['we', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "3. Punctuation: ['we', 'consider', 'the', 'problem', 'of', 'reranking', 'the', 'topk', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '']\n",
      "4. Lemmatization: ['we', 'consider', 'the', 'problem', 'of', 'reranking', 'the', 'topk', 'document', 'return', 'by', 'a', 'retrieval', 'system', 'give', 'some', 'search', 'query', '']\n",
      "5. Stopwords: ['consider', 'problem', 'reranking', 'topk', 'document', 'return', 'retrieval', 'system', 'give', 'search', 'query']\n",
      "\n",
      "\n",
      "In this paper, we combine the learning-to-rank paradigm with the recent developments on axioms for information retrieval.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'combine', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'combine', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'development', 'on', 'axiom', 'for', 'information', 'retrieval', '']\n",
      "5. Stopwords: ['paper', 'combine', 'learningtorank', 'paradigm', 'recent', 'development', 'axiom', 'information', 'retrieval']\n",
      "\n",
      "\n",
      "We outline important details on cross-validation techniques that can enhance the performance.\n",
      "\n",
      "1. Tokenization: ['We', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "2. Lowercasing: ['we', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "3. Punctuation: ['we', 'outline', 'important', 'details', 'on', 'crossvalidation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '']\n",
      "4. Lemmatization: ['we', 'outline', 'important', 'detail', 'on', 'crossvalidation', 'technique', 'that', 'can', 'enhance', 'the', 'performance', '']\n",
      "5. Stopwords: ['outline', 'important', 'detail', 'crossvalidation', 'technique', 'enhance', 'performance']\n",
      "\n",
      "\n",
      "“Next-fit” allocation differs from first-fit in that a first-fit allocator commences its search for free space at a fixed end of memory, whereas a next-fit allocator commences its search wherever it previously stopped searching.\n",
      "\n",
      "1. Tokenization: ['“', 'Next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "2. Lowercasing: ['“', 'next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "3. Punctuation: ['“', 'nextfit', '”', 'allocation', 'differs', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', '', 'whereas', 'a', 'nextfit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '']\n",
      "4. Lemmatization: ['“', 'nextfit', '”', 'allocation', 'differs', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'allocator', 'commences', 'it', 'search', 'for', 'free', 'space', 'at', 'a', 'fix', 'end', 'of', 'memory', '', 'whereas', 'a', 'nextfit', 'allocator', 'commences', 'it', 'search', 'wherever', 'it', 'previously', 'stop', 'search', '']\n",
      "5. Stopwords: ['“', 'nextfit', '”', 'allocation', 'differs', 'firstfit', 'firstfit', 'allocator', 'commences', 'search', 'free', 'space', 'fix', 'end', 'memory', 'whereas', 'nextfit', 'allocator', 'commences', 'search', 'wherever', 'previously', 'stop', 'search']\n",
      "\n",
      "\n",
      "It is important to choose an appropriate network structure because simple networks are likely to under-fit while complex networks are less plastic and more computationally expensive to train.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "2. Lowercasing: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "3. Punctuation: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'underfit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '']\n",
      "4. Lemmatization: ['it', 'be', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'network', 'be', 'likely', 'to', 'underfit', 'while', 'complex', 'network', 'be', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '']\n",
      "5. Stopwords: ['important', 'choose', 'appropriate', 'network', 'structure', 'simple', 'network', 'likely', 'underfit', 'complex', 'network', 'less', 'plastic', 'computationally', 'expensive', 'train']\n",
      "\n",
      "\n",
      "Both of the LL-based algorithms in these papers attempt to minimize the reparsing on the original parse tree and the parse table.\n",
      "\n",
      "1. Tokenization: ['Both', 'of', 'the', 'LL-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "2. Lowercasing: ['both', 'of', 'the', 'll-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "3. Punctuation: ['both', 'of', 'the', 'llbased', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '']\n",
      "4. Lemmatization: ['both', 'of', 'the', 'llbased', 'algorithm', 'in', 'these', 'paper', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '']\n",
      "5. Stopwords: ['llbased', 'algorithm', 'paper', 'attempt', 'minimize', 'reparsing', 'original', 'parse', 'tree', 'parse', 'table']\n",
      "\n",
      "\n",
      "In this paper, L2-norm Deep Belief Network (L2DBN) is proposed, which uses L2-norm regularization to optimize the network parameters of DBN.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'L2-norm', 'Deep', 'Belief', 'Network', '(', 'L2DBN', ')', 'is', 'proposed', ',', 'which', 'uses', 'L2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'DBN', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'l2-norm', 'deep', 'belief', 'network', '(', 'l2dbn', ')', 'is', 'proposed', ',', 'which', 'uses', 'l2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'proposed', '', 'which', 'uses', 'l2norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'be', 'propose', '', 'which', 'us', 'l2norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameter', 'of', 'dbn', '']\n",
      "5. Stopwords: ['paper', 'l2norm', 'deep', 'belief', 'network', 'l2dbn', 'propose', 'us', 'l2norm', 'regularization', 'optimize', 'network', 'parameter', 'dbn']\n",
      "\n",
      "\n",
      "The dramatic development of IT technology has increased absolute amount of data to store, analyze, and process for computers and it has also rapidly increased the amount of realtime processing for data stream\n",
      "\n",
      "1. Tokenization: ['The', 'dramatic', 'development', 'of', 'IT', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "2. Lowercasing: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "3. Punctuation: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', '', 'analyze', '', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "4. Lemmatization: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'have', 'increase', 'absolute', 'amount', 'of', 'data', 'to', 'store', '', 'analyze', '', 'and', 'process', 'for', 'computer', 'and', 'it', 'have', 'also', 'rapidly', 'increase', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "5. Stopwords: ['dramatic', 'development', 'technology', 'increase', 'absolute', 'amount', 'data', 'store', 'analyze', 'process', 'computer', 'also', 'rapidly', 'increase', 'amount', 'realtime', 'processing', 'data', 'stream']\n",
      "\n",
      "\n",
      "We present an algorithm achieving gathering in O(n2) rounds in expectation.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'O', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '', 'n2', '', 'rounds', 'in', 'expectation', '']\n",
      "4. Lemmatization: ['we', 'present', 'an', 'algorithm', 'achieve', 'gathering', 'in', 'o', '', 'n2', '', 'round', 'in', 'expectation', '']\n",
      "5. Stopwords: ['present', 'algorithm', 'achieve', 'gathering', 'n2', 'round', 'expectation']\n",
      "\n",
      "\n",
      "Let A and B two n×n matrices over a ring R (e.g., the reals or the integers) each containing at most m nonzero elements.\n",
      "\n",
      "1. Tokenization: ['Let', 'A', 'and', 'B', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'R', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "2. Lowercasing: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "3. Punctuation: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'reals', 'or', 'the', 'integers', '', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '']\n",
      "4. Lemmatization: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrix', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'real', 'or', 'the', 'integer', '', 'each', 'contain', 'at', 'most', 'm', 'nonzero', 'element', '']\n",
      "5. Stopwords: ['let', 'b', 'two', 'n×n', 'matrix', 'ring', 'r', 'eg', 'real', 'integer', 'contain', 'nonzero', 'element']\n",
      "\n",
      "\n",
      "Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited.\n",
      "\n",
      "1. Tokenization: ['Firmware', 'is', 'the', 'enable', 'software', 'of', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'IoT', 'devices', 'being', 'exploited', '.']\n",
      "2. Lowercasing: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '(', 'iot', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '.']\n",
      "3. Punctuation: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '', 'iot', '', 'devices', '', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '']\n",
      "4. Lemmatization: ['firmware', 'be', 'the', 'enable', 'software', 'of', 'internet', 'of', 'thing', '', 'iot', '', 'device', '', 'and', 'it', 'software', 'vulnerability', 'be', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'device', 'be', 'exploit', '']\n",
      "5. Stopwords: ['firmware', 'enable', 'software', 'internet', 'thing', 'iot', 'device', 'software', 'vulnerability', 'one', 'primary', 'reason', 'iot', 'device', 'exploit']\n",
      "\n",
      "\n",
      "We present a new algorithm that multiplies A and B using O(m0.7n1.2+n2+o(1)) algebraic operations (i.e., multiplications, additions and subtractions) over R.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'A', 'and', 'B', 'using', 'O', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'R', '.']\n",
      "2. Lowercasing: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'r', '.']\n",
      "3. Punctuation: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebraic', 'operations', '', 'ie', '', 'multiplications', '', 'additions', 'and', 'subtractions', '', 'over', 'r', '']\n",
      "4. Lemmatization: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'use', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebraic', 'operation', '', 'ie', '', 'multiplication', '', 'addition', 'and', 'subtraction', '', 'over', 'r', '']\n",
      "5. Stopwords: ['present', 'new', 'algorithm', 'multiplies', 'b', 'use', 'm07n12n2o', '1', 'algebraic', 'operation', 'ie', 'multiplication', 'addition', 'subtraction', 'r']\n",
      "\n",
      "\n",
      "Efforts in “explainable AI” are under way, hopefully eliminating the “black-box” concept in future clinical decision tools.\n",
      "\n",
      "1. Tokenization: ['Efforts', 'in', '“', 'explainable', 'AI', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "2. Lowercasing: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "3. Punctuation: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', '', 'hopefully', 'eliminating', 'the', '“', 'blackbox', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '']\n",
      "4. Lemmatization: ['effort', 'in', '“', 'explainable', 'ai', '”', 'be', 'under', 'way', '', 'hopefully', 'eliminate', 'the', '“', 'blackbox', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tool', '']\n",
      "5. Stopwords: ['effort', '“', 'explainable', 'ai', '”', 'way', 'hopefully', 'eliminate', '“', 'blackbox', '”', 'concept', 'future', 'clinical', 'decision', 'tool']\n",
      "\n",
      "\n",
      "Target distance (D) and target width (W), traditionally treated as independent variables in Fitts' target acquisition paradigm, are shown to suffer inextricable confounds with task difficulty.\n",
      "\n",
      "1. Tokenization: ['Target', 'distance', '(', 'D', ')', 'and', 'target', 'width', '(', 'W', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'Fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "2. Lowercasing: ['target', 'distance', '(', 'd', ')', 'and', 'target', 'width', '(', 'w', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "3. Punctuation: ['target', 'distance', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', '', 'target', 'acquisition', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '']\n",
      "4. Lemmatization: ['target', 'distance', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'traditionally', 'treat', 'a', 'independent', 'variable', 'in', 'fitts', '', 'target', 'acquisition', 'paradigm', '', 'be', 'show', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '']\n",
      "5. Stopwords: ['target', 'distance', 'target', 'width', 'w', 'traditionally', 'treat', 'independent', 'variable', 'fitts', 'target', 'acquisition', 'paradigm', 'show', 'suffer', 'inextricable', 'confounds', 'task', 'difficulty']\n",
      "\n",
      "\n",
      "This paper describes an NSF-funded initiative involving 600 underrepresented high school students and 60 teachers designed to introduce underrepresented students to the numerous and varied career opportunities in the computing sciences.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'an', 'NSF-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'an', 'nsf-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'an', 'nsffunded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '']\n",
      "4. Lemmatization: ['this', 'paper', 'describes', 'an', 'nsffunded', 'initiative', 'involve', '600', 'underrepresented', 'high', 'school', 'student', 'and', '60', 'teacher', 'design', 'to', 'introduce', 'underrepresented', 'student', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunity', 'in', 'the', 'compute', 'science', '']\n",
      "5. Stopwords: ['paper', 'describes', 'nsffunded', 'initiative', 'involve', '600', 'underrepresented', 'high', 'school', 'student', '60', 'teacher', 'design', 'introduce', 'underrepresented', 'student', 'numerous', 'varied', 'career', 'opportunity', 'compute', 'science']\n",
      "\n",
      "\n",
      "The gathering problem, where n autonomous robots with restricted capabilities are required to meet in a single point of the plane, is widely studied.\n",
      "\n",
      "1. Tokenization: ['The', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "2. Lowercasing: ['the', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "3. Punctuation: ['the', 'gathering', 'problem', '', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', '', 'is', 'widely', 'studied', '']\n",
      "4. Lemmatization: ['the', 'gathering', 'problem', '', 'where', 'n', 'autonomous', 'robot', 'with', 'restrict', 'capability', 'be', 'require', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', '', 'be', 'widely', 'study', '']\n",
      "5. Stopwords: ['gathering', 'problem', 'n', 'autonomous', 'robot', 'restrict', 'capability', 'require', 'meet', 'single', 'point', 'plane', 'widely', 'study']\n",
      "\n",
      "\n",
      "The information captured by IoT present an unprecedented opportunity to solve large-scale problems in those application domains to deliver services\n",
      "\n",
      "1. Tokenization: ['The', 'information', 'captured', 'by', 'IoT', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "2. Lowercasing: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "3. Punctuation: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'largescale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "4. Lemmatization: ['the', 'information', 'capture', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'largescale', 'problem', 'in', 'those', 'application', 'domain', 'to', 'deliver', 'service']\n",
      "5. Stopwords: ['information', 'capture', 'iot', 'present', 'unprecedented', 'opportunity', 'solve', 'largescale', 'problem', 'application', 'domain', 'deliver', 'service']\n",
      "\n",
      "\n",
      "Compliance with the information system (IS) security policy is an established theme in IS research for protecting the IS from user actions.\n",
      "\n",
      "1. Tokenization: ['Compliance', 'with', 'the', 'information', 'system', '(', 'IS', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'IS', 'research', 'for', 'protecting', 'the', 'IS', 'from', 'user', 'actions', '.']\n",
      "2. Lowercasing: ['compliance', 'with', 'the', 'information', 'system', '(', 'is', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '.']\n",
      "3. Punctuation: ['compliance', 'with', 'the', 'information', 'system', '', 'is', '', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '']\n",
      "4. Lemmatization: ['compliance', 'with', 'the', 'information', 'system', '', 'be', '', 'security', 'policy', 'be', 'an', 'establish', 'theme', 'in', 'be', 'research', 'for', 'protect', 'the', 'be', 'from', 'user', 'action', '']\n",
      "5. Stopwords: ['compliance', 'information', 'system', 'security', 'policy', 'establish', 'theme', 'research', 'protect', 'user', 'action']\n",
      "\n",
      "\n",
      "In this paper, we develop Re-Vibe, the first system that re-identifies people through footstep-induced floor vibrations.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'develop', 'Re-Vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'develop', 're-vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'develop', 'revibe', '', 'the', 'first', 'system', 'that', 'reidentifies', 'people', 'through', 'footstepinduced', 'floor', 'vibrations', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'develop', 'revibe', '', 'the', 'first', 'system', 'that', 'reidentifies', 'people', 'through', 'footstepinduced', 'floor', 'vibration', '']\n",
      "5. Stopwords: ['paper', 'develop', 'revibe', 'first', 'system', 'reidentifies', 'people', 'footstepinduced', 'floor', 'vibration']\n",
      "\n",
      "\n",
      "I/O is emerging as a major bottleneck for machine learning training, especially in distributed environments.\n",
      "\n",
      "1. Tokenization: ['I/O', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "2. Lowercasing: ['i/o', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "3. Punctuation: ['io', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', '', 'especially', 'in', 'distributed', 'environments', '']\n",
      "4. Lemmatization: ['io', 'be', 'emerge', 'a', 'a', 'major', 'bottleneck', 'for', 'machine', 'learn', 'training', '', 'especially', 'in', 'distribute', 'environment', '']\n",
      "5. Stopwords: ['io', 'emerge', 'major', 'bottleneck', 'machine', 'learn', 'training', 'especially', 'distribute', 'environment']\n",
      "\n",
      "\n",
      "In this paper, we attempt to improve the query likelihood function by bringing back the negative query generation.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bring', 'back', 'the', 'negative', 'query', 'generation', '']\n",
      "5. Stopwords: ['paper', 'attempt', 'improve', 'query', 'likelihood', 'function', 'bring', 'back', 'negative', 'query', 'generation']\n",
      "\n",
      "\n",
      "In this paper, we propose a new on-chip interconnect scheme called Y-architecture, which can utilize the on-chip routing resources more efficiently than traditional Manhattan interconnect architecture by allowing wires routed in three directions (0°, 60°, and 120°).\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'Y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'Manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'propose', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'called', 'yarchitecture', '', 'which', 'can', 'utilize', 'the', 'onchip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'propose', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'call', 'yarchitecture', '', 'which', 'can', 'utilize', 'the', 'onchip', 'rout', 'resource', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allow', 'wire', 'rout', 'in', 'three', 'direction', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "5. Stopwords: ['paper', 'propose', 'new', 'onchip', 'interconnect', 'scheme', 'call', 'yarchitecture', 'utilize', 'onchip', 'rout', 'resource', 'efficiently', 'traditional', 'manhattan', 'interconnect', 'architecture', 'allow', 'wire', 'rout', 'three', 'direction', '0°', '60°', '120°']\n",
      "\n",
      "\n",
      "We describe an algorithm for Byzantine agreement that is scalable in the sense that each processor sends only O(√n) bits, where n is the total number of processors.\n",
      "\n",
      "1. Tokenization: ['We', 'describe', 'an', 'algorithm', 'for', 'Byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'O', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "2. Lowercasing: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "3. Punctuation: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '', '√n', '', 'bits', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '']\n",
      "4. Lemmatization: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'be', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '', '√n', '', 'bit', '', 'where', 'n', 'be', 'the', 'total', 'number', 'of', 'processor', '']\n",
      "5. Stopwords: ['describe', 'algorithm', 'byzantine', 'agreement', 'scalable', 'sense', 'processor', 'sends', '√n', 'bit', 'n', 'total', 'number', 'processor']\n",
      "\n",
      "\n",
      "This paper presents an image-based rendering (IBR) system based on RGB-D images.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'IBR', ')', 'system', 'based', 'on', 'RGB-D', 'images', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'ibr', ')', 'system', 'based', 'on', 'rgb-d', 'images', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'imagebased', 'rendering', '', 'ibr', '', 'system', 'based', 'on', 'rgbd', 'images', '']\n",
      "4. Lemmatization: ['this', 'paper', 'present', 'an', 'imagebased', 'render', '', 'ibr', '', 'system', 'base', 'on', 'rgbd', 'image', '']\n",
      "5. Stopwords: ['paper', 'present', 'imagebased', 'render', 'ibr', 'system', 'base', 'rgbd', 'image']\n",
      "\n",
      "\n",
      "In this paper we present a framework and methodology for aligning the business strategy and IT/IS for an organization offering an e-service in a multi-organizational setting.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'IT/IS', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'it/is', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'itis', 'for', 'an', 'organization', 'offering', 'an', 'eservice', 'in', 'a', 'multiorganizational', 'setting', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'align', 'the', 'business', 'strategy', 'and', 'itis', 'for', 'an', 'organization', 'offering', 'an', 'eservice', 'in', 'a', 'multiorganizational', 'set', '']\n",
      "5. Stopwords: ['paper', 'present', 'framework', 'methodology', 'align', 'business', 'strategy', 'itis', 'organization', 'offering', 'eservice', 'multiorganizational', 'set']\n",
      "\n",
      "\n",
      "The D Programming Language is a hybrid of C++ and modern scripting languages: it compiles statically to native code, but is also garbage collected.\n",
      "\n",
      "1. Tokenization: ['The', 'D', 'Programming', 'Language', 'is', 'a', 'hybrid', 'of', 'C++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "2. Lowercasing: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "3. Punctuation: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'scripting', 'languages', '', 'it', 'compiles', 'statically', 'to', 'native', 'code', '', 'but', 'is', 'also', 'garbage', 'collected', '']\n",
      "4. Lemmatization: ['the', 'd', 'program', 'language', 'be', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'script', 'language', '', 'it', 'compiles', 'statically', 'to', 'native', 'code', '', 'but', 'be', 'also', 'garbage', 'collect', '']\n",
      "5. Stopwords: ['program', 'language', 'hybrid', 'c', 'modern', 'script', 'language', 'compiles', 'statically', 'native', 'code', 'also', 'garbage', 'collect']\n",
      "\n",
      "\n",
      "However, the main aim is precisely to present an algorithm which gives the so-called minimal solutions: Boolean matrices M satisfying the equation with the least possible number of unity entries.\n",
      "\n",
      "1. Tokenization: ['However', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'Boolean', 'matrices', 'M', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "2. Lowercasing: ['however', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "3. Punctuation: ['however', '', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'socalled', 'minimal', 'solutions', '', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '']\n",
      "4. Lemmatization: ['however', '', 'the', 'main', 'aim', 'be', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'give', 'the', 'socalled', 'minimal', 'solution', '', 'boolean', 'matrix', 'm', 'satisfy', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entry', '']\n",
      "5. Stopwords: ['however', 'main', 'aim', 'precisely', 'present', 'algorithm', 'give', 'socalled', 'minimal', 'solution', 'boolean', 'matrix', 'satisfy', 'equation', 'least', 'possible', 'number', 'unity', 'entry']\n",
      "\n",
      "\n",
      "A number of algorithms have been proposed for LR incremental parsers, but few have been proposed for LL incremental parsers [1, 2].\n",
      "\n",
      "1. Tokenization: ['A', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'LR', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'LL', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "2. Lowercasing: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "3. Punctuation: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', '', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '', '1', '', '2', '', '']\n",
      "4. Lemmatization: ['a', 'number', 'of', 'algorithm', 'have', 'be', 'propose', 'for', 'lr', 'incremental', 'parser', '', 'but', 'few', 'have', 'be', 'propose', 'for', 'll', 'incremental', 'parser', '', '1', '', '2', '', '']\n",
      "5. Stopwords: ['number', 'algorithm', 'propose', 'lr', 'incremental', 'parser', 'propose', 'incremental', 'parser', '1', '2']\n",
      "\n",
      "\n",
      "I will discuss the use of graphical models for data mining.\n",
      "\n",
      "1. Tokenization: ['I', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "2. Lowercasing: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "3. Punctuation: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '']\n",
      "4. Lemmatization: ['i', 'will', 'discus', 'the', 'use', 'of', 'graphical', 'model', 'for', 'data', 'mining', '']\n",
      "5. Stopwords: ['discus', 'use', 'graphical', 'model', 'data', 'mining']\n",
      "\n",
      "\n",
      "Over the past decade, a pair of synchronization instructions known as LL/SC has emerged as the most suitable set of instructions to be used in the design of lock-free algorithms.\n",
      "\n",
      "1. Tokenization: ['Over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'LL/SC', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "2. Lowercasing: ['over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'll/sc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "3. Punctuation: ['over', 'the', 'past', 'decade', '', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'llsc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lockfree', 'algorithms', '']\n",
      "4. Lemmatization: ['over', 'the', 'past', 'decade', '', 'a', 'pair', 'of', 'synchronization', 'instruction', 'know', 'a', 'llsc', 'have', 'emerge', 'a', 'the', 'most', 'suitable', 'set', 'of', 'instruction', 'to', 'be', 'use', 'in', 'the', 'design', 'of', 'lockfree', 'algorithm', '']\n",
      "5. Stopwords: ['past', 'decade', 'pair', 'synchronization', 'instruction', 'know', 'llsc', 'emerge', 'suitable', 'set', 'instruction', 'use', 'design', 'lockfree', 'algorithm']\n",
      "\n",
      "\n",
      "Our algorithm has latency that is polylogarithmic in n. \n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '']\n",
      "4. Lemmatization: ['our', 'algorithm', 'have', 'latency', 'that', 'be', 'polylogarithmic', 'in', 'n', '']\n",
      "5. Stopwords: ['algorithm', 'latency', 'polylogarithmic', 'n']\n",
      "\n",
      "\n",
      "Typical person re-identification (re-ID) systems rely on cameras to match the same person across different locations.\n",
      "\n",
      "1. Tokenization: ['Typical', 'person', 're-identification', '(', 're-ID', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "2. Lowercasing: ['typical', 'person', 're-identification', '(', 're-id', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "3. Punctuation: ['typical', 'person', 'reidentification', '', 'reid', '', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '']\n",
      "4. Lemmatization: ['typical', 'person', 'reidentification', '', 'reid', '', 'system', 'rely', 'on', 'camera', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'location', '']\n",
      "5. Stopwords: ['typical', 'person', 'reidentification', 'reid', 'system', 'rely', 'camera', 'match', 'person', 'across', 'different', 'location']\n",
      "\n",
      "\n",
      "The Internet of Things (IoT) is the latest Internet evolution that incorporates a diverse range of things such as sensors, actuators, and services deployed by different organizations and individuals to support a variety of applications.\n",
      "\n",
      "1. Tokenization: ['The', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'is', 'the', 'latest', 'Internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "2. Lowercasing: ['the', 'internet', 'of', 'things', '(', 'iot', ')', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "3. Punctuation: ['the', 'internet', 'of', 'things', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', '', 'actuators', '', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '']\n",
      "4. Lemmatization: ['the', 'internet', 'of', 'thing', '', 'iot', '', 'be', 'the', 'late', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'thing', 'such', 'a', 'sensor', '', 'actuator', '', 'and', 'service', 'deployed', 'by', 'different', 'organization', 'and', 'individual', 'to', 'support', 'a', 'variety', 'of', 'application', '']\n",
      "5. Stopwords: ['internet', 'thing', 'iot', 'late', 'internet', 'evolution', 'incorporates', 'diverse', 'range', 'thing', 'sensor', 'actuator', 'service', 'deployed', 'different', 'organization', 'individual', 'support', 'variety', 'application']\n",
      "\n",
      "\n",
      "This subsection compares the state of the art methods and the proposed WRA-Net for the BoniRob dataset through the same method.\n",
      "\n",
      "1. Tokenization: ['This', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'WRA-Net', 'for', 'the', 'BoniRob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "2. Lowercasing: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wra-net', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "3. Punctuation: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "4. Lemmatization: ['this', 'subsection', 'compare', 'the', 'state', 'of', 'the', 'art', 'method', 'and', 'the', 'propose', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "5. Stopwords: ['subsection', 'compare', 'state', 'art', 'method', 'propose', 'wranet', 'bonirob', 'dataset', 'method']\n",
      "\n",
      "\n",
      "Hence, WRA-Net achieved higher values for PSNRand SSIM compared to the state-of-the-art methods.\n",
      "\n",
      "1. Tokenization: ['Hence', ',', 'WRA-Net', 'achieved', 'higher', 'values', 'for', 'PSNRand', 'SSIM', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "2. Lowercasing: ['hence', ',', 'wra-net', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "3. Punctuation: ['hence', '', 'wranet', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'stateoftheart', 'methods', '']\n",
      "4. Lemmatization: ['hence', '', 'wranet', 'achieve', 'high', 'value', 'for', 'psnrand', 'ssim', 'compare', 'to', 'the', 'stateoftheart', 'method', '']\n",
      "5. Stopwords: ['hence', 'wranet', 'achieve', 'high', 'value', 'psnrand', 'ssim', 'compare', 'stateoftheart', 'method']\n",
      "\n",
      "\n",
      "It is shown that relative movement amplitude D/W(which determines difficulty) and absolute movement amplitude D (or scale) are the only two variables that can be manipulated independently in a Fitts' task experiment.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'D/W', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'D', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'Fitts', \"'\", 'task', 'experiment', '.']\n",
      "2. Lowercasing: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'd/w', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'd', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', \"'\", 'task', 'experiment', '.']\n",
      "3. Punctuation: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'dw', '', 'which', 'determines', 'difficulty', '', 'and', 'absolute', 'movement', 'amplitude', 'd', '', 'or', 'scale', '', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', '', 'task', 'experiment', '']\n",
      "4. Lemmatization: ['it', 'be', 'show', 'that', 'relative', 'movement', 'amplitude', 'dw', '', 'which', 'determines', 'difficulty', '', 'and', 'absolute', 'movement', 'amplitude', 'd', '', 'or', 'scale', '', 'be', 'the', 'only', 'two', 'variable', 'that', 'can', 'be', 'manipulate', 'independently', 'in', 'a', 'fitts', '', 'task', 'experiment', '']\n",
      "5. Stopwords: ['show', 'relative', 'movement', 'amplitude', 'dw', 'determines', 'difficulty', 'absolute', 'movement', 'amplitude', 'scale', 'two', 'variable', 'manipulate', 'independently', 'fitts', 'task', 'experiment']\n",
      "\n",
      "\n",
      "The area-under-the-curve (AUC) was the chosen performance metric for comparison and cross-validation was performed.\n",
      "\n",
      "1. Tokenization: ['The', 'area-under-the-curve', '(', 'AUC', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "2. Lowercasing: ['the', 'area-under-the-curve', '(', 'auc', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "3. Punctuation: ['the', 'areaunderthecurve', '', 'auc', '', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'crossvalidation', 'was', 'performed', '']\n",
      "4. Lemmatization: ['the', 'areaunderthecurve', '', 'auc', '', 'be', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'crossvalidation', 'be', 'perform', '']\n",
      "5. Stopwords: ['areaunderthecurve', 'auc', 'chosen', 'performance', 'metric', 'comparison', 'crossvalidation', 'perform']\n",
      "\n",
      "\n",
      "In this paper we present a tool to assist in teaching top-down and bottom-up analysis algorithms. The tool provides simulation for the following analysis algorithms: LL, SLR, LALR and LR.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'The', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'LL', ',', 'SLR', ',', 'LALR', 'and', 'LR', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'll', ',', 'slr', ',', 'lalr', 'and', 'lr', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'topdown', 'and', 'bottomup', 'analysis', 'algorithms', '', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teach', 'topdown', 'and', 'bottomup', 'analysis', 'algorithm', '', 'the', 'tool', 'provide', 'simulation', 'for', 'the', 'follow', 'analysis', 'algorithm', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "5. Stopwords: ['paper', 'present', 'tool', 'assist', 'teach', 'topdown', 'bottomup', 'analysis', 'algorithm', 'tool', 'provide', 'simulation', 'follow', 'analysis', 'algorithm', 'slr', 'lalr', 'lr']\n",
      "\n",
      "\n",
      "In this paper, we formally show that standard gradient methods never overfit on separable data: If we run these methods for T iterations on a dataset of size m, both the empirical risk and the generalization error decrease at an essentially optimal rate of Õ(1/γ2T) up till T ∼ m.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'If', 'we', 'run', 'these', 'methods', 'for', 'T', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'Õ', '(', '1/γ2T', ')', 'up', 'till', 'T', '∼', 'm', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '(', '1/γ2t', ')', 'up', 'till', 't', '∼', 'm', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', '', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'method', 'never', 'overfit', 'on', 'separable', 'data', '', 'if', 'we', 'run', 'these', 'method', 'for', 't', 'iteration', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "5. Stopwords: ['paper', 'formally', 'show', 'standard', 'gradient', 'method', 'never', 'overfit', 'separable', 'data', 'run', 'method', 'iteration', 'dataset', 'size', 'empirical', 'risk', 'generalization', 'error', 'decrease', 'essentially', 'optimal', 'rate', 'õ', '1γ2t', 'till', '∼']\n",
      "\n",
      "\n",
      "This paper is concerned with goodness of fit evaluation for virtual commissioning modelling purposes.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "3. Punctuation: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '']\n",
      "4. Lemmatization: ['this', 'paper', 'be', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commission', 'model', 'purpose', '']\n",
      "5. Stopwords: ['paper', 'concerned', 'goodness', 'fit', 'evaluation', 'virtual', 'commission', 'model', 'purpose']\n",
      "\n",
      "\n",
      "The results showed that the optimal performance was achieved under natural complexification of the EANN and that back-propagation tended to over fit the data.\n",
      "\n",
      "1. Tokenization: ['The', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'EANN', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "2. Lowercasing: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "3. Punctuation: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'backpropagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "4. Lemmatization: ['the', 'result', 'show', 'that', 'the', 'optimal', 'performance', 'be', 'achieve', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'backpropagation', 'tend', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "5. Stopwords: ['result', 'show', 'optimal', 'performance', 'achieve', 'natural', 'complexification', 'eann', 'backpropagation', 'tend', 'fit', 'data']\n",
      "\n",
      "\n",
      "The second approach does not train models that generalize across tasks, but rather over-fit a single instance of a problem\n",
      "\n",
      "1. Tokenization: ['The', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "2. Lowercasing: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "3. Punctuation: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', '', 'but', 'rather', 'overfit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "4. Lemmatization: ['the', 'second', 'approach', 'do', 'not', 'train', 'model', 'that', 'generalize', 'across', 'task', '', 'but', 'rather', 'overfit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "5. Stopwords: ['second', 'approach', 'train', 'model', 'generalize', 'across', 'task', 'rather', 'overfit', 'single', 'instance', 'problem']\n",
      "\n",
      "\n",
      "This paper describes two significant contributions to the NILM community in an effort towards reproducible state-of-the-art research.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'NILM', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'stateoftheart', 'research', '']\n",
      "4. Lemmatization: ['this', 'paper', 'describes', 'two', 'significant', 'contribution', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'stateoftheart', 'research', '']\n",
      "5. Stopwords: ['paper', 'describes', 'two', 'significant', 'contribution', 'nilm', 'community', 'effort', 'towards', 'reproducible', 'stateoftheart', 'research']\n",
      "\n",
      "\n",
      "Accurate numerical results for a definite integral are easily obtained by simple substitutions of upper and lower bounds of integral into obtained approximate symbolic results\n",
      "\n",
      "1. Tokenization: ['Accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "2. Lowercasing: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "3. Punctuation: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "4. Lemmatization: ['accurate', 'numerical', 'result', 'for', 'a', 'definite', 'integral', 'be', 'easily', 'obtain', 'by', 'simple', 'substitution', 'of', 'upper', 'and', 'low', 'bound', 'of', 'integral', 'into', 'obtain', 'approximate', 'symbolic', 'result']\n",
      "5. Stopwords: ['accurate', 'numerical', 'result', 'definite', 'integral', 'easily', 'obtain', 'simple', 'substitution', 'upper', 'low', 'bound', 'integral', 'obtain', 'approximate', 'symbolic', 'result']\n",
      "\n",
      "\n",
      "This paper presents an optimal algorithm for jumper insertion under the ratio upper-bound.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "4. Lemmatization: ['this', 'paper', 'present', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "5. Stopwords: ['paper', 'present', 'optimal', 'algorithm', 'jumper', 'insertion', 'ratio', 'upperbound']\n",
      "\n",
      "\n",
      "In particular, we argue that inter-tagger agreement is not a real upperbound for the Basque WSD task.\n",
      "\n",
      "1. Tokenization: ['In', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'Basque', 'WSD', 'task', '.']\n",
      "2. Lowercasing: ['in', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '.']\n",
      "3. Punctuation: ['in', 'particular', '', 'we', 'argue', 'that', 'intertagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '']\n",
      "4. Lemmatization: ['in', 'particular', '', 'we', 'argue', 'that', 'intertagger', 'agreement', 'be', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '']\n",
      "5. Stopwords: ['particular', 'argue', 'intertagger', 'agreement', 'real', 'upperbound', 'basque', 'wsd', 'task']\n",
      "\n",
      "\n",
      "Lexicalized context-free grammar(LCFG) is an attractive compromise between the parsing efficiency of context-free grammar (CFG) and the elegance and lexical sensitivity of lexicalized tree adjoining grammar (LTAG).\n",
      "\n",
      "1. Tokenization: ['Lexicalized', 'context-free', 'grammar', '(', 'LCFG', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'CFG', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'LTAG', ')', '.']\n",
      "2. Lowercasing: ['lexicalized', 'context-free', 'grammar', '(', 'lcfg', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'cfg', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'ltag', ')', '.']\n",
      "3. Punctuation: ['lexicalized', 'contextfree', 'grammar', '', 'lcfg', '', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'contextfree', 'grammar', '', 'cfg', '', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '', 'ltag', '', '']\n",
      "4. Lemmatization: ['lexicalize', 'contextfree', 'grammar', '', 'lcfg', '', 'be', 'an', 'attractive', 'compromise', 'between', 'the', 'parse', 'efficiency', 'of', 'contextfree', 'grammar', '', 'cfg', '', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalize', 'tree', 'adjoin', 'grammar', '', 'ltag', '', '']\n",
      "5. Stopwords: ['lexicalize', 'contextfree', 'grammar', 'lcfg', 'attractive', 'compromise', 'parse', 'efficiency', 'contextfree', 'grammar', 'cfg', 'elegance', 'lexical', 'sensitivity', 'lexicalize', 'tree', 'adjoin', 'grammar', 'ltag']\n",
      "\n",
      "\n",
      "Formulation of Low-Order Dominant Poles for Y-Matrix of Interconnects: This paper presents an efficient approach to compute the dominant poles for the reduced-order admittance (Y parameter) matrix of lossy interconnects.\n",
      "\n",
      "1. Tokenization: ['Formulation', 'of', 'Low-Order', 'Dominant', 'Poles', 'for', 'Y-Matrix', 'of', 'Interconnects', ':', 'This', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'Y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "2. Lowercasing: ['formulation', 'of', 'low-order', 'dominant', 'poles', 'for', 'y-matrix', 'of', 'interconnects', ':', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "3. Punctuation: ['formulation', 'of', 'loworder', 'dominant', 'poles', 'for', 'ymatrix', 'of', 'interconnects', '', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reducedorder', 'admittance', '', 'y', 'parameter', '', 'matrix', 'of', 'lossy', 'interconnects', '']\n",
      "4. Stemming: ['formul', 'of', 'loword', 'domin', 'pole', 'for', 'ymatrix', 'of', 'interconnect', '', 'thi', 'paper', 'present', 'an', 'effici', 'approach', 'to', 'comput', 'the', 'domin', 'pole', 'for', 'the', 'reducedord', 'admitt', '', 'y', 'paramet', '', 'matrix', 'of', 'lossi', 'interconnect', '']\n",
      "\n",
      "\n",
      "Our algorithm succeeds with high probability against an adaptive adversary, which can take over processors at any time during the protocol, up to the point of taking over arbitrarily close to a 1/3 fraction.\n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', '', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "4. Stemming: ['our', 'algorithm', 'succe', 'with', 'high', 'probabl', 'against', 'an', 'adapt', 'adversari', '', 'which', 'can', 'take', 'over', 'processor', 'at', 'ani', 'time', 'dure', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'take', 'over', 'arbitrarili', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "\n",
      "\n",
      "We present an all-pairs shortest path algorithm whose running time on a complete directed graph on n vertices whose edge weights are chosen independently and uniformly at random from [0,1] is O(n2), in expectation and with high probability.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'O', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'o', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'allpairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expectation', 'and', 'with', 'high', 'probability', '']\n",
      "4. Stemming: ['we', 'present', 'an', 'allpair', 'shortest', 'path', 'algorithm', 'whose', 'run', 'time', 'on', 'a', 'complet', 'direct', 'graph', 'on', 'n', 'vertic', 'whose', 'edg', 'weight', 'are', 'chosen', 'independ', 'and', 'uniformli', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expect', 'and', 'with', 'high', 'probabl', '']\n",
      "\n",
      "\n",
      "We consider the problem of re-ranking the top-k documents returned by a retrieval system given some search query.\n",
      "\n",
      "1. Tokenization: ['We', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "2. Lowercasing: ['we', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "3. Punctuation: ['we', 'consider', 'the', 'problem', 'of', 'reranking', 'the', 'topk', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '']\n",
      "4. Stemming: ['we', 'consid', 'the', 'problem', 'of', 'rerank', 'the', 'topk', 'document', 'return', 'by', 'a', 'retriev', 'system', 'given', 'some', 'search', 'queri', '']\n",
      "\n",
      "\n",
      "In this paper, we combine the learning-to-rank paradigm with the recent developments on axioms for information retrieval.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'combine', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'combin', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'develop', 'on', 'axiom', 'for', 'inform', 'retriev', '']\n",
      "\n",
      "\n",
      "We outline important details on cross-validation techniques that can enhance the performance.\n",
      "\n",
      "1. Tokenization: ['We', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "2. Lowercasing: ['we', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "3. Punctuation: ['we', 'outline', 'important', 'details', 'on', 'crossvalidation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '']\n",
      "4. Stemming: ['we', 'outlin', 'import', 'detail', 'on', 'crossvalid', 'techniqu', 'that', 'can', 'enhanc', 'the', 'perform', '']\n",
      "\n",
      "\n",
      "“Next-fit” allocation differs from first-fit in that a first-fit allocator commences its search for free space at a fixed end of memory, whereas a next-fit allocator commences its search wherever it previously stopped searching.\n",
      "\n",
      "1. Tokenization: ['“', 'Next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "2. Lowercasing: ['“', 'next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "3. Punctuation: ['“', 'nextfit', '”', 'allocation', 'differs', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', '', 'whereas', 'a', 'nextfit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '']\n",
      "4. Stemming: ['“', 'nextfit', '”', 'alloc', 'differ', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'alloc', 'commenc', 'it', 'search', 'for', 'free', 'space', 'at', 'a', 'fix', 'end', 'of', 'memori', '', 'wherea', 'a', 'nextfit', 'alloc', 'commenc', 'it', 'search', 'wherev', 'it', 'previous', 'stop', 'search', '']\n",
      "\n",
      "\n",
      "It is important to choose an appropriate network structure because simple networks are likely to under-fit while complex networks are less plastic and more computationally expensive to train.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "2. Lowercasing: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "3. Punctuation: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'underfit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '']\n",
      "4. Stemming: ['it', 'is', 'import', 'to', 'choos', 'an', 'appropri', 'network', 'structur', 'becaus', 'simpl', 'network', 'are', 'like', 'to', 'underfit', 'while', 'complex', 'network', 'are', 'less', 'plastic', 'and', 'more', 'comput', 'expens', 'to', 'train', '']\n",
      "\n",
      "\n",
      "Both of the LL-based algorithms in these papers attempt to minimize the reparsing on the original parse tree and the parse table.\n",
      "\n",
      "1. Tokenization: ['Both', 'of', 'the', 'LL-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "2. Lowercasing: ['both', 'of', 'the', 'll-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "3. Punctuation: ['both', 'of', 'the', 'llbased', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '']\n",
      "4. Stemming: ['both', 'of', 'the', 'llbase', 'algorithm', 'in', 'these', 'paper', 'attempt', 'to', 'minim', 'the', 'repars', 'on', 'the', 'origin', 'pars', 'tree', 'and', 'the', 'pars', 'tabl', '']\n",
      "\n",
      "\n",
      "In this paper, L2-norm Deep Belief Network (L2DBN) is proposed, which uses L2-norm regularization to optimize the network parameters of DBN.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'L2-norm', 'Deep', 'Belief', 'Network', '(', 'L2DBN', ')', 'is', 'proposed', ',', 'which', 'uses', 'L2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'DBN', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'l2-norm', 'deep', 'belief', 'network', '(', 'l2dbn', ')', 'is', 'proposed', ',', 'which', 'uses', 'l2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'proposed', '', 'which', 'uses', 'l2norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'propos', '', 'which', 'use', 'l2norm', 'regular', 'to', 'optim', 'the', 'network', 'paramet', 'of', 'dbn', '']\n",
      "\n",
      "\n",
      "The dramatic development of IT technology has increased absolute amount of data to store, analyze, and process for computers and it has also rapidly increased the amount of realtime processing for data stream\n",
      "\n",
      "1. Tokenization: ['The', 'dramatic', 'development', 'of', 'IT', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "2. Lowercasing: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "3. Punctuation: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', '', 'analyze', '', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "4. Stemming: ['the', 'dramat', 'develop', 'of', 'it', 'technolog', 'ha', 'increas', 'absolut', 'amount', 'of', 'data', 'to', 'store', '', 'analyz', '', 'and', 'process', 'for', 'comput', 'and', 'it', 'ha', 'also', 'rapidli', 'increas', 'the', 'amount', 'of', 'realtim', 'process', 'for', 'data', 'stream']\n",
      "\n",
      "\n",
      "We present an algorithm achieving gathering in O(n2) rounds in expectation.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'O', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '', 'n2', '', 'rounds', 'in', 'expectation', '']\n",
      "4. Stemming: ['we', 'present', 'an', 'algorithm', 'achiev', 'gather', 'in', 'o', '', 'n2', '', 'round', 'in', 'expect', '']\n",
      "\n",
      "\n",
      "Let A and B two n×n matrices over a ring R (e.g., the reals or the integers) each containing at most m nonzero elements.\n",
      "\n",
      "1. Tokenization: ['Let', 'A', 'and', 'B', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'R', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "2. Lowercasing: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "3. Punctuation: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'reals', 'or', 'the', 'integers', '', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '']\n",
      "4. Stemming: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matric', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'real', 'or', 'the', 'integ', '', 'each', 'contain', 'at', 'most', 'm', 'nonzero', 'element', '']\n",
      "\n",
      "\n",
      "Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited.\n",
      "\n",
      "1. Tokenization: ['Firmware', 'is', 'the', 'enable', 'software', 'of', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'IoT', 'devices', 'being', 'exploited', '.']\n",
      "2. Lowercasing: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '(', 'iot', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '.']\n",
      "3. Punctuation: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '', 'iot', '', 'devices', '', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '']\n",
      "4. Stemming: ['firmwar', 'is', 'the', 'enabl', 'softwar', 'of', 'internet', 'of', 'thing', '', 'iot', '', 'devic', '', 'and', 'it', 'softwar', 'vulner', 'are', 'one', 'of', 'the', 'primari', 'reason', 'of', 'iot', 'devic', 'be', 'exploit', '']\n",
      "\n",
      "\n",
      "We present a new algorithm that multiplies A and B using O(m0.7n1.2+n2+o(1)) algebraic operations (i.e., multiplications, additions and subtractions) over R.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'A', 'and', 'B', 'using', 'O', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'R', '.']\n",
      "2. Lowercasing: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'r', '.']\n",
      "3. Punctuation: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebraic', 'operations', '', 'ie', '', 'multiplications', '', 'additions', 'and', 'subtractions', '', 'over', 'r', '']\n",
      "4. Stemming: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multipli', 'a', 'and', 'b', 'use', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebra', 'oper', '', 'ie', '', 'multipl', '', 'addit', 'and', 'subtract', '', 'over', 'r', '']\n",
      "\n",
      "\n",
      "Efforts in “explainable AI” are under way, hopefully eliminating the “black-box” concept in future clinical decision tools.\n",
      "\n",
      "1. Tokenization: ['Efforts', 'in', '“', 'explainable', 'AI', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "2. Lowercasing: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "3. Punctuation: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', '', 'hopefully', 'eliminating', 'the', '“', 'blackbox', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '']\n",
      "4. Stemming: ['effort', 'in', '“', 'explain', 'ai', '”', 'are', 'under', 'way', '', 'hope', 'elimin', 'the', '“', 'blackbox', '”', 'concept', 'in', 'futur', 'clinic', 'decis', 'tool', '']\n",
      "\n",
      "\n",
      "Target distance (D) and target width (W), traditionally treated as independent variables in Fitts' target acquisition paradigm, are shown to suffer inextricable confounds with task difficulty.\n",
      "\n",
      "1. Tokenization: ['Target', 'distance', '(', 'D', ')', 'and', 'target', 'width', '(', 'W', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'Fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "2. Lowercasing: ['target', 'distance', '(', 'd', ')', 'and', 'target', 'width', '(', 'w', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "3. Punctuation: ['target', 'distance', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', '', 'target', 'acquisition', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '']\n",
      "4. Stemming: ['target', 'distanc', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'tradit', 'treat', 'as', 'independ', 'variabl', 'in', 'fitt', '', 'target', 'acquisit', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextric', 'confound', 'with', 'task', 'difficulti', '']\n",
      "\n",
      "\n",
      "This paper describes an NSF-funded initiative involving 600 underrepresented high school students and 60 teachers designed to introduce underrepresented students to the numerous and varied career opportunities in the computing sciences.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'an', 'NSF-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'an', 'nsf-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'an', 'nsffunded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '']\n",
      "4. Stemming: ['thi', 'paper', 'describ', 'an', 'nsffund', 'initi', 'involv', '600', 'underrepres', 'high', 'school', 'student', 'and', '60', 'teacher', 'design', 'to', 'introduc', 'underrepres', 'student', 'to', 'the', 'numer', 'and', 'vari', 'career', 'opportun', 'in', 'the', 'comput', 'scienc', '']\n",
      "\n",
      "\n",
      "The gathering problem, where n autonomous robots with restricted capabilities are required to meet in a single point of the plane, is widely studied.\n",
      "\n",
      "1. Tokenization: ['The', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "2. Lowercasing: ['the', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "3. Punctuation: ['the', 'gathering', 'problem', '', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', '', 'is', 'widely', 'studied', '']\n",
      "4. Stemming: ['the', 'gather', 'problem', '', 'where', 'n', 'autonom', 'robot', 'with', 'restrict', 'capabl', 'are', 'requir', 'to', 'meet', 'in', 'a', 'singl', 'point', 'of', 'the', 'plane', '', 'is', 'wide', 'studi', '']\n",
      "\n",
      "\n",
      "The information captured by IoT present an unprecedented opportunity to solve large-scale problems in those application domains to deliver services\n",
      "\n",
      "1. Tokenization: ['The', 'information', 'captured', 'by', 'IoT', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "2. Lowercasing: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "3. Punctuation: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'largescale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "4. Stemming: ['the', 'inform', 'captur', 'by', 'iot', 'present', 'an', 'unpreced', 'opportun', 'to', 'solv', 'largescal', 'problem', 'in', 'those', 'applic', 'domain', 'to', 'deliv', 'servic']\n",
      "\n",
      "\n",
      "Compliance with the information system (IS) security policy is an established theme in IS research for protecting the IS from user actions.\n",
      "\n",
      "1. Tokenization: ['Compliance', 'with', 'the', 'information', 'system', '(', 'IS', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'IS', 'research', 'for', 'protecting', 'the', 'IS', 'from', 'user', 'actions', '.']\n",
      "2. Lowercasing: ['compliance', 'with', 'the', 'information', 'system', '(', 'is', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '.']\n",
      "3. Punctuation: ['compliance', 'with', 'the', 'information', 'system', '', 'is', '', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '']\n",
      "4. Stemming: ['complianc', 'with', 'the', 'inform', 'system', '', 'is', '', 'secur', 'polici', 'is', 'an', 'establish', 'theme', 'in', 'is', 'research', 'for', 'protect', 'the', 'is', 'from', 'user', 'action', '']\n",
      "\n",
      "\n",
      "In this paper, we develop Re-Vibe, the first system that re-identifies people through footstep-induced floor vibrations.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'develop', 'Re-Vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'develop', 're-vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'develop', 'revibe', '', 'the', 'first', 'system', 'that', 'reidentifies', 'people', 'through', 'footstepinduced', 'floor', 'vibrations', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'develop', 'revib', '', 'the', 'first', 'system', 'that', 'reidentifi', 'peopl', 'through', 'footstepinduc', 'floor', 'vibrat', '']\n",
      "\n",
      "\n",
      "I/O is emerging as a major bottleneck for machine learning training, especially in distributed environments.\n",
      "\n",
      "1. Tokenization: ['I/O', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "2. Lowercasing: ['i/o', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "3. Punctuation: ['io', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', '', 'especially', 'in', 'distributed', 'environments', '']\n",
      "4. Stemming: ['io', 'is', 'emerg', 'as', 'a', 'major', 'bottleneck', 'for', 'machin', 'learn', 'train', '', 'especi', 'in', 'distribut', 'environ', '']\n",
      "\n",
      "\n",
      "In this paper, we attempt to improve the query likelihood function by bringing back the negative query generation.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'attempt', 'to', 'improv', 'the', 'queri', 'likelihood', 'function', 'by', 'bring', 'back', 'the', 'neg', 'queri', 'gener', '']\n",
      "\n",
      "\n",
      "In this paper, we propose a new on-chip interconnect scheme called Y-architecture, which can utilize the on-chip routing resources more efficiently than traditional Manhattan interconnect architecture by allowing wires routed in three directions (0°, 60°, and 120°).\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'Y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'Manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'propose', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'called', 'yarchitecture', '', 'which', 'can', 'utilize', 'the', 'onchip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'propos', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'call', 'yarchitectur', '', 'which', 'can', 'util', 'the', 'onchip', 'rout', 'resourc', 'more', 'effici', 'than', 'tradit', 'manhattan', 'interconnect', 'architectur', 'by', 'allow', 'wire', 'rout', 'in', 'three', 'direct', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "\n",
      "\n",
      "We describe an algorithm for Byzantine agreement that is scalable in the sense that each processor sends only O(√n) bits, where n is the total number of processors.\n",
      "\n",
      "1. Tokenization: ['We', 'describe', 'an', 'algorithm', 'for', 'Byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'O', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "2. Lowercasing: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "3. Punctuation: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '', '√n', '', 'bits', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '']\n",
      "4. Stemming: ['we', 'describ', 'an', 'algorithm', 'for', 'byzantin', 'agreement', 'that', 'is', 'scalabl', 'in', 'the', 'sens', 'that', 'each', 'processor', 'send', 'onli', 'o', '', '√n', '', 'bit', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processor', '']\n",
      "\n",
      "\n",
      "This paper presents an image-based rendering (IBR) system based on RGB-D images.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'IBR', ')', 'system', 'based', 'on', 'RGB-D', 'images', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'ibr', ')', 'system', 'based', 'on', 'rgb-d', 'images', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'imagebased', 'rendering', '', 'ibr', '', 'system', 'based', 'on', 'rgbd', 'images', '']\n",
      "4. Stemming: ['thi', 'paper', 'present', 'an', 'imagebas', 'render', '', 'ibr', '', 'system', 'base', 'on', 'rgbd', 'imag', '']\n",
      "\n",
      "\n",
      "In this paper we present a framework and methodology for aligning the business strategy and IT/IS for an organization offering an e-service in a multi-organizational setting.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'IT/IS', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'it/is', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'itis', 'for', 'an', 'organization', 'offering', 'an', 'eservice', 'in', 'a', 'multiorganizational', 'setting', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodolog', 'for', 'align', 'the', 'busi', 'strategi', 'and', 'iti', 'for', 'an', 'organ', 'offer', 'an', 'eservic', 'in', 'a', 'multiorganiz', 'set', '']\n",
      "\n",
      "\n",
      "The D Programming Language is a hybrid of C++ and modern scripting languages: it compiles statically to native code, but is also garbage collected.\n",
      "\n",
      "1. Tokenization: ['The', 'D', 'Programming', 'Language', 'is', 'a', 'hybrid', 'of', 'C++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "2. Lowercasing: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "3. Punctuation: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'scripting', 'languages', '', 'it', 'compiles', 'statically', 'to', 'native', 'code', '', 'but', 'is', 'also', 'garbage', 'collected', '']\n",
      "4. Stemming: ['the', 'd', 'program', 'languag', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'script', 'languag', '', 'it', 'compil', 'static', 'to', 'nativ', 'code', '', 'but', 'is', 'also', 'garbag', 'collect', '']\n",
      "\n",
      "\n",
      "However, the main aim is precisely to present an algorithm which gives the so-called minimal solutions: Boolean matrices M satisfying the equation with the least possible number of unity entries.\n",
      "\n",
      "1. Tokenization: ['However', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'Boolean', 'matrices', 'M', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "2. Lowercasing: ['however', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "3. Punctuation: ['however', '', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'socalled', 'minimal', 'solutions', '', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '']\n",
      "4. Stemming: ['howev', '', 'the', 'main', 'aim', 'is', 'precis', 'to', 'present', 'an', 'algorithm', 'which', 'give', 'the', 'socal', 'minim', 'solut', '', 'boolean', 'matric', 'm', 'satisfi', 'the', 'equat', 'with', 'the', 'least', 'possibl', 'number', 'of', 'uniti', 'entri', '']\n",
      "\n",
      "\n",
      "A number of algorithms have been proposed for LR incremental parsers, but few have been proposed for LL incremental parsers [1, 2].\n",
      "\n",
      "1. Tokenization: ['A', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'LR', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'LL', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "2. Lowercasing: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "3. Punctuation: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', '', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '', '1', '', '2', '', '']\n",
      "4. Stemming: ['a', 'number', 'of', 'algorithm', 'have', 'been', 'propos', 'for', 'lr', 'increment', 'parser', '', 'but', 'few', 'have', 'been', 'propos', 'for', 'll', 'increment', 'parser', '', '1', '', '2', '', '']\n",
      "\n",
      "\n",
      "I will discuss the use of graphical models for data mining.\n",
      "\n",
      "1. Tokenization: ['I', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "2. Lowercasing: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "3. Punctuation: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '']\n",
      "4. Stemming: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphic', 'model', 'for', 'data', 'mine', '']\n",
      "\n",
      "\n",
      "Over the past decade, a pair of synchronization instructions known as LL/SC has emerged as the most suitable set of instructions to be used in the design of lock-free algorithms.\n",
      "\n",
      "1. Tokenization: ['Over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'LL/SC', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "2. Lowercasing: ['over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'll/sc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "3. Punctuation: ['over', 'the', 'past', 'decade', '', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'llsc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lockfree', 'algorithms', '']\n",
      "4. Stemming: ['over', 'the', 'past', 'decad', '', 'a', 'pair', 'of', 'synchron', 'instruct', 'known', 'as', 'llsc', 'ha', 'emerg', 'as', 'the', 'most', 'suitabl', 'set', 'of', 'instruct', 'to', 'be', 'use', 'in', 'the', 'design', 'of', 'lockfre', 'algorithm', '']\n",
      "\n",
      "\n",
      "Our algorithm has latency that is polylogarithmic in n. \n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '']\n",
      "4. Stemming: ['our', 'algorithm', 'ha', 'latenc', 'that', 'is', 'polylogarithm', 'in', 'n', '']\n",
      "\n",
      "\n",
      "Typical person re-identification (re-ID) systems rely on cameras to match the same person across different locations.\n",
      "\n",
      "1. Tokenization: ['Typical', 'person', 're-identification', '(', 're-ID', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "2. Lowercasing: ['typical', 'person', 're-identification', '(', 're-id', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "3. Punctuation: ['typical', 'person', 'reidentification', '', 'reid', '', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '']\n",
      "4. Stemming: ['typic', 'person', 'reidentif', '', 'reid', '', 'system', 'reli', 'on', 'camera', 'to', 'match', 'the', 'same', 'person', 'across', 'differ', 'locat', '']\n",
      "\n",
      "\n",
      "The Internet of Things (IoT) is the latest Internet evolution that incorporates a diverse range of things such as sensors, actuators, and services deployed by different organizations and individuals to support a variety of applications.\n",
      "\n",
      "1. Tokenization: ['The', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'is', 'the', 'latest', 'Internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "2. Lowercasing: ['the', 'internet', 'of', 'things', '(', 'iot', ')', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "3. Punctuation: ['the', 'internet', 'of', 'things', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', '', 'actuators', '', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '']\n",
      "4. Stemming: ['the', 'internet', 'of', 'thing', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolut', 'that', 'incorpor', 'a', 'divers', 'rang', 'of', 'thing', 'such', 'as', 'sensor', '', 'actuat', '', 'and', 'servic', 'deploy', 'by', 'differ', 'organ', 'and', 'individu', 'to', 'support', 'a', 'varieti', 'of', 'applic', '']\n",
      "\n",
      "\n",
      "This subsection compares the state of the art methods and the proposed WRA-Net for the BoniRob dataset through the same method.\n",
      "\n",
      "1. Tokenization: ['This', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'WRA-Net', 'for', 'the', 'BoniRob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "2. Lowercasing: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wra-net', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "3. Punctuation: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "4. Stemming: ['thi', 'subsect', 'compar', 'the', 'state', 'of', 'the', 'art', 'method', 'and', 'the', 'propos', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "\n",
      "\n",
      "Hence, WRA-Net achieved higher values for PSNRand SSIM compared to the state-of-the-art methods.\n",
      "\n",
      "1. Tokenization: ['Hence', ',', 'WRA-Net', 'achieved', 'higher', 'values', 'for', 'PSNRand', 'SSIM', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "2. Lowercasing: ['hence', ',', 'wra-net', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "3. Punctuation: ['hence', '', 'wranet', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'stateoftheart', 'methods', '']\n",
      "4. Stemming: ['henc', '', 'wranet', 'achiev', 'higher', 'valu', 'for', 'psnrand', 'ssim', 'compar', 'to', 'the', 'stateoftheart', 'method', '']\n",
      "\n",
      "\n",
      "It is shown that relative movement amplitude D/W(which determines difficulty) and absolute movement amplitude D (or scale) are the only two variables that can be manipulated independently in a Fitts' task experiment.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'D/W', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'D', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'Fitts', \"'\", 'task', 'experiment', '.']\n",
      "2. Lowercasing: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'd/w', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'd', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', \"'\", 'task', 'experiment', '.']\n",
      "3. Punctuation: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'dw', '', 'which', 'determines', 'difficulty', '', 'and', 'absolute', 'movement', 'amplitude', 'd', '', 'or', 'scale', '', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', '', 'task', 'experiment', '']\n",
      "4. Stemming: ['it', 'is', 'shown', 'that', 'rel', 'movement', 'amplitud', 'dw', '', 'which', 'determin', 'difficulti', '', 'and', 'absolut', 'movement', 'amplitud', 'd', '', 'or', 'scale', '', 'are', 'the', 'onli', 'two', 'variabl', 'that', 'can', 'be', 'manipul', 'independ', 'in', 'a', 'fitt', '', 'task', 'experi', '']\n",
      "\n",
      "\n",
      "The area-under-the-curve (AUC) was the chosen performance metric for comparison and cross-validation was performed.\n",
      "\n",
      "1. Tokenization: ['The', 'area-under-the-curve', '(', 'AUC', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "2. Lowercasing: ['the', 'area-under-the-curve', '(', 'auc', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "3. Punctuation: ['the', 'areaunderthecurve', '', 'auc', '', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'crossvalidation', 'was', 'performed', '']\n",
      "4. Stemming: ['the', 'areaunderthecurv', '', 'auc', '', 'wa', 'the', 'chosen', 'perform', 'metric', 'for', 'comparison', 'and', 'crossvalid', 'wa', 'perform', '']\n",
      "\n",
      "\n",
      "In this paper we present a tool to assist in teaching top-down and bottom-up analysis algorithms. The tool provides simulation for the following analysis algorithms: LL, SLR, LALR and LR.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'The', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'LL', ',', 'SLR', ',', 'LALR', 'and', 'LR', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'll', ',', 'slr', ',', 'lalr', 'and', 'lr', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'topdown', 'and', 'bottomup', 'analysis', 'algorithms', '', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teach', 'topdown', 'and', 'bottomup', 'analysi', 'algorithm', '', 'the', 'tool', 'provid', 'simul', 'for', 'the', 'follow', 'analysi', 'algorithm', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "\n",
      "\n",
      "In this paper, we formally show that standard gradient methods never overfit on separable data: If we run these methods for T iterations on a dataset of size m, both the empirical risk and the generalization error decrease at an essentially optimal rate of Õ(1/γ2T) up till T ∼ m.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'If', 'we', 'run', 'these', 'methods', 'for', 'T', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'Õ', '(', '1/γ2T', ')', 'up', 'till', 'T', '∼', 'm', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '(', '1/γ2t', ')', 'up', 'till', 't', '∼', 'm', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', '', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "4. Stemming: ['in', 'thi', 'paper', '', 'we', 'formal', 'show', 'that', 'standard', 'gradient', 'method', 'never', 'overfit', 'on', 'separ', 'data', '', 'if', 'we', 'run', 'these', 'method', 'for', 't', 'iter', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empir', 'risk', 'and', 'the', 'gener', 'error', 'decreas', 'at', 'an', 'essenti', 'optim', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "\n",
      "\n",
      "This paper is concerned with goodness of fit evaluation for virtual commissioning modelling purposes.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "3. Punctuation: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '']\n",
      "4. Stemming: ['thi', 'paper', 'is', 'concern', 'with', 'good', 'of', 'fit', 'evalu', 'for', 'virtual', 'commiss', 'model', 'purpos', '']\n",
      "\n",
      "\n",
      "The results showed that the optimal performance was achieved under natural complexification of the EANN and that back-propagation tended to over fit the data.\n",
      "\n",
      "1. Tokenization: ['The', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'EANN', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "2. Lowercasing: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "3. Punctuation: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'backpropagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "4. Stemming: ['the', 'result', 'show', 'that', 'the', 'optim', 'perform', 'wa', 'achiev', 'under', 'natur', 'complexif', 'of', 'the', 'eann', 'and', 'that', 'backpropag', 'tend', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "\n",
      "\n",
      "The second approach does not train models that generalize across tasks, but rather over-fit a single instance of a problem\n",
      "\n",
      "1. Tokenization: ['The', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "2. Lowercasing: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "3. Punctuation: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', '', 'but', 'rather', 'overfit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "4. Stemming: ['the', 'second', 'approach', 'doe', 'not', 'train', 'model', 'that', 'gener', 'across', 'task', '', 'but', 'rather', 'overfit', 'a', 'singl', 'instanc', 'of', 'a', 'problem']\n",
      "\n",
      "\n",
      "This paper describes two significant contributions to the NILM community in an effort towards reproducible state-of-the-art research.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'NILM', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'stateoftheart', 'research', '']\n",
      "4. Stemming: ['thi', 'paper', 'describ', 'two', 'signific', 'contribut', 'to', 'the', 'nilm', 'commun', 'in', 'an', 'effort', 'toward', 'reproduc', 'stateoftheart', 'research', '']\n",
      "\n",
      "\n",
      "Accurate numerical results for a definite integral are easily obtained by simple substitutions of upper and lower bounds of integral into obtained approximate symbolic results\n",
      "\n",
      "1. Tokenization: ['Accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "2. Lowercasing: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "3. Punctuation: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "4. Stemming: ['accur', 'numer', 'result', 'for', 'a', 'definit', 'integr', 'are', 'easili', 'obtain', 'by', 'simpl', 'substitut', 'of', 'upper', 'and', 'lower', 'bound', 'of', 'integr', 'into', 'obtain', 'approxim', 'symbol', 'result']\n",
      "\n",
      "\n",
      "This paper presents an optimal algorithm for jumper insertion under the ratio upper-bound.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "4. Stemming: ['thi', 'paper', 'present', 'an', 'optim', 'algorithm', 'for', 'jumper', 'insert', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "\n",
      "\n",
      "In particular, we argue that inter-tagger agreement is not a real upperbound for the Basque WSD task.\n",
      "\n",
      "1. Tokenization: ['In', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'Basque', 'WSD', 'task', '.']\n",
      "2. Lowercasing: ['in', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '.']\n",
      "3. Punctuation: ['in', 'particular', '', 'we', 'argue', 'that', 'intertagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '']\n",
      "4. Stemming: ['in', 'particular', '', 'we', 'argu', 'that', 'intertagg', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basqu', 'wsd', 'task', '']\n",
      "\n",
      "\n",
      "Lexicalized context-free grammar(LCFG) is an attractive compromise between the parsing efficiency of context-free grammar (CFG) and the elegance and lexical sensitivity of lexicalized tree adjoining grammar (LTAG).\n",
      "\n",
      "1. Tokenization: ['Lexicalized', 'context-free', 'grammar', '(', 'LCFG', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'CFG', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'LTAG', ')', '.']\n",
      "2. Lowercasing: ['lexicalized', 'context-free', 'grammar', '(', 'lcfg', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'cfg', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'ltag', ')', '.']\n",
      "3. Punctuation: ['lexicalized', 'contextfree', 'grammar', '', 'lcfg', '', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'contextfree', 'grammar', '', 'cfg', '', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '', 'ltag', '', '']\n",
      "4. Stemming: ['lexic', 'contextfre', 'grammar', '', 'lcfg', '', 'is', 'an', 'attract', 'compromis', 'between', 'the', 'pars', 'effici', 'of', 'contextfre', 'grammar', '', 'cfg', '', 'and', 'the', 'eleg', 'and', 'lexic', 'sensit', 'of', 'lexic', 'tree', 'adjoin', 'grammar', '', 'ltag', '', '']\n",
      "\n",
      "\n",
      "Formulation of Low-Order Dominant Poles for Y-Matrix of Interconnects: This paper presents an efficient approach to compute the dominant poles for the reduced-order admittance (Y parameter) matrix of lossy interconnects.\n",
      "\n",
      "1. Tokenization: ['Formulation', 'of', 'Low-Order', 'Dominant', 'Poles', 'for', 'Y-Matrix', 'of', 'Interconnects', ':', 'This', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'Y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "2. Lowercasing: ['formulation', 'of', 'low-order', 'dominant', 'poles', 'for', 'y-matrix', 'of', 'interconnects', ':', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "3. Punctuation: ['formulation', 'of', 'loworder', 'dominant', 'poles', 'for', 'ymatrix', 'of', 'interconnects', '', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reducedorder', 'admittance', '', 'y', 'parameter', '', 'matrix', 'of', 'lossy', 'interconnects', '']\n",
      "4. Stemming: ['formul', 'of', 'loword', 'domin', 'pole', 'for', 'ymatrix', 'of', 'interconnect', '', 'this', 'paper', 'present', 'an', 'effici', 'approach', 'to', 'comput', 'the', 'domin', 'pole', 'for', 'the', 'reducedord', 'admitt', '', 'y', 'paramet', '', 'matrix', 'of', 'lossi', 'interconnect', '']\n",
      "\n",
      "\n",
      "Our algorithm succeeds with high probability against an adaptive adversary, which can take over processors at any time during the protocol, up to the point of taking over arbitrarily close to a 1/3 fraction.\n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', '', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "4. Stemming: ['our', 'algorithm', 'succeed', 'with', 'high', 'probabl', 'against', 'an', 'adapt', 'adversari', '', 'which', 'can', 'take', 'over', 'processor', 'at', 'ani', 'time', 'dure', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'take', 'over', 'arbitrarili', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "\n",
      "\n",
      "We present an all-pairs shortest path algorithm whose running time on a complete directed graph on n vertices whose edge weights are chosen independently and uniformly at random from [0,1] is O(n2), in expectation and with high probability.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'O', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'o', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'allpairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expectation', 'and', 'with', 'high', 'probability', '']\n",
      "4. Stemming: ['we', 'present', 'an', 'allpair', 'shortest', 'path', 'algorithm', 'whose', 'run', 'time', 'on', 'a', 'complet', 'direct', 'graph', 'on', 'n', 'vertic', 'whose', 'edg', 'weight', 'are', 'chosen', 'independ', 'and', 'uniform', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expect', 'and', 'with', 'high', 'probabl', '']\n",
      "\n",
      "\n",
      "We consider the problem of re-ranking the top-k documents returned by a retrieval system given some search query.\n",
      "\n",
      "1. Tokenization: ['We', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "2. Lowercasing: ['we', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "3. Punctuation: ['we', 'consider', 'the', 'problem', 'of', 'reranking', 'the', 'topk', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '']\n",
      "4. Stemming: ['we', 'consid', 'the', 'problem', 'of', 'rerank', 'the', 'topk', 'document', 'return', 'by', 'a', 'retriev', 'system', 'given', 'some', 'search', 'queri', '']\n",
      "\n",
      "\n",
      "In this paper, we combine the learning-to-rank paradigm with the recent developments on axioms for information retrieval.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'combine', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'combin', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'develop', 'on', 'axiom', 'for', 'inform', 'retriev', '']\n",
      "\n",
      "\n",
      "We outline important details on cross-validation techniques that can enhance the performance.\n",
      "\n",
      "1. Tokenization: ['We', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "2. Lowercasing: ['we', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "3. Punctuation: ['we', 'outline', 'important', 'details', 'on', 'crossvalidation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '']\n",
      "4. Stemming: ['we', 'outlin', 'import', 'detail', 'on', 'crossvalid', 'techniqu', 'that', 'can', 'enhanc', 'the', 'perform', '']\n",
      "\n",
      "\n",
      "“Next-fit” allocation differs from first-fit in that a first-fit allocator commences its search for free space at a fixed end of memory, whereas a next-fit allocator commences its search wherever it previously stopped searching.\n",
      "\n",
      "1. Tokenization: ['“', 'Next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "2. Lowercasing: ['“', 'next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "3. Punctuation: ['“', 'nextfit', '”', 'allocation', 'differs', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', '', 'whereas', 'a', 'nextfit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '']\n",
      "4. Stemming: ['“', 'nextfit', '”', 'alloc', 'differ', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'alloc', 'commenc', 'it', 'search', 'for', 'free', 'space', 'at', 'a', 'fix', 'end', 'of', 'memori', '', 'wherea', 'a', 'nextfit', 'alloc', 'commenc', 'it', 'search', 'wherev', 'it', 'previous', 'stop', 'search', '']\n",
      "\n",
      "\n",
      "It is important to choose an appropriate network structure because simple networks are likely to under-fit while complex networks are less plastic and more computationally expensive to train.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "2. Lowercasing: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "3. Punctuation: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'underfit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '']\n",
      "4. Stemming: ['it', 'is', 'import', 'to', 'choos', 'an', 'appropri', 'network', 'structur', 'becaus', 'simpl', 'network', 'are', 'like', 'to', 'underfit', 'while', 'complex', 'network', 'are', 'less', 'plastic', 'and', 'more', 'comput', 'expens', 'to', 'train', '']\n",
      "\n",
      "\n",
      "Both of the LL-based algorithms in these papers attempt to minimize the reparsing on the original parse tree and the parse table.\n",
      "\n",
      "1. Tokenization: ['Both', 'of', 'the', 'LL-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "2. Lowercasing: ['both', 'of', 'the', 'll-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "3. Punctuation: ['both', 'of', 'the', 'llbased', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '']\n",
      "4. Stemming: ['both', 'of', 'the', 'llbase', 'algorithm', 'in', 'these', 'paper', 'attempt', 'to', 'minim', 'the', 'repars', 'on', 'the', 'origin', 'pars', 'tree', 'and', 'the', 'pars', 'tabl', '']\n",
      "\n",
      "\n",
      "In this paper, L2-norm Deep Belief Network (L2DBN) is proposed, which uses L2-norm regularization to optimize the network parameters of DBN.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'L2-norm', 'Deep', 'Belief', 'Network', '(', 'L2DBN', ')', 'is', 'proposed', ',', 'which', 'uses', 'L2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'DBN', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'l2-norm', 'deep', 'belief', 'network', '(', 'l2dbn', ')', 'is', 'proposed', ',', 'which', 'uses', 'l2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'proposed', '', 'which', 'uses', 'l2norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'propos', '', 'which', 'use', 'l2norm', 'regular', 'to', 'optim', 'the', 'network', 'paramet', 'of', 'dbn', '']\n",
      "\n",
      "\n",
      "The dramatic development of IT technology has increased absolute amount of data to store, analyze, and process for computers and it has also rapidly increased the amount of realtime processing for data stream\n",
      "\n",
      "1. Tokenization: ['The', 'dramatic', 'development', 'of', 'IT', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "2. Lowercasing: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "3. Punctuation: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', '', 'analyze', '', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "4. Stemming: ['the', 'dramat', 'develop', 'of', 'it', 'technolog', 'has', 'increas', 'absolut', 'amount', 'of', 'data', 'to', 'store', '', 'analyz', '', 'and', 'process', 'for', 'comput', 'and', 'it', 'has', 'also', 'rapid', 'increas', 'the', 'amount', 'of', 'realtim', 'process', 'for', 'data', 'stream']\n",
      "\n",
      "\n",
      "We present an algorithm achieving gathering in O(n2) rounds in expectation.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'O', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '', 'n2', '', 'rounds', 'in', 'expectation', '']\n",
      "4. Stemming: ['we', 'present', 'an', 'algorithm', 'achiev', 'gather', 'in', 'o', '', 'n2', '', 'round', 'in', 'expect', '']\n",
      "\n",
      "\n",
      "Let A and B two n×n matrices over a ring R (e.g., the reals or the integers) each containing at most m nonzero elements.\n",
      "\n",
      "1. Tokenization: ['Let', 'A', 'and', 'B', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'R', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "2. Lowercasing: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "3. Punctuation: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'reals', 'or', 'the', 'integers', '', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '']\n",
      "4. Stemming: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matric', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'real', 'or', 'the', 'integ', '', 'each', 'contain', 'at', 'most', 'm', 'nonzero', 'element', '']\n",
      "\n",
      "\n",
      "Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited.\n",
      "\n",
      "1. Tokenization: ['Firmware', 'is', 'the', 'enable', 'software', 'of', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'IoT', 'devices', 'being', 'exploited', '.']\n",
      "2. Lowercasing: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '(', 'iot', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '.']\n",
      "3. Punctuation: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '', 'iot', '', 'devices', '', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '']\n",
      "4. Stemming: ['firmwar', 'is', 'the', 'enabl', 'softwar', 'of', 'internet', 'of', 'thing', '', 'iot', '', 'devic', '', 'and', 'it', 'softwar', 'vulner', 'are', 'one', 'of', 'the', 'primari', 'reason', 'of', 'iot', 'devic', 'be', 'exploit', '']\n",
      "\n",
      "\n",
      "We present a new algorithm that multiplies A and B using O(m0.7n1.2+n2+o(1)) algebraic operations (i.e., multiplications, additions and subtractions) over R.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'A', 'and', 'B', 'using', 'O', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'R', '.']\n",
      "2. Lowercasing: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'r', '.']\n",
      "3. Punctuation: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebraic', 'operations', '', 'ie', '', 'multiplications', '', 'additions', 'and', 'subtractions', '', 'over', 'r', '']\n",
      "4. Stemming: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multipli', 'a', 'and', 'b', 'use', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebra', 'oper', '', 'ie', '', 'multipl', '', 'addit', 'and', 'subtract', '', 'over', 'r', '']\n",
      "\n",
      "\n",
      "Efforts in “explainable AI” are under way, hopefully eliminating the “black-box” concept in future clinical decision tools.\n",
      "\n",
      "1. Tokenization: ['Efforts', 'in', '“', 'explainable', 'AI', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "2. Lowercasing: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "3. Punctuation: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', '', 'hopefully', 'eliminating', 'the', '“', 'blackbox', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '']\n",
      "4. Stemming: ['effort', 'in', '“', 'explain', 'ai', '”', 'are', 'under', 'way', '', 'hope', 'elimin', 'the', '“', 'blackbox', '”', 'concept', 'in', 'futur', 'clinic', 'decis', 'tool', '']\n",
      "\n",
      "\n",
      "Target distance (D) and target width (W), traditionally treated as independent variables in Fitts' target acquisition paradigm, are shown to suffer inextricable confounds with task difficulty.\n",
      "\n",
      "1. Tokenization: ['Target', 'distance', '(', 'D', ')', 'and', 'target', 'width', '(', 'W', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'Fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "2. Lowercasing: ['target', 'distance', '(', 'd', ')', 'and', 'target', 'width', '(', 'w', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "3. Punctuation: ['target', 'distance', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', '', 'target', 'acquisition', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '']\n",
      "4. Stemming: ['target', 'distanc', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'tradit', 'treat', 'as', 'independ', 'variabl', 'in', 'fitt', '', 'target', 'acquisit', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextric', 'confound', 'with', 'task', 'difficulti', '']\n",
      "\n",
      "\n",
      "This paper describes an NSF-funded initiative involving 600 underrepresented high school students and 60 teachers designed to introduce underrepresented students to the numerous and varied career opportunities in the computing sciences.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'an', 'NSF-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'an', 'nsf-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'an', 'nsffunded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '']\n",
      "4. Stemming: ['this', 'paper', 'describ', 'an', 'nsffund', 'initi', 'involv', '600', 'underrepres', 'high', 'school', 'student', 'and', '60', 'teacher', 'design', 'to', 'introduc', 'underrepres', 'student', 'to', 'the', 'numer', 'and', 'vari', 'career', 'opportun', 'in', 'the', 'comput', 'scienc', '']\n",
      "\n",
      "\n",
      "The gathering problem, where n autonomous robots with restricted capabilities are required to meet in a single point of the plane, is widely studied.\n",
      "\n",
      "1. Tokenization: ['The', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "2. Lowercasing: ['the', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "3. Punctuation: ['the', 'gathering', 'problem', '', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', '', 'is', 'widely', 'studied', '']\n",
      "4. Stemming: ['the', 'gather', 'problem', '', 'where', 'n', 'autonom', 'robot', 'with', 'restrict', 'capabl', 'are', 'requir', 'to', 'meet', 'in', 'a', 'singl', 'point', 'of', 'the', 'plane', '', 'is', 'wide', 'studi', '']\n",
      "\n",
      "\n",
      "The information captured by IoT present an unprecedented opportunity to solve large-scale problems in those application domains to deliver services\n",
      "\n",
      "1. Tokenization: ['The', 'information', 'captured', 'by', 'IoT', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "2. Lowercasing: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "3. Punctuation: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'largescale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "4. Stemming: ['the', 'inform', 'captur', 'by', 'iot', 'present', 'an', 'unpreced', 'opportun', 'to', 'solv', 'largescal', 'problem', 'in', 'those', 'applic', 'domain', 'to', 'deliv', 'servic']\n",
      "\n",
      "\n",
      "Compliance with the information system (IS) security policy is an established theme in IS research for protecting the IS from user actions.\n",
      "\n",
      "1. Tokenization: ['Compliance', 'with', 'the', 'information', 'system', '(', 'IS', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'IS', 'research', 'for', 'protecting', 'the', 'IS', 'from', 'user', 'actions', '.']\n",
      "2. Lowercasing: ['compliance', 'with', 'the', 'information', 'system', '(', 'is', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '.']\n",
      "3. Punctuation: ['compliance', 'with', 'the', 'information', 'system', '', 'is', '', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '']\n",
      "4. Stemming: ['complianc', 'with', 'the', 'inform', 'system', '', 'is', '', 'secur', 'polici', 'is', 'an', 'establish', 'theme', 'in', 'is', 'research', 'for', 'protect', 'the', 'is', 'from', 'user', 'action', '']\n",
      "\n",
      "\n",
      "In this paper, we develop Re-Vibe, the first system that re-identifies people through footstep-induced floor vibrations.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'develop', 'Re-Vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'develop', 're-vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'develop', 'revibe', '', 'the', 'first', 'system', 'that', 'reidentifies', 'people', 'through', 'footstepinduced', 'floor', 'vibrations', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'develop', 'revib', '', 'the', 'first', 'system', 'that', 'reidentifi', 'peopl', 'through', 'footstepinduc', 'floor', 'vibrat', '']\n",
      "\n",
      "\n",
      "I/O is emerging as a major bottleneck for machine learning training, especially in distributed environments.\n",
      "\n",
      "1. Tokenization: ['I/O', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "2. Lowercasing: ['i/o', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "3. Punctuation: ['io', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', '', 'especially', 'in', 'distributed', 'environments', '']\n",
      "4. Stemming: ['io', 'is', 'emerg', 'as', 'a', 'major', 'bottleneck', 'for', 'machin', 'learn', 'train', '', 'especi', 'in', 'distribut', 'environ', '']\n",
      "\n",
      "\n",
      "In this paper, we attempt to improve the query likelihood function by bringing back the negative query generation.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improv', 'the', 'queri', 'likelihood', 'function', 'by', 'bring', 'back', 'the', 'negat', 'queri', 'generat', '']\n",
      "\n",
      "\n",
      "In this paper, we propose a new on-chip interconnect scheme called Y-architecture, which can utilize the on-chip routing resources more efficiently than traditional Manhattan interconnect architecture by allowing wires routed in three directions (0°, 60°, and 120°).\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'Y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'Manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'propose', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'called', 'yarchitecture', '', 'which', 'can', 'utilize', 'the', 'onchip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'propos', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'call', 'yarchitectur', '', 'which', 'can', 'util', 'the', 'onchip', 'rout', 'resourc', 'more', 'effici', 'than', 'tradit', 'manhattan', 'interconnect', 'architectur', 'by', 'allow', 'wire', 'rout', 'in', 'three', 'direct', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "\n",
      "\n",
      "We describe an algorithm for Byzantine agreement that is scalable in the sense that each processor sends only O(√n) bits, where n is the total number of processors.\n",
      "\n",
      "1. Tokenization: ['We', 'describe', 'an', 'algorithm', 'for', 'Byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'O', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "2. Lowercasing: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "3. Punctuation: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '', '√n', '', 'bits', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '']\n",
      "4. Stemming: ['we', 'describ', 'an', 'algorithm', 'for', 'byzantin', 'agreement', 'that', 'is', 'scalabl', 'in', 'the', 'sens', 'that', 'each', 'processor', 'send', 'onli', 'o', '', '√n', '', 'bit', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processor', '']\n",
      "\n",
      "\n",
      "This paper presents an image-based rendering (IBR) system based on RGB-D images.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'IBR', ')', 'system', 'based', 'on', 'RGB-D', 'images', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'ibr', ')', 'system', 'based', 'on', 'rgb-d', 'images', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'imagebased', 'rendering', '', 'ibr', '', 'system', 'based', 'on', 'rgbd', 'images', '']\n",
      "4. Stemming: ['this', 'paper', 'present', 'an', 'imagebas', 'render', '', 'ibr', '', 'system', 'base', 'on', 'rgbd', 'imag', '']\n",
      "\n",
      "\n",
      "In this paper we present a framework and methodology for aligning the business strategy and IT/IS for an organization offering an e-service in a multi-organizational setting.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'IT/IS', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'it/is', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'itis', 'for', 'an', 'organization', 'offering', 'an', 'eservice', 'in', 'a', 'multiorganizational', 'setting', '']\n",
      "4. Stemming: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodolog', 'for', 'align', 'the', 'busi', 'strategi', 'and', 'iti', 'for', 'an', 'organ', 'offer', 'an', 'eservic', 'in', 'a', 'multiorganiz', 'set', '']\n",
      "\n",
      "\n",
      "The D Programming Language is a hybrid of C++ and modern scripting languages: it compiles statically to native code, but is also garbage collected.\n",
      "\n",
      "1. Tokenization: ['The', 'D', 'Programming', 'Language', 'is', 'a', 'hybrid', 'of', 'C++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "2. Lowercasing: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "3. Punctuation: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'scripting', 'languages', '', 'it', 'compiles', 'statically', 'to', 'native', 'code', '', 'but', 'is', 'also', 'garbage', 'collected', '']\n",
      "4. Stemming: ['the', 'd', 'program', 'languag', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'script', 'languag', '', 'it', 'compil', 'static', 'to', 'nativ', 'code', '', 'but', 'is', 'also', 'garbag', 'collect', '']\n",
      "\n",
      "\n",
      "However, the main aim is precisely to present an algorithm which gives the so-called minimal solutions: Boolean matrices M satisfying the equation with the least possible number of unity entries.\n",
      "\n",
      "1. Tokenization: ['However', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'Boolean', 'matrices', 'M', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "2. Lowercasing: ['however', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "3. Punctuation: ['however', '', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'socalled', 'minimal', 'solutions', '', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '']\n",
      "4. Stemming: ['howev', '', 'the', 'main', 'aim', 'is', 'precis', 'to', 'present', 'an', 'algorithm', 'which', 'give', 'the', 'socal', 'minim', 'solut', '', 'boolean', 'matric', 'm', 'satisfi', 'the', 'equat', 'with', 'the', 'least', 'possibl', 'number', 'of', 'uniti', 'entri', '']\n",
      "\n",
      "\n",
      "A number of algorithms have been proposed for LR incremental parsers, but few have been proposed for LL incremental parsers [1, 2].\n",
      "\n",
      "1. Tokenization: ['A', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'LR', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'LL', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "2. Lowercasing: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "3. Punctuation: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', '', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '', '1', '', '2', '', '']\n",
      "4. Stemming: ['a', 'number', 'of', 'algorithm', 'have', 'been', 'propos', 'for', 'lr', 'increment', 'parser', '', 'but', 'few', 'have', 'been', 'propos', 'for', 'll', 'increment', 'parser', '', '1', '', '2', '', '']\n",
      "\n",
      "\n",
      "I will discuss the use of graphical models for data mining.\n",
      "\n",
      "1. Tokenization: ['I', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "2. Lowercasing: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "3. Punctuation: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '']\n",
      "4. Stemming: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphic', 'model', 'for', 'data', 'mine', '']\n",
      "\n",
      "\n",
      "Over the past decade, a pair of synchronization instructions known as LL/SC has emerged as the most suitable set of instructions to be used in the design of lock-free algorithms.\n",
      "\n",
      "1. Tokenization: ['Over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'LL/SC', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "2. Lowercasing: ['over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'll/sc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "3. Punctuation: ['over', 'the', 'past', 'decade', '', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'llsc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lockfree', 'algorithms', '']\n",
      "4. Stemming: ['over', 'the', 'past', 'decad', '', 'a', 'pair', 'of', 'synchron', 'instruct', 'known', 'as', 'llsc', 'has', 'emerg', 'as', 'the', 'most', 'suitabl', 'set', 'of', 'instruct', 'to', 'be', 'use', 'in', 'the', 'design', 'of', 'lockfre', 'algorithm', '']\n",
      "\n",
      "\n",
      "Our algorithm has latency that is polylogarithmic in n. \n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '']\n",
      "4. Stemming: ['our', 'algorithm', 'has', 'latenc', 'that', 'is', 'polylogarithm', 'in', 'n', '']\n",
      "\n",
      "\n",
      "Typical person re-identification (re-ID) systems rely on cameras to match the same person across different locations.\n",
      "\n",
      "1. Tokenization: ['Typical', 'person', 're-identification', '(', 're-ID', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "2. Lowercasing: ['typical', 'person', 're-identification', '(', 're-id', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "3. Punctuation: ['typical', 'person', 'reidentification', '', 'reid', '', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '']\n",
      "4. Stemming: ['typic', 'person', 'reidentif', '', 'reid', '', 'system', 'reli', 'on', 'camera', 'to', 'match', 'the', 'same', 'person', 'across', 'differ', 'locat', '']\n",
      "\n",
      "\n",
      "The Internet of Things (IoT) is the latest Internet evolution that incorporates a diverse range of things such as sensors, actuators, and services deployed by different organizations and individuals to support a variety of applications.\n",
      "\n",
      "1. Tokenization: ['The', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'is', 'the', 'latest', 'Internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "2. Lowercasing: ['the', 'internet', 'of', 'things', '(', 'iot', ')', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "3. Punctuation: ['the', 'internet', 'of', 'things', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', '', 'actuators', '', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '']\n",
      "4. Stemming: ['the', 'internet', 'of', 'thing', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolut', 'that', 'incorpor', 'a', 'divers', 'rang', 'of', 'thing', 'such', 'as', 'sensor', '', 'actuat', '', 'and', 'servic', 'deploy', 'by', 'differ', 'organ', 'and', 'individu', 'to', 'support', 'a', 'varieti', 'of', 'applic', '']\n",
      "\n",
      "\n",
      "This subsection compares the state of the art methods and the proposed WRA-Net for the BoniRob dataset through the same method.\n",
      "\n",
      "1. Tokenization: ['This', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'WRA-Net', 'for', 'the', 'BoniRob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "2. Lowercasing: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wra-net', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "3. Punctuation: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "4. Stemming: ['this', 'subsect', 'compar', 'the', 'state', 'of', 'the', 'art', 'method', 'and', 'the', 'propos', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "\n",
      "\n",
      "Hence, WRA-Net achieved higher values for PSNRand SSIM compared to the state-of-the-art methods.\n",
      "\n",
      "1. Tokenization: ['Hence', ',', 'WRA-Net', 'achieved', 'higher', 'values', 'for', 'PSNRand', 'SSIM', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "2. Lowercasing: ['hence', ',', 'wra-net', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "3. Punctuation: ['hence', '', 'wranet', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'stateoftheart', 'methods', '']\n",
      "4. Stemming: ['henc', '', 'wranet', 'achiev', 'higher', 'valu', 'for', 'psnrand', 'ssim', 'compar', 'to', 'the', 'stateoftheart', 'method', '']\n",
      "\n",
      "\n",
      "It is shown that relative movement amplitude D/W(which determines difficulty) and absolute movement amplitude D (or scale) are the only two variables that can be manipulated independently in a Fitts' task experiment.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'D/W', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'D', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'Fitts', \"'\", 'task', 'experiment', '.']\n",
      "2. Lowercasing: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'd/w', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'd', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', \"'\", 'task', 'experiment', '.']\n",
      "3. Punctuation: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'dw', '', 'which', 'determines', 'difficulty', '', 'and', 'absolute', 'movement', 'amplitude', 'd', '', 'or', 'scale', '', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', '', 'task', 'experiment', '']\n",
      "4. Stemming: ['it', 'is', 'shown', 'that', 'relat', 'movement', 'amplitud', 'dw', '', 'which', 'determin', 'difficulti', '', 'and', 'absolut', 'movement', 'amplitud', 'd', '', 'or', 'scale', '', 'are', 'the', 'onli', 'two', 'variabl', 'that', 'can', 'be', 'manipul', 'independ', 'in', 'a', 'fitt', '', 'task', 'experi', '']\n",
      "\n",
      "\n",
      "The area-under-the-curve (AUC) was the chosen performance metric for comparison and cross-validation was performed.\n",
      "\n",
      "1. Tokenization: ['The', 'area-under-the-curve', '(', 'AUC', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "2. Lowercasing: ['the', 'area-under-the-curve', '(', 'auc', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "3. Punctuation: ['the', 'areaunderthecurve', '', 'auc', '', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'crossvalidation', 'was', 'performed', '']\n",
      "4. Stemming: ['the', 'areaunderthecurv', '', 'auc', '', 'was', 'the', 'chosen', 'perform', 'metric', 'for', 'comparison', 'and', 'crossvalid', 'was', 'perform', '']\n",
      "\n",
      "\n",
      "In this paper we present a tool to assist in teaching top-down and bottom-up analysis algorithms. The tool provides simulation for the following analysis algorithms: LL, SLR, LALR and LR.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'The', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'LL', ',', 'SLR', ',', 'LALR', 'and', 'LR', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'll', ',', 'slr', ',', 'lalr', 'and', 'lr', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'topdown', 'and', 'bottomup', 'analysis', 'algorithms', '', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "4. Stemming: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teach', 'topdown', 'and', 'bottomup', 'analysi', 'algorithm', '', 'the', 'tool', 'provid', 'simul', 'for', 'the', 'follow', 'analysi', 'algorithm', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "\n",
      "\n",
      "In this paper, we formally show that standard gradient methods never overfit on separable data: If we run these methods for T iterations on a dataset of size m, both the empirical risk and the generalization error decrease at an essentially optimal rate of Õ(1/γ2T) up till T ∼ m.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'If', 'we', 'run', 'these', 'methods', 'for', 'T', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'Õ', '(', '1/γ2T', ')', 'up', 'till', 'T', '∼', 'm', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '(', '1/γ2t', ')', 'up', 'till', 't', '∼', 'm', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', '', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "4. Stemming: ['in', 'this', 'paper', '', 'we', 'formal', 'show', 'that', 'standard', 'gradient', 'method', 'never', 'overfit', 'on', 'separ', 'data', '', 'if', 'we', 'run', 'these', 'method', 'for', 't', 'iter', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empir', 'risk', 'and', 'the', 'general', 'error', 'decreas', 'at', 'an', 'essenti', 'optim', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "\n",
      "\n",
      "This paper is concerned with goodness of fit evaluation for virtual commissioning modelling purposes.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "3. Punctuation: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '']\n",
      "4. Stemming: ['this', 'paper', 'is', 'concern', 'with', 'good', 'of', 'fit', 'evalu', 'for', 'virtual', 'commiss', 'model', 'purpos', '']\n",
      "\n",
      "\n",
      "The results showed that the optimal performance was achieved under natural complexification of the EANN and that back-propagation tended to over fit the data.\n",
      "\n",
      "1. Tokenization: ['The', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'EANN', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "2. Lowercasing: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "3. Punctuation: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'backpropagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "4. Stemming: ['the', 'result', 'show', 'that', 'the', 'optim', 'perform', 'was', 'achiev', 'under', 'natur', 'complexif', 'of', 'the', 'eann', 'and', 'that', 'backpropag', 'tend', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "\n",
      "\n",
      "The second approach does not train models that generalize across tasks, but rather over-fit a single instance of a problem\n",
      "\n",
      "1. Tokenization: ['The', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "2. Lowercasing: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "3. Punctuation: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', '', 'but', 'rather', 'overfit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "4. Stemming: ['the', 'second', 'approach', 'doe', 'not', 'train', 'model', 'that', 'general', 'across', 'task', '', 'but', 'rather', 'overfit', 'a', 'singl', 'instanc', 'of', 'a', 'problem']\n",
      "\n",
      "\n",
      "This paper describes two significant contributions to the NILM community in an effort towards reproducible state-of-the-art research.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'NILM', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'stateoftheart', 'research', '']\n",
      "4. Stemming: ['this', 'paper', 'describ', 'two', 'signific', 'contribut', 'to', 'the', 'nilm', 'communiti', 'in', 'an', 'effort', 'toward', 'reproduc', 'stateoftheart', 'research', '']\n",
      "\n",
      "\n",
      "Accurate numerical results for a definite integral are easily obtained by simple substitutions of upper and lower bounds of integral into obtained approximate symbolic results\n",
      "\n",
      "1. Tokenization: ['Accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "2. Lowercasing: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "3. Punctuation: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "4. Stemming: ['accur', 'numer', 'result', 'for', 'a', 'definit', 'integr', 'are', 'easili', 'obtain', 'by', 'simpl', 'substitut', 'of', 'upper', 'and', 'lower', 'bound', 'of', 'integr', 'into', 'obtain', 'approxim', 'symbol', 'result']\n",
      "\n",
      "\n",
      "This paper presents an optimal algorithm for jumper insertion under the ratio upper-bound.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "4. Stemming: ['this', 'paper', 'present', 'an', 'optim', 'algorithm', 'for', 'jumper', 'insert', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "\n",
      "\n",
      "In particular, we argue that inter-tagger agreement is not a real upperbound for the Basque WSD task.\n",
      "\n",
      "1. Tokenization: ['In', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'Basque', 'WSD', 'task', '.']\n",
      "2. Lowercasing: ['in', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '.']\n",
      "3. Punctuation: ['in', 'particular', '', 'we', 'argue', 'that', 'intertagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '']\n",
      "4. Stemming: ['in', 'particular', '', 'we', 'argu', 'that', 'intertagg', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basqu', 'wsd', 'task', '']\n",
      "\n",
      "\n",
      "Lexicalized context-free grammar(LCFG) is an attractive compromise between the parsing efficiency of context-free grammar (CFG) and the elegance and lexical sensitivity of lexicalized tree adjoining grammar (LTAG).\n",
      "\n",
      "1. Tokenization: ['Lexicalized', 'context-free', 'grammar', '(', 'LCFG', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'CFG', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'LTAG', ')', '.']\n",
      "2. Lowercasing: ['lexicalized', 'context-free', 'grammar', '(', 'lcfg', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'cfg', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'ltag', ')', '.']\n",
      "3. Punctuation: ['lexicalized', 'contextfree', 'grammar', '', 'lcfg', '', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'contextfree', 'grammar', '', 'cfg', '', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '', 'ltag', '', '']\n",
      "4. Stemming: ['lexic', 'contextfre', 'grammar', '', 'lcfg', '', 'is', 'an', 'attract', 'compromis', 'between', 'the', 'pars', 'effici', 'of', 'contextfre', 'grammar', '', 'cfg', '', 'and', 'the', 'eleg', 'and', 'lexic', 'sensit', 'of', 'lexic', 'tree', 'adjoin', 'grammar', '', 'ltag', '', '']\n",
      "\n",
      "\n",
      "Formulation of Low-Order Dominant Poles for Y-Matrix of Interconnects: This paper presents an efficient approach to compute the dominant poles for the reduced-order admittance (Y parameter) matrix of lossy interconnects.\n",
      "\n",
      "1. Tokenization: ['Formulation', 'of', 'Low-Order', 'Dominant', 'Poles', 'for', 'Y-Matrix', 'of', 'Interconnects', ':', 'This', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'Y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "2. Lowercasing: ['formulation', 'of', 'low-order', 'dominant', 'poles', 'for', 'y-matrix', 'of', 'interconnects', ':', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reduced-order', 'admittance', '(', 'y', 'parameter', ')', 'matrix', 'of', 'lossy', 'interconnects', '.']\n",
      "3. Punctuation: ['formulation', 'of', 'loworder', 'dominant', 'poles', 'for', 'ymatrix', 'of', 'interconnects', '', 'this', 'paper', 'presents', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'poles', 'for', 'the', 'reducedorder', 'admittance', '', 'y', 'parameter', '', 'matrix', 'of', 'lossy', 'interconnects', '']\n",
      "4. Lemmatization: ['formulation', 'of', 'loworder', 'dominant', 'pole', 'for', 'ymatrix', 'of', 'interconnects', '', 'this', 'paper', 'present', 'an', 'efficient', 'approach', 'to', 'compute', 'the', 'dominant', 'pole', 'for', 'the', 'reducedorder', 'admittance', '', 'y', 'parameter', '', 'matrix', 'of', 'lossy', 'interconnects', '']\n",
      "\n",
      "\n",
      "Our algorithm succeeds with high probability against an adaptive adversary, which can take over processors at any time during the protocol, up to the point of taking over arbitrarily close to a 1/3 fraction.\n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', ',', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', ',', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '1/3', 'fraction', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'succeeds', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', '', 'which', 'can', 'take', 'over', 'processors', 'at', 'any', 'time', 'during', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'taking', 'over', 'arbitrarily', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "4. Lemmatization: ['our', 'algorithm', 'succeed', 'with', 'high', 'probability', 'against', 'an', 'adaptive', 'adversary', '', 'which', 'can', 'take', 'over', 'processor', 'at', 'any', 'time', 'during', 'the', 'protocol', '', 'up', 'to', 'the', 'point', 'of', 'take', 'over', 'arbitrarily', 'close', 'to', 'a', '13', 'fraction', '']\n",
      "\n",
      "\n",
      "We present an all-pairs shortest path algorithm whose running time on a complete directed graph on n vertices whose edge weights are chosen independently and uniformly at random from [0,1] is O(n2), in expectation and with high probability.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'O', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'all-pairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '[', '0,1', ']', 'is', 'o', '(', 'n2', ')', ',', 'in', 'expectation', 'and', 'with', 'high', 'probability', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'allpairs', 'shortest', 'path', 'algorithm', 'whose', 'running', 'time', 'on', 'a', 'complete', 'directed', 'graph', 'on', 'n', 'vertices', 'whose', 'edge', 'weights', 'are', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '', '01', '', 'is', 'o', '', 'n2', '', '', 'in', 'expectation', 'and', 'with', 'high', 'probability', '']\n",
      "4. Lemmatization: ['we', 'present', 'an', 'allpairs', 'shortest', 'path', 'algorithm', 'whose', 'run', 'time', 'on', 'a', 'complete', 'direct', 'graph', 'on', 'n', 'vertex', 'whose', 'edge', 'weight', 'be', 'chosen', 'independently', 'and', 'uniformly', 'at', 'random', 'from', '', '01', '', 'be', 'o', '', 'n2', '', '', 'in', 'expectation', 'and', 'with', 'high', 'probability', '']\n",
      "\n",
      "\n",
      "We consider the problem of re-ranking the top-k documents returned by a retrieval system given some search query.\n",
      "\n",
      "1. Tokenization: ['We', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "2. Lowercasing: ['we', 'consider', 'the', 'problem', 'of', 're-ranking', 'the', 'top-k', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '.']\n",
      "3. Punctuation: ['we', 'consider', 'the', 'problem', 'of', 'reranking', 'the', 'topk', 'documents', 'returned', 'by', 'a', 'retrieval', 'system', 'given', 'some', 'search', 'query', '']\n",
      "4. Lemmatization: ['we', 'consider', 'the', 'problem', 'of', 'reranking', 'the', 'topk', 'document', 'return', 'by', 'a', 'retrieval', 'system', 'give', 'some', 'search', 'query', '']\n",
      "\n",
      "\n",
      "In this paper, we combine the learning-to-rank paradigm with the recent developments on axioms for information retrieval.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'combine', 'the', 'learning-to-rank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'combine', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'developments', 'on', 'axioms', 'for', 'information', 'retrieval', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'combine', 'the', 'learningtorank', 'paradigm', 'with', 'the', 'recent', 'development', 'on', 'axiom', 'for', 'information', 'retrieval', '']\n",
      "\n",
      "\n",
      "We outline important details on cross-validation techniques that can enhance the performance.\n",
      "\n",
      "1. Tokenization: ['We', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "2. Lowercasing: ['we', 'outline', 'important', 'details', 'on', 'cross-validation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '.']\n",
      "3. Punctuation: ['we', 'outline', 'important', 'details', 'on', 'crossvalidation', 'techniques', 'that', 'can', 'enhance', 'the', 'performance', '']\n",
      "4. Lemmatization: ['we', 'outline', 'important', 'detail', 'on', 'crossvalidation', 'technique', 'that', 'can', 'enhance', 'the', 'performance', '']\n",
      "\n",
      "\n",
      "“Next-fit” allocation differs from first-fit in that a first-fit allocator commences its search for free space at a fixed end of memory, whereas a next-fit allocator commences its search wherever it previously stopped searching.\n",
      "\n",
      "1. Tokenization: ['“', 'Next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "2. Lowercasing: ['“', 'next-fit', '”', 'allocation', 'differs', 'from', 'first-fit', 'in', 'that', 'a', 'first-fit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', ',', 'whereas', 'a', 'next-fit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '.']\n",
      "3. Punctuation: ['“', 'nextfit', '”', 'allocation', 'differs', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'allocator', 'commences', 'its', 'search', 'for', 'free', 'space', 'at', 'a', 'fixed', 'end', 'of', 'memory', '', 'whereas', 'a', 'nextfit', 'allocator', 'commences', 'its', 'search', 'wherever', 'it', 'previously', 'stopped', 'searching', '']\n",
      "4. Lemmatization: ['“', 'nextfit', '”', 'allocation', 'differs', 'from', 'firstfit', 'in', 'that', 'a', 'firstfit', 'allocator', 'commences', 'it', 'search', 'for', 'free', 'space', 'at', 'a', 'fix', 'end', 'of', 'memory', '', 'whereas', 'a', 'nextfit', 'allocator', 'commences', 'it', 'search', 'wherever', 'it', 'previously', 'stop', 'search', '']\n",
      "\n",
      "\n",
      "It is important to choose an appropriate network structure because simple networks are likely to under-fit while complex networks are less plastic and more computationally expensive to train.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "2. Lowercasing: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'under-fit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '.']\n",
      "3. Punctuation: ['it', 'is', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'networks', 'are', 'likely', 'to', 'underfit', 'while', 'complex', 'networks', 'are', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '']\n",
      "4. Lemmatization: ['it', 'be', 'important', 'to', 'choose', 'an', 'appropriate', 'network', 'structure', 'because', 'simple', 'network', 'be', 'likely', 'to', 'underfit', 'while', 'complex', 'network', 'be', 'less', 'plastic', 'and', 'more', 'computationally', 'expensive', 'to', 'train', '']\n",
      "\n",
      "\n",
      "Both of the LL-based algorithms in these papers attempt to minimize the reparsing on the original parse tree and the parse table.\n",
      "\n",
      "1. Tokenization: ['Both', 'of', 'the', 'LL-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "2. Lowercasing: ['both', 'of', 'the', 'll-based', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '.']\n",
      "3. Punctuation: ['both', 'of', 'the', 'llbased', 'algorithms', 'in', 'these', 'papers', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '']\n",
      "4. Lemmatization: ['both', 'of', 'the', 'llbased', 'algorithm', 'in', 'these', 'paper', 'attempt', 'to', 'minimize', 'the', 'reparsing', 'on', 'the', 'original', 'parse', 'tree', 'and', 'the', 'parse', 'table', '']\n",
      "\n",
      "\n",
      "In this paper, L2-norm Deep Belief Network (L2DBN) is proposed, which uses L2-norm regularization to optimize the network parameters of DBN.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'L2-norm', 'Deep', 'Belief', 'Network', '(', 'L2DBN', ')', 'is', 'proposed', ',', 'which', 'uses', 'L2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'DBN', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'l2-norm', 'deep', 'belief', 'network', '(', 'l2dbn', ')', 'is', 'proposed', ',', 'which', 'uses', 'l2-norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'is', 'proposed', '', 'which', 'uses', 'l2norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameters', 'of', 'dbn', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'l2norm', 'deep', 'belief', 'network', '', 'l2dbn', '', 'be', 'propose', '', 'which', 'us', 'l2norm', 'regularization', 'to', 'optimize', 'the', 'network', 'parameter', 'of', 'dbn', '']\n",
      "\n",
      "\n",
      "The dramatic development of IT technology has increased absolute amount of data to store, analyze, and process for computers and it has also rapidly increased the amount of realtime processing for data stream\n",
      "\n",
      "1. Tokenization: ['The', 'dramatic', 'development', 'of', 'IT', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "2. Lowercasing: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', ',', 'analyze', ',', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "3. Punctuation: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'has', 'increased', 'absolute', 'amount', 'of', 'data', 'to', 'store', '', 'analyze', '', 'and', 'process', 'for', 'computers', 'and', 'it', 'has', 'also', 'rapidly', 'increased', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "4. Lemmatization: ['the', 'dramatic', 'development', 'of', 'it', 'technology', 'have', 'increase', 'absolute', 'amount', 'of', 'data', 'to', 'store', '', 'analyze', '', 'and', 'process', 'for', 'computer', 'and', 'it', 'have', 'also', 'rapidly', 'increase', 'the', 'amount', 'of', 'realtime', 'processing', 'for', 'data', 'stream']\n",
      "\n",
      "\n",
      "We present an algorithm achieving gathering in O(n2) rounds in expectation.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'O', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "2. Lowercasing: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '(', 'n2', ')', 'rounds', 'in', 'expectation', '.']\n",
      "3. Punctuation: ['we', 'present', 'an', 'algorithm', 'achieving', 'gathering', 'in', 'o', '', 'n2', '', 'rounds', 'in', 'expectation', '']\n",
      "4. Lemmatization: ['we', 'present', 'an', 'algorithm', 'achieve', 'gathering', 'in', 'o', '', 'n2', '', 'round', 'in', 'expectation', '']\n",
      "\n",
      "\n",
      "Let A and B two n×n matrices over a ring R (e.g., the reals or the integers) each containing at most m nonzero elements.\n",
      "\n",
      "1. Tokenization: ['Let', 'A', 'and', 'B', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'R', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "2. Lowercasing: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '(', 'e.g.', ',', 'the', 'reals', 'or', 'the', 'integers', ')', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '.']\n",
      "3. Punctuation: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrices', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'reals', 'or', 'the', 'integers', '', 'each', 'containing', 'at', 'most', 'm', 'nonzero', 'elements', '']\n",
      "4. Lemmatization: ['let', 'a', 'and', 'b', 'two', 'n×n', 'matrix', 'over', 'a', 'ring', 'r', '', 'eg', '', 'the', 'real', 'or', 'the', 'integer', '', 'each', 'contain', 'at', 'most', 'm', 'nonzero', 'element', '']\n",
      "\n",
      "\n",
      "Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited.\n",
      "\n",
      "1. Tokenization: ['Firmware', 'is', 'the', 'enable', 'software', 'of', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'IoT', 'devices', 'being', 'exploited', '.']\n",
      "2. Lowercasing: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '(', 'iot', ')', 'devices', ',', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '.']\n",
      "3. Punctuation: ['firmware', 'is', 'the', 'enable', 'software', 'of', 'internet', 'of', 'things', '', 'iot', '', 'devices', '', 'and', 'its', 'software', 'vulnerabilities', 'are', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'devices', 'being', 'exploited', '']\n",
      "4. Lemmatization: ['firmware', 'be', 'the', 'enable', 'software', 'of', 'internet', 'of', 'thing', '', 'iot', '', 'device', '', 'and', 'it', 'software', 'vulnerability', 'be', 'one', 'of', 'the', 'primary', 'reason', 'of', 'iot', 'device', 'be', 'exploit', '']\n",
      "\n",
      "\n",
      "We present a new algorithm that multiplies A and B using O(m0.7n1.2+n2+o(1)) algebraic operations (i.e., multiplications, additions and subtractions) over R.\n",
      "\n",
      "1. Tokenization: ['We', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'A', 'and', 'B', 'using', 'O', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'R', '.']\n",
      "2. Lowercasing: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '(', 'm0.7n1.2+n2+o', '(', '1', ')', ')', 'algebraic', 'operations', '(', 'i.e.', ',', 'multiplications', ',', 'additions', 'and', 'subtractions', ')', 'over', 'r', '.']\n",
      "3. Punctuation: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'using', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebraic', 'operations', '', 'ie', '', 'multiplications', '', 'additions', 'and', 'subtractions', '', 'over', 'r', '']\n",
      "4. Lemmatization: ['we', 'present', 'a', 'new', 'algorithm', 'that', 'multiplies', 'a', 'and', 'b', 'use', 'o', '', 'm07n12n2o', '', '1', '', '', 'algebraic', 'operation', '', 'ie', '', 'multiplication', '', 'addition', 'and', 'subtraction', '', 'over', 'r', '']\n",
      "\n",
      "\n",
      "Efforts in “explainable AI” are under way, hopefully eliminating the “black-box” concept in future clinical decision tools.\n",
      "\n",
      "1. Tokenization: ['Efforts', 'in', '“', 'explainable', 'AI', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "2. Lowercasing: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', ',', 'hopefully', 'eliminating', 'the', '“', 'black-box', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '.']\n",
      "3. Punctuation: ['efforts', 'in', '“', 'explainable', 'ai', '”', 'are', 'under', 'way', '', 'hopefully', 'eliminating', 'the', '“', 'blackbox', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tools', '']\n",
      "4. Lemmatization: ['effort', 'in', '“', 'explainable', 'ai', '”', 'be', 'under', 'way', '', 'hopefully', 'eliminate', 'the', '“', 'blackbox', '”', 'concept', 'in', 'future', 'clinical', 'decision', 'tool', '']\n",
      "\n",
      "\n",
      "Target distance (D) and target width (W), traditionally treated as independent variables in Fitts' target acquisition paradigm, are shown to suffer inextricable confounds with task difficulty.\n",
      "\n",
      "1. Tokenization: ['Target', 'distance', '(', 'D', ')', 'and', 'target', 'width', '(', 'W', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'Fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "2. Lowercasing: ['target', 'distance', '(', 'd', ')', 'and', 'target', 'width', '(', 'w', ')', ',', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', \"'\", 'target', 'acquisition', 'paradigm', ',', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '.']\n",
      "3. Punctuation: ['target', 'distance', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'traditionally', 'treated', 'as', 'independent', 'variables', 'in', 'fitts', '', 'target', 'acquisition', 'paradigm', '', 'are', 'shown', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '']\n",
      "4. Lemmatization: ['target', 'distance', '', 'd', '', 'and', 'target', 'width', '', 'w', '', '', 'traditionally', 'treat', 'a', 'independent', 'variable', 'in', 'fitts', '', 'target', 'acquisition', 'paradigm', '', 'be', 'show', 'to', 'suffer', 'inextricable', 'confounds', 'with', 'task', 'difficulty', '']\n",
      "\n",
      "\n",
      "This paper describes an NSF-funded initiative involving 600 underrepresented high school students and 60 teachers designed to introduce underrepresented students to the numerous and varied career opportunities in the computing sciences.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'an', 'NSF-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'an', 'nsf-funded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'an', 'nsffunded', 'initiative', 'involving', '600', 'underrepresented', 'high', 'school', 'students', 'and', '60', 'teachers', 'designed', 'to', 'introduce', 'underrepresented', 'students', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunities', 'in', 'the', 'computing', 'sciences', '']\n",
      "4. Lemmatization: ['this', 'paper', 'describes', 'an', 'nsffunded', 'initiative', 'involve', '600', 'underrepresented', 'high', 'school', 'student', 'and', '60', 'teacher', 'design', 'to', 'introduce', 'underrepresented', 'student', 'to', 'the', 'numerous', 'and', 'varied', 'career', 'opportunity', 'in', 'the', 'compute', 'science', '']\n",
      "\n",
      "\n",
      "The gathering problem, where n autonomous robots with restricted capabilities are required to meet in a single point of the plane, is widely studied.\n",
      "\n",
      "1. Tokenization: ['The', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "2. Lowercasing: ['the', 'gathering', 'problem', ',', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', ',', 'is', 'widely', 'studied', '.']\n",
      "3. Punctuation: ['the', 'gathering', 'problem', '', 'where', 'n', 'autonomous', 'robots', 'with', 'restricted', 'capabilities', 'are', 'required', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', '', 'is', 'widely', 'studied', '']\n",
      "4. Lemmatization: ['the', 'gathering', 'problem', '', 'where', 'n', 'autonomous', 'robot', 'with', 'restrict', 'capability', 'be', 'require', 'to', 'meet', 'in', 'a', 'single', 'point', 'of', 'the', 'plane', '', 'be', 'widely', 'study', '']\n",
      "\n",
      "\n",
      "The information captured by IoT present an unprecedented opportunity to solve large-scale problems in those application domains to deliver services\n",
      "\n",
      "1. Tokenization: ['The', 'information', 'captured', 'by', 'IoT', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "2. Lowercasing: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'large-scale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "3. Punctuation: ['the', 'information', 'captured', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'largescale', 'problems', 'in', 'those', 'application', 'domains', 'to', 'deliver', 'services']\n",
      "4. Lemmatization: ['the', 'information', 'capture', 'by', 'iot', 'present', 'an', 'unprecedented', 'opportunity', 'to', 'solve', 'largescale', 'problem', 'in', 'those', 'application', 'domain', 'to', 'deliver', 'service']\n",
      "\n",
      "\n",
      "Compliance with the information system (IS) security policy is an established theme in IS research for protecting the IS from user actions.\n",
      "\n",
      "1. Tokenization: ['Compliance', 'with', 'the', 'information', 'system', '(', 'IS', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'IS', 'research', 'for', 'protecting', 'the', 'IS', 'from', 'user', 'actions', '.']\n",
      "2. Lowercasing: ['compliance', 'with', 'the', 'information', 'system', '(', 'is', ')', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '.']\n",
      "3. Punctuation: ['compliance', 'with', 'the', 'information', 'system', '', 'is', '', 'security', 'policy', 'is', 'an', 'established', 'theme', 'in', 'is', 'research', 'for', 'protecting', 'the', 'is', 'from', 'user', 'actions', '']\n",
      "4. Lemmatization: ['compliance', 'with', 'the', 'information', 'system', '', 'be', '', 'security', 'policy', 'be', 'an', 'establish', 'theme', 'in', 'be', 'research', 'for', 'protect', 'the', 'be', 'from', 'user', 'action', '']\n",
      "\n",
      "\n",
      "In this paper, we develop Re-Vibe, the first system that re-identifies people through footstep-induced floor vibrations.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'develop', 'Re-Vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'develop', 're-vibe', ',', 'the', 'first', 'system', 'that', 're-identifies', 'people', 'through', 'footstep-induced', 'floor', 'vibrations', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'develop', 'revibe', '', 'the', 'first', 'system', 'that', 'reidentifies', 'people', 'through', 'footstepinduced', 'floor', 'vibrations', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'develop', 'revibe', '', 'the', 'first', 'system', 'that', 'reidentifies', 'people', 'through', 'footstepinduced', 'floor', 'vibration', '']\n",
      "\n",
      "\n",
      "I/O is emerging as a major bottleneck for machine learning training, especially in distributed environments.\n",
      "\n",
      "1. Tokenization: ['I/O', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "2. Lowercasing: ['i/o', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', ',', 'especially', 'in', 'distributed', 'environments', '.']\n",
      "3. Punctuation: ['io', 'is', 'emerging', 'as', 'a', 'major', 'bottleneck', 'for', 'machine', 'learning', 'training', '', 'especially', 'in', 'distributed', 'environments', '']\n",
      "4. Lemmatization: ['io', 'be', 'emerge', 'a', 'a', 'major', 'bottleneck', 'for', 'machine', 'learn', 'training', '', 'especially', 'in', 'distribute', 'environment', '']\n",
      "\n",
      "\n",
      "In this paper, we attempt to improve the query likelihood function by bringing back the negative query generation.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bringing', 'back', 'the', 'negative', 'query', 'generation', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'attempt', 'to', 'improve', 'the', 'query', 'likelihood', 'function', 'by', 'bring', 'back', 'the', 'negative', 'query', 'generation', '']\n",
      "\n",
      "\n",
      "In this paper, we propose a new on-chip interconnect scheme called Y-architecture, which can utilize the on-chip routing resources more efficiently than traditional Manhattan interconnect architecture by allowing wires routed in three directions (0°, 60°, and 120°).\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'Y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'Manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'propose', 'a', 'new', 'on-chip', 'interconnect', 'scheme', 'called', 'y-architecture', ',', 'which', 'can', 'utilize', 'the', 'on-chip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '(', '0°', ',', '60°', ',', 'and', '120°', ')', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'propose', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'called', 'yarchitecture', '', 'which', 'can', 'utilize', 'the', 'onchip', 'routing', 'resources', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allowing', 'wires', 'routed', 'in', 'three', 'directions', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'propose', 'a', 'new', 'onchip', 'interconnect', 'scheme', 'call', 'yarchitecture', '', 'which', 'can', 'utilize', 'the', 'onchip', 'rout', 'resource', 'more', 'efficiently', 'than', 'traditional', 'manhattan', 'interconnect', 'architecture', 'by', 'allow', 'wire', 'rout', 'in', 'three', 'direction', '', '0°', '', '60°', '', 'and', '120°', '', '']\n",
      "\n",
      "\n",
      "We describe an algorithm for Byzantine agreement that is scalable in the sense that each processor sends only O(√n) bits, where n is the total number of processors.\n",
      "\n",
      "1. Tokenization: ['We', 'describe', 'an', 'algorithm', 'for', 'Byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'O', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "2. Lowercasing: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '(', '√n', ')', 'bits', ',', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '.']\n",
      "3. Punctuation: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'is', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '', '√n', '', 'bits', '', 'where', 'n', 'is', 'the', 'total', 'number', 'of', 'processors', '']\n",
      "4. Lemmatization: ['we', 'describe', 'an', 'algorithm', 'for', 'byzantine', 'agreement', 'that', 'be', 'scalable', 'in', 'the', 'sense', 'that', 'each', 'processor', 'sends', 'only', 'o', '', '√n', '', 'bit', '', 'where', 'n', 'be', 'the', 'total', 'number', 'of', 'processor', '']\n",
      "\n",
      "\n",
      "This paper presents an image-based rendering (IBR) system based on RGB-D images.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'IBR', ')', 'system', 'based', 'on', 'RGB-D', 'images', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'image-based', 'rendering', '(', 'ibr', ')', 'system', 'based', 'on', 'rgb-d', 'images', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'imagebased', 'rendering', '', 'ibr', '', 'system', 'based', 'on', 'rgbd', 'images', '']\n",
      "4. Lemmatization: ['this', 'paper', 'present', 'an', 'imagebased', 'render', '', 'ibr', '', 'system', 'base', 'on', 'rgbd', 'image', '']\n",
      "\n",
      "\n",
      "In this paper we present a framework and methodology for aligning the business strategy and IT/IS for an organization offering an e-service in a multi-organizational setting.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'IT/IS', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'it/is', 'for', 'an', 'organization', 'offering', 'an', 'e-service', 'in', 'a', 'multi-organizational', 'setting', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'aligning', 'the', 'business', 'strategy', 'and', 'itis', 'for', 'an', 'organization', 'offering', 'an', 'eservice', 'in', 'a', 'multiorganizational', 'setting', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', 'we', 'present', 'a', 'framework', 'and', 'methodology', 'for', 'align', 'the', 'business', 'strategy', 'and', 'itis', 'for', 'an', 'organization', 'offering', 'an', 'eservice', 'in', 'a', 'multiorganizational', 'set', '']\n",
      "\n",
      "\n",
      "The D Programming Language is a hybrid of C++ and modern scripting languages: it compiles statically to native code, but is also garbage collected.\n",
      "\n",
      "1. Tokenization: ['The', 'D', 'Programming', 'Language', 'is', 'a', 'hybrid', 'of', 'C++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "2. Lowercasing: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c++', 'and', 'modern', 'scripting', 'languages', ':', 'it', 'compiles', 'statically', 'to', 'native', 'code', ',', 'but', 'is', 'also', 'garbage', 'collected', '.']\n",
      "3. Punctuation: ['the', 'd', 'programming', 'language', 'is', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'scripting', 'languages', '', 'it', 'compiles', 'statically', 'to', 'native', 'code', '', 'but', 'is', 'also', 'garbage', 'collected', '']\n",
      "4. Lemmatization: ['the', 'd', 'program', 'language', 'be', 'a', 'hybrid', 'of', 'c', 'and', 'modern', 'script', 'language', '', 'it', 'compiles', 'statically', 'to', 'native', 'code', '', 'but', 'be', 'also', 'garbage', 'collect', '']\n",
      "\n",
      "\n",
      "However, the main aim is precisely to present an algorithm which gives the so-called minimal solutions: Boolean matrices M satisfying the equation with the least possible number of unity entries.\n",
      "\n",
      "1. Tokenization: ['However', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'Boolean', 'matrices', 'M', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "2. Lowercasing: ['however', ',', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'so-called', 'minimal', 'solutions', ':', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '.']\n",
      "3. Punctuation: ['however', '', 'the', 'main', 'aim', 'is', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'gives', 'the', 'socalled', 'minimal', 'solutions', '', 'boolean', 'matrices', 'm', 'satisfying', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entries', '']\n",
      "4. Lemmatization: ['however', '', 'the', 'main', 'aim', 'be', 'precisely', 'to', 'present', 'an', 'algorithm', 'which', 'give', 'the', 'socalled', 'minimal', 'solution', '', 'boolean', 'matrix', 'm', 'satisfy', 'the', 'equation', 'with', 'the', 'least', 'possible', 'number', 'of', 'unity', 'entry', '']\n",
      "\n",
      "\n",
      "A number of algorithms have been proposed for LR incremental parsers, but few have been proposed for LL incremental parsers [1, 2].\n",
      "\n",
      "1. Tokenization: ['A', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'LR', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'LL', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "2. Lowercasing: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', ',', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '[', '1', ',', '2', ']', '.']\n",
      "3. Punctuation: ['a', 'number', 'of', 'algorithms', 'have', 'been', 'proposed', 'for', 'lr', 'incremental', 'parsers', '', 'but', 'few', 'have', 'been', 'proposed', 'for', 'll', 'incremental', 'parsers', '', '1', '', '2', '', '']\n",
      "4. Lemmatization: ['a', 'number', 'of', 'algorithm', 'have', 'be', 'propose', 'for', 'lr', 'incremental', 'parser', '', 'but', 'few', 'have', 'be', 'propose', 'for', 'll', 'incremental', 'parser', '', '1', '', '2', '', '']\n",
      "\n",
      "\n",
      "I will discuss the use of graphical models for data mining.\n",
      "\n",
      "1. Tokenization: ['I', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "2. Lowercasing: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '.']\n",
      "3. Punctuation: ['i', 'will', 'discuss', 'the', 'use', 'of', 'graphical', 'models', 'for', 'data', 'mining', '']\n",
      "4. Lemmatization: ['i', 'will', 'discus', 'the', 'use', 'of', 'graphical', 'model', 'for', 'data', 'mining', '']\n",
      "\n",
      "\n",
      "Over the past decade, a pair of synchronization instructions known as LL/SC has emerged as the most suitable set of instructions to be used in the design of lock-free algorithms.\n",
      "\n",
      "1. Tokenization: ['Over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'LL/SC', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "2. Lowercasing: ['over', 'the', 'past', 'decade', ',', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'll/sc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lock-free', 'algorithms', '.']\n",
      "3. Punctuation: ['over', 'the', 'past', 'decade', '', 'a', 'pair', 'of', 'synchronization', 'instructions', 'known', 'as', 'llsc', 'has', 'emerged', 'as', 'the', 'most', 'suitable', 'set', 'of', 'instructions', 'to', 'be', 'used', 'in', 'the', 'design', 'of', 'lockfree', 'algorithms', '']\n",
      "4. Lemmatization: ['over', 'the', 'past', 'decade', '', 'a', 'pair', 'of', 'synchronization', 'instruction', 'know', 'a', 'llsc', 'have', 'emerge', 'a', 'the', 'most', 'suitable', 'set', 'of', 'instruction', 'to', 'be', 'use', 'in', 'the', 'design', 'of', 'lockfree', 'algorithm', '']\n",
      "\n",
      "\n",
      "Our algorithm has latency that is polylogarithmic in n. \n",
      "\n",
      "1. Tokenization: ['Our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "2. Lowercasing: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '.']\n",
      "3. Punctuation: ['our', 'algorithm', 'has', 'latency', 'that', 'is', 'polylogarithmic', 'in', 'n', '']\n",
      "4. Lemmatization: ['our', 'algorithm', 'have', 'latency', 'that', 'be', 'polylogarithmic', 'in', 'n', '']\n",
      "\n",
      "\n",
      "Typical person re-identification (re-ID) systems rely on cameras to match the same person across different locations.\n",
      "\n",
      "1. Tokenization: ['Typical', 'person', 're-identification', '(', 're-ID', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "2. Lowercasing: ['typical', 'person', 're-identification', '(', 're-id', ')', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '.']\n",
      "3. Punctuation: ['typical', 'person', 'reidentification', '', 'reid', '', 'systems', 'rely', 'on', 'cameras', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'locations', '']\n",
      "4. Lemmatization: ['typical', 'person', 'reidentification', '', 'reid', '', 'system', 'rely', 'on', 'camera', 'to', 'match', 'the', 'same', 'person', 'across', 'different', 'location', '']\n",
      "\n",
      "\n",
      "The Internet of Things (IoT) is the latest Internet evolution that incorporates a diverse range of things such as sensors, actuators, and services deployed by different organizations and individuals to support a variety of applications.\n",
      "\n",
      "1. Tokenization: ['The', 'Internet', 'of', 'Things', '(', 'IoT', ')', 'is', 'the', 'latest', 'Internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "2. Lowercasing: ['the', 'internet', 'of', 'things', '(', 'iot', ')', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', ',', 'actuators', ',', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '.']\n",
      "3. Punctuation: ['the', 'internet', 'of', 'things', '', 'iot', '', 'is', 'the', 'latest', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'things', 'such', 'as', 'sensors', '', 'actuators', '', 'and', 'services', 'deployed', 'by', 'different', 'organizations', 'and', 'individuals', 'to', 'support', 'a', 'variety', 'of', 'applications', '']\n",
      "4. Lemmatization: ['the', 'internet', 'of', 'thing', '', 'iot', '', 'be', 'the', 'late', 'internet', 'evolution', 'that', 'incorporates', 'a', 'diverse', 'range', 'of', 'thing', 'such', 'a', 'sensor', '', 'actuator', '', 'and', 'service', 'deployed', 'by', 'different', 'organization', 'and', 'individual', 'to', 'support', 'a', 'variety', 'of', 'application', '']\n",
      "\n",
      "\n",
      "This subsection compares the state of the art methods and the proposed WRA-Net for the BoniRob dataset through the same method.\n",
      "\n",
      "1. Tokenization: ['This', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'WRA-Net', 'for', 'the', 'BoniRob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "2. Lowercasing: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wra-net', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '.']\n",
      "3. Punctuation: ['this', 'subsection', 'compares', 'the', 'state', 'of', 'the', 'art', 'methods', 'and', 'the', 'proposed', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "4. Lemmatization: ['this', 'subsection', 'compare', 'the', 'state', 'of', 'the', 'art', 'method', 'and', 'the', 'propose', 'wranet', 'for', 'the', 'bonirob', 'dataset', 'through', 'the', 'same', 'method', '']\n",
      "\n",
      "\n",
      "Hence, WRA-Net achieved higher values for PSNRand SSIM compared to the state-of-the-art methods.\n",
      "\n",
      "1. Tokenization: ['Hence', ',', 'WRA-Net', 'achieved', 'higher', 'values', 'for', 'PSNRand', 'SSIM', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "2. Lowercasing: ['hence', ',', 'wra-net', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'state-of-the-art', 'methods', '.']\n",
      "3. Punctuation: ['hence', '', 'wranet', 'achieved', 'higher', 'values', 'for', 'psnrand', 'ssim', 'compared', 'to', 'the', 'stateoftheart', 'methods', '']\n",
      "4. Lemmatization: ['hence', '', 'wranet', 'achieve', 'high', 'value', 'for', 'psnrand', 'ssim', 'compare', 'to', 'the', 'stateoftheart', 'method', '']\n",
      "\n",
      "\n",
      "It is shown that relative movement amplitude D/W(which determines difficulty) and absolute movement amplitude D (or scale) are the only two variables that can be manipulated independently in a Fitts' task experiment.\n",
      "\n",
      "1. Tokenization: ['It', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'D/W', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'D', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'Fitts', \"'\", 'task', 'experiment', '.']\n",
      "2. Lowercasing: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'd/w', '(', 'which', 'determines', 'difficulty', ')', 'and', 'absolute', 'movement', 'amplitude', 'd', '(', 'or', 'scale', ')', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', \"'\", 'task', 'experiment', '.']\n",
      "3. Punctuation: ['it', 'is', 'shown', 'that', 'relative', 'movement', 'amplitude', 'dw', '', 'which', 'determines', 'difficulty', '', 'and', 'absolute', 'movement', 'amplitude', 'd', '', 'or', 'scale', '', 'are', 'the', 'only', 'two', 'variables', 'that', 'can', 'be', 'manipulated', 'independently', 'in', 'a', 'fitts', '', 'task', 'experiment', '']\n",
      "4. Lemmatization: ['it', 'be', 'show', 'that', 'relative', 'movement', 'amplitude', 'dw', '', 'which', 'determines', 'difficulty', '', 'and', 'absolute', 'movement', 'amplitude', 'd', '', 'or', 'scale', '', 'be', 'the', 'only', 'two', 'variable', 'that', 'can', 'be', 'manipulate', 'independently', 'in', 'a', 'fitts', '', 'task', 'experiment', '']\n",
      "\n",
      "\n",
      "The area-under-the-curve (AUC) was the chosen performance metric for comparison and cross-validation was performed.\n",
      "\n",
      "1. Tokenization: ['The', 'area-under-the-curve', '(', 'AUC', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "2. Lowercasing: ['the', 'area-under-the-curve', '(', 'auc', ')', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'cross-validation', 'was', 'performed', '.']\n",
      "3. Punctuation: ['the', 'areaunderthecurve', '', 'auc', '', 'was', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'crossvalidation', 'was', 'performed', '']\n",
      "4. Lemmatization: ['the', 'areaunderthecurve', '', 'auc', '', 'be', 'the', 'chosen', 'performance', 'metric', 'for', 'comparison', 'and', 'crossvalidation', 'be', 'perform', '']\n",
      "\n",
      "\n",
      "In this paper we present a tool to assist in teaching top-down and bottom-up analysis algorithms. The tool provides simulation for the following analysis algorithms: LL, SLR, LALR and LR.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'The', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'LL', ',', 'SLR', ',', 'LALR', 'and', 'LR', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'top-down', 'and', 'bottom-up', 'analysis', 'algorithms', '.', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', ':', 'll', ',', 'slr', ',', 'lalr', 'and', 'lr', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teaching', 'topdown', 'and', 'bottomup', 'analysis', 'algorithms', '', 'the', 'tool', 'provides', 'simulation', 'for', 'the', 'following', 'analysis', 'algorithms', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', 'we', 'present', 'a', 'tool', 'to', 'assist', 'in', 'teach', 'topdown', 'and', 'bottomup', 'analysis', 'algorithm', '', 'the', 'tool', 'provide', 'simulation', 'for', 'the', 'follow', 'analysis', 'algorithm', '', 'll', '', 'slr', '', 'lalr', 'and', 'lr', '']\n",
      "\n",
      "\n",
      "In this paper, we formally show that standard gradient methods never overfit on separable data: If we run these methods for T iterations on a dataset of size m, both the empirical risk and the generalization error decrease at an essentially optimal rate of Õ(1/γ2T) up till T ∼ m.\n",
      "\n",
      "1. Tokenization: ['In', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'If', 'we', 'run', 'these', 'methods', 'for', 'T', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'Õ', '(', '1/γ2T', ')', 'up', 'till', 'T', '∼', 'm', '.']\n",
      "2. Lowercasing: ['in', 'this', 'paper', ',', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', ':', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', ',', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '(', '1/γ2t', ')', 'up', 'till', 't', '∼', 'm', '.']\n",
      "3. Punctuation: ['in', 'this', 'paper', '', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'methods', 'never', 'overfit', 'on', 'separable', 'data', '', 'if', 'we', 'run', 'these', 'methods', 'for', 't', 'iterations', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "4. Lemmatization: ['in', 'this', 'paper', '', 'we', 'formally', 'show', 'that', 'standard', 'gradient', 'method', 'never', 'overfit', 'on', 'separable', 'data', '', 'if', 'we', 'run', 'these', 'method', 'for', 't', 'iteration', 'on', 'a', 'dataset', 'of', 'size', 'm', '', 'both', 'the', 'empirical', 'risk', 'and', 'the', 'generalization', 'error', 'decrease', 'at', 'an', 'essentially', 'optimal', 'rate', 'of', 'õ', '', '1γ2t', '', 'up', 'till', 't', '∼', 'm', '']\n",
      "\n",
      "\n",
      "This paper is concerned with goodness of fit evaluation for virtual commissioning modelling purposes.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '.']\n",
      "3. Punctuation: ['this', 'paper', 'is', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commissioning', 'modelling', 'purposes', '']\n",
      "4. Lemmatization: ['this', 'paper', 'be', 'concerned', 'with', 'goodness', 'of', 'fit', 'evaluation', 'for', 'virtual', 'commission', 'model', 'purpose', '']\n",
      "\n",
      "\n",
      "The results showed that the optimal performance was achieved under natural complexification of the EANN and that back-propagation tended to over fit the data.\n",
      "\n",
      "1. Tokenization: ['The', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'EANN', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "2. Lowercasing: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'back-propagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '.']\n",
      "3. Punctuation: ['the', 'results', 'showed', 'that', 'the', 'optimal', 'performance', 'was', 'achieved', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'backpropagation', 'tended', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "4. Lemmatization: ['the', 'result', 'show', 'that', 'the', 'optimal', 'performance', 'be', 'achieve', 'under', 'natural', 'complexification', 'of', 'the', 'eann', 'and', 'that', 'backpropagation', 'tend', 'to', 'over', 'fit', 'the', 'data', '']\n",
      "\n",
      "\n",
      "The second approach does not train models that generalize across tasks, but rather over-fit a single instance of a problem\n",
      "\n",
      "1. Tokenization: ['The', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "2. Lowercasing: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', ',', 'but', 'rather', 'over-fit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "3. Punctuation: ['the', 'second', 'approach', 'does', 'not', 'train', 'models', 'that', 'generalize', 'across', 'tasks', '', 'but', 'rather', 'overfit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "4. Lemmatization: ['the', 'second', 'approach', 'do', 'not', 'train', 'model', 'that', 'generalize', 'across', 'task', '', 'but', 'rather', 'overfit', 'a', 'single', 'instance', 'of', 'a', 'problem']\n",
      "\n",
      "\n",
      "This paper describes two significant contributions to the NILM community in an effort towards reproducible state-of-the-art research.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'NILM', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'state-of-the-art', 'research', '.']\n",
      "3. Punctuation: ['this', 'paper', 'describes', 'two', 'significant', 'contributions', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'stateoftheart', 'research', '']\n",
      "4. Lemmatization: ['this', 'paper', 'describes', 'two', 'significant', 'contribution', 'to', 'the', 'nilm', 'community', 'in', 'an', 'effort', 'towards', 'reproducible', 'stateoftheart', 'research', '']\n",
      "\n",
      "\n",
      "Accurate numerical results for a definite integral are easily obtained by simple substitutions of upper and lower bounds of integral into obtained approximate symbolic results\n",
      "\n",
      "1. Tokenization: ['Accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "2. Lowercasing: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "3. Punctuation: ['accurate', 'numerical', 'results', 'for', 'a', 'definite', 'integral', 'are', 'easily', 'obtained', 'by', 'simple', 'substitutions', 'of', 'upper', 'and', 'lower', 'bounds', 'of', 'integral', 'into', 'obtained', 'approximate', 'symbolic', 'results']\n",
      "4. Lemmatization: ['accurate', 'numerical', 'result', 'for', 'a', 'definite', 'integral', 'be', 'easily', 'obtain', 'by', 'simple', 'substitution', 'of', 'upper', 'and', 'low', 'bound', 'of', 'integral', 'into', 'obtain', 'approximate', 'symbolic', 'result']\n",
      "\n",
      "\n",
      "This paper presents an optimal algorithm for jumper insertion under the ratio upper-bound.\n",
      "\n",
      "1. Tokenization: ['This', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "2. Lowercasing: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upper-bound', '.']\n",
      "3. Punctuation: ['this', 'paper', 'presents', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "4. Lemmatization: ['this', 'paper', 'present', 'an', 'optimal', 'algorithm', 'for', 'jumper', 'insertion', 'under', 'the', 'ratio', 'upperbound', '']\n",
      "\n",
      "\n",
      "In particular, we argue that inter-tagger agreement is not a real upperbound for the Basque WSD task.\n",
      "\n",
      "1. Tokenization: ['In', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'Basque', 'WSD', 'task', '.']\n",
      "2. Lowercasing: ['in', 'particular', ',', 'we', 'argue', 'that', 'inter-tagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '.']\n",
      "3. Punctuation: ['in', 'particular', '', 'we', 'argue', 'that', 'intertagger', 'agreement', 'is', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '']\n",
      "4. Lemmatization: ['in', 'particular', '', 'we', 'argue', 'that', 'intertagger', 'agreement', 'be', 'not', 'a', 'real', 'upperbound', 'for', 'the', 'basque', 'wsd', 'task', '']\n",
      "\n",
      "\n",
      "Lexicalized context-free grammar(LCFG) is an attractive compromise between the parsing efficiency of context-free grammar (CFG) and the elegance and lexical sensitivity of lexicalized tree adjoining grammar (LTAG).\n",
      "\n",
      "1. Tokenization: ['Lexicalized', 'context-free', 'grammar', '(', 'LCFG', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'CFG', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'LTAG', ')', '.']\n",
      "2. Lowercasing: ['lexicalized', 'context-free', 'grammar', '(', 'lcfg', ')', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'context-free', 'grammar', '(', 'cfg', ')', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '(', 'ltag', ')', '.']\n",
      "3. Punctuation: ['lexicalized', 'contextfree', 'grammar', '', 'lcfg', '', 'is', 'an', 'attractive', 'compromise', 'between', 'the', 'parsing', 'efficiency', 'of', 'contextfree', 'grammar', '', 'cfg', '', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalized', 'tree', 'adjoining', 'grammar', '', 'ltag', '', '']\n",
      "4. Lemmatization: ['lexicalize', 'contextfree', 'grammar', '', 'lcfg', '', 'be', 'an', 'attractive', 'compromise', 'between', 'the', 'parse', 'efficiency', 'of', 'contextfree', 'grammar', '', 'cfg', '', 'and', 'the', 'elegance', 'and', 'lexical', 'sensitivity', 'of', 'lexicalize', 'tree', 'adjoin', 'grammar', '', 'ltag', '', '']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "import time\n",
    "\n",
    "with open(\"acmdocuments.txt\", 'r', encoding='utf-8') as file:\n",
    "    documents = file.readlines()\n",
    "\n",
    "# This function tries to determine the word type of English words\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Function to preprocess a single document\n",
    "def preprocess_text(text, baseform_method, stopwords_check=True, printing=True, print_result_only=False):\n",
    "    # Tokenization\n",
    "    if printing:\n",
    "        print(\"\\n\")\n",
    "        print(text) \n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    if printing and not print_result_only:\n",
    "        print(f\"1. Tokenization: {tokens}\")\n",
    "\n",
    "    # Lowercasing\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    if printing and not print_result_only:\n",
    "        print(f\"2. Lowercasing: {tokens}\")\n",
    "\n",
    "    # Removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [token.translate(table) for token in tokens]\n",
    "    if printing and not print_result_only:\n",
    "        print(f\"3. Punctuation: {tokens}\")\n",
    "\n",
    "    # Porter Stemming\n",
    "    if baseform_method == \"PorterStemmer\":\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        if printing and not print_result_only:\n",
    "            print(f\"4. Stemming: {tokens}\")\n",
    "\n",
    "    # Snowball stemmer    \n",
    "    if baseform_method == \"SnowballStemmer\":\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        if printing and not print_result_only:\n",
    "            print(f\"4. Stemming: {tokens}\")\n",
    "    \n",
    "    # Lemmatization\n",
    "    if baseform_method == \"Lemmatizer\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "        if printing and not print_result_only:\n",
    "            print(f\"4. Lemmatization: {tokens}\")\n",
    "\n",
    "    # Removing stop words and empty tokens\n",
    "    if stopwords_check:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.add('')\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        if printing and not print_result_only:\n",
    "            print(f\"5. Stopwords: {tokens}\")\n",
    "        \n",
    "        if printing and print_result_only:\n",
    "            print(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Function to preprocess a list of documents\n",
    "def preprocess_documents(documents, baseform_method, stopwords_check):\n",
    "    preprocessed_documents = []\n",
    "    for doc in documents:\n",
    "        preprocessed_doc = preprocess_text(doc, baseform_method, stopwords_check=stopwords_check, printing=True, print_result_only=False)\n",
    "        preprocessed_documents.append(preprocessed_doc)\n",
    "    return preprocessed_documents\n",
    "\n",
    "# Example usage with a list of documents\n",
    "# documents = ['Your first document text.', 'Your second document text.', ...]\n",
    "\n",
    "# writing to a result text file\n",
    "with open(\"preprocessed_PortStemmer_stopwords_removed.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "    preprocessed_documents = preprocess_documents(documents, \"PorterStemmer\", stopwords_check=True)\n",
    "    for index, doc in enumerate(preprocessed_documents):\n",
    "        file.write(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "        file.write(\"\\n\" + documents[index] + \"\\n\")\n",
    "        file.write(\"| \" + \" | \".join(doc) + \" |\" + \"\\n\")\n",
    "\n",
    "with open(\"preprocessed_SnowballStemmer_stopwords_removed.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "    preprocessed_documents = preprocess_documents(documents, \"SnowballStemmer\", stopwords_check=True)\n",
    "    for index, doc in enumerate(preprocessed_documents):\n",
    "        file.write(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "        file.write(\"\\n\" + documents[index] + \"\\n\")\n",
    "        file.write(\"| \" + \" | \".join(doc) + \" |\" + \"\\n\")\n",
    "\n",
    "with open(\"preprocessed_Lemmatizer_stopwords_removed.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "    preprocessed_documents = preprocess_documents(documents, \"Lemmatizer\", stopwords_check=True)\n",
    "    for index, doc in enumerate(preprocessed_documents):\n",
    "        file.write(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "        file.write(\"\\n\" + documents[index] + \"\\n\")\n",
    "        file.write(\"| \" + \" | \".join(doc) + \" |\" + \"\\n\")\n",
    "\n",
    "\n",
    "with open(\"preprocessed_PortStemmer_stopwords_not_removed.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "    preprocessed_documents = preprocess_documents(documents, \"PorterStemmer\", stopwords_check=False)\n",
    "    for index, doc in enumerate(preprocessed_documents):\n",
    "        file.write(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "        file.write(\"\\n\" + documents[index] + \"\\n\")\n",
    "        file.write(\"| \" + \" | \".join(doc) + \" |\" + \"\\n\")\n",
    "\n",
    "with open(\"preprocessed_SnowballStemmer_stopwords_not_removed.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "    preprocessed_documents = preprocess_documents(documents, \"SnowballStemmer\", stopwords_check=False)\n",
    "    for index, doc in enumerate(preprocessed_documents):\n",
    "        file.write(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "        file.write(\"\\n\" + documents[index] + \"\\n\")\n",
    "        file.write(\"| \" + \" | \".join(doc) + \" |\" + \"\\n\")\n",
    "\n",
    "with open(\"preprocessed_Lemmatizer_stopwords_not_removed.txt\", \"w\", encoding=\"UTF-8\") as file:\n",
    "    preprocessed_documents = preprocess_documents(documents, \"Lemmatizer\", stopwords_check=False)\n",
    "    for index, doc in enumerate(preprocessed_documents):\n",
    "        file.write(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "        file.write(\"\\n\" + documents[index] + \"\\n\")\n",
    "        file.write(\"| \" + \" | \".join(doc) + \" |\" + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On inspection, it appears that the five steps have been applied successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if you skipped lowercasing or performed stemming before stopword removal?\n",
    "\n",
    "- If we skip lowercasing before stopword removal, stopwords in their original uppercase form may not be recognized and removed. This could result in tokens such as \"The\" or \"the\" being retained in the text, which affects the effectiveness of stopword removal if \"the\" is contained in the stop words while \"The\" is not. In other words, \"the\" is removed from the results but \"The\" is not, which is not ideal.\n",
    "\n",
    "- Performing stemming before stopword removal may lead to some stopwords not being removed. Stemming can transform stopwords into different variations, and if these variations are not included in the list of stopwords, they might not be removed as expected. This can result in leftover stopwords in the text. For example, \"this\" is a stopword, but the Porter Stemmer may reduce it to \"thi\", which is not a stopword and will not be removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All results below are reported with the Porter Stemmer, unless otherwise specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Check stopword removal and search examples of two types of errors: \n",
    "\n",
    "- (i) Stopwords (or other common and useless words in this context) that remain in the text\n",
    "\n",
    "I think some common useless words left in the text are the words \"also\", \"let\", \"thi\" and \"paper\"\n",
    "\n",
    "The word \"paper\" is useless because it is a common word in the context of scientific papers, and does not contribute to the meaning of the text.\n",
    "\n",
    "The word \"thi\" is useless because it is a misspelling of the word \"this\" due to the stemmer, and does not contribute to the meaning of the text. It is not removed as \"thi\" is not included in the list of stop words\n",
    "\n",
    "- (ii) important words that are removed as stopwords (Hint: look important computer science abbreviations and notations in the NLTK stopword list). Estimate how serious these errors are (assuming we had a larger corpus of similar documents). How could you fix the (most serious) errors?\n",
    "\n",
    "The notation O(n2) is truncated to n2 while the O symbol is removed. This is a serious error because it changes the meaning of the text. The notation O(n2) is a common notation in computer science, and is used to describe the running time of algorithms. Removing the O symbol changes the meaning of the notation to n2, which is not the same as O(n2).\n",
    "\n",
    "Symbol \"A\" for matrix or some notations is removed due to being mistaken for the article \"a\"\n",
    "\n",
    "\"IT\" (information technology) is removed due to being mistaken for the pronoun \"it\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Check the quality of stemming (with Porter stemmer). Can you find errors where either \n",
    "\n",
    "- (i) two words having the same basic forms are reduced to different stems \n",
    "\n",
    "\n",
    "- (ii) two words with different roots are reduced to the same stem? \n",
    "\n",
    "\n",
    "- Test if the Snowball stemmer would do a better job! \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Are there errors where lemmatization could help?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter succeed\n",
      "Snowball succeed\n",
      "Lemmatizer succeed\n",
      "Porter neg\n",
      "Snowball negat\n",
      "Lemmatizer negative\n",
      "Porter gener\n",
      "Snowball general\n",
      "Lemmatizer general\n",
      "Porter discuss\n",
      "Snowball discuss\n",
      "Lemmatizer discus\n",
      "Porter relat\n",
      "Snowball relat\n",
      "Lemmatizer relation\n",
      "Porter commun\n",
      "Snowball communiti\n",
      "Lemmatizer community\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "# succeed, negative, general, discuss, relation, community\n",
    "# Create stemmer instances\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Stemming with both stemmers\n",
    "def test_words(word):\n",
    "    porter_stems = porter_stemmer.stem(word)\n",
    "    snowball_stems = snowball_stemmer.stem(word)\n",
    "    lemma_stems = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "    print(\"Porter\", porter_stems)\n",
    "    print(\"Snowball\", snowball_stems)\n",
    "    print(\"Lemmatizer\", lemma_stems)\n",
    "\n",
    "# test_words(words)\n",
    "test_words(\"succeed\")\n",
    "test_words(\"negative\")\n",
    "test_words(\"general\")\n",
    "test_words(\"discuss\")\n",
    "test_words(\"relation\")\n",
    "test_words(\"community\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Check punctuation removal. Can you find errors where either \n",
    "- (i) punctuation that should be removed has remained \n",
    "\n",
    "They are the double quotation marks “ and ”, which are often single tokens on their own.\n",
    "The percent sign % is also a single token on its own but it is not removed.\n",
    "\n",
    "- (ii) punctuation that is important for the meaning of the term has been removed? No need to check hyphenated words, yet.\n",
    "\n",
    "1/3 fraction is turned into 13 fraction\n",
    "\n",
    "[0,1] is turned into 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Evaluate collocations/compound words (phrases of multiple consecutive words). Can you find examples of important collocations that occur in different forms: \n",
    "\n",
    "- (i) closed (constituent words catenated together)\n",
    "\n",
    "underrepresented, interconnect\n",
    "\n",
    "- (ii) hyphenated (hyphen between words)\n",
    "\n",
    "Y-Matrix, NSF-funded, L2-norm\n",
    "\n",
    "- (iii) open (space between words)\n",
    "\n",
    "Internet of Things, machine learning\n",
    "\n",
    "\n",
    "Suggest a solution how to handle them!\n",
    "\n",
    "Solution: Use a tokenizer that can handle collocations, such as the MWETokenizer. This tokenizer is able to tokenize collocations such as \"Internet of Things\" as a single token, instead of splitting it into \"Inter\", \"of\" and \"Things\". This allows the collocation to be treated as a single token in text analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Internet_of_Things', 'is', 'used', 'in', 'machine_learning']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# Create a tokenizer and add the multi-word expression\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('Internet', 'of', 'Things'))\n",
    "tokenizer.add_mwe(('machine', 'learning'))\n",
    "\n",
    "# Tokenize a text containing the specified multi-word expression\n",
    "tokens = tokenizer.tokenize('Internet of Things is used in machine learning'.split())\n",
    "\n",
    "print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
